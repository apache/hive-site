<!doctype html><html><!doctype html>
<html>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content>
<meta name=author content>
<title>Apache Hive : HCatalog InputOutput</title>
<link rel=icon href=/images/hive.svg sizes=any type=image/svg+xml>
<link rel=stylesheet href=https://hive.apache.org/css/hive-theme.css>
<link rel=stylesheet href=https://hive.apache.org/css/font-awesome.all.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/bootstrap.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/termynal.css>
<link rel=apple-touch-icon sizes=180x180 href=https://hive.apache.org/images/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=https://hive.apache.org/images/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=https://hive.apache.org/images/favicon-16x16.png>
<link rel=manifest href=https://hive.apache.org/images/site.webmanifest>
<link rel=mask-icon href=https://hive.apache.org/images/safari-pinned-tab.svg color=#5bbad5>
<meta name=msapplication-TileColor content="#da532c">
<meta name=theme-color content="#ffffff">
<script>var _paq=window._paq=window._paq||[];_paq.push(['disableCookies']),_paq.push(['trackPageView']),_paq.push(['enableLinkTracking']),function(){var b="https://analytics.apache.org/",c,a,d;_paq.push(['setTrackerUrl',b+'matomo.php']),_paq.push(['setSiteId','30']),c=document,a=c.createElement('script'),d=c.getElementsByTagName('script')[0],a.async=!0,a.src=b+'matomo.js',d.parentNode.insertBefore(a,d)}()</script>
</head>
<body>
<body>
<header>
<menu style=background:#000;margin:0>
<nav class="navbar navbar-expand-lg navbar-dark bg-black">
<div class=container-fluid>
<a href=https://hive.apache.org> <img src=https://hive.apache.org/images/hive.svg width=60 height=35 alt="Apache Software Foundation"></a>
<a class="header-text navbar-brand" href=https://hive.apache.org>Apache Hive</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse" id=navbarSupportedContent>
<ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item dropdown">
<a class=nav-link href=/general/downloads id=navbarDropdown role=button aria-expanded=false>
Releases
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/Document id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Documentation
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/docs/latest/>Latest</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/docs/javadocs/>Javadocs</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual>Language Manual</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/general id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
General
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/licenses/LICENSE-2.0.html>License</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/general/privacypolicy/>Privacy Policy</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Development
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://hive.apache.org/development/gettingstarted/>Getting Started</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/quickstart/>Quickstart with Docker</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/DesignDocs>Design Docs</a></li>
<li><a class=dropdown-item href=https://issues.apache.org/jira/projects/HIVE/issues>Hive JIRA</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ>Hive Developer FAQ</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing>Precommit Patch Testing</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/versioncontrol/>Version Control</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Community
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/community/becomingcommitter/>Becoming A Committer</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToContribute>How To Contribute</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Home#Home-ResourcesforContributors>Resources for Contributors</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/mailinglists/>Mailing Lists</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/issuetracking/>Issue Tracking</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/people/>People</a></li>
<li>
<hr class=dropdown-divider>
</li>
<li><a class=dropdown-item href=/community/bylaws/>By Laws</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToRelease>How To Release</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class=nav-link href=https://hive.blog.apache.org/ id=navbarDropdown role=button aria-expanded=false>
Blogs
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
ASF
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/foundation/contributing.html>Donations</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/thanks.html>Thanks</a></li>
<li><a class=dropdown-item href=https://www.apache.org/>Website</a></li>
</ul>
</li>
<li>
<form action=/search method=get class=search-bar>
<input type=search name=q id=search-query placeholder=Search... class=search-input>
<button type=submit class=search-button>Search</button>
</form>
</li>
</ul>
</div>
</div>
</nav>
</menu>
</header>
<div class=content>
<div class=docs>
<h1 id=apache-hive--hcatalog-inputoutput>Apache Hive : HCatalog InputOutput</h1>
<h1 id=input-and-output-interfaces>Input and Output Interfaces</h1>
<ul>
<li><a href=#input-and-output-interfaces>Input and Output Interfaces</a>
<ul>
<li><a href=#set-up>Set Up</a></li>
<li><a href=#hcatinputformat>HCatInputFormat</a>
<ul>
<li><a href=#api>API</a></li>
</ul>
</li>
<li><a href=#hcatoutputformat>HCatOutputFormat</a>
<ul>
<li><a href=#api>API</a></li>
</ul>
</li>
<li><a href=#hcatrecord>HCatRecord</a></li>
<li><a href=#running-mapreduce-with-hcatalog>Running MapReduce with HCatalog</a>
<ul>
<li><a href=#authentication>Authentication</a></li>
<li><a href=#read-example>Read Example</a></li>
<li><a href=#filter-operators>Filter Operators</a></li>
<li><a href=#scan-filter>Scan Filter</a></li>
<li><a href=#write-filter>Write Filter</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id=set-up>Set Up</h2>
<p>No HCatalog-specific setup is required for the HCatInputFormat and HCatOutputFormat interfaces.</p>
<p><strong>Note</strong>: HCatalog is not thread safe.</p>
<h2 id=hcatinputformat>HCatInputFormat</h2>
<p>The HCatInputFormat is used with MapReduce jobs to read data from HCatalog-managed tables.</p>
<p>HCatInputFormat exposes a Hadoop 0.20 MapReduce API for reading data as if it had been published to a table.</p>
<h3 id=api>API</h3>
<p>The API exposed by HCatInputFormat is shown below. It includes:</p>
<ul>
<li><code>setInput</code></li>
<li><code>setOutputSchema</code></li>
<li><code>getTableSchema</code></li>
</ul>
<p>To use HCatInputFormat to read data, first instantiate an <code>InputJobInfo</code> with the necessary information from the table being read and then call <code>setInput</code> with the <code>InputJobInfo</code>.</p>
<p>You can use the <code>setOutputSchema</code> method to include a projection schema, to specify the output fields. If a schema is not specified, all the columns in the table will be returned.</p>
<p>You can use the <code>getTableSchema</code> method to determine the table schema for a specified input table.</p>
<pre tabindex=0><code>  /**
   * Set the input to use for the Job. This queries the metadata server with
   * the specified partition predicates, gets the matching partitions, puts
   * the information in the conf object. The inputInfo object is updated with
   * information needed in the client context
   * @param job the job object
   * @param inputJobInfo the input info for table to read
   * @throws IOException the exception in communicating with the metadata server
   */
  public static void setInput(Job job,
      InputJobInfo inputJobInfo) throws IOException;

  /**
   * Set the schema for the HCatRecord data returned by HCatInputFormat.
   * @param job the job object
   * @param hcatSchema the schema to use as the consolidated schema
   */
  public static void setOutputSchema(Job job,HCatSchema hcatSchema)
    throws IOException;

  /**
   * Get the HCatTable schema for the table specified in the HCatInputFormat.setInput
   * call on the specified job context. This information is available only after
   * HCatInputFormat.setInput has been called for a JobContext.
   * @param context the context
   * @return the table schema
   * @throws IOException if HCatInputFormat.setInput has not been called
   *                     for the current context
   */
  public static HCatSchema getTableSchema(JobContext context)
    throws IOException;

</code></pre><h2 id=hcatoutputformat>HCatOutputFormat</h2>
<p>HCatOutputFormat is used with MapReduce jobs to write data to HCatalog-managed tables.</p>
<p>HCatOutputFormat exposes a Hadoop 0.20 MapReduce API for writing data to a table. When a MapReduce job uses HCatOutputFormat to write output, the default OutputFormat configured for the table is used and the new partition is published to the table after the job completes.</p>
<h3 id=api-1>API</h3>
<p>The API exposed by HCatOutputFormat is shown below. It includes:</p>
<ul>
<li><code>setOutput</code></li>
<li><code>setSchema</code></li>
<li><code>getTableSchema</code></li>
</ul>
<p>The first call on the HCatOutputFormat must be <code>setOutput</code>; any other call will throw an exception saying the output format is not initialized. The schema for the data being written out is specified by the <code>setSchema</code> method. You must call this method, providing the schema of data you are writing. If your data has the same schema as the table schema, you can use <code>HCatOutputFormat.getTableSchema()</code> to get the table schema and then pass that along to <code>setSchema()</code>.</p>
<pre tabindex=0><code>  /**
   * Set the information about the output to write for the job. This queries the metadata
   * server to find the StorageHandler to use for the table. It throws an error if the
   * partition is already published.
   * @param job the job object
   * @param outputJobInfo the table output information for the job
   * @throws IOException the exception in communicating with the metadata server
   */
  @SuppressWarnings(&quot;unchecked&quot;)
  public static void setOutput(Job job, OutputJobInfo outputJobInfo) throws IOException;

  /**
   * Set the schema for the data being written out to the partition. The
   * table schema is used by default for the partition if this is not called.
   * @param job the job object
   * @param schema the schema for the data
   * @throws IOException
   */
  public static void setSchema(final Job job, final HCatSchema schema) throws IOException;

  /**
   * Get the table schema for the table specified in the HCatOutputFormat.setOutput call
   * on the specified job context.
   * @param context the context
   * @return the table schema
   * @throws IOException if HCatOutputFormat.setOutput has not been called
   *                     for the passed context
   */
  public static HCatSchema getTableSchema(JobContext context) throws IOException;

</code></pre><h2 id=hcatrecord>HCatRecord</h2>
<p>HCatRecord is the type supported for storing values in HCatalog tables.</p>
<p>The types in an HCatalog table schema determine the types of objects returned for different fields in HCatRecord. This table shows the mappings between Java classes for MapReduce programs and HCatalog data types:</p>
<table>
<thead>
<tr>
<th>HCatalog Data Type</th>
<th>Java Class in MapReduce</th>
<th>Values</th>
</tr>
</thead>
<tbody>
<tr>
<td>TINYINT</td>
<td>java.lang.Byte</td>
<td>-128 to 127</td>
</tr>
<tr>
<td>SMALLINT</td>
<td>java.lang.Short</td>
<td>-(2^15) to (2^15)-1, which is -32,768 to 32,767</td>
</tr>
<tr>
<td>INT</td>
<td>java.lang.Integer</td>
<td>-(2^31) to (2^31)-1, which is -2,147,483,648 to 2,147,483,647</td>
</tr>
<tr>
<td>BIGINT</td>
<td>java.lang.Long</td>
<td>-(2^63) to (2^63)-1, which is -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>java.lang.Boolean</td>
<td>true or false</td>
</tr>
<tr>
<td>FLOAT</td>
<td>java.lang.Float</td>
<td>single-precision floating-point value</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>java.lang.Double</td>
<td>double-precision floating-point value</td>
</tr>
<tr>
<td>DECIMAL</td>
<td>java.math.BigDecimal</td>
<td>exact floating-point value with 38-digit precision</td>
</tr>
<tr>
<td>BINARY</td>
<td>byte[]</td>
<td>binary data</td>
</tr>
<tr>
<td>STRING</td>
<td>java.lang.String</td>
<td>character string</td>
</tr>
<tr>
<td>STRUCT</td>
<td>java.util.List</td>
<td>structured data</td>
</tr>
<tr>
<td>ARRAY</td>
<td>java.util.List</td>
<td>values of one data type</td>
</tr>
<tr>
<td>MAP</td>
<td>java.util.Map</td>
<td>key-value pairs</td>
</tr>
</tbody>
</table>
<p>For general information about Hive data types, see <a href=https://hive.apache.org/docs/latest/languagemanual-types_27838462/>Hive Data Types</a> and <a href=#type-system>Type System</a>.</p>
<h2 id=running-mapreduce-with-hcatalog>Running MapReduce with HCatalog</h2>
<p>Your MapReduce program needs to be told where the Thrift server is. The easiest way to do this is to pass the location as an argument to your Java program. You need to pass the Hive and HCatalog jars to MapReduce as well, via the -libjars argument.</p>
<pre tabindex=0><code>export HADOOP_HOME=&lt;path_to_hadoop_install&gt;
export HCAT_HOME=&lt;path_to_hcat_install&gt;
export HIVE_HOME=&lt;path_to_hive_install&gt;
export LIB_JARS=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar,
$HIVE_HOME/lib/hive-metastore-0.10.0.jar,
$HIVE_HOME/lib/libthrift-0.7.0.jar,
$HIVE_HOME/lib/hive-exec-0.10.0.jar,
$HIVE_HOME/lib/libfb303-0.7.0.jar,
$HIVE_HOME/lib/jdo2-api-2.3-ec.jar,
$HIVE_HOME/lib/slf4j-api-1.6.1.jar

export HADOOP_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar:
$HIVE_HOME/lib/hive-metastore-0.10.0.jar:
$HIVE_HOME/lib/libthrift-0.7.0.jar:
$HIVE_HOME/lib/hive-exec-0.10.0.jar:
$HIVE_HOME/lib/libfb303-0.7.0.jar:
$HIVE_HOME/lib/jdo2-api-2.3-ec.jar:
$HIVE_HOME/conf:$HADOOP_HOME/conf:
$HIVE_HOME/lib/slf4j-api-1.6.1.jar

$HADOOP_HOME/bin/hadoop --config $HADOOP_HOME/conf jar &lt;path_to_jar&gt;
&lt;main_class&gt; -libjars $LIB_JARS &lt;program_arguments&gt;

</code></pre><p>This works but Hadoop will ship libjars every time you run the MapReduce program, treating the files as different cache entries, which is not efficient and may deplete the Hadoop distributed cache.</p>
<p>Instead, you can optimize to ship libjars using HDFS locations. By doing this, Hadoop will reuse the entries in the distributed cache.</p>
<pre tabindex=0><code>bin/hadoop fs -copyFromLocal $HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar /tmp
bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/hive-metastore-0.10.0.jar /tmp
bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/libthrift-0.7.0.jar /tmp
bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/hive-exec-0.10.0.jar /tmp
bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/libfb303-0.7.0.jar /tmp
bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/jdo2-api-2.3-ec.jar /tmp
bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/slf4j-api-1.6.1.jar /tmp

export LIB_JARS=hdfs:///tmp/hcatalog-core-0.5.0.jar,
hdfs:///tmp/hive-metastore-0.10.0.jar,
hdfs:///tmp/libthrift-0.7.0.jar,
hdfs:///tmp/hive-exec-0.10.0.jar,
hdfs:///tmp/libfb303-0.7.0.jar,
hdfs:///tmp/jdo2-api-2.3-ec.jar,
hdfs:///tmp/slf4j-api-1.6.1.jar

# (Other statements remain the same.)

</code></pre><h3 id=authentication>Authentication</h3>
<p>If a failure results in a message like &ldquo;2010-11-03 16:17:28,225 WARN hive.metastore &mldr; - Unable to connect metastore with URI thrift://&mldr;&rdquo; in <code>/tmp/</code><em></em><code>/hive.log</code>, then make sure you have run &ldquo;<code>kinit</code> <em></em><code>@FOO.COM</code>&rdquo; to get a Kerberos ticket and to be able to authenticate to the HCatalog server.</p>
<h3 id=read-example>Read Example</h3>
<p>The following very simple MapReduce program reads data from one table which it assumes to have an integer in the second column (&ldquo;column 1&rdquo;), and counts how many instances of each distinct value it finds. That is, it does the equivalent of &ldquo;select col1, count(*) from $table group by col1;&rdquo;.</p>
<p>For example, if the values in the second column are {<code>1,1,1,3,3,5</code>} the program will produce this output of values and counts:</p>
<p><code>1, 3</code><br>
<code>3, 2</code><br>
<code>5, 1</code></p>
<pre tabindex=0><code>public class GroupByAge extends Configured implements Tool {

    public static class Map extends
            Mapper&lt;WritableComparable, HCatRecord, IntWritable, IntWritable&gt; {

        int age;

        @Override
        protected void map(
                WritableComparable key,
                HCatRecord value,
                org.apache.hadoop.mapreduce.Mapper&lt;WritableComparable, HCatRecord,
                        IntWritable, IntWritable&gt;.Context context)
                throws IOException, InterruptedException {
            age = (Integer) value.get(1);
            context.write(new IntWritable(age), new IntWritable(1));
        }
    }

    public static class Reduce extends Reducer&lt;IntWritable, IntWritable,
    WritableComparable, HCatRecord&gt; {

      @Override
      protected void reduce(
              IntWritable key,
              java.lang.Iterable&lt;IntWritable&gt; values,
              org.apache.hadoop.mapreduce.Reducer&lt;IntWritable, IntWritable,
                      WritableComparable, HCatRecord&gt;.Context context)
              throws IOException, InterruptedException {
          int sum = 0;
          Iterator&lt;IntWritable&gt; iter = values.iterator();
          while (iter.hasNext()) {
              sum++;
              iter.next();
          }
          HCatRecord record = new DefaultHCatRecord(2);
          record.set(0, key.get());
          record.set(1, sum);

          context.write(null, record);
        }
    }

    public int run(String[] args) throws Exception {
        Configuration conf = getConf();
        args = new GenericOptionsParser(conf, args).getRemainingArgs();

        String inputTableName = args[0];
        String outputTableName = args[1];
        String dbName = null;

        Job job = new Job(conf, &quot;GroupByAge&quot;);
        HCatInputFormat.setInput(job, InputJobInfo.create(dbName,
                inputTableName, null));
        // initialize HCatOutputFormat

        job.setInputFormatClass(HCatInputFormat.class);
        job.setJarByClass(GroupByAge.class);
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);
        job.setMapOutputKeyClass(IntWritable.class);
        job.setMapOutputValueClass(IntWritable.class);
        job.setOutputKeyClass(WritableComparable.class);
        job.setOutputValueClass(DefaultHCatRecord.class);
        HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName,
                outputTableName, null));
        HCatSchema s = HCatOutputFormat.getTableSchema(job);
        System.err.println(&quot;INFO: output schema explicitly set for writing:&quot;
                + s);
        HCatOutputFormat.setSchema(job, s);
        job.setOutputFormatClass(HCatOutputFormat.class);
        return (job.waitForCompletion(true) ? 0 : 1);
    }

    public static void main(String[] args) throws Exception {
        int exitCode = ToolRunner.run(new GroupByAge(), args);
        System.exit(exitCode);
    }
}

</code></pre><p>Notice a number of important points about this program:</p>
<ol>
<li>The implementation of Map takes HCatRecord as an input and the implementation of Reduce produces it as an output.</li>
<li>This example program assumes the schema of the input, but it could also retrieve the schema via <code>HCatOutputFormat.getOutputSchema()</code> and retrieve fields based on the results of that call.</li>
<li>The input descriptor for the table to be read is created by calling <code>InputJobInfo.create</code>. It requires the database name, table name, and partition filter. In this example the partition filter is null, so all partitions of the table will be read.</li>
<li>The output descriptor for the table to be written is created by calling <code>OutputJobInfo.create</code>. It requires the database name, the table name, and a Map of partition keys and values that describe the partition being written. In this example the table is assumed to be unpartitioned, so this Map is null.</li>
</ol>
<p>To scan just selected partitions of a table, a filter describing the desired partitions can be passed to <code>InputJobInfo.create</code>. To scan a single partition, the filter string should look like: &ldquo;<code>ds=20120401</code>&rdquo; where the datestamp &ldquo;<code>ds</code>&rdquo; is the partition column name and &ldquo;<code>20120401</code>&rdquo; is the value you want to read (year, month, and day).</p>
<h3 id=filter-operators>Filter Operators</h3>
<p>A filter can contain the operators &lsquo;and&rsquo;, &lsquo;or&rsquo;, &lsquo;like&rsquo;, &lsquo;()&rsquo;, &lsquo;=&rsquo;, &lsquo;&lt;>&rsquo; (<em>not equal</em>), &lsquo;&lt;&rsquo;, &lsquo;>&rsquo;, &lsquo;&lt;=&rsquo; and &lsquo;>=&rsquo;.</p>
<p>For example:</p>
<ul>
<li><code>ds > "20110924"</code></li>
<li><code>ds &lt; "20110925"</code></li>
<li><code>ds &lt;= "20110925" and ds >= "20110924"</code></li>
</ul>
<h3 id=scan-filter>Scan Filter</h3>
<p>Assume for example you have a web_logs table that is partitioned by the column &ldquo;<code>ds</code>&rdquo;. You could select one partition of the table by changing</p>
<pre tabindex=0><code>HCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, null));

</code></pre><p>to</p>
<pre tabindex=0><code>HCatInputFormat.setInput(job,
                         InputJobInfo.create(dbName, inputTableName, &quot;ds=\&quot;20110924\&quot;&quot;));

</code></pre><p>This filter must reference only partition columns. Values from other columns will cause the job to fail.</p>
<h3 id=write-filter>Write Filter</h3>
<p>To write to a single partition you can change the above example to have a Map of key value pairs that describe all of the partition keys and values for that partition. In our example web_logs table, there is only one partition column (<code>ds</code>), so our Map will have only one entry. Change</p>
<pre tabindex=0><code>HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, null));

</code></pre><p>to</p>
<pre tabindex=0><code>Map partitions = new HashMap&lt;String, String&gt;(1);
partitions.put(&quot;ds&quot;, &quot;20110924&quot;);
HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, partitions));

</code></pre><p>To write multiple partitions simultaneously you can leave the Map null, but all of the partitioning columns must be present in the data you are writing.</p>
<p> </p>
<p><strong>Navigation Links</strong>
Previous: <a href=https://hive.apache.org/docs/latest/hcatalog-loadstore_34013511/>Load and Store Interfaces</a><br>
Next: <a href=https://hive.apache.org/docs/latest/hcatalog-readerwriter_34013921/>Reader and Writer Interfaces</a></p>
<p>General: <a href=https://hive.apache.org/docs/latest/hcatalog_33299065/>HCatalog Manual</a> – <a href=https://hive.apache.org/docs/latest/webhcat_33299069/>WebHCat Manual</a> – <a href=https://hive.apache.org/docs/latest/home_27362069/>Hive Wiki Home</a> – <a href=http://hive.apache.org/>Hive Project Site</a></p>
</div>
</div>
<footer class="black-background static-bottom" style=padding:30px>
<div class=row>
<div class=col-3>
<a href=https://www.apache.org/>
<img src=https://hive.apache.org/images/asf_logo.png width=270 height=100 alt="Apache Software Foundation"></a>
</a>
</div>
<div class=col-9>
<p class=footer-text>Apache is a non-profit organization helping open-source
software projects released under the Apache
<a href=https://www.apache.org/licenses/>license</a>
and managed with
<a href=https://www.apache.org/foundation/how-it-works.html>
open governance</a> and
<a href=https://privacy.apache.org/policies/privacy-policy-public.html>
privacy policy</a>. See upcoming
<a href=https://www.apache.org/events/current-event>Apache Events</a>.
If you discover any
<a href=https://www.apache.org/security/>security</a> vulnerabilities, please
report them privately. Finally,
<a href=https://www.apache.org/foundation/sponsorship.html>thanks
</a> to the sponsors who
<a href=https://www.apache.org/foundation/contributing.html>
donate</a> to the Apache Foundation.
</p>
</div>
</div>
<div class="copyright row">
<a href=https://hive.apache.org style=color:grey>
The contents of this website are © 2023 Apache Software Foundation under the terms of the Apache License v2. Apache Hive and its logo are trademarks of the Apache Software Foundation.
</a>
</div>
</footer>
<script src=https://hive.apache.org/js/bootstrap.bundle.min.js></script>
</body>
</html>