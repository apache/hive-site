<!doctype html><html><!doctype html>
<html>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content>
<meta name=author content>
<title>Apache Hive : Hybrid Hybrid Grace Hash Join, v1.0</title>
<link rel=icon href=/images/hive.svg sizes=any type=image/svg+xml>
<link rel=stylesheet href=https://hive.apache.org/css/hive-theme.css>
<link rel=stylesheet href=https://hive.apache.org/css/font-awesome.all.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/bootstrap.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/termynal.css>
<link rel=apple-touch-icon sizes=180x180 href=https://hive.apache.org/images/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=https://hive.apache.org/images/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=https://hive.apache.org/images/favicon-16x16.png>
<link rel=manifest href=https://hive.apache.org/images/site.webmanifest>
<link rel=mask-icon href=https://hive.apache.org/images/safari-pinned-tab.svg color=#5bbad5>
<meta name=msapplication-TileColor content="#da532c">
<meta name=theme-color content="#ffffff">
<script>var _paq=window._paq=window._paq||[];_paq.push(['disableCookies']),_paq.push(['trackPageView']),_paq.push(['enableLinkTracking']),function(){var b="https://analytics.apache.org/",c,a,d;_paq.push(['setTrackerUrl',b+'matomo.php']),_paq.push(['setSiteId','30']),c=document,a=c.createElement('script'),d=c.getElementsByTagName('script')[0],a.async=!0,a.src=b+'matomo.js',d.parentNode.insertBefore(a,d)}()</script>
</head>
<body>
<body>
<header>
<menu style=background:#000;margin:0>
<nav class="navbar navbar-expand-lg navbar-dark bg-black">
<div class=container-fluid>
<a href=https://hive.apache.org> <img src=https://hive.apache.org/images/hive.svg width=60 height=35 alt="Apache Software Foundation"></a>
<a class="header-text navbar-brand" href=https://hive.apache.org>Apache Hive</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse" id=navbarSupportedContent>
<ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item dropdown">
<a class=nav-link href=/general/downloads id=navbarDropdown role=button aria-expanded=false>
Releases
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/Document id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Documentation
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/docs/latest/>Latest</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/docs/javadocs/>Javadocs</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual>Language Manual</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/general id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
General
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/licenses/LICENSE-2.0.html>License</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/general/privacypolicy/>Privacy Policy</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Development
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://hive.apache.org/development/gettingstarted/>Getting Started</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/quickstart/>Quickstart with Docker</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/DesignDocs>Design Docs</a></li>
<li><a class=dropdown-item href=https://issues.apache.org/jira/projects/HIVE/issues>Hive JIRA</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ>Hive Developer FAQ</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing>Precommit Patch Testing</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/versioncontrol/>Version Control</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Community
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/community/becomingcommitter/>Becoming A Committer</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToContribute>How To Contribute</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Home#Home-ResourcesforContributors>Resources for Contributors</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/mailinglists/>Mailing Lists</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/issuetracking/>Issue Tracking</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/people/>People</a></li>
<li>
<hr class=dropdown-divider>
</li>
<li><a class=dropdown-item href=/community/bylaws/>By Laws</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToRelease>How To Release</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class=nav-link href=https://hive.blog.apache.org/ id=navbarDropdown role=button aria-expanded=false>
Blogs
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
ASF
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/foundation/contributing.html>Donations</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/thanks.html>Thanks</a></li>
<li><a class=dropdown-item href=https://www.apache.org/>Website</a></li>
</ul>
</li>
<li>
<form action=/search method=get class=search-bar>
<input type=search name=q id=search-query placeholder=Search... class=search-input>
<button type=submit class=search-button>Search</button>
</form>
</li>
</ul>
</div>
</div>
</nav>
</menu>
</header>
<div class=content>
<div class=docs>
<h1 id=apache-hive--hybrid-hybrid-grace-hash-join-v10>Apache Hive : Hybrid Hybrid Grace Hash Join, v1.0</h1>
<ul>
<li><a href=#overview>Overview</a></li>
<li><a href=#scope>Scope</a></li>
<li><a href=#notation-and-assumptions>Notation and Assumptions</a></li>
<li><a href=#brief-review-on-hash-join-algorithms>Brief Review on Hash Join Algorithms</a>
<ul>
<li><a href=#simple-hash-join>Simple Hash Join</a></li>
<li><a href=#grace-hash-join>GRACE Hash Join</a></li>
<li><a href=#hybrid-grace-hash-join>Hybrid GRACE Hash Join</a></li>
<li><a href=#hash-join-in-hive>Hash Join in Hive</a></li>
</ul>
</li>
<li><a href=#motivation-for-hybrid-hybrid-grace-hash-join>Motivation for “Hybrid Hybrid GRACE Hash Join”</a></li>
<li><a href=#algorithm>Algorithm</a></li>
<li><a href=#recursive-hashing-and-spilling>Recursive Hashing and Spilling</a></li>
<li><a href=#skewed-data-distribution>Skewed Data Distribution</a></li>
<li><a href=#bloom-filter>Bloom Filter</a></li>
<li><a href=#references>References</a></li>
</ul>
<h1 id=overview>Overview</h1>
<p>We are proposing an enhanced hash join algorithm called “hybrid hybrid grace hash join”. We can benefit from this feature as illustrated below:</p>
<ul>
<li>The query will not fail even if the estimated memory requirement is slightly wrong.</li>
<li>Expensive garbage collection overhead can be avoided when hash table grows.</li>
<li>Join execution using a Map join operator even though the small table doesn&rsquo;t fit in memory, as spilling some data from the build and probe sides will still be cheaper than having to shuffle the large fact table.</li>
</ul>
<p>The design was based on Hadoop’s parallel processing capability and significant amount of memory available.</p>
<p>See <a href=https://issues.apache.org/jira/browse/HIVE-9277>HIVE-9277</a> for the current status of this work.</p>
<h1 id=scope>Scope</h1>
<p>This new join algorithm will only work with Tez. It does not support Map Reduce currently.</p>
<h1 id=notation-and-assumptions>Notation and Assumptions</h1>
<p>The goal is to compute the equi-join result of two relations labeled R and S. We assume R is the small table with smaller cardinality, and S is the big table with bigger cardinality.</p>
<h1 id=brief-review-on-hash-join-algorithms>Brief Review on Hash Join Algorithms</h1>
<h2 id=simple-hash-join>Simple Hash Join</h2>
<p>Also known as Classic Hash Join, it is used when the hash table for R can entirely fit into the memory.</p>
<p>In general, all hash join algorithms described here have two phases: Build phase and Probe phase. During the build phase, a hash table is built into memory based on the joining column(s) from the small table R. During the probe phase, the big table S is scanned sequentially and for each row of S, the hash table is probed for matching rows. If a match is found, output the pair, otherwise drop the row from S and continue scanning S.</p>
<p>When the hash table for R cannot wholly fit in the memory, only part of the hash table for R will be put in memory first and S is scanned against the partial hash table. After the scan on S is completed, the memory is cleared and another part of hash table of R is put in memory, and S is scanned again. This process can repeat more times if there are more parts of the hash table.</p>
<h2 id=grace-hash-join>GRACE Hash Join</h2>
<p>Apparently when the size of small table R is much bigger than memory, we end up having many partial hash tables of R loaded in the memory one by one, and the big table S being scanned multiple times, which is very expensive.</p>
<p>GRACE hash join brings rounds of scanning the big table from many times down to just twice. One for partitioning, the other for row matching. Similarly, the small table will also be scanned twice.</p>
<p>Here is the detailed process.</p>
<ol>
<li>Small table R is scanned, and during the scan a hash function is used to distribute the rows into different output buffers (or partitions). The size of each output buffer should be specified as close as possible to the memory limit but no more than that. After R has been completely scanned, all output buffers are flushed to disk.</li>
<li>Similarly, big table S is scanned, partitioned and flushed to disk the same way. The hash function used here is the same one as in the previous step.</li>
<li>Load one partition of R into memory and build a hash table for it. This is the build phase.</li>
<li>Hash each row of the corresponding partition of S, and probe for a match in R’s hash table. If a match is found, output the pair to the result, otherwise, proceed with the next row in the current S partition. This is the probe phase.</li>
<li>Repeat loading and building hash table for partitions of R and probing partitions of S, until both are exhausted.</li>
</ol>
<p>It can be seen there are extra partitioning steps in this algorithm (1 & 2). The rest of the steps are the same as Classic Hash Join, i.e., building and probing. One assumption here is all partitions of R can completely fit into the memory.</p>
<h2 id=hybrid-grace-hash-join>Hybrid GRACE Hash Join</h2>
<p>It is a hybrid of Classic Hash Join and GRACE Hash Join. The idea is to build an in-memory hash table for the first partition of R during the partitioning phase, without the need to write this partition to disk. Similarly, while partitioning S, the first partition does not have to be put to the disk since probing can be directly done against the in-memory first partition of R. So at the end of the partitioning phase, the join of the first pair of partitions of R and S has already been done.</p>
<p>Comparing to GRACE hash join, Hybrid GRACE hash join has an advantage of avoiding writing the first partitions of R and S to disk during the partitioning phase and reading them back in again during the probing phase.</p>
<h2 id=hash-join-in-hive>Hash Join in Hive</h2>
<p>It is also known as replicated join, map-side join or mapjoin. When one side of the data is small enough to fit in memory of a mapper, it can be copied to all the mappers and the join will be performed only in the map phase. There is no reduce phase needed.</p>
<h1 id=motivation-for-hybrid-hybrid-grace-hash-join>Motivation for “Hybrid Hybrid GRACE Hash Join”</h1>
<p>Current implementation of hash join in Hive is the Classic Hash Join, which can only handle the case when the small table can be entirely fit in memory. Otherwise hash join cannot be performed. This feature is trying to lift that limitation by making hash join more general, so that even if the size of small table is larger than memory, we can still do the hash join, in an elegant way.</p>
<p>It’s obvious that GRACE Hash Join uses main memory as a staging area during the partitioning phase, which unconditionally scans both relations from disk, then partitions and puts them back to disk (Hybrid GRACE Hash Join puts one pair less), and finally reads them back to find matches.</p>
<p>This feature tries to avoid the unnecessary write-back of partitions to disk as much as possible, and will only do that when necessary. The idea is to fully utilize the main memory to hold existing partitions of hash tables.</p>
<p>The key factor that will impact the performance of this algorithm is whether the data can be evenly distributed into different hash partitions. If we have skewed values, which will result in a few very big partitions, then an extra partitioning step is needed to divide the big partitions down to many. This can happen recursively. Refer to <a href=#recursive-hashing-and-spilling>Recursive Hashing and Spilling</a> below for more details. </p>
<h1 id=algorithm>Algorithm</h1>
<p>Like other hash joins, there are a build phase and a probe phase. But there are several new terms to be defined.</p>
<ul>
<li><em>Partition</em>. Instead of putting all keys of the small table into a single hash table, they will be put into many hash tables, based on their hash value. Those hash tables are known as partitions. Partitions can be either in memory when initially created, or spilled to disk when the memory is used up.</li>
<li><em>Sidefile</em>. It is a row container on disk to hold rows from the small table targeted to a specific partition that has been already spilled to disk. There can be zero or one sidefile for each partition spilled to disk.</li>
<li><em>Matchfile</em>. It is a row container on disk to hold rows from the big table that have possible matching keys with partitions spilled to disk. It also has a one-to-one relationship to the partitions.</li>
</ul>
<p>When the entire small table can fit in memory, everything is similar to Classic Hash Join, except there are multiple hash tables instead of one in memory.</p>
<p><img src=https://lh6.googleusercontent.com/iVa2tD1kTSoWQKxWxwPhPxJHnvnlr1uJa4HwrkHqNPB4osG3xzVU-sjmCUKfI0c9f3I3rpNqx9M_qmjelxDfWYUp7165k7k2O44C1Y2A8ajc9eK0SonMgD-y5Z6ia0Ydfg alt></p>
<p><img src=https://lh3.googleusercontent.com/VnwA074O47g4crdCqIxW3CFyxVpWgfKD6-2iKAbdQGn2u_IR1LWW-nT5t261ljhY8Y_zOmm2Mj3GGJY6yhxkUbNcwSh5-r4Pwo3mUoj8Yqo3C9a4ZaFYwRQgGisKuWZ42A alt></p>
<p>When the small table cannot fit in memory, things are a little more complicated.</p>
<p>Small table keys keep getting hashed into different partitions until at some point the memory limit is reached. Then the biggest partition in memory will be moved to disk so that memory is freed to some extent. The new key that should have been put into that partition will be put in the corresponding sidefile.</p>
<p><img src=https://lh4.googleusercontent.com/DahwmR-yyQSks0veoua0FOsmp7XoV_8fS1KIb3UGo8cdjcPFJ4ani_40TzYmIQHzXmaERJduVwkFqn8bZd7bCTz-bcxy66Rhn4dQ5OUrcQ09s7AIQax7UfyGlTzQOV_eDA alt></p>
<p>During probing, if matches are found from in-memory partitions, they will be directly put into the result.</p>
<p><img src=https://lh5.googleusercontent.com/zGTjywxVuEClExTogYw2oe16Wn3CVjc1l64CTGUUKUlHB0SN74LVJe3h57udeghBzswaQtcztekGgH4st8-DIzLRKMAaoMmZv8KmqkhxK-DqzYBlABAwRrM2yolCS-Ad_w alt></p>
<p>If a possible match is found, i.e., the key hash of big table falls into an on-disk partition, it will be put into an on-disk matchfile. No matching is done at this moment, but will be done later.</p>
<p><img src=https://lh5.googleusercontent.com/hPDypZAfTtCBI1Ocdvp6kkS40MLnCH_yadfef7kgk5firhV5EJ9g1DLr4CWrdgxc1nsGvc1HnY4US7TwOiAnT1oEQ1snrXH2Tp0sneJaxXWITp2JIB-rntB9A0aMV5C5FQ alt></p>
<p>After the big table has been exhausted, all the in-memory partitions are no longer needed and purged. Purging is appropriate because those keys that should match have been output to result, and those that don’t match won’t be relevant to the on-disk structures.</p>
<p>Now there may be many on-disk triplets (partition, sidefile and matchfile). For each triplet, merge the partition and sidefile and put them back in memory to form a new hash table. Then scan the matchfile against the newly created hash table looking for matches. The logic here is similar to Classic Hash Join.</p>
<p><img src=https://lh4.googleusercontent.com/Gpbx1cZh37iFIvqMUBWjgce2GxAPvrJ-XldCMs6NgvqatlGa4x4try2eED-VLgdYiAW7tjZElEhmyzcWzFLo_k1t5j2kT-CGM0eRPHm_cV0gcA3oPUEyf-bu6fABiE3xeQ alt></p>
<p>Repeat until all on-disk triplets have been exhausted. At this moment, the join between small table and big table is completed.</p>
<h1 id=recursive-hashing-and-spilling>Recursive Hashing and Spilling</h1>
<p>There are cases when the hash function is not working well for distributing values evenly among the hash partitions, or the values themselves are skewed, which will result in very big sidefiles. At the time when the on-disk partition and sidefile are to be merged back into memory, the size of the newly combined hash partition will exceed the memory limit. In that case, another round of hashing and spilling is necessary. The process is illustrated below.</p>
<p>Assume there’s only 1GB memory space available for the join. Hash function h1 distributes keys into two partitions HT1 and HT2. At some point in time, the memory is full and HT1 is moved to disk. Assume all the future keys all hash to HT1, so they go to Sidefile1.</p>
<p><a href=https://drive.draw.io/#G0B72RYNz9voGccmlLNEhjcW4wUFU><img src=https://lh3.googleusercontent.com/ppKL4CGZnDUQWJRT7CGepJ8I5_WDCe2FlQhSEoZ135-YCvbZkeRtmj3Yl8zcoQytCD9ZVQ3bkujvj30V9gOuYDqSRiOKHHxXa1k5fqg2RGWBIEuMB3Y3rqveW5w2gb8Kog alt></a></p>
<p>As normal, during the probe phase big table values will be either matched and put into result or not matched and put into a Matchfile.</p>
<p><a href=https://drive.draw.io/#G0B72RYNz9voGcVnVzeWwzR0hyUDA><img src=https://lh6.googleusercontent.com/a-QU_o2xvnLaVn7hKuhHX2VnmK0EYmi_0BaY-1H32wEAho0pJ1wmSaGOoO-VmNpwvj-MlVNdNA30JUEKGkuWydAKho6igYEo2D72W9aTRArWBvFRja91prI7I664CiSLvw alt></a></p>
<p>After the big table is exhausted, all the values from big table should be either output (matched) to the result, or staged into matchfiles. Then it is time to merge back pairs of on-disk hash partition and sidefile, and build a new in-memory hash table, and probe the corresponding matchfile against it.</p>
<p>In this example, since the predicted size of the merge is greater than the memory limit (800MB + 300MB > 1GB), we cannot simply merge them back to memory. Instead, we need to rehash them by using a different hash function h2.</p>
<p><a href=https://drive.draw.io/#G0B72RYNz9voGcVnVzeWwzR0hyUDA><img src=https://lh6.googleusercontent.com/9-F7k7Auay7iqSl6gdwA8ImAwKMIyaixtcrHHJB65tmld3Yw_DASBXdpTlNfx21X0rAjnL7KvA_KVWEshvRWRTlCldMeztPMLINjJVO4gvMSLv2WxTGY450LwQYtCOlM5w alt></a></p>
<p> </p>
<p>Now we probe using Matchfile 1 against HT 3 (in memory) and HT 4 (on disk). Matching values for HT 3 go into result. Possibly matching values for HT 4 go to Matchfile 4.</p>
<p><img src=https://lh4.googleusercontent.com/BMAqCXS1nBeb-v_XdLfVswpMP4S7cRD0x976n7sGcmrF3zsPfD7OTd-SvmQYkTp7M6kbLxIp_LEdOQMZTakG3oRkSs0SCYEYF-izz10dLn3wOUz20eAOMgBXJWgD4THamg alt></p>
<p>This process can continue recursively if the size of HT 4 plus size of Sidefile 4 is still greater than the memory limit, i.e., hashing HT 4 and Sidefile 4 using a third hash function h3, and probing Matchfile 4 using h3. In this example, the size of HT 4 plus size of Sidefile 4 is smaller than memory limit, so we are done.</p>
<h1 id=skewed-data-distribution>Skewed Data Distribution</h1>
<p>Sometimes it happens that all or most rows in one hash partition share the same value. In that case, no matter how we change the hash function, it won’t help anyway.</p>
<p>Several approaches can be considered to handle this problem. If we have reliable statistics about the data, like detailed histograms, we can process rows with the same value using a new join algorithm (using or dropping rows sharing the same value all in one shot). Or we can divide the rows into pieces such that each piece can fit into memory, and perform the probing in several passes. This probably will bring performance impact. Further, we can resort to regular shuffle join as a fallback option once we figure out Mapjoin cannot handle this situation.</p>
<h1 id=bloom-filter>Bloom Filter</h1>
<p>As of <a href=https://issues.apache.org/jira/browse/HIVE-11306>Hive 2.0.0</a>, a cheap Bloom filter is built during the build phase of the Hybrid hashtable, which is consulted against before spilling a row into the matchfile. The goal is to minimize the number of records which end up being spilled to disk, which may not have any matches in the spilled hashtables. The optimization also benefits left outer joins since the row which entered the hybrid join can be immediately generated as output with appropriate nulls indicating a lack of match, while without the filter it would have to be serialized onto disk only to be reloaded without a match at the end of the probe.</p>
<h1 id=references>References</h1>
<ul>
<li>Hybrid Hybrid Grace Hash Join presentation by Mostafa</li>
<li>MapJoinOptimization <a href=https://cwiki.apache.org/confluence/display/Hive/MapJoinOptimization>https://cwiki.apache.org/confluence/display/Hive/MapJoinOptimization</a></li>
<li><a href=https://issues.apache.org/jira/browse/HIVE-1641>HIVE-1641</a> add map joined table to distributed cache</li>
<li><a href=https://issues.apache.org/jira/browse/HIVE-1642>HIVE-1642</a> Convert join queries to map-join based on size of table/row</li>
<li>Database Management Systems, 3rd ed</li>
<li>Kitsuregawa, M.  Application of Hash to Data Base Machine and Its Architecture</li>
<li>Shapiro, L. D.  Join Processing in Database Systems with Large Main Memories</li>
<li>Dewitt, David J.  Implementation techniques for main memory database systems</li>
<li>Jimmy Lin and Chris Dyer  Data-Intensive Text Processing with MapReduce</li>
</ul>
</div>
</div>
<footer class="black-background static-bottom" style=padding:30px>
<div class=row>
<div class=col-3>
<a href=https://www.apache.org/>
<img src=https://hive.apache.org/images/asf_logo.png width=270 height=100 alt="Apache Software Foundation"></a>
</a>
</div>
<div class=col-9>
<p class=footer-text>Apache is a non-profit organization helping open-source
software projects released under the Apache
<a href=https://www.apache.org/licenses/>license</a>
and managed with
<a href=https://www.apache.org/foundation/how-it-works.html>
open governance</a> and
<a href=https://privacy.apache.org/policies/privacy-policy-public.html>
privacy policy</a>. See upcoming
<a href=https://www.apache.org/events/current-event>Apache Events</a>.
If you discover any
<a href=https://www.apache.org/security/>security</a> vulnerabilities, please
report them privately. Finally,
<a href=https://www.apache.org/foundation/sponsorship.html>thanks
</a> to the sponsors who
<a href=https://www.apache.org/foundation/contributing.html>
donate</a> to the Apache Foundation.
</p>
</div>
</div>
<div class="copyright row">
<a href=https://hive.apache.org style=color:grey>
The contents of this website are © 2023 Apache Software Foundation under the terms of the Apache License v2. Apache Hive and its logo are trademarks of the Apache Software Foundation.
</a>
</div>
</footer>
<script src=https://hive.apache.org/js/bootstrap.bundle.min.js></script>
</body>
</html>