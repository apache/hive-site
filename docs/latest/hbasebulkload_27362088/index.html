<!doctype html><html><!doctype html>
<html>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content>
<meta name=author content>
<title>Apache Hive : HBaseBulkLoad</title>
<link rel=icon href=/images/hive.svg sizes=any type=image/svg+xml>
<link rel=stylesheet href=https://hive.apache.org/css/hive-theme.css>
<link rel=stylesheet href=https://hive.apache.org/css/font-awesome.all.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/bootstrap.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/termynal.css>
<link rel=apple-touch-icon sizes=180x180 href=https://hive.apache.org/images/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=https://hive.apache.org/images/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=https://hive.apache.org/images/favicon-16x16.png>
<link rel=manifest href=https://hive.apache.org/images/site.webmanifest>
<link rel=mask-icon href=https://hive.apache.org/images/safari-pinned-tab.svg color=#5bbad5>
<meta name=msapplication-TileColor content="#da532c">
<meta name=theme-color content="#ffffff">
<script>var _paq=window._paq=window._paq||[];_paq.push(['disableCookies']),_paq.push(['trackPageView']),_paq.push(['enableLinkTracking']),function(){var b="https://analytics.apache.org/",c,a,d;_paq.push(['setTrackerUrl',b+'matomo.php']),_paq.push(['setSiteId','30']),c=document,a=c.createElement('script'),d=c.getElementsByTagName('script')[0],a.async=!0,a.src=b+'matomo.js',d.parentNode.insertBefore(a,d)}()</script>
</head>
<body>
<body>
<header>
<menu style=background:#000;margin:0>
<nav class="navbar navbar-expand-lg navbar-dark bg-black">
<div class=container-fluid>
<a href=https://hive.apache.org> <img src=https://hive.apache.org/images/hive.svg width=60 height=35 alt="Apache Software Foundation"></a>
<a class="header-text navbar-brand" href=https://hive.apache.org>Apache Hive</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse" id=navbarSupportedContent>
<ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item dropdown">
<a class=nav-link href=/general/downloads id=navbarDropdown role=button aria-expanded=false>
Releases
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/Document id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Documentation
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/docs/latest/>Latest</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/docs/javadocs/>Javadocs</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual>Language Manual</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/general id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
General
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/licenses/LICENSE-2.0.html>License</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/general/privacypolicy/>Privacy Policy</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Development
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://hive.apache.org/development/gettingstarted/>Getting Started</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/quickstart/>Quickstart with Docker</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/DesignDocs>Design Docs</a></li>
<li><a class=dropdown-item href=https://issues.apache.org/jira/projects/HIVE/issues>Hive JIRA</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ>Hive Developer FAQ</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing>Precommit Patch Testing</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/versioncontrol/>Version Control</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Community
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/community/becomingcommitter/>Becoming A Committer</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToContribute>How To Contribute</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Home#Home-ResourcesforContributors>Resources for Contributors</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/mailinglists/>Mailing Lists</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/issuetracking/>Issue Tracking</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/people/>People</a></li>
<li>
<hr class=dropdown-divider>
</li>
<li><a class=dropdown-item href=/community/bylaws/>By Laws</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToRelease>How To Release</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class=nav-link href=https://hive.blog.apache.org/ id=navbarDropdown role=button aria-expanded=false>
Blogs
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
ASF
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/foundation/contributing.html>Donations</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/thanks.html>Thanks</a></li>
<li><a class=dropdown-item href=https://www.apache.org/>Website</a></li>
</ul>
</li>
<li>
<form action=/search method=get class=search-bar>
<input type=search name=q id=search-query placeholder=Search... class=search-input>
<button type=submit class=search-button>Search</button>
</form>
</li>
</ul>
</div>
</div>
</nav>
</menu>
</header>
<div class=content>
<div class=docs>
<h1 id=apache-hive--hbasebulkload>Apache Hive : HBaseBulkLoad</h1>
<h1 id=hive-hbase-bulk-load>Hive HBase Bulk Load</h1>
<ul>
<li><a href=#hive-hbase-bulk-load>Hive HBase Bulk Load</a>
<ul>
<li><a href=#overview>Overview</a></li>
<li><a href=#decide-on-target-hbase-schema>Decide on Target HBase Schema</a></li>
<li><a href=#estimate-resources-needed>Estimate Resources Needed</a></li>
<li><a href=#add-necessary-jars>Add necessary JARs</a></li>
<li><a href=#prepare-range-partitioning>Prepare Range Partitioning</a></li>
<li><a href=#prepare-staging-location>Prepare Staging Location</a></li>
<li><a href=#sort-data>Sort Data</a></li>
<li><a href=#run-hbase-script>Run HBase Script</a></li>
<li><a href=#map-new-table-back-into-hive>Map New Table Back Into Hive</a></li>
<li><a href=#followups-needed>Followups Needed</a></li>
</ul>
</li>
</ul>
<p>This page explains how to use Hive to bulk load data into a new (empty) HBase table per <a href=https://issues.apache.org/jira/browse/HIVE-1295>HIVE-1295</a>. (If you&rsquo;re not using a build which contains this functionality yet, you&rsquo;ll need to build from source and make sure this patch and HIVE-1321 are both applied.)</p>
<h2 id=overview>Overview</h2>
<p>Ideally, bulk load from Hive into HBase would be part of <a href=https://hive.apache.org/docs/latest/hbaseintegration_27362089/>HBaseIntegration</a>, making it as simple as this:</p>
<pre tabindex=0><code>CREATE TABLE new_hbase_table(rowkey string, x int, y int) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:x,cf:y&quot;);

SET hive.hbase.bulk=true;

INSERT OVERWRITE TABLE new_hbase_table
SELECT rowkey_expression, x, y FROM ...any_hive_query...;

</code></pre><p>However, things aren&rsquo;t <em>quite</em> as straightforward as that yet. Instead, a procedure involving a series of SQL commands is required. It should still be a lot easier and more flexible than writing your own map/reduce program, and over time we hope to enhance Hive to move closer to the ideal.</p>
<p>The procedure is based on <a href=http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#bulk>underlying HBase recommendations</a>, and involves the following steps:</p>
<ol>
<li>Decide how you want the data to look once it has been loaded into HBase.</li>
<li>Decide on the number of reducers you&rsquo;re planning to use for parallelizing the sorting and HFile creation. This depends on the size of your data as well as cluster resources available.</li>
<li>Run Hive sampling commands which will create a file containing &ldquo;splitter&rdquo; keys which will be used for range-partitioning the data during sort.</li>
<li>Prepare a staging location in HDFS where the HFiles will be generated.</li>
<li>Run Hive commands which will execute the sort and generate the HFiles.</li>
<li>(Optional: if HBase and Hive are running in different clusters, distcp the generated files from the Hive cluster to the HBase cluster.)</li>
<li>Run HBase script <code>loadtable.rb</code> to move the files into a new HBase table.</li>
<li>(Optional: register the HBase table as an external table in Hive so you can access it from there.)</li>
</ol>
<p>The rest of this page explains each step in greater detail.</p>
<h2 id=decide-on-target-hbase-schema>Decide on Target HBase Schema</h2>
<p>Currently there are a number of constraints here:</p>
<ul>
<li>The target table must be new (you can&rsquo;t bulk load into an existing table)</li>
<li>The target table can only have a single column family (<a href=http://issues.apache.org/jira/browse/HBASE-1861>HBASE-1861</a>)</li>
<li>The target table cannot be sparse (every row will have the same set of columns); this should be easy to fix by either allowing a MAP value to be read from Hive, and/or by allowing rows to be read from Hive in pivoted form (one row per HBase cell)</li>
</ul>
<p>Besides dealing with these constraints, probably the most important work here is deciding on how you want to assign an HBase row key to each row coming from Hive. To avoid inconsistencies between lexical and binary comparators, it is simplest to design a string row key and use it consistently all the way through. If you want to combine multiple columns into the key, use Hive&rsquo;s string concat expression for this purpose. You can use CREATE VIEW to tack on your rowkey logically without having to update any existing data in Hive.</p>
<h2 id=estimate-resources-needed>Estimate Resources Needed</h2>
<p>TBD: provide some example numbers based on Facebook experiments; also reference <a href=http://www.hpl.hp.com/hosted/sortbenchmark/YahooHadoop.pdf>Hadoop Terasort</a></p>
<h2 id=add-necessary-jars>Add necessary JARs</h2>
<p>You will need to add a couple jar files to your path. First, put them in DFS:</p>
<pre tabindex=0><code>hadoop dfs -put /usr/lib/hive/lib/hbase-VERSION.jar /user/hive/hbase-VERSION.jar
hadoop dfs -put /usr/lib/hive/lib/hive-hbase-handler-VERSION.jar /user/hive/hive-hbase-handler-VERSION.jar

</code></pre><p>Then add them to your hive-site.xml:</p>
<pre tabindex=0><code>&lt;property&gt;
  &lt;name&gt;hive.aux.jars.path&lt;/name&gt;
  &lt;value&gt;/user/hive/hbase-VERSION.jar,/user/hive/hive-hbase-handler-VERSION.jar&lt;/value&gt;
&lt;/property&gt;

</code></pre><h2 id=prepare-range-partitioning>Prepare Range Partitioning</h2>
<p>In order to perform a parallel sort on the data, we need to range-partition it. The idea is to divide the space of row keys up into nearly equal-sized ranges, one per reducer which will be used in the parallel sort. The details will vary according to your source data, and you may need to run a number of exploratory Hive queries in order to come up with a good enough set of ranges. Here&rsquo;s one example:</p>
<pre tabindex=0><code>add jar lib/hive-contrib-0.7.0.jar;
set mapred.reduce.tasks=1;
create temporary function row_sequence as 
'org.apache.hadoop.hive.contrib.udf.UDFRowSequence';
select transaction_id from
(select transaction_id
from transactions
tablesample(bucket 1 out of 10000 on transaction_id) s 
order by transaction_id 
limit 10000000) x
where (row_sequence() % 910000)=0
order by transaction_id
limit 11;

</code></pre><p>This works by ordering all of the rows in a .01% sample of the table (using a single reducer), and then selecting every nth row (here n=910000). The value of n is chosen by dividing the total number of rows in the sample by the desired number of ranges, e.g. 12 in this case (one more than the number of partitioning keys produced by the LIMIT clause). The assumption here is that the distribution in the sample matches the overall distribution in the table; if this is not the case, the resulting partition keys will lead to skew in the parallel sort.</p>
<p>Once you have your sampling query defined, the next step is to save its results to a properly formatted file which will be used in a subsequent step. To do this, run commands like the following:</p>
<pre tabindex=0><code>create external table hb_range_keys(transaction_id_range_start string)
row format serde 
'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe'
stored as 
inputformat 
'org.apache.hadoop.mapred.TextInputFormat'
outputformat 
'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat'
location '/tmp/hb_range_keys';

insert overwrite table hb_range_keys
select transaction_id from
(select transaction_id
from transactions
tablesample(bucket 1 out of 10000 on transaction_id) s 
order by transaction_id 
limit 10000000) x
where (row_sequence() % 910000)=0
order by transaction_id
limit 11;

</code></pre><p>The first command creates an external table defining the format of the file to be created; be sure to set the serde and inputformat/outputformat exactly as specified.</p>
<p>The second command populates it (using the sampling query previously defined). Usage of ORDER BY guarantees that a single file will be produced in directory <code>/tmp/hb_range_keys</code>. The filename is unknown, but it is necessary to reference the file by name later, so run a command such as the following to copy it to a specific name:</p>
<pre tabindex=0><code>dfs -cp /tmp/hb_range_keys/* /tmp/hb_range_key_list;

</code></pre><h2 id=prepare-staging-location>Prepare Staging Location</h2>
<p>The sort is going to produce a lot of data, so make sure you have sufficient space in your HDFS cluster, and choose the location where the files will be staged. We&rsquo;ll use <code>/tmp/hbsort</code> in this example.</p>
<p>The directory does not actually need to exist (it will be automatically created in the next step), but if it does exist, it should be empty.</p>
<pre tabindex=0><code>dfs -rmr /tmp/hbsort;
dfs -mkdir /tmp/hbsort;

</code></pre><h2 id=sort-data>Sort Data</h2>
<p>Now comes the big step: running a sort over all of the data to be bulk loaded. Make sure that your Hive instance has the HBase jars available on its auxpath.</p>
<pre tabindex=0><code>set hive.execution.engine=mr;
set mapred.reduce.tasks=12;
set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
set total.order.partitioner.path=/tmp/hb_range_key_list;
set hfile.compression=gz;

create table hbsort(transaction_id string, user_name string, amount double, ...)
stored as
INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat'
TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf');

insert overwrite table hbsort
select transaction_id, user_name, amount, ...
from transactions
cluster by transaction_id;

</code></pre><p>The CREATE TABLE creates a dummy table which controls how the output of the sort is written. Note that it uses <code>HiveHFileOutputFormat</code> to do this, with the table property <code>hfile.family.path</code> used to control the destination directory for the output. Again, be sure to set the inputformat/outputformat exactly as specified. In the example above, we select gzip (gz) compression for the result files; if you don&rsquo;t set the <code>hfile.compression</code> parameter, no compression will be performed. (The other method available is lzo, which compresses less aggressively but does not require as much CPU power.)</p>
<p>Note that the number of reduce tasks is one more than the number of partitions - this <strong>must</strong> be true or else you will get a &ldquo;Wrong number of partitions in keyset&rdquo; error.</p>
<p>There is a parameter <code>hbase.hregion.max.filesize</code> (default 256MB) which affects how HFiles are generated. If the amount of data (pre-compression) produced by a reducer exceeds this limit, more than one HFile will be generated for that reducer. This will lead to unbalanced region files. This will not cause any correctness problems, but if you want to get balanced region files, either use more reducers or set this parameter to a larger value. Note that when compression is enabled, you may see multiple files generated whose sizes are well below the limit; this is because the overflow check is done pre-compression.</p>
<p>The <code>cf</code> in the path specifies the name of the column family which will be created in HBase, so the directory name you choose here is important. (Note that we&rsquo;re not actually using an HBase table here; <code>HiveHFileOutputFormat</code> writes directly to files.)</p>
<p>The CLUSTER BY clause provides the keys to be used by the partitioner; be sure that it matches the range partitioning that you came up with in the earlier step.</p>
<p>The first column in the SELECT list is interpreted as the rowkey; subsequent columns become cell values (all in a single column family, so their column names are important).</p>
<h2 id=run-hbase-script>Run HBase Script</h2>
<p>Once the sort job completes successfully, one final step is required for importing the result files into HBase. Again, we don&rsquo;t know the name of the file, so we copy it over:</p>
<pre tabindex=0><code>dfs -copyToLocal /tmp/hbsort/cf/* /tmp/hbout

</code></pre><p>If Hive and HBase are running in different clusters, use <a href=http://hadoop.apache.org/common/docs/current/distcp.html>distcp</a> to copy the files from one to the other.</p>
<p>If you are using HBase 0.90.2 or newer, you can use the <a href=http://hbase.apache.org/bulk-loads.html>completebulkload</a> utility to load the data into HBase</p>
<pre tabindex=0><code>hadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /tmp/hbout transactions

</code></pre><p>In older versions of HBase, use the <code>bin/loadtable.rb</code> script to import them:</p>
<pre tabindex=0><code>hbase org.jruby.Main loadtable.rb transactions /tmp/hbout

</code></pre><p>The first argument (<code>transactions</code>) specifies the name of the new HBase table. For the second argument, pass the staging directory name, not the the column family child directory.</p>
<p>After this script finishes, you may need to wait a minute or two for the new table to be picked up by the HBase meta scanner. Use the hbase shell to verify that the new table was created correctly, and do some sanity queries to locate individual cells and make sure they can be found.</p>
<h2 id=map-new-table-back-into-hive>Map New Table Back Into Hive</h2>
<p>Finally, if you&rsquo;d like to access the HBase table you just created via Hive:</p>
<pre tabindex=0><code>CREATE EXTERNAL TABLE hbase_transactions(transaction_id string, user_name string, amount double, ...) 
STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (&quot;hbase.columns.mapping&quot; = &quot;:key,cf:user_name,cf:amount,...&quot;)
TBLPROPERTIES(&quot;hbase.table.name&quot; = &quot;transactions&quot;);

</code></pre><h2 id=followups-needed>Followups Needed</h2>
<ul>
<li>Support sparse tables</li>
<li>Support loading binary data representations once HIVE-1245 is fixed</li>
<li>Support assignment of timestamps</li>
<li>Provide control over file parameters such as compression</li>
<li>Support multiple column families once HBASE-1861 is implemented</li>
<li>Support loading into existing tables once HBASE-1923 is implemented</li>
<li>Wrap it all up into the ideal single-INSERT-with-auto-sampling job&mldr;</li>
</ul>
</div>
</div>
<footer class="black-background static-bottom" style=padding:30px>
<div class=row>
<div class=col-3>
<a href=https://www.apache.org/>
<img src=https://hive.apache.org/images/asf_logo.png width=270 height=100 alt="Apache Software Foundation"></a>
</a>
</div>
<div class=col-9>
<p class=footer-text>Apache is a non-profit organization helping open-source
software projects released under the Apache
<a href=https://www.apache.org/licenses/>license</a>
and managed with
<a href=https://www.apache.org/foundation/how-it-works.html>
open governance</a> and
<a href=https://privacy.apache.org/policies/privacy-policy-public.html>
privacy policy</a>. See upcoming
<a href=https://www.apache.org/events/current-event>Apache Events</a>.
If you discover any
<a href=https://www.apache.org/security/>security</a> vulnerabilities, please
report them privately. Finally,
<a href=https://www.apache.org/foundation/sponsorship.html>thanks
</a> to the sponsors who
<a href=https://www.apache.org/foundation/contributing.html>
donate</a> to the Apache Foundation.
</p>
</div>
</div>
<div class="copyright row">
<a href=https://hive.apache.org style=color:grey>
The contents of this website are © 2023 Apache Software Foundation under the terms of the Apache License v2. Apache Hive and its logo are trademarks of the Apache Software Foundation.
</a>
</div>
</footer>
<script src=https://hive.apache.org/js/bootstrap.bundle.min.js></script>
</body>
</html>