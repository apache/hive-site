<!doctype html><html><!doctype html>
<html>
<head>
<meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<meta name=description content>
<meta name=author content>
<title>Apache Hive : GettingStarted</title>
<link rel=icon href=/images/hive.svg sizes=any type=image/svg+xml>
<link rel=stylesheet href=https://hive.apache.org/css/hive-theme.css>
<link rel=stylesheet href=https://hive.apache.org/css/font-awesome.all.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/bootstrap.min.css>
<link rel=stylesheet href=https://hive.apache.org/css/termynal.css>
<link rel=apple-touch-icon sizes=180x180 href=https://hive.apache.org/images/apple-touch-icon.png>
<link rel=icon type=image/png sizes=32x32 href=https://hive.apache.org/images/favicon-32x32.png>
<link rel=icon type=image/png sizes=16x16 href=https://hive.apache.org/images/favicon-16x16.png>
<link rel=manifest href=https://hive.apache.org/images/site.webmanifest>
<link rel=mask-icon href=https://hive.apache.org/images/safari-pinned-tab.svg color=#5bbad5>
<meta name=msapplication-TileColor content="#da532c">
<meta name=theme-color content="#ffffff">
<script>var _paq=window._paq=window._paq||[];_paq.push(['disableCookies']),_paq.push(['trackPageView']),_paq.push(['enableLinkTracking']),function(){var b="https://analytics.apache.org/",c,a,d;_paq.push(['setTrackerUrl',b+'matomo.php']),_paq.push(['setSiteId','30']),c=document,a=c.createElement('script'),d=c.getElementsByTagName('script')[0],a.async=!0,a.src=b+'matomo.js',d.parentNode.insertBefore(a,d)}()</script>
</head>
<body>
<body>
<header>
<menu style=background:#000;margin:0>
<nav class="navbar navbar-expand-lg navbar-dark bg-black">
<div class=container-fluid>
<a href=https://hive.apache.org> <img src=https://hive.apache.org/images/hive.svg width=60 height=35 alt="Apache Software Foundation"></a>
<a class="header-text navbar-brand" href=https://hive.apache.org>Apache Hive</a>
<button class=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#navbarSupportedContent aria-controls=navbarSupportedContent aria-expanded=false aria-label="Toggle navigation">
<span class=navbar-toggler-icon></span>
</button>
<div class="collapse navbar-collapse" id=navbarSupportedContent>
<ul class="navbar-nav me-auto mb-2 mb-lg-0">
<li class="nav-item dropdown">
<a class=nav-link href=/general/downloads id=navbarDropdown role=button aria-expanded=false>
Releases
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/Document id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Documentation
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/docs/latest/>Latest</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/docs/javadocs/>Javadocs</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/LanguageManual>Language Manual</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=/general id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
General
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/licenses/LICENSE-2.0.html>License</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/general/privacypolicy/>Privacy Policy</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Development
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://hive.apache.org/development/gettingstarted/>Getting Started</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/quickstart/>Quickstart with Docker</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/DesignDocs>Design Docs</a></li>
<li><a class=dropdown-item href=https://issues.apache.org/jira/projects/HIVE/issues>Hive JIRA</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HiveDeveloperFAQ>Hive Developer FAQ</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Hive+PreCommit+Patch+Testing>Precommit Patch Testing</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/development/versioncontrol/>Version Control</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
Community
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=/community/becomingcommitter/>Becoming A Committer</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToContribute>How To Contribute</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/Home#Home-ResourcesforContributors>Resources for Contributors</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/mailinglists/>Mailing Lists</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/issuetracking/>Issue Tracking</a></li>
<li><a class=dropdown-item href=https://hive.apache.org/community/people/>People</a></li>
<li>
<hr class=dropdown-divider>
</li>
<li><a class=dropdown-item href=/community/bylaws/>By Laws</a></li>
<li><a class=dropdown-item href=https://cwiki.apache.org/confluence/display/Hive/HowToRelease>How To Release</a></li>
</ul>
</li>
<li class="nav-item dropdown">
<a class=nav-link href=https://hive.blog.apache.org/ id=navbarDropdown role=button aria-expanded=false>
Blogs
</a>
</li>
<li class="nav-item dropdown">
<a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-expanded=false>
ASF
</a>
<ul class=dropdown-menu aria-labelledby=navbarDropdown>
<li><a class=dropdown-item href=https://www.apache.org/foundation/contributing.html>Donations</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a></li>
<li><a class=dropdown-item href=https://www.apache.org/foundation/thanks.html>Thanks</a></li>
<li><a class=dropdown-item href=https://www.apache.org/>Website</a></li>
</ul>
</li>
<li>
<form action=/search method=get class=search-bar>
<input type=search name=q id=search-query placeholder=Search... class=search-input>
<button type=submit class=search-button>Search</button>
</form>
</li>
</ul>
</div>
</div>
</nav>
</menu>
</header>
<div class=content>
<div class=docs>
<h1 id=apache-hive--gettingstarted>Apache Hive : GettingStarted</h1>
<p>Table of Contents</p>
<ul>
<li>
<p><a href=#installation-and-configuration>Installation and Configuration</a></p>
<pre><code>  - [Running HiveServer2 and Beeline](#running-hiveserver2-and-beeline)+ [Requirements](#requirements)
</code></pre>
<ul>
<li><a href=#installing-hive-from-a-stable-release>Installing Hive from a Stable Release</a></li>
<li><a href=#building-hive-from-source>Building Hive from Source</a>
<ul>
<li><a href=#compile-hive-on-master>Compile Hive on master</a></li>
<li><a href=#compile-hive-on-branch-1>Compile Hive on branch-1</a></li>
<li><a href=#compile-hive-prior-to-013-on-hadoop-020>Compile Hive Prior to 0.13 on Hadoop 0.20</a></li>
<li><a href=#compile-hive-prior-to-013-on-hadoop-023>Compile Hive Prior to 0.13 on Hadoop 0.23</a></li>
</ul>
</li>
<li><a href=#running-hive>Running Hive</a>
<ul>
<li><a href=#running-hive-cli>Running Hive CLI</a></li>
<li><a href=#running-hiveserver2-and-beeline>Running HiveServer2 and Beeline</a></li>
<li><a href=#running-hcatalog>Running HCatalog</a></li>
<li><a href=#running-webhcat-templeton>Running WebHCat (Templeton)</a></li>
</ul>
</li>
<li><a href=#configuration-management-overview>Configuration Management Overview</a></li>
<li><a href=#runtime-configuration>Runtime Configuration</a></li>
<li><a href=#hive-map-reduce-and-local-mode>Hive, Map-Reduce and Local-Mode</a></li>
<li><a href=#hive-logging>Hive Logging</a>
<ul>
<li><a href=#hiveserver2-logs>HiveServer2 Logs</a></li>
<li><a href=#audit-logs>Audit Logs</a></li>
<li><a href=#perf-logger>Perf Logger</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href=#ddl-operations>DDL Operations</a></p>
<ul>
<li><a href=#creating-hive-tables>Creating Hive Tables</a></li>
<li><a href=#browsing-through-tables>Browsing through Tables</a></li>
<li><a href=#altering-and-dropping-tables>Altering and Dropping Tables</a></li>
<li><a href=#metadata-store>Metadata Store</a></li>
</ul>
</li>
<li>
<p><a href=#dml-operations>DML Operations</a></p>
</li>
<li>
<p><a href=#sql-operations>SQL Operations</a></p>
<ul>
<li><a href=#example-queries>Example Queries</a>
<ul>
<li><a href=#selects-and-filters>SELECTS and FILTERS</a></li>
<li><a href=#group-by>GROUP BY</a></li>
<li><a href=#join>JOIN</a></li>
<li><a href=#multitable-insert>MULTITABLE INSERT</a></li>
<li><a href=#streaming>STREAMING</a></li>
</ul>
</li>
</ul>
</li>
<li>
<p><a href=#simple-example-use-cases>Simple Example Use Cases</a></p>
<ul>
<li><a href=#movielens-user-ratings>MovieLens User Ratings</a></li>
<li><a href=#apache-weblog-data>Apache Weblog Data</a></li>
</ul>
</li>
</ul>
<h2 id=installation-and-configuration>Installation and Configuration</h2>
<p>You can install a stable release of Hive by downloading a tarball, or you can download the source code and build Hive from that.</p>
<h4 id=running-hiveserver2-and-beeline>Running HiveServer2 and Beeline</h4>
<h3 id=requirements>Requirements</h3>
<ul>
<li>Java 1.7<br>
<em>Note:</em>  Hive versions <a href="https://issues.apache.org/jira/browse/HIVE/fixforversion/12329345/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel">1.2</a> onward require Java 1.7 or newer. Hive versions 0.14 to 1.1 work with Java 1.6 as well. Users are strongly advised to start moving to Java 1.8 (see <a href=https://issues.apache.org/jira/browse/HIVE-8607>HIVE-8607</a>).</li>
<li>Hadoop 2.x (preferred), 1.x (not supported by Hive 2.0.0 onward).<br>
Hive versions up to 0.13 also supported Hadoop 0.20.x, 0.23.x.</li>
<li>Hive is commonly used in production Linux and Windows environment. Mac is a commonly used development environment. The instructions in this document are applicable to Linux and Mac. Using it on Windows would require slightly different steps.</li>
</ul>
<h3 id=installing-hive-from-a-stable-release>Installing Hive from a Stable Release</h3>
<p>Start by downloading the most recent stable release of Hive from one of the Apache download mirrors (see <a href=https://hive.apache.org/downloads.html>Hive Releases</a>).</p>
<p>Next you need to unpack the tarball. This will result in the creation of a subdirectory named <code>hive-x.y.z</code> (where <code>x.y.z</code> is the release number):</p>
<pre tabindex=0><code>  $ tar -xzvf hive-x.y.z.tar.gz

</code></pre><p>Set the environment variable <code>HIVE_HOME</code> to point to the installation directory:</p>
<pre tabindex=0><code>  $ cd hive-x.y.z
  $ export HIVE_HOME={{pwd}}

</code></pre><p>Finally, add <code>$HIVE_HOME/bin</code> to your <code>PATH</code>:</p>
<pre tabindex=0><code>  $ export PATH=$HIVE_HOME/bin:$PATH

</code></pre><h3 id=building-hive-from-source>Building Hive from Source</h3>
<p>The Hive GIT repository for the most recent Hive code is located here: <code>git clone &lt;https://git-wip-us.apache.org/repos/asf/hive.git></code> (the master branch).</p>
<p>All release versions are in branches named &ldquo;branch-0.#&rdquo; or &ldquo;branch-1.#&rdquo; or the upcoming &ldquo;branch-2.#&rdquo;, with the exception of release 0.8.1 which is in &ldquo;branch-0.8-r2&rdquo;. Any branches with other names are feature branches for works-in-progress. See <a href=https://cwiki.apache.org/confluence/display/Hive/HowToContribute#HowToContribute-UnderstandingHiveBranches>Understanding Hive Branches</a> for details.</p>
<p>As of 0.13, Hive is built using <a href=http://maven.apache.org>Apache Maven</a>.</p>
<h4 id=compile-hive-on-master>Compile Hive on master</h4>
<p>To build the current Hive code from the master branch:</p>
<pre tabindex=0><code>  $ git clone https://git-wip-us.apache.org/repos/asf/hive.git
  $ cd hive
  $ mvn clean package -Pdist [-DskipTests -Dmaven.javadoc.skip=true]
  $ cd packaging/target/apache-hive-{version}-SNAPSHOT-bin/apache-hive-{version}-SNAPSHOT-bin
  $ ls
  LICENSE
  NOTICE
  README.txt
  RELEASE_NOTES.txt
  bin/ (all the shell scripts)
  lib/ (required jar files)
  conf/ (configuration files)
  examples/ (sample input and query files)
  hcatalog / (hcatalog installation)
  scripts / (upgrade scripts for hive-metastore)
</code></pre><p>Here, {version} refers to the current Hive version.</p>
<p>If building Hive source using Maven (mvn), we will refer to the directory &ldquo;/packaging/target/apache-hive-{version}-SNAPSHOT-bin/apache-hive-{version}-SNAPSHOT-bin&rdquo; as for the rest of the page.</p>
<h4 id=compile-hive-on-branch-1>Compile Hive on branch-1</h4>
<p>In branch-1, Hive supports both Hadoop 1.x and 2.x.  You will need to specify which version of Hadoop to build against via a Maven profile.  To build against Hadoop 1.x use the profile <code>hadoop-1</code>; for Hadoop 2.x use <code>hadoop-2</code>.  For example to build against Hadoop 1.x, the above mvn command becomes:</p>
<pre tabindex=0><code>  $ mvn clean package -Phadoop-1,dist

</code></pre><h4 id=compile-hive-prior-to-013-on-hadoop-020>Compile Hive Prior to 0.13 on Hadoop 0.20</h4>
<p>Prior to Hive 0.13, Hive was built using <a href=http://ant.apache.org/>Apache Ant</a>.  To build an older version of Hive on Hadoop 0.20:</p>
<pre tabindex=0><code>  $ svn co http://svn.apache.org/repos/asf/hive/branches/branch-{version} hive
  $ cd hive
  $ ant clean package
  $ cd build/dist
  # ls
  LICENSE
  NOTICE
  README.txt
  RELEASE_NOTES.txt
  bin/ (all the shell scripts)
  lib/ (required jar files)
  conf/ (configuration files)
  examples/ (sample input and query files)
  hcatalog / (hcatalog installation)
  scripts / (upgrade scripts for hive-metastore)
</code></pre><p>If using Ant, we will refer to the directory &ldquo;<code>build/dist</code>&rdquo; as <code>&lt;install-dir></code>.</p>
<h4 id=compile-hive-prior-to-013-on-hadoop-023>Compile Hive Prior to 0.13 on Hadoop 0.23</h4>
<p>To build Hive in Ant against Hadoop 0.23, 2.0.0, or other version, build with the appropriate flag; some examples below:</p>
<pre tabindex=0><code>  $ ant clean package -Dhadoop.version=0.23.3 -Dhadoop-0.23.version=0.23.3 -Dhadoop.mr.rev=23
  $ ant clean package -Dhadoop.version=2.0.0-alpha -Dhadoop-0.23.version=2.0.0-alpha -Dhadoop.mr.rev=23

</code></pre><h3 id=running-hive>Running Hive</h3>
<p>Hive uses Hadoop, so:</p>
<ul>
<li>you must have Hadoop in your path OR</li>
<li><code>export HADOOP_HOME=&lt;hadoop-install-dir></code></li>
</ul>
<p>In addition, you must use below HDFS commands to create <code>/tmp</code> and <code>/user/hive/warehouse</code> (aka <code>hive.metastore.warehouse.dir</code>) and set them <code>chmod g+w</code> before you can create a table in Hive.</p>
<pre tabindex=0><code>  $ $HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
  $ $HADOOP_HOME/bin/hadoop fs -mkdir       /user/hive/warehouse
  $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
  $ $HADOOP_HOME/bin/hadoop fs -chmod g+w   /user/hive/warehouse

</code></pre><p>You may find it useful, though it&rsquo;s not necessary, to set <code>HIVE_HOME</code>:</p>
<pre tabindex=0><code>  $ export HIVE_HOME=&lt;hive-install-dir&gt;

</code></pre><h4 id=running-hive-cli>Running Hive CLI</h4>
<p>To use the Hive <a href=https://hive.apache.org/docs/latest/languagemanual-cli_27362033/>command line interface</a> (CLI) from the shell:</p>
<pre tabindex=0><code>  $ $HIVE_HOME/bin/hive

</code></pre><h4 id=running-hiveserver2-and-beeline-1>Running HiveServer2 and Beeline</h4>
<p>Starting from Hive 2.1, we need to run the schematool command below as an initialization step. For example, we can use &ldquo;derby&rdquo; as db type. </p>
<pre tabindex=0><code>  $ $HIVE_HOME/bin/schematool -dbType &lt;db type&gt; -initSchema

</code></pre><p><a href=https://hive.apache.org/docs/latest/setting-up-hiveserver2_30758712/>HiveServer2</a> (introduced in Hive 0.11) has its own CLI called <a href=https://hive.apache.org/docs/latest/hiveserver2-clients_30758725/>Beeline</a>.  HiveCLI is now deprecated in favor of Beeline, as it lacks the multi-user, security, and other capabilities of HiveServer2.  To run HiveServer2 and Beeline from shell:</p>
<pre tabindex=0><code>  $ $HIVE_HOME/bin/hiveserver2

  $ $HIVE_HOME/bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT
</code></pre><p>Beeline is started with the JDBC URL of the HiveServer2, which depends on the address and port where HiveServer2 was started.  By default, it will be (localhost:10000), so the address will look like jdbc:hive2://localhost:10000.</p>
<p>Or to start Beeline and HiveServer2 in the same process for testing purpose, for a similar user experience to HiveCLI:</p>
<pre tabindex=0><code>  $ $HIVE_HOME/bin/beeline -u jdbc:hive2://

</code></pre><h4 id=running-hcatalog>Running HCatalog</h4>
<p>To run the HCatalog server from the shell in Hive release 0.11.0 and later:</p>
<pre tabindex=0><code>  $ $HIVE_HOME/hcatalog/sbin/hcat_server.sh

</code></pre><p>To use the HCatalog command line interface (CLI) in Hive release 0.11.0 and later:</p>
<pre tabindex=0><code>  $ $HIVE_HOME/hcatalog/bin/hcat

</code></pre><p>For more information, see <a href=https://hive.apache.org/docs/latest/hcatalog-installhcat_34013403/>HCatalog Installation from Tarball</a> and <a href=https://hive.apache.org/docs/latest/hcatalog-cli_34013932/>HCatalog CLI</a> in the <a href=https://hive.apache.org/docs/latest/hcatalog_33299065/>HCatalog manual</a>.</p>
<h4 id=running-webhcat-templeton>Running WebHCat (Templeton)</h4>
<p>To run the WebHCat server from the shell in Hive release 0.11.0 and later:</p>
<pre tabindex=0><code>  $ $HIVE_HOME/hcatalog/sbin/webhcat_server.sh

</code></pre><p>For more information, see <a href=https://hive.apache.org/docs/latest/webhcat-installwebhcat_34015585/>WebHCat Installation</a> in the <a href=https://hive.apache.org/docs/latest/webhcat_33299069/>WebHCat manual</a>.</p>
<h3 id=configuration-management-overview>Configuration Management Overview</h3>
<ul>
<li>Hive by default gets its configuration from <code>&lt;install-dir>/conf/hive-default.xml</code></li>
<li>The location of the Hive configuration directory can be changed by setting the <code>HIVE_CONF_DIR</code> environment variable.</li>
<li>Configuration variables can be changed by (re-)defining them in <code>&lt;install-dir>/conf/hive-site.xml</code></li>
<li>Log4j configuration is stored in <code>&lt;install-dir>/conf/hive-log4j.properties</code></li>
<li>Hive configuration is an overlay on top of Hadoop – it inherits the Hadoop configuration variables by default.</li>
<li>Hive configuration can be manipulated by:
<ul>
<li>Editing hive-site.xml and defining any desired variables (including Hadoop variables) in it</li>
<li>Using the set command (see next section)</li>
<li>Invoking Hive (deprecated), Beeline or HiveServer2 using the syntax:
<ul>
<li><code>$ bin/hive --hiveconf x1=y1 --hiveconf x2=y2  //this sets the variables x1 and x2 to y1 and y2 respectively</code></li>
<li>$ bin/hiveserver2 &ndash;hiveconf x1=y1 &ndash;hiveconf x2=y2  //this sets server-side variables x1 and x2 to y1 and y2 respectively</li>
<li>$ bin/beeline &ndash;hiveconf x1=y1 &ndash;hiveconf x2=y2  //this sets client-side variables x1 and x2 to y1 and y2 respectively.</li>
</ul>
</li>
<li>Setting the <code>HIVE_OPTS</code> environment variable to &ldquo;<code>--hiveconf x1=y1 --hiveconf x2=y2</code>&rdquo; which does the same as above.</li>
</ul>
</li>
</ul>
<h3 id=runtime-configuration>Runtime Configuration</h3>
<ul>
<li>Hive queries are executed using map-reduce queries and, therefore, the behavior of such queries can be controlled by the Hadoop configuration variables.</li>
<li>The HiveCLI (deprecated) and Beeline command &lsquo;SET&rsquo; can be used to set any Hadoop (or Hive) configuration variable. For example:</li>
</ul>
<pre tabindex=0><code>    beeline&gt; SET mapred.job.tracker=myhost.mycompany.com:50030;
    beeline&gt; SET -v;

</code></pre><p>The latter shows all the current settings. Without the <code>-v</code> option only the variables that differ from the base Hadoop configuration are displayed.</p>
<h3 id=hive-map-reduce-and-local-mode>Hive, Map-Reduce and Local-Mode</h3>
<p>Hive compiler generates map-reduce jobs for most queries. These jobs are then submitted to the Map-Reduce cluster indicated by the variable:</p>
<pre tabindex=0><code>  mapred.job.tracker

</code></pre><p>While this usually points to a map-reduce cluster with multiple nodes, Hadoop also offers a nifty option to run map-reduce jobs locally on the user&rsquo;s workstation. This can be very useful to run queries over small data sets – in such cases local mode execution is usually significantly faster than submitting jobs to a large cluster. Data is accessed transparently from HDFS. Conversely, local mode only runs with one reducer and can be very slow processing larger data sets.</p>
<p>Starting with release 0.7, Hive fully supports local mode execution. To enable this, the user can enable the following option:</p>
<pre tabindex=0><code>  hive&gt; SET mapreduce.framework.name=local;
</code></pre><p>In addition, <code>mapred.local.dir</code> should point to a path that&rsquo;s valid on the local machine (for example <code>/tmp/&lt;username>/mapred/local</code>). (Otherwise, the user will get an exception allocating local disk space.)</p>
<p>Starting with release 0.7, Hive also supports a mode to run map-reduce jobs in local-mode automatically. The relevant options are <code>hive.exec.mode.local.auto</code>, <code>hive.exec.mode.local.auto.inputbytes.max</code>, and <code>hive.exec.mode.local.auto.tasks.max</code>:</p>
<pre tabindex=0><code>  hive&gt; SET hive.exec.mode.local.auto=false;

</code></pre><p>Note that this feature is <em>disabled</em> by default. If enabled, Hive analyzes the size of each map-reduce job in a query and may run it locally if the following thresholds are satisfied:</p>
<ul>
<li>The total input size of the job is lower than: <code>hive.exec.mode.local.auto.inputbytes.max</code> (128MB by default)</li>
<li>The total number of map-tasks is less than: <code>hive.exec.mode.local.auto.tasks.max</code> (4 by default)</li>
<li>The total number of reduce tasks required is 1 or 0.</li>
</ul>
<p>So for queries over small data sets, or for queries with multiple map-reduce jobs where the input to subsequent jobs is substantially smaller (because of reduction/filtering in the prior job), jobs may be run locally.</p>
<p>Note that there may be differences in the runtime environment of Hadoop server nodes and the machine running the Hive client (because of different jvm versions or different software libraries). This can cause unexpected behavior/errors while running in local mode. Also note that local mode execution is done in a separate, child jvm (of the Hive client). If the user so wishes, the maximum amount of memory for this child jvm can be controlled via the option <code>hive.mapred.local.mem</code>. By default, it&rsquo;s set to zero, in which case Hive lets Hadoop determine the default memory limits of the child jvm.</p>
<h3 id=hive-logging>Hive Logging</h3>
<p>Hive uses log4j for logging. By default logs are not emitted to the console by the CLI. The default logging level is <code>WARN</code> for Hive releases prior to 0.13.0. Starting with Hive 0.13.0, the default logging level is <code>INFO</code>.</p>
<p>The logs are stored in the directory <code>/tmp/&lt;*user.name*></code>:</p>
<ul>
<li><code>/tmp/&lt;*user.name*>/hive.log</code><br>
Note: In <a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=82706478#GettingStarted-Hive,Map-ReduceandLocal-Mode">local mode</a>, prior to Hive 0.13.0 the log file name was &ldquo;<code>.log</code>&rdquo; instead of &ldquo;<code>hive.log</code>&rdquo;. This bug was fixed in release 0.13.0 (see <a href=https://issues.apache.org/jira/browse/HIVE-5528>HIVE-5528</a> and <a href=https://issues.apache.org/jira/browse/HIVE-5676>HIVE-5676</a>).</li>
</ul>
<p>To configure a different log location, set <code>hive.log.dir</code> in $HIVE_HOME/conf/hive-log4j.properties. Make sure the directory has the sticky bit set (<code>chmod 1777 &lt;*dir*></code>).</p>
<ul>
<li><code>hive.log.dir=*&lt;other_location>*</code></li>
</ul>
<p>If the user wishes, the logs can be emitted to the console by adding the arguments shown below:</p>
<ul>
<li><code>bin/hive --hiveconf hive.root.logger=INFO,console  //for HiveCLI (deprecated)</code></li>
<li><code>bin/hiveserver2 --hiveconf hive.root.logger=INFO,console</code></li>
</ul>
<p>Alternatively, the user can change the logging level only by using:</p>
<ul>
<li><code>bin/hive --hiveconf hive.root.logger=INFO,DRFA //for HiveCLI (deprecated)</code></li>
<li><code>bin/hiveserver2 --hiveconf hive.root.logger=INFO,DRFA</code></li>
</ul>
<p>Another option for logging is TimeBasedRollingPolicy (applicable for Hive 1.1.0 and above, <a href=https://issues.apache.org/jira/browse/HIVE-9001>HIVE-9001</a>) by providing DAILY option as shown below:</p>
<ul>
<li><code>bin/hive --hiveconf hive.root.logger=INFO,DAILY //for HiveCLI (deprecated)</code></li>
<li><code>bin/hiveserver2 --hiveconf hive.root.logger=INFO,DAILY</code></li>
</ul>
<p>Note that setting <code>hive.root.logger</code> via the &lsquo;set&rsquo; command does not change logging properties since they are determined at initialization time.</p>
<p>Hive also stores query logs on a per Hive session basis in <code>/tmp/&lt;user.name>/</code>, but can be configured in <a href=https://hive.apache.org/docs/latest/adminmanual-configuration_27362070/>hive-site.xml</a> with the <code>[hive.querylog.location](#hive-querylog-location)</code> property.  Starting with Hive 1.1.0, <a href=#explain-extended>EXPLAIN EXTENDED</a> output for queries can be logged at the INFO level by setting the <code>[hive.log.explain.output](#hive-log-explain-output)</code> property to true.</p>
<p>Logging during Hive execution on a Hadoop cluster is controlled by Hadoop configuration. Usually Hadoop will produce one log file per map and reduce task stored on the cluster machine(s) where the task was executed. The log files can be obtained by clicking through to the Task Details page from the Hadoop JobTracker Web UI.</p>
<p>When using local mode (using <code>mapreduce.framework.name=local</code>), Hadoop/Hive execution logs are produced on the client machine itself. Starting with release 0.6 – Hive uses the <code>hive-exec-log4j.properties</code> (falling back to <code>hive-log4j.properties</code> only if it&rsquo;s missing) to determine where these logs are delivered by default. The default configuration file produces one log file per query executed in local mode and stores it under <code>/tmp/&lt;user.name></code>. The intent of providing a separate configuration file is to enable administrators to centralize execution log capture if desired (on a NFS file server for example). Execution logs are invaluable for debugging run-time errors.</p>
<p>For information about WebHCat errors and logging, see <a href=#error-codes-and-responses>Error Codes and Responses</a> and <a href=#log-files>Log Files</a> in the <a href=https://hive.apache.org/docs/latest/webhcat_33299069/>WebHCat manual</a>.</p>
<p>Error logs are very useful to debug problems. Please send them with any bugs (of which there are many!) to <code>hive-dev@hadoop.apache.org</code>.</p>
<p>From Hive 2.1.0 onwards (with <a href=https://issues.apache.org/jira/browse/HIVE-13027>HIVE-13027</a>), Hive uses Log4j2&rsquo;s asynchronous logger by default. Setting <a href=#hive-async-log-enabled>hive.async.log.enabled</a> to false will disable asynchronous logging and fallback to synchronous logging. Asynchronous logging can give significant performance improvement as logging will be handled in a separate thread that uses the LMAX disruptor queue for buffering log messages. Refer to <a href=https://logging.apache.org/log4j/2.x/manual/async.html>https://logging.apache.org/log4j/2.x/manual/async.html</a> for benefits and drawbacks.</p>
<h4 id=hiveserver2-logs>HiveServer2 Logs</h4>
<p>HiveServer2 operation logs are available to clients starting in Hive 0.14. See <a href=#hiveserver2-logging>HiveServer2 Logging</a> for configuration.</p>
<h4 id=audit-logs>Audit Logs</h4>
<p>Audit logs are logged from the Hive metastore server for every metastore API invocation.</p>
<p>An audit log has the function and some of the relevant function arguments logged in the metastore log file. It is logged at the INFO level of log4j, so you need to make sure that the logging at the INFO level is enabled (see <a href=https://issues.apache.org/jira/browse/HIVE-3505>HIVE-3505</a>). The name of the log entry is &ldquo;HiveMetaStore.audit&rdquo;.</p>
<p>Audit logs were added in Hive 0.7 for secure client connections (<a href=https://issues.apache.org/jira/browse/HIVE-1948>HIVE-1948</a>) and in Hive 0.10 for non-secure connections (<a href=https://issues.apache.org/jira/browse/HIVE-3277>HIVE-3277</a>; also see <a href=https://issues.apache.org/jira/browse/HIVE-2797>HIVE-2797</a>).</p>
<h4 id=perf-logger>Perf Logger</h4>
<p>In order to obtain the performance metrics via the PerfLogger, you need to set DEBUG level logging for the PerfLogger class (<a href=https://issues.apache.org/jira/browse/HIVE-12675>HIVE-12675</a>). This can be achieved by setting the following in the log4j properties file.</p>
<p><code>log4j.logger.org.apache.hadoop.hive.ql.log.PerfLogger=DEBUG</code></p>
<p>If the logger level has already been set to DEBUG at root via hive.root.logger, the above setting is not required to see the performance logs.</p>
<h2 id=ddl-operations>DDL Operations</h2>
<p> The Hive DDL operations are documented in <a href=https://hive.apache.org/docs/latest/languagemanual-ddl_27362034/>Hive Data Definition Language</a>.</p>
<h3 id=creating-hive-tables>Creating Hive Tables</h3>
<pre tabindex=0><code>  hive&gt; CREATE TABLE pokes (foo INT, bar STRING);

</code></pre><p>creates a table called pokes with two columns, the first being an integer and the other a string.</p>
<pre tabindex=0><code>  hive&gt; CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING);

</code></pre><p>creates a table called invites with two columns and a partition column called ds. The partition column is a virtual column. It is not part of the data itself but is derived from the partition that a particular dataset is loaded into.</p>
<p>By default, tables are assumed to be of text input format and the delimiters are assumed to be ^A(ctrl-a).</p>
<h3 id=browsing-through-tables>Browsing through Tables</h3>
<pre tabindex=0><code>  hive&gt; SHOW TABLES;

</code></pre><p>lists all the tables.</p>
<pre tabindex=0><code>  hive&gt; SHOW TABLES '.*s';

</code></pre><p>lists all the table that end with &rsquo;s'. The pattern matching follows Java regular expressions. Check out this link for documentation <a href=http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html>http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html</a>.</p>
<pre tabindex=0><code>hive&gt; DESCRIBE invites;

</code></pre><p>shows the list of columns.</p>
<h3 id=altering-and-dropping-tables>Altering and Dropping Tables</h3>
<p>Table names can be <a href=#changed>changed</a> and columns can be <a href=#added-or-replaced>added or replaced</a>:</p>
<pre tabindex=0><code>  hive&gt; ALTER TABLE events RENAME TO 3koobecaf;
  hive&gt; ALTER TABLE pokes ADD COLUMNS (new_col INT);
  hive&gt; ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment');
  hive&gt; ALTER TABLE invites REPLACE COLUMNS (foo INT, bar STRING, baz INT COMMENT 'baz replaces new_col2');

</code></pre><p>Note that REPLACE COLUMNS replaces all existing columns and only changes the table&rsquo;s schema, not the data. The table must use a native SerDe. REPLACE COLUMNS can also be used to drop columns from the table&rsquo;s schema:</p>
<pre tabindex=0><code>  hive&gt; ALTER TABLE invites REPLACE COLUMNS (foo INT COMMENT 'only keep the first column');

</code></pre><p>Dropping tables:</p>
<pre tabindex=0><code>  hive&gt; DROP TABLE pokes;

</code></pre><h3 id=metadata-store>Metadata Store</h3>
<p>Metadata is in an embedded Derby database whose disk storage location is determined by the Hive configuration variable named <code>javax.jdo.option.ConnectionURL</code>. By default this location is <code>./metastore_db</code> (see <code>conf/hive-default.xml</code>).</p>
<p>Right now, in the default configuration, this metadata can only be seen by one user at a time.</p>
<p>Metastore can be stored in any database that is supported by JPOX. The location and the type of the RDBMS can be controlled by the two variables <code>javax.jdo.option.ConnectionURL</code> and <code>javax.jdo.option.ConnectionDriverName</code>. Refer to JDO (or JPOX) documentation for more details on supported databases. The database schema is defined in JDO metadata annotations file <code>package.jdo</code> at <code>src/contrib/hive/metastore/src/model</code>.</p>
<p>In the future, the metastore itself can be a standalone server.</p>
<p>If you want to run the metastore as a network server so it can be accessed from multiple nodes, see <a href=https://hive.apache.org/docs/latest/hivederbyservermode_27362068/>Hive Using Derby in Server Mode</a>.</p>
<h2 id=dml-operations>DML Operations</h2>
<p>The Hive DML operations are documented in <a href=https://hive.apache.org/docs/latest/languagemanual-dml_27362036/>Hive Data Manipulation Language</a>.</p>
<p>Loading data from flat files into Hive:</p>
<pre tabindex=0><code>  hive&gt; LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes;

</code></pre><p>Loads a file that contains two columns separated by ctrl-a into pokes table. &lsquo;LOCAL&rsquo; signifies that the input file is on the local file system. If &lsquo;LOCAL&rsquo; is omitted then it looks for the file in HDFS.</p>
<p>The keyword &lsquo;OVERWRITE&rsquo; signifies that existing data in the table is deleted. If the &lsquo;OVERWRITE&rsquo; keyword is omitted, data files are appended to existing data sets.</p>
<p>NOTES:</p>
<ul>
<li>NO verification of data against the schema is performed by the load command.</li>
<li>If the file is in hdfs, it is moved into the Hive-controlled file system namespace.<br>
The root of the Hive directory is specified by the option <code>hive.metastore.warehouse.dir</code> in <code>hive-default.xml</code>. We advise users to create this directory before trying to create tables via Hive.</li>
</ul>
<pre tabindex=0><code>  hive&gt; LOAD DATA LOCAL INPATH './examples/files/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');
  hive&gt; LOAD DATA LOCAL INPATH './examples/files/kv3.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-08');

</code></pre><p>The two LOAD statements above load data into two different partitions of the table invites. Table invites must be created as partitioned by the key ds for this to succeed.</p>
<pre tabindex=0><code>  hive&gt; LOAD DATA INPATH '/user/myname/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15');

</code></pre><p>The above command will load data from an HDFS file/directory to the table.<br>
Note that loading data from HDFS will result in moving the file/directory. As a result, the operation is almost instantaneous.</p>
<h2 id=sql-operations>SQL Operations</h2>
<p>The Hive query operations are documented in <a href=https://hive.apache.org/docs/latest/languagemanual-select_27362043/>Select</a>.</p>
<h3 id=example-queries>Example Queries</h3>
<p>Some example queries are shown below. They are available in <code>build/dist/examples/queries</code>.<br>
More are available in the Hive sources at <code>ql/src/test/queries/positive</code>.</p>
<h4 id=selects-and-filters>SELECTS and FILTERS</h4>
<pre tabindex=0><code>  hive&gt; SELECT a.foo FROM invites a WHERE a.ds='2008-08-15';

</code></pre><p>selects column &lsquo;foo&rsquo; from all rows of partition <code>ds=2008-08-15</code> of the <code>invites</code> table. The results are not stored anywhere, but are displayed on the console.</p>
<p>Note that in all the examples that follow, <code>INSERT</code> (into a Hive table, local directory or HDFS directory) is optional.</p>
<pre tabindex=0><code>  hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15';

</code></pre><p>selects all rows from partition <code>ds=2008-08-15</code> of the <code>invites</code> table into an HDFS directory. The result data is in files (depending on the number of mappers) in that directory.<br>
NOTE: partition columns if any are selected by the use of *. They can also be specified in the projection clauses.</p>
<p>Partitioned tables must always have a partition selected in the <code>WHERE</code> clause of the statement.</p>
<pre tabindex=0><code>  hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/local_out' SELECT a.* FROM pokes a;

</code></pre><p>selects all rows from pokes table into a local directory.</p>
<pre tabindex=0><code>  hive&gt; INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a;
  hive&gt; INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a WHERE a.key &lt; 100;
  hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/reg_3' SELECT a.* FROM events a;
  hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_4' select a.invites, a.pokes FROM profiles a;
  hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT COUNT(*) FROM invites a WHERE a.ds='2008-08-15';
  hive&gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT a.foo, a.bar FROM invites a;
  hive&gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/sum' SELECT SUM(a.pc) FROM pc1 a;

</code></pre><p>selects the sum of a column. The avg, min, or max can also be used. Note that for versions of Hive which don&rsquo;t include <a href=https://issues.apache.org/jira/browse/HIVE-287>HIVE-287</a>, you&rsquo;ll need to use <code>COUNT(1)</code> in place of <code>COUNT(*)</code>.</p>
<h4 id=group-by>GROUP BY</h4>
<pre tabindex=0><code>  hive&gt; FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo &gt; 0 GROUP BY a.bar;
  hive&gt; INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo &gt; 0 GROUP BY a.bar;

</code></pre><p>Note that for versions of Hive which don&rsquo;t include <a href=https://issues.apache.org/jira/browse/HIVE-287>HIVE-287</a>, you&rsquo;ll need to use <code>COUNT(1)</code> in place of <code>COUNT(*)</code>.</p>
<h4 id=join>JOIN</h4>
<pre tabindex=0><code>  hive&gt; FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo;

</code></pre><h4 id=multitable-insert>MULTITABLE INSERT</h4>
<pre tabindex=0><code>  FROM src
  INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key &lt; 100
  INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key &gt;= 100 and src.key &lt; 200
  INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key &gt;= 200 and src.key &lt; 300
  INSERT OVERWRITE LOCAL DIRECTORY '/tmp/dest4.out' SELECT src.value WHERE src.key &gt;= 300;

</code></pre><h4 id=streaming>STREAMING</h4>
<pre tabindex=0><code>  hive&gt; FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds &gt; '2008-08-09';

</code></pre><p>This streams the data in the map phase through the script <code>/bin/cat</code> (like Hadoop streaming).<br>
Similarly – streaming can be used on the reduce side (please see the <a href=https://cwiki.apache.org/confluence/display/Hive/Tutorial#Tutorial-Custommap%2Freducescripts>Hive Tutorial</a> for examples).</p>
<h2 id=simple-example-use-cases>Simple Example Use Cases</h2>
<h3 id=movielens-user-ratings>MovieLens User Ratings</h3>
<p>First, create a table with tab-delimited text file format:</p>
<pre tabindex=0><code>CREATE TABLE u_data (
  userid INT,
  movieid INT,
  rating INT,
  unixtime STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;

</code></pre><p>Then, download the data files from <strong>MovieLens 100k</strong> on the <a href=http://grouplens.org/datasets/movielens/>GroupLens datasets</a> page (which also has a README.txt file and index of unzipped files):</p>
<pre tabindex=0><code>wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
</code></pre><p>or:</p>
<pre tabindex=0><code>curl --remote-name http://files.grouplens.org/datasets/movielens/ml-100k.zip
</code></pre><p>Note:  If the link to <a href=http://grouplens.org/datasets/movielens/>GroupLens datasets</a> does not work, please report it on <a href=https://issues.apache.org/jira/browse/HIVE-5341>HIVE-5341</a> or send a message to the <a href=http://hive.apache.org/mailing_lists.html>user@hive.apache.org mailing list</a>.</p>
<p>Unzip the data files:</p>
<pre tabindex=0><code>unzip ml-100k.zip
</code></pre><p>And load <code>u.data</code> into the table that was just created:</p>
<pre tabindex=0><code>LOAD DATA LOCAL INPATH '&lt;path&gt;/u.data'
OVERWRITE INTO TABLE u_data;

</code></pre><p>Count the number of rows in table u_data:</p>
<pre tabindex=0><code>SELECT COUNT(*) FROM u_data;

</code></pre><p>Note that for older versions of Hive which don&rsquo;t include <a href=https://issues.apache.org/jira/browse/HIVE-287>HIVE-287</a>, you&rsquo;ll need to use COUNT(1) in place of COUNT(*).</p>
<p>Now we can do some complex data analysis on the table <code>u_data</code>:</p>
<p>Create <code>weekday_mapper.py</code>:</p>
<pre tabindex=0><code>import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  userid, movieid, rating, unixtime = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([userid, movieid, rating, str(weekday)])

</code></pre><p>Use the mapper script:</p>
<pre tabindex=0><code>CREATE TABLE u_data_new (
  userid INT,
  movieid INT,
  rating INT,
  weekday INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add FILE weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (userid, movieid, rating, unixtime)
  USING 'python weekday_mapper.py'
  AS (userid, movieid, rating, weekday)
FROM u_data;

SELECT weekday, COUNT(*)
FROM u_data_new
GROUP BY weekday;

</code></pre><p>Note that if you&rsquo;re using Hive 0.5.0 or earlier you will need to use <code>COUNT(1)</code> in place of <code>COUNT(*)</code>.</p>
<h3 id=apache-weblog-data>Apache Weblog Data</h3>
<p>The format of Apache weblog is customizable, while most webmasters use the default.<br>
For default Apache weblog, we can create a table with the following command.</p>
<p>More about RegexSerDe can be found here in <a href=https://issues.apache.org/jira/browse/HIVE-662>HIVE-662</a> and <a href=https://issues.apache.org/jira/browse/HIVE-1719>HIVE-1719</a>.</p>
<pre tabindex=0><code>CREATE TABLE apachelog (
  host STRING,
  identity STRING,
  user STRING,
  time STRING,
  request STRING,
  status STRING,
  size STRING,
  referer STRING,
  agent STRING)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'
WITH SERDEPROPERTIES (
  &quot;input.regex&quot; = &quot;([^]*) ([^]*) ([^]*) (-|\\[^\\]*\\]) ([^ \&quot;]*|\&quot;[^\&quot;]*\&quot;) (-|[0-9]*) (-|[0-9]*)(?: ([^ \&quot;]*|\&quot;.*\&quot;) ([^ \&quot;]*|\&quot;.*\&quot;))?&quot;
)
STORED AS TEXTFILE;

</code></pre>
</div>
</div>
<footer class="black-background static-bottom" style=padding:30px>
<div class=row>
<div class=col-3>
<a href=https://www.apache.org/>
<img src=https://hive.apache.org/images/asf_logo.png width=270 height=100 alt="Apache Software Foundation"></a>
</a>
</div>
<div class=col-9>
<p class=footer-text>Apache is a non-profit organization helping open-source
software projects released under the Apache
<a href=https://www.apache.org/licenses/>license</a>
and managed with
<a href=https://www.apache.org/foundation/how-it-works.html>
open governance</a> and
<a href=https://privacy.apache.org/policies/privacy-policy-public.html>
privacy policy</a>. See upcoming
<a href=https://www.apache.org/events/current-event>Apache Events</a>.
If you discover any
<a href=https://www.apache.org/security/>security</a> vulnerabilities, please
report them privately. Finally,
<a href=https://www.apache.org/foundation/sponsorship.html>thanks
</a> to the sponsors who
<a href=https://www.apache.org/foundation/contributing.html>
donate</a> to the Apache Foundation.
</p>
</div>
</div>
<div class="copyright row">
<a href=https://hive.apache.org style=color:grey>
The contents of this website are © 2023 Apache Software Foundation under the terms of the Apache License v2. Apache Hive and its logo are trademarks of the Apache Software Foundation.
</a>
</div>
</footer>
<script src=https://hive.apache.org/js/bootstrap.bundle.min.js></script>
</body>
</html>