[{"categories":null,"contents":"Query File Test(qtest) Query File Test is a JUnit-based integration test suite for Apache Hive. Developers write any SQL; the testing framework runs it and verifies the result and output.\n Tutorial: How to run a specific test case  Preparation Run a test case   Tutorial: How to add a new test case  Add a QFile Generate a result file Verify the new result file   Commandline options  Test options Test Iceberg, Accumulo, or Kudu   QTestOptionHandler: pre/post-processor  Using test data Mask non-deterministic outputs   Advanced  Locations of log files Negative tests How to specify drivers How to use PostgreSQL/MySQL/Oracle as a backend database for Hive Metastore Remote debug   Tips for Adding New Tests in Hive    Tutorial: How to run a specific test case Preparation You have to compile Hive\u0026rsquo;s source codes ahead of time.\n$ mvn clean install -Dmaven.javadoc.skip=true -DskipTests -Pitests Run a test case Let\u0026rsquo;s try to run alter1.q.\n$ mvn test -Pitests -pl itests/qtest -Dtest=TestMiniLlapLocalCliDriver -Dqfile=alter1.q The test should successfully finish.\n[INFO] Results: [INFO] [INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 Tutorial: How to add a new test case Add a QFile A QFile includes a set of SQL statements that you want to test. Typically, we should put a new file in ql/src/test/queries/clientpositive.\nLet\u0026rsquo;s say you created the following file.\n$ cat ql/src/test/queries/clientpositive/aaa.q SELECT 1; Generate a result file You can generate the expected output with -Dtest.output.overwrite=true.\n$ mvn test -Pitests -pl itests/qtest -Dtest=TestMiniLlapLocalCliDriver -Dtest.output.overwrite=true -Dqfile=aaa.q ... $ cat ql/src/test/results/clientpositive/llap/aaa.q.out PREHOOK: query: SELECT 1 PREHOOK: type: QUERY PREHOOK: Input: _dummy_database@_dummy_table #### A masked pattern was here #### POSTHOOK: query: SELECT 1 POSTHOOK: type: QUERY POSTHOOK: Input: _dummy_database@_dummy_table #### A masked pattern was here #### 1 Verify the new result file You can ensure the generated result file is correct by rerunning the test case without -Dtest.output.overwrite=true.\n$ mvn test -Pitests -pl itests/qtest -Dtest=TestMiniLlapLocalCliDriver -Dqfile=aaa.q Commandline options Test options    Option Description Example     test The class name of the test driver -Dtest=TestMiniLlapLocalCliDriver   qfile The name(s) of Query Files -Dqfile=alter1.q, -Dqfile=alter1.q,alter2.q   qfile_regex The pattern to list Query Files -Dqfile_regex=alter[0-9]   test.output.overwrite Whether you want to (re)generate result files or not -Dtest.output.overwrite=true   test.metastore.db Which RDBMS to be used as a metastore backend See How to use PostgreSQL/MySQL/Oracle as a backend database for Hive Metastore    Test Iceberg, Accumulo, or Kudu Most test drivers are available in the itest/qtest project. However, there are some exceptional ones.\n   Driver Project     TestAccumuloCliDriver itest/qtest-accumulo   TestIcebergCliDriver itest/qtest-iceberg   TestIcebergLlapLocalCliDriver itest/qtest-iceberg   TestIcebergLlapLocalCompactorCliDriver itest/qtest-iceberg   TestIcebergNegativeCliDriver itest/qtest-iceberg   TestKuduCliDriver itest/qtest-kudu   TestKuduNegativeCliDriver itest/qtest-kudu    When you use TestIcebergLlapLocalCliDriver, you have to specify -pl itest/qtest-iceberg.\n$ mvn test -Pitests -pl itests/qtest-iceberg -Dtest=TestIcebergLlapLocalCliDriver -Dqfile_regex=iceberg_bucket_map_join_8 QTestOptionHandler: pre/post-processor We extend JUnit by implementing QTestOptionHandlers, which are custom pre-processors and post-processors. This section explains a couple of typical processors.\nUsing test data Adding --! qt:dataset:{table name}, QTestDatasetHandler automatically sets up a test table. You can find the table definitions here.\n--! qt:dataset:src SELECT * FROM src; Mask non-deterministic outputs Some test cases generate random results. QTestReplaceHandler masks such a non-deterministic part. You can use it with a special comment prefixed with --! qt:replace:.\nFor example, the result of CURRENT_DATE changes every day. Using the comment, the output will be non-deterministic-output #Masked#, which is stable across executions.\n--! qt:replace:/(non-deterministic-output\\s)[0-9]{4}-[0-9]{2}-[0-9]{2}/$1#Masked#/ SELECT \u0026#39;non-deterministic-output\u0026#39;, CURRENT_DATE(); Advanced Locations of log files The Query File Test framework outputs log files in the following paths.\n itests/{qtest, qtest-accumulo, qtest-iceberg, qtest-kudu}/target/surefire-reports From the root of the source tree: find . -name hive.log  Negative tests Negative drivers allow us to make sure that a test case fails expectedly. For example, the query in strict_timestamp_to_numeric.q must fail based on Hive’s specifications. We can use TestNegativeLlapLocalCliDriver, TestIcebergNegativeCliDriver, and so on.\n$ mvn -Pitests -pl itests/qtest test -Dtest=TestNegativeLlapLocalCliDriver -Dqfile=strict_timestamp_to_numeric.q How to specify drivers We define the default mapping of Query Files and test drivers using testconfiguration.properties and CliConfigs. For example, TestMiniLlapLocalCliDriver is the default driver for query files stored in ql/src/test/queries/clientpositive. The hive-precommit Jenkins job also follows the mapping.\nYou can override the mapping through testconfiguration.properties. For example, if you want to test ql/src/test/queries/clientpositive/aaa.q not by LLAP but by Tez, you must include the file name in minitez.query.files and generate the result file with -Dtest=TestMiniTezCliDriver.\nIn most cases, we should use TestMiniLlapLocalCliDriver for positive tests and TestNegativeLlapLocalCliDriver for negative tests.\nHow to use PostgreSQL/MySQL/Oracle as a backend database for Hive Metastore To run a test with a specified DB, it is possible by adding the \u0026ldquo;-Dtest.metastore.db\u0026rdquo; parameter like in the following commands:\n$ mvn test -Pitests -pl itests/qtest -Dtest=TestCliDriver -Dqfile=partition_params_postgres.q -Dtest.metastore.db=postgres $ mvn test -Pitests -pl itests/qtest -Dtest=TestCliDriver -Dqfile=partition_params_postgres.q -Dtest.metastore.db=mssql $ mvn test -Pitests -pl itests/qtest -Dtest=TestCliDriver -Dqfile=partition_params_postgres.q -Dtest.metastore.db=mysql $ mvn test -Pitests -pl itests/qtest -Dtest=TestCliDriver -Dqfile=partition_params_postgres.q -Dtest.metastore.db=oracle -Ditest.jdbc.jars=/path/to/your/god/damn/oracle/jdbc/driver/ojdbc6.jar Remote debug Remote debugging with Query File Test is a potent tool for debugging Hive. With the following command, Query File Test listens to port 5005 and waits for a debugger to be attached.\n$ mvn -Pitests -pl itests/qtest -Dmaven.surefire.debug test -Dtest=TestMiniLlapLocalCliDriver -Dqfile=alter1.q Tips for Adding New Tests in Hive Following are a few rules of thumb that should be followed when adding new test cases in Hive that require the introduction of new query file(s). Of course, these rules should not be applied if they invalidate the purpose of your test to begin with. These are generally helpful in keeping the test queries concise, minimizing the redundancies where possible, and ensuring that cascading failures due to a single test failure do not occur.\n Instead of creating your own data file for loading into a new table, use existing data from staged tables like src. If your test requires a SELECT query, keep it as simple as possible, and minimize the number of queries to keep overall test time down; avoid repeating scenarios which are already covered by existing tests. When you do need to use a SELECT statement, make sure you use the ORDER BY clause to minimize the chances of spurious diffs due to output order differences leading to test failures. Limit your test to one table unless you require multiple tables specifically. Make sure that you name your query file appropriately with a descriptive name.  ","date":"28","image":null,"permalink":"https://hive.apache.org/development/qtest/","tags":null,"title":"Query File Test(qtest)"},{"categories":null,"contents":"Apache Hive : AccessServer Design Proposal AccessServer Proposal Author: Carl Steinbach Overview The technical approach described in the this document addresses the following high-level requirements:\n Make Apache Hive’s data model and metadata services accessible to users of the Apache Pig dataflow programming language as well as other Hadoop language runtimes. Make it possible for Hive users and users of other Hadoop language runtimes to share data stored in Hive’s HDFS data warehouse. Accomplish requirements (1) and (2) while also enforcing Hive’s fine-grained authorization model. Accomplish (1) and (2) while also ensuring the accuracy and consistency of secondary metadata artifacts such as indexes and statistics.  In order to satisfy these requirements we propose extending HiveServer2 in order to make it capable of hosting the Pig runtime execution engine in addition to the Hive runtime execution engine. Note that requirements (3) and (4) assume completion of the parallel project to implement consistent Hive authorization.\nDoes HCatalog address these requirements? HCatalog “is a table and storage management layer for Hadoop that enables users with different data processing tools - Pig, MapReduce, and Hive – to more easily read and write data on the grid \u0026hellip; HCatalog is built on top of the Hive metastore and incorporates Hive’s DDL.” “HCatalog provides read and write interfaces for Pig and MapReduce” built on top of Hive’s SerDes “and uses Hive’s command line interface for issuing data definition and metadata export commands.” This can also be summarized by saying that HCatalog is a set of wrapper APIs designed to make Hive’s MetaStore service and SerDe format handlers accessible to Pig and MapReduce. The MetaStore API wrappers and the SerDe wrappers are useful in making Hive metadata available to Pig and MapReduce programs.\nWhere does HCatalog in its current incarnation fall short?\nAuthorization in HCatalog HCatalog has a pluggable authorization system, with the only currently implemented plugin assuming a storage-based authorization model that uses the permissions of the underlying file system (HDFS) as the basis for determining the read and write permissions of each database, table, or partition. For example, in order to determine if a user has permission to read data from a table, HCatalog checks to see if the user has permission to read the corresponding subdirectory of Hive’s warehouse directory. With this authorization model, the read/write permissions that a user or group has for a particular database, table, or partition are determined by the filesystem permissions on the corresponding subdirectory in Hive’s warehouse.\nColumn-level Access Controls The file system approach cannot support column-level access controls. This is a consequence of the fact that Hive’s data model makes it possible to store table data in flat files where each file may contain multiple columns. In order for HCatalog to support column-level ACLs it will need to defer to Hive for authorization decisions, i.e. it will need Hive authorization.\nAccessServer will support a deployment choice of using Hive authorization or using file system authorization (because some sites prefer it). File system authorization will happen through DoAs proxying.\nIntegrity of Indexes and Statistics Another problem with file system authorization is that it makes it difficult to ensure the consistency and integrity of Hive’s indexes and statistics. HCatalog’s file system authorization model allows users to directly access and manipulate the contents of the warehouse directory without going through Hive or HCatalog. Adding, removing, or altering a file in any of these directories means that any indexes or statistics that were previously computed based on the contents of that directory now need to be invalidated and regenerated. There is no mechanism for notifying Hive that changes have been made, and Hive is precluded from determining this on its own due to the bedrock design principle that it does not track individual files in the warehouse subdirectories.\nData Model Impedance Mismatch Hive has a powerful data model that allows users to map logical tables and partitions onto physical directories located on HDFS file systems. As was mentioned earlier, one of the bedrock design principles of this data model is that Hive does not track the individual files that are located in these directories, and instead delegates this task to the HDFS NameNode. The primary motivation for this restriction is that it allows the Metastore to scale by reducing the FS metadata load. However, problems arise when we try to reconcile this data model with an authorization model that depends on the underlying file system permissions, and which consequently can\u0026rsquo;t ignore the permissions applied to individual files located in those directories.\nHCatalog\u0026rsquo;s Storage Based Authorization model is explained in more detail in the HCatalog documentation, but the following set of quotes provides a good high-level overview:\n \u0026hellip; when a file system is used for storage, there is a directory corresponding to a database or a table. With this authorization model, the read/write permissions a user or group has for this directory determine the permissions a user has on the database or table.\n\u0026hellip;\nFor example, an alter table operation would check if the user has permissions on the table directory before allowing the operation, even if it might not change anything on the file system.\n\u0026hellip;\nWhen the database or table is backed by a file system that has a Unix/POSIX-style permissions model (like HDFS), there are read(r) and write(w) permissions you can set for the owner user, group and ‘other’. The file system’s logic for determining if a user has permission on the directory or file will be used by Hive.\n There are several problems with this approach, the first of which is actually hinted at by the inconsistency highlighted in the preceding quote. To determine whether a particular user has read permission on table foo, HCatalog\u0026rsquo;s HdfsAuthorizationProvider class checks to see if the user has read permission on the corresponding HDFS directory /hive/warehouse/foo that contains the table\u0026rsquo;s data. However, in HDFS having read permission on a directory only implies that you have the ability to list the contents of the directory – it doesn\u0026rsquo;t have any affect on your ability to read the files contained in the directory.\nExecution container HCatalog includes a subproject Templeton which exposes two sets of REST APIs: a set to access Hive metadata and a set to launch and manage MapReduce jobs. A metadata REST API is something we want for AccessServer. HCatalog is not the right place for job management. Templeton has copied the Oozie code for job submission and management. We think users should use Oozie\u0026rsquo;s REST APIs to submit jobs to Oozie. The HCatalog plan was to implement JDBC and ODBC on top of the Templeton job control REST API. That would be significant work (while we already have JDBC and ODBC for HiveServer2 that can be used for Pig as well) and would not allow for interactive JDBC or ODBC usage since Templeton executes each instruction as an Oozie job.\nTechnical Approach We will modify HiveServer2 in order to make it capable of supporting language runtimes other than HQL, in effect converting HiveServer2 into an application server for pluggable modules with the immediate goal of supporting Pig. The end result of these efforts will be called AccessServer.\nBefore discussing these modifications it is important to first understand the basic design of HiveServer2.\nHiveServer2 Design Overview The following diagram is a block-level representation of the major submodules in HiveServer2 with horizontal boundaries signifying dependencies. Green modules existed in Hive before the HiveServer2 project commenced, while blue modules were implemented as part of the HiveServer2 project.\nThe core of HiveServer2 is the HiveSession class. This class provides a container for user session state and also manages the lifecycle of operations triggered by the user. In this context an operation is any command exposed through the CLIService API that can generate a result set. This includes the ExecuteStatement() operation and metadata operations such as GetTables() and GetSchemas(). Each of these operations is implemented by a specific Operation subclass. In order to execute Hive queries the ExecuteStatementOperation makes use of the pre-existing HiveDriver class. HiveDriver encapsulates Hive’s compiler and plan execution engine, and in most respects is very similar to Pig’s PigServer class.\nAccessServer Design The following diagram gives a quick overview of the changes required to support the Pig runtime engine in AccessServer. For simplicity we have removed Hive-specific components from the diagram such as the HiveOperation and HiveSession classes.\nIn the diagram blue denotes existing components in HiveServer2 that do not require modification. This includes the Thrift interface, JDBC/ODBC drivers, CLIService, and the Metastore.\nOrange denotes existing HiveServer2 components that must be modified. We need to modify the SessionManager in order to support pluggable session factories. Note that the CLIService API already allows clients to request a specific Session type via a set of configuration parameters that are part of the OpenSession call.\nYellow denotes new components that must be fashioned from scratch: the PigSession class and the set of Pig Operation classes. The following implementation details relevant to these classes are worth noting:\n We will need to provide Pig-specific implementations of the metadata operations defined in the CLIService API, e.g. GetTables, GetSchemas, GetTypeInfo, etc. In some cases we will be able to reuse the Hive version of these operations without modification (e.g. GetSchemas). Other metadata operations such as GetTables can be based on the corresponding Hive versions, but must be modified in order to filter out catalog objects such as indexes and views that Pig does not support. The Pig version of the ExecuteStatementOperation will likely require the most effort to implement. This class will function as an adaptor between the AccessServer Session API and an instance of the PigServer class.  Finally, red is used in the preceding diagram to highlight HCatalog components we plan to use: the HCatStorer and HCatLoader modules, and the REST API. These classes function as an adaptor layer that makes Hive’s metadata and SerDes accessible to Pig.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/accessserver-design-proposal_31823045/","tags":null,"title":"Apache Hive : AccessServer Design Proposal"},{"categories":null,"contents":"Apache Hive : AccumuloIntegration Hive Accumulo Integration  Hive Accumulo Integration  Overview Implementation Accumulo Configuration Usage Column Mapping Indexing Other options Examples  Override the Accumulo table name Store a Hive map with binary serialization Register an external table Create an indexed table   Acknowledgements    Overview Apache Accumulo is a sorted, distributed key-value store based on the Google BigTable paper. The API methods that Accumulo provides are in terms of Keys and Values which present the highest level of flexibility in reading and writing data; however, higher-level query abstractions are typically an exercise left to the user. Leveraging Apache Hive as a SQL interface to Accumulo complements its existing high-throughput batch access and low-latency random lookups.\nImplementation The initial implementation was added to Hive 0.14 in HIVE-7068 and is designed to work with Accumulo 1.6.x. There are two main components which make up the implementation: the AccumuloStorageHandler and the AccumuloPredicateHandler. The AccumuloStorageHandler is a StorageHandler implementation. The primary roles of this class are to manage the mapping of Hive table to Accumulo table and configures Hive queries. The AccumuloPredicateHandler is used push down filter operations to the Accumulo for more efficient reduction of data.\nAccumulo Configuration The only additional Accumulo configuration necessary is the inclusion of the hive-accumulo-handler.jar, provided as a part of the Hive distribution, to be included in the Accumulo server classpath. This can be accomplished a variety of ways: copying/symlink the jar into $ACCUMULO_HOME/lib or $ACCUMULO_HOME/lib/ext or include the path to the jar in general.classpaths in accumulo-site.xml. Be sure to restart the Accumulo tabletservers if the jar is added to the classpath in a non-dynamic fashion (using $ACCUMULO_HOME/lib or general.classpaths in accumulo-site.xml).\nUsage To issue queries against Accumulo using Hive, four parameters must be provided by the Hive configuration:\n    Connection Parameters     accumulo.instance.name   accumulo.zookeepers   accumulo.user.name   accumulo.user.pass     For those familiar with Accumulo, these four configurations are the normal configuration values necessary to connect to Accumulo: the Accumulo instance name, the ZooKeeper quorum (comma-separated list of hosts), and Accumulo username and password. The easiest way to provide these values is by using the -hiveconf option to the hive command. It is expected that the Accumulo user provided either has the ability to create new tables, or that the Hive queries will only be accessing existing Accumulo tables.\nhive -hiveconf accumulo.instance.name=accumulo -hiveconf accumulo.zookeepers=localhost -hiveconf accumulo.user.name=hive -hiveconf accumulo.user.pass=hive To access Accumulo tables, a Hive table must be created using the CREATE command with the STORED BY clause. If the EXTERNAL keyword is omitted from the CREATE call, the lifecycle of the Accumulo table is tied to the lifetime of the Hive table: if the Hive table is deleted, so is the Accumulo table. This is the default case. Providing the EXTERNAL keyword will create a Hive table that references an Accumulo table but will not remove the underlying Accumulo table if the Hive table is dropped.\nEach Hive row maps to a set of Accumulo keys with the same row ID. One column in the Hive row is designated as a \u0026ldquo;special\u0026rdquo; column which is used as the Accumulo row ID. All other Hive columns in the row have some mapping to Accumulo column (column family and qualifier) where the Hive column value is placed in the Accumulo value.\nCREATE TABLE accumulo_table(rowid STRING, name STRING, age INT, weight DOUBLE, height INT) STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler' WITH SERDEPROPERTIES('accumulo.columns.mapping' = ':rowid,person:name,person:age,person:weight,person:height'); In the above statement, normal Hive column name and type pairs are provided as is the case with normal create table statements. The full AccumuloStorageHandler class name is provided to inform Hive that Accumulo will back this Hive table. A number of properties can be provided to configure the AccumuloStorageHandler via SERDEPROPERTIES or TBLPROPERTIES. The most important property is \u0026ldquo;accumulo.columns.mapping\u0026rdquo; which controls how the Hive columns map to Accumulo columns. In this case, the \u0026ldquo;row\u0026rdquo; Hive column is used to populate the Accumulo row ID component of the Accumulo Key, while the other Hive columns (name, age, weight and height) are all columns within the Accumulo row.\nFor the above schema in the \u0026ldquo;accumulo_table\u0026rdquo;, we could envision a single row in the table:\nhive\u0026gt; select * from accumulo_table; row1\tSteve\t32\t200\t72 The above record would be serialized into Accumulo Key-Value pairs in the following manner given the declared accumulo.columns.mapping:\nuser@accumulo accumulo_table\u0026gt; scan row1\tperson:age []\t32 row1\tperson:height []\t72 row1\tperson:name []\tSteve row1\tperson:weight []\t200 The power of the column mapping is that multiple Hive tables with differing column mappings can interact with the same Accumulo table and produce different results. When columns are excluded, the performance of Hive queries can be improved through the use of Accumulo locality groups to filter out unwanted data at the server-side.\nColumn Mapping The column mapping string is comma-separated list of encoded values whose offset corresponds to the Hive schema for the table. The order of the columns in the Hive schema can be arbitrary as long as the elements in the column mapping align to the intended Hive column. For those familiar with Accumulo, each element in the column mapping string resembles a column_family:column_qualifier; however, there are a few different variants that allow for different control.\n A single column  This places the value for the Hive column into the Accumulo value with the given column family and column qualifier.   A column qualifier map  A column family is provided and a column qualifier prefix of any length is allowed, follow by an asterisk. The Hive column type is expected to be a Map, the key of the Hive map is appended to the column qualifier prefix The value of the Hive map is placed in the Accumulo value.   The rowid  Controls which Hive column is used as the Accumulo rowid. Exactly one \u0026ldquo;:rowid\u0026rdquo; element must exist in each column mapping \u0026ldquo;:rowid\u0026rdquo; is case insensitive (:rowID is equivalent to :rowId)    Additionally, a serialization option can be provided to each element in the column mapping which will control how the value is serialized. Currently, the options are:\n \u0026lsquo;binary\u0026rsquo; or \u0026lsquo;b\u0026rsquo; \u0026lsquo;string\u0026rsquo; or \u0026rsquo;s'  These are set by including a pound sign ('#') after the column mapping element with either the long or short serialization value. The default serialization is \u0026lsquo;string\u0026rsquo;. For example, for the value 10, \u0026ldquo;person:age#s\u0026rdquo; is synonymous with the \u0026ldquo;person:age\u0026rdquo; and would serialize the value as the literal string \u0026ldquo;10\u0026rdquo;. If \u0026ldquo;person:age#b\u0026rdquo; was used instead, the value would be serialized as four bytes: \\x00\\x00\\x00\\xA0.\nIndexing Starting in Hive 3.0.0 with HIVE-15795, indexing support has been added to Accumulo-backed Hive tables. Indexing works by using another Accumulo table to store the field value mapping to rowId of the data table. The index table is automatically populated on record insertion via Hive.\nUsing index tables greatly improve performance of non-rowId predicate queries by eliminating full table scans. Indexing works for both internally and externally managed tables using either the Tez or Map Reduce query engines. The following options control indexing behavior.\n   Option Name Description     accumulo.indextable.name (Required) The name of the index table in Accumulo.   accumulo.indexed.columns (Optional) A comma separated list of hive columns to index, or * which indexes all columns (default: *)   accumulo.index.rows.max (Optional) The maximum number of predicate values to scan from the index for each search predicate (default: 20000) See this note about this value   accumulo.index.scanner (Optional) The index scanner implementation. (default: org.apache.hadoop.hive.accumulo.AccumuloDefaultIndexScanner)    The indexes are stored in the index table using the following format:\nrowId = [field value in data table]\ncolumn_family = [field column family in data table] + ‘_’ + [field column quantifier in data table]\ncolumn_quantifier = [field rowId in data table]\nvisibility = [field visibility in data table]\nvalue = [empty byte array]\n When using a string encoded table, the indexed field value is encoded using Accumulo Lexicoder methods for numeric types. Otherwise, values are encoding using native binary encoding. This information will allow applications to insert data and index values into Accumulo outside of Hive but still require high performance queries from within Hive.\nIt is important to note when inserting data and indexes outside of Hive it is important to update both tables within the same unit of work. If the Hive query does not find indexes matches for the any of the query predicates, the query will short circuit and return empty results without searching the data table.\nIf the search predicate matches more entries than defined by the option accumulo.index.rows.max (default 20000), the index search results will be abandoned and the query will fall back to a full scan of the data table with predicate filtering. Remember using large values for this option or having very large data table rowId values may require increasing hive memory to prevent memory errors.\nOther options The following options are also valid to be used with SERDEPROPERTIES or TABLEPROPERTIES for further control over the actions of the AccumuloStorageHandler:\n    Option Name Description     accumulo.iterator.pushdown Should filter predicates be satisfied within Accumulo using Iterators (default: true)   accumulo.default.storage The default storage serialization method for values (default: string)   accumulo.visibility.label A static ColumnVisibility string to use when writing any records to Accumulo (default: empty string)   accumulo.authorizations A comma-separated list of authorizations to use when scanning Accumulo (default: no authorizations).Note that the Accumulo user provided to connect to Accumulo must have all authorizations provided.   accumulo.composite.rowid.factory Extension point which allows a custom class to be provided when constructing LazyObjects from the rowid without changingthe ObjectInspector for the rowid column.   accumulo.composite.rowid Extension point which allows for custom parsing of the rowid column into a LazyObject.   accumulo.table.name Controls what Accumulo table name is used (default: the Hive table name)   accumulo.mock.instance Use a MockAccumulo instance instead of connecting to a real instance (default: false). Useful for testing.    Examples Override the Accumulo table name Create a user table, consisting of some unique key for a user, a user ID, and a username. The Accumulo row ID is from the Hive column, the user ID column is written to the \u0026ldquo;f\u0026rdquo; column family and \u0026ldquo;userid\u0026rdquo; column qualifier, and the username column to the \u0026ldquo;f\u0026rdquo; column family and the \u0026ldquo;nickname\u0026rdquo; column qualifier. Instead of using the \u0026ldquo;users\u0026rdquo; Accumulo table, it is overridden in the TBLPROPERTIES to use the Accumulo table \u0026ldquo;hive_users\u0026rdquo; instead.\nCREATE TABLE users(key int, userid int, username string) STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler' WITH SERDEPROPERTIES (\u0026quot;accumulo.columns.mapping\u0026quot; = \u0026quot;:rowID,f:userid,f:nickname\u0026quot;) WITH TBLPROPERTIES (\u0026quot;accumulo.table.name\u0026quot; = \u0026quot;hive_users\u0026quot;); Store a Hive map with binary serialization Using an asterisk in the column mapping string, a Hive map can be expanded from a single Accumulo Key-Value pair to multiple Key-Value pairs. The Hive Map is a parameterized type: in the below case, the key is a string, and the value integer. The default serialization is overriden from \u0026lsquo;string\u0026rsquo; to \u0026lsquo;binary\u0026rsquo; which means that the integers in the value of the Hive map will be stored as a series of bytes instead of the UTF-8 string representation.\nCREATE TABLE hive_map(key int, value map\u0026lt;string,int\u0026gt;) STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;accumulo.columns.mapping\u0026quot; = \u0026quot;:rowID,cf:*\u0026quot;, \u0026quot;accumulo.default.storage\u0026quot; = \u0026quot;binary\u0026quot; ); Register an external table Creating the Hive table with the external keyword decouples the lifecycle of the Accumulo table from that of the Hive table. Creating this table assumes that the Accumulo table \u0026ldquo;countries\u0026rdquo; already exists. This is a very useful way to use Hive to manage tables that are created and populated by some external tool (e.g. A MapReduce job). When the Hive table countries is deleted, the Accumulo table will not be deleted. Additionally, the external keyword can also be useful when creating multiple Hive tables with different options that operate on the same underlying Accumulo table.\nCREATE EXTERNAL TABLE countries(key string, name string, country string, country_id int) STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler' WITH SERDEPROPERTIES (\u0026quot;accumulo.columns.mapping\u0026quot; = \u0026quot;:rowID,info:name,info:country,info:country_id\u0026quot;); Create an indexed table To take advantage of indexing, Hive uses another Accumulo table is used to create a lexicographically-sorted search term index for each field allowing for very efficient exact match and bounded range searches.\nCREATE TABLE company_stats ( rowid string, active_entry boolean, num_offices tinyint, num_personel smallint, total_manhours int, num_shareholders bigint, eff_rating float, err_rating double, yearly_production decimal, start_date date, address varchar(100), phone char(13), last_update timestamp ) ROW FORMAT SERDE 'org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe' STORED BY 'org.apache.hadoop.hive.accumulo.AccumuloStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;accumulo.columns.mapping\u0026quot; = \u0026quot;:rowID,a:act,a:off,a:per,a:mhs,a:shs,a:eff,a:err,a:yp,a:sd,a:addr,a:ph,a:lu”, \u0026quot;accumulo.table.name\u0026quot;=\u0026quot;company_stats\u0026quot;, \u0026quot;accumulo.indextable.name\u0026quot;=\u0026quot;company_stats_idx\u0026quot; ); Acknowledgements I would be remiss to not mention the efforts made by Brian Femiano that were the basis for this storage handler. His initial prototype for Accumulo-Hive integration was the base for this work.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/accumulointegration_46633569/","tags":null,"title":"Apache Hive : AccumuloIntegration"},{"categories":null,"contents":"Apache Hive : AdminManual Hive Administrator\u0026rsquo;s Manual  Installing Hive Configuring Hive Setting up Metastore Setting up Hive Server (JDBC, ODBC, Thrift, etc) Hive on Amazon Web Services  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/adminmanual_27362071/","tags":null,"title":"Apache Hive : AdminManual"},{"categories":null,"contents":"Apache Hive : AdminManual Configuration  Configuring Hive  hive-site.xml and hive-default.xml.template Temporary Folders Log Files Derby Server Mode Configuration Variables   Removing Hive Metastore Password from Hive Configuration Configuring HCatalog and WebHCat  HCatalog WebHCat    Configuring Hive A number of configuration variables in Hive can be used by the administrator to change the behavior for their installations and user sessions. These variables can be configured in any of the following ways, shown in the order of preference:\n Using the set command in the CLI or Beeline for setting session level values for the configuration variable for all statements subsequent to the set command. For example, the following command sets the scratch directory (which is used by Hive to store temporary output and plans) to /tmp/mydir for all subsequent statements:   set hive.exec.scratchdir=/tmp/mydir;  Using the --hiveconf option of the [hive](#hive) command (in the CLI) or [beeline](#beeline) command for the entire session. For example:   bin/hive --hiveconf hive.exec.scratchdir=/tmp/mydir  In hive-site.xml. This is used for setting values for the entire Hive configuration (see hive-site.xml and hive-default.xml.template below). For example:   \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exec.scratchdir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/tmp/mydir\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Scratch space for Hive jobs\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt;  In server-specific configuration files (supported starting Hive 0.14). You can set metastore-specific configuration values in hivemetastore-site.xml, and HiveServer2-specific configuration values in hiveserver2-site.xml.\nThe server-specific configuration file is useful in two situations:     You want a different configuration for one type of server (for example – enabling authorization only in HiveServer2 and not CLI). You want to set a configuration value only in a server-specific configuration file (for example – setting the metastore database password only in the metastore server configuration file).\nHiveMetastore server reads hive-site.xml as well as hivemetastore-site.xml configuration files that are available in the $HIVE_CONF_DIR or in the classpath. If the metastore is being used in embedded mode (i.e., hive.metastore.uris is not set or empty) in hive commandline or HiveServer2, the hivemetastore-site.xml gets loaded by the parent process as well.\nThe value of hive.metastore.uris is examined to determine this, and the value should be set appropriately in hive-site.xml .\nCertain metastore configuration parameters like hive.metastore.sasl.enabled, hive.metastore.kerberos.principal, hive.metastore.execute.setugi, and hive.metastore.thrift.framed.transport.enabled are used by the metastore client as well as server. For such common parameters it is better to set the values in hive-site.xml, that will help in keeping them consistent.  HiveServer2 reads hive-site.xml as well as hiveserver2-site.xml that are available in the $HIVE_CONF_DIR or in the classpath.\nIf HiveServer2 is using the metastore in embedded mode, hivemetastore-site.xml also is loaded.\nThe order of precedence of the config files is as follows (later one has higher precedence) –\nhive-site.xml -\u0026gt; hivemetastore-site.xml -\u0026gt; hiveserver2-site.xml -\u0026gt; \u0026lsquo;-hiveconf\u0026rsquo; commandline parameters.\n  hive-site.xml and hive-default.xml.template hive-default.xml.template contains the default values for various configuration variables that come prepackaged in a Hive distribution. In order to override any of the values, create hive-site.xml instead and set the value in that file as shown above.\nhive-default.xml.template is located in the conf directory in your installation root, and hive-site.xml should also be created in the same directory.\nPlease note that the template file hive-default.xml.template is not used by Hive at all (as of Hive 0.9.0) – the canonical list of configuration options is only managed in the HiveConf java class. The template file has the formatting needed for hive-site.xml, so you can paste configuration variables from the template file into hive-site.xml and then change their values to the desired configuration.\nIn Hive releases 0.9.0 through 0.13.1, the template file does not necessarily contain all configuration options found in HiveConf.java and some of its values and descriptions might be out of date or out of sync with the actual values and descriptions. However, as of Hive 0.14.0 the template file is generated directly from HiveConf.java and therefore it is a reliable source for configuration variables and their defaults.\nThe administrative configuration variables are listed below. User variables are listed in Hive Configuration Properties. As of Hive 0.14.0 you can display information about a configuration variable with the SHOW CONF command.\nTemporary Folders Hive uses temporary folders both on the machine running the Hive client and the default HDFS instance. These folders are used to store per-query temporary/intermediate data sets and are normally cleaned up by the hive client when the query is finished. However, in cases of abnormal hive client termination, some data may be left behind. The configuration details are as follows:\n On the HDFS cluster this is set to /tmp/hive- by default and is controlled by the configuration variable hive.exec.scratchdir On the client machine, this is hardcoded to /tmp/  Note that when writing data to a table/partition, Hive will first write to a temporary location on the target table\u0026rsquo;s filesystem (using hive.exec.scratchdir as the temporary location) and then move the data to the target table. This applies in all cases - whether tables are stored in HDFS (normal case) or in file systems like S3 or even NFS.\nLog Files Hive client produces logs and history files on the client machine. Please see Hive Logging for configuration details.\nFor WebHCat logs, see Log Files in the WebHCat manual.\nDerby Server Mode Derby is the default database for the Hive metastore (Metadata Store). To run Derby as a network server for multiple users, see Hive Using Derby in Server Mode.\nConfiguration Variables Broadly the configuration variables for Hive administration are categorized into:\n Hive Configuration Variables Hive Metastore Configuration Variables Configuration Variables Used to Interact with Hadoop Hive Variables Used to Pass Run Time Information  Also see Hive Configuration Properties in the Language Manual for non-administrative configuration variables.\nVersion information: Metrics\n A new Hive metrics system based on Codahale is introduced in releases 1.3.0 and 2.0.0 by HIVE-10761. To configure it or revert to the old metrics system, see the Metrics section of Hive Configuration Properties.\nHive Configuration Variables    Variable Name Description Default Value     hive.ddl.output.format The data format to use for DDL output (e.g. DESCRIBE table). One of \u0026ldquo;text\u0026rdquo; (for human readable text) or \u0026ldquo;json\u0026rdquo; (for a json object). (As of Hive 0.9.0.) text   hive.exec.script.wrapper Wrapper around any invocations to script operator e.g. if this is set to python, the script passed to the script operator will be invoked as python \u0026lt;script command\u0026gt;. If the value is null or not set, the script is invoked as \u0026lt;script command\u0026gt;. null   hive.exec.plan   null   hive.exec.scratchdir This directory is used by Hive to store the plans for different map/reduce stages for the query as well as to stored the intermediate outputs of these stages.Hive 0.14.0 and later: HDFS root scratch directory for Hive jobs, which gets created with write all (733) permission. For each connecting user, an HDFS scratch directory ${hive.exec.scratchdir}/is created with ${hive.scratch.dir.permission}. /tmp/\u0026lt;user.name\u0026gt;/hive (Hive 0.8.0 and earlier) /tmp/hive-\u0026lt;user.name\u0026gt; (as of Hive 0.8.1 to 0.14.0)/tmp/hive (Hive 0.14.0 and later)   hive.scratch.dir.permission The permission for the user-specific scratch directories that get created in the root scratch directory ${hive.exec.scratchdir}. (As of Hive 0.12.0.) 700 (Hive 0.12.0 and later)   hive.exec.local.scratchdir This directory is used for temporary files when Hive runs in local mode. (As of Hive 0.10.0.) /tmp/\u0026lt;user.name\u0026gt;   hive.exec.submitviachild Determines whether the map/reduce jobs should be submitted through a separate jvm in the non local mode. false - By default jobs are submitted through the same jvm as the compiler   hive.exec.script.maxerrsize Maximum number of serialization errors allowed in a user script invoked through TRANSFORM or MAP or REDUCE constructs. 100000   hive.exec.compress.output Determines whether the output of the final map/reduce job in a query is compressed or not. false   hive.exec.compress.intermediate Determines whether the output of the intermediate map/reduce jobs in a query is compressed or not. false   hive.resource.use.hdfs.location Reference HDFS based files/jars directly instead of copying to session based HDFS scratch directory. (As of Hive 2.2.1.) true   hive.jar.path The location of hive_cli.jar that is used when submitting jobs in a separate jvm.     hive.aux.jars.path The location of the plugin jars that contain implementations of user defined functions and SerDes.     hive.reloadable.aux.jars.path The location of plugin jars that can be renewed (added, removed, or updated) by executing the Beeline reload command, without having to restart HiveServer2. These jars can be used just like the auxiliary classes in hive.aux.jars.pathfor creating UDFs or SerDes. (As of Hive 0.14.0.)    hive.partition.pruning A strict value for this variable indicates that an error is thrown by the compiler in case no partition predicate is provided on a partitioned table. This is used to protect against a user inadvertently issuing a query against all the partitions of the table. nonstrict   hive.map.aggr Determines whether the map side aggregation is on or not. true   hive.join.emit.interval   1000   hive.map.aggr.hash.percentmemory   (float)0.5   hive.default.fileformat Default file format for CREATE TABLE statement. Options are TextFile, SequenceFile, RCFile, and Orc. TextFile   hive.merge.mapfiles Merge small files at the end of a map-only job. true   hive.merge.mapredfiles Merge small files at the end of a map-reduce job. false   hive.merge.size.per.task Size of merged files at the end of the job. 256000000   hive.merge.smallfiles.avgsize When the average output file size of a job is less than this number, Hive will start an additional map-reduce job to merge the output files into bigger files. This is only done for map-only jobs if hive.merge.mapfiles is true, and for map-reduce jobs if hive.merge.mapredfiles is true. 16000000   hive.querylog.enable.plan.progress Whether to log the plan\u0026rsquo;s progress every time a job\u0026rsquo;s progress is checked. These logs are written to the location specified by hive.querylog.location. (As of Hive 0.10.) true   hive.querylog.location Directory where structured hive query logs are created. One file per session is created in this directory. If this variable set to empty string structured log will not be created. /tmp/\u0026lt;user.name\u0026gt;   hive.querylog.plan.progress.interval The interval to wait between logging the plan\u0026rsquo;s progress in milliseconds. If there is a whole number percentage change in the progress of the mappers or the reducers, the progress is logged regardless of this value. The actual interval will be the ceiling of (this value divided by the value of hive.exec.counters.pull.interval) multiplied by the value of hive.exec.counters.pull.interval i.e. if it is not divide evenly by the value of hive.exec.counters.pull.interval it will be logged less frequently than specified. This only has an effect if hive.querylog.enable.plan.progress is set to true. (As of Hive 0.10.) 60000   hive.stats.autogather A flag to gather statistics automatically during the INSERT OVERWRITE command. (As of Hive 0.7.0.) true   hive.stats.dbclass The default database that stores temporary hive statistics. Valid values are hbase and jdbc while jdbc should have a specification of the Database to use, separated by a colon (e.g. jdbc:mysql). (As of Hive 0.7.0.) jdbc:derby   hive.stats.dbconnectionstring The default connection string for the database that stores temporary hive statistics. (As of Hive 0.7.0.) jdbc:derby:;databaseName=TempStatsStore;create=true   hive.stats.jdbcdriver The JDBC driver for the database that stores temporary hive statistics. (As of Hive 0.7.0.) org.apache.derby.jdbc.EmbeddedDriver   hive.stats.reliable Whether queries will fail because stats cannot be collected completely accurately. If this is set to true, reading/writing from/into a partition may fail becuase the stats could not be computed accurately. (As of Hive 0.10.0.) false   hive.enforce.bucketing If enabled, enforces inserts into bucketed tables to also be bucketed. (Hive 0.6.0 through Hive 1.x.x only) false   hive.variable.substitute Substitutes variables in Hive statements which were previously set using the set command, system variables or environment variables. See HIVE-1096 for details. (As of Hive 0.7.0.) true   hive.variable.substitute.depth The maximum replacements the substitution engine will do. (As of Hive 0.10.0.) 40   hive.vectorized.execution.enabled This flag controls the vectorized mode of query execution as documented in HIVE-4160. (As of Hive 0.13.0.) false    Hive Metastore Configuration Variables Please see Hive Metastore Administration for information about the configuration variables used to set up the metastore in local, remote, or embedded mode. Also see descriptions in the Metastore section of the Language Manual\u0026rsquo;s Hive Configuration Properties.\nFor security configuration (Hive 0.10 and later), see the Hive Metastore Security section in the Language Manual\u0026rsquo;s Hive Configuration Properties.\nConfiguration Variables Used to Interact with Hadoop    Variable Name Description Default Value     hadoop.bin.path The location of the Hadoop script which is used to submit jobs to Hadoop when submitting through a separate JVM. $HADOOP_HOME/bin/hadoop   hadoop.config.dir The location of the configuration directory of the Hadoop installation. $HADOOP_HOME/conf   fs.default.name The default name of the filesystem (for example, localhost for hdfs://:8020).For YARN this configuration variable is called fs.defaultFS. file:///   map.input.file The filename the map is reading from. null   mapred.job.tracker The URL to the jobtracker. If this is set to local then map/reduce is run in the local mode. local   mapred.reduce.tasks The number of reducers for each map/reduce stage in the query plan. 1   mapred.job.name The name of the map/reduce job. null   mapreduce.input.fileinputformat.split.maxsize For splittable data this changes the portion of the data that each mapper is assigned. By default, each mapper is assigned based on the block sizes of the source files. Entering a value larger than the block size will decrease the number of splits which creates fewer mappers. Entering a value smaller than the block size will increase the number of splits which creates more mappers. empty   fs.trash.interval The interval, in minutes, after which a trash checkpoint directory is deleted. (This is also the interval between checkpoints.) The checkpoint directory is located in .Trash under the user\u0026rsquo;s home directory and contains files and directories that were removed since the previous checkpoint.Any setting greater than 0 enables the trash feature of HDFS.When using the Transparent Data Encryption (TDE) feature, set this to 0 in Hadoop core-site.xml as documented in HIVE-10978. 0    Hive Variables Used to Pass Run Time Information    Variable Name Description Default Value     hive.session.id The id of the Hive Session.     hive.query.string The query string passed to the map/reduce job.     hive.query.planid The id of the plan for the map/reduce stage.     hive.jobname.length The maximum length of the jobname. 50   hive.table.name The name of the Hive table. This is passed to the user scripts through the script operator.     hive.partition.name The name of the Hive partition. This is passed to the user scripts through the script operator.     hive.alias The alias being processed. This is also passed to the user scripts through the script operator.      Removing Hive Metastore Password from Hive Configuration Support for this was added in Hive 0.14.0 with HIVE-7634 and HADOOP-10904. By setting up a CredentialProvider to handle storing/retrieval of passwords, you can remove the need to keep the Hive metastore password in cleartext in the Hive configuration.\n Set up the CredentialProvider to store the Hive Metastore password, using the key javax.jdo.option.ConnectionPassword (the same key as used in the Hive configuration). For example, the following command adds the metastore password to a JCEKS keystore file at /usr/lib/hive/conf/hive.jceks:  $ hadoop credential create javax.jdo.option.ConnectionPassword -provider jceks://file/usr/lib/hive/conf/hive.jceks Enter password: Enter password again: javax.jdo.option.ConnectionPassword has been successfully created. org.apache.hadoop.security.alias.JavaKeyStoreProvider has been updated. Make sure to restrict access to this file to just the user running the Hive Metastore server/HiveServer2.\nSee http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html#credential for more information. 2. Update the Hive configuration to use the designated CredentialProvider. For example to use our /usr/lib/hive/conf/hive.jceks file:\n \u0026lt;!-- Configure credential store for passwords--\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.security.credential.provider.path\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jceks://file/usr/lib/hive/conf/hive.jceks\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; This configures the CredentialProvider used by http://hadoop.apache.org/docs/current/api/org/apache/hadoop/conf/Configuration.html#getPassword(java.lang.String), which is used by Hive to retrieve the metastore password. 3. Remove the Hive Metastore password entry (javax.jdo.option.ConnectionPassword) from the Hive configuration. The CredentialProvider will be used instead. 4. Restart Hive Metastore Server/HiveServer2.\nConfiguring HCatalog and WebHCat HCatalog Starting in Hive release 0.11.0, HCatalog is installed and configured with Hive. The HCatalog server is the same as the Hive metastore.\n See Hive Metastore Administration for metastore configuration properties. See HCatalog Installation from Tarball for additional information.  For Hive releases prior to 0.11.0, see the \u0026ldquo;Thrift Server Setup\u0026rdquo; section in the HCatalog 0.5.0 document Installation from Tarball.\nWebHCat For information about configuring WebHCat, see WebHCat Configuration.\n  Save\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/adminmanual-configuration_27362070/","tags":null,"title":"Apache Hive : AdminManual Configuration"},{"categories":null,"contents":"Apache Hive : AdminManual Installation  Installing Hive  Installing from a Tarball Installing from Source Code (Hive 1.2.0 and Later) Installing from Source Code (Hive 0.13.0 and Later) Installing from Source Code (Hive 0.12.0 and Earlier)   Next Steps  Hive CLI and Beeline CLI Hive Metastore   HCatalog and WebHCat  HCatalog WebHCat (Templeton)    Installing Hive You can install a stable release of Hive by downloading and unpacking a tarball, or you can download the source code and build Hive using Maven (release 0.13 and later) or Ant (release 0.12 and earlier).\nHive installation has these requirements:\n Java 1.7 (preferred).\nNote: Hive versions 1.2 onward require Java 1.7 or newer. Hive versions 0.14 to 1.1 work with Java 1.6, but prefer 1.7. Users are strongly advised to start moving to Java 1.8 (see HIVE-8607). Hadoop 2.x (preferred), 1.x (not supported by Hive 2.0.0 onward).\nHive versions up to 0.13 also supported Hadoop 0.20.x, 0.23.x. Hive is commonly used in production Linux and Windows environment. Mac is a commonly used development environment. The instructions in this document are applicable to Linux and Mac. Using it on Windows would require slightly different steps.  Installing from a Tarball Start by downloading the most recent stable release of Hive from one of the Apache download mirrors (see Hive Releases).\nNext you need to unpack the tarball. This will result in the creation of a subdirectory named hive-x.y.z (where x.y.z is the release number):\n $ tar -xzvf hive-x.y.z.tar.gz Set the environment variable HIVE_HOME to point to the installation directory:\n $ cd hive-x.y.z $ export HIVE_HOME={{pwd}} Finally, add $HIVE_HOME/bin to your PATH:\n $ export PATH=$HIVE_HOME/bin:$PATH Installing from Source Code (Hive 1.2.0 and Later) Version information\nTo build Hive 1.2.0 and later releases with Apache Maven, see Getting Started: Building Hive from Source. You will need Java 1.7 or newer.\nInstalling from Source Code (Hive 0.13.0 and Later) Version information\nTo build Hive 0.13.0 and later releases with Apache Maven, see Getting Started: Building Hive from Source.\nInstalling from Source Code (Hive 0.12.0 and Earlier) Version information\nThis section describes installation for Hive 0.12.0 and earlier releases, which use Apache Ant to build Hive.\nInstalling Hive is simple and only requires having Java 1.6 and Ant installed on your machine (for Hive 0.12 and earlier).\nHive is available via SVN at http://svn.apache.org/repos/asf/hive/branches. You can download it by running the following command.\n$ svn co http://svn.apache.org/repos/asf/hive/branches/branch-#.# hive where #.# is the Hive release number. For release 0.8.1, use \u0026ldquo;branch-0.8-r2\u0026rdquo;.\nTo build Hive, execute the following command on the base directory:\n$ ant package It will create the subdirectory build/dist with the following contents:\n README.txt: readme file. bin/: directory containing all the shell scripts lib/: directory containing all required jar files conf/: directory with configuration files examples/: directory with sample input and query files  Subdirectory build/dist should contain all the files necessary to run Hive. You can run it from there or copy it to a different location, if you prefer.\nIn order to run Hive, you must have Hadoop in your path or have defined the environment variable HADOOP_HOME with the Hadoop installation directory.\nMoreover, we strongly advise users to create the HDFS directories /tmp and /user/hive/warehouse (also known as hive.metastore.warehouse.dir) and set them chmod g+w before tables are created in Hive.\nNext Steps You can begin using Hive as soon as it is installed, although you will probably want to configure it first.\nHive CLI and Beeline CLI To use the Hive command line interface (CLI) go to the Hive home directory and execute the following command:\n$ bin/hive The Hive home directory is the one with the contents of build/dist for Hive 0.12 and earlier; for Hive 0.13 and later it is packaging/target/apache-hive-\u0026lt;release_string\u0026gt;-bin/apache-hive-\u0026lt;release_string\u0026gt;-bin/.\nHiveServer2 (introduced in Hive 0.11) has a new CLI called Beeline (see Beeline – New Command Line Shell). To use Beeline, execute the following command in the Hive home directory:\n$ bin/beeline Hive Metastore Metadata is stored in an embedded Derby database whose disk storage location is determined by the Hive configuration variable named javax.jdo.option.ConnectionURL. By default, this location is ./metastore_db (see conf/hive-default.xml).\nUsing Derby in embedded mode allows at most one user at a time. To configure Derby to run in server mode, see Hive Using Derby in Server Mode.\nTo configure a database other than Derby for the Hive metastore, see Hive Metastore Administration.\nNext Step: Configuring Hive.\nHCatalog and WebHCat HCatalog Version\nHCatalog is installed with Hive, starting with Hive release 0.11.0.\nIf you install Hive from the binary tarball, the hcat command is available in the hcatalog/bin directory. However, most hcat commands can be issued as hive commands except for \u0026ldquo;hcat -g\u0026rdquo; and \u0026ldquo;hcat -p\u0026rdquo;. Note that the hcat command uses the -p flag for permissions but hive uses it to specify a port number. The HCatalog CLI is documented here and the Hive CLI is documented here.\nHCatalog installation is documented here.\nWebHCat (Templeton) Version\nWebHCat is installed with Hive, starting with Hive release 0.11.0.\nIf you install Hive from the binary tarball, the WebHCat server command webhcat_server.sh is in the hcatalog/sbin directory.\nWebHCat installation is documented here.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/adminmanual-installation_27362077/","tags":null,"title":"Apache Hive : AdminManual Installation"},{"categories":null,"contents":"Apache Hive : AdminManual Metastore 3.0 Administration Metastore 3.0 Administration\n Version Note Introduction  Changes From Hive 2 to Hive 3   General Configuration RDBMS  Option 1: Embedding Derby Option 2: External RDBMS  Supported RDBMSs   Installing and Upgrading the Metastore Schema   Running the Metastore  Embedded Mode Metastore Server  High Availability Securing the Service     Running the Metastore Without Hive Performance Optimizations  CachedStore   Less Commonly Changed Configuration Parameters  Version Note This document applies only to the Metastore in Hive 3.0 and later releases. For Hive 0, 1, and 2 releases please see the Metastore Administration document.\nIntroduction The definition of Hive objects such as databases, tables, and functions are stored in the Metastore. Depending on how the system is configured, statistics and authorization records may also be stored there. Hive, and other execution engines, use this data at runtime to determine how to parse, authorize, and efficiently execute user queries. The Metastore persists the object definitions to a relational database (RDBMS) via DataNucleus, a Java JDO based Object Relational Mapping (ORM) layer. See Supported RDBMSs below for a list of supported RDBMSs that can be used.\nThe Metastore can be configured to embed the Apache Derby RDBMS or connect to a external RDBMS. The Metastore itself can be embedded entirely in a user process or run as a service for other processes to connect to. Each of these options will be discussed in turn below.\nChanges From Hive 2 to Hive 3 Beginning in Hive 3.0, the Metastore can be run without the rest of Hive being installed. It is provided as a separate release in order to allow non-Hive systems to easily integrate with it. (It is, however, still included in the Hive release for convenience.) Making the Metastore a standalone service involved changing a number of configuration parameter names and tool names. All of the old configuration parameters and tools still work, in order to maximize backwards compatibility. This document will cover both the old and new names. As new functionality is added old, Hive style names will not be added.\nFor details on using the Metastore without Hive, see Running the Metastore Without Hive below.\nGeneral Configuration The metastore reads its configuration from the file metastore-site.xml. It expects to find this file in $METASTORE_HOME/conf where $METASTORE_HOME is an environment variable. For backwards compatibility it will also read any hive-site.xml or hive-metastoresite.xmlfiles found in HIVE_HOME/conf. Configuration options can also be defined on the command line (see Starting and Stopping the Service below).\nConfiguration values specific to running the Metastore with various RDBMSs, embedded or as a service, and without Hive are discussed in the relevant sections. The following configuration values apply to the Metastore regardless of how it is being run. This table covers only commonly customized configuration values. For less commonly changed configuration values see Less Commonly Changed Configuration Parameters.\n    Parameter Hive 2 Parameter Default Value Description     metastore.warehouse.dir hive.metastore.warehouse.dir  URI of the default location for tables in the default catalog and database.   datanucleus.schema.autoCreateAll datanucleus.schema.autoCreateAll false Auto creates the necessary schema in the RDBMS at startup if one does not exist. Set this to false after creating it once. To enable auto create also set hive.metastore.schema.verification=false. Auto creation is not recommended in production; run schematool instead.   metastore.schema.verification hive.metastore.schema.verification true Enforce metastore schema version consistency. When set to true: verify that version information stored in the RDBMS is compatible with the version of the Metastore jar. Also disable automatic schema migration. Users are required to manually migrate the schema after upgrade, which ensures proper schema migration. This setting is strongly recommended in production.When set to false: warn if the version information stored in RDBMS doesn\u0026rsquo;t match the version of the Metastore jar and allow auto schema migration.   metastore.hmshandler.retry.attempts hive.hmshandler.retry.attempts 10 The number of times to retry a call to the meastore when there is a connection error.   metastore.hmshandler.retry.interval hive.hmshandler.retry.interval 2 sec Time between retry attempts.   metastore.log4j.file hive.log4j.file none Log4j configuration file. If unset will look for metastore-log4j2.properties in $METASTORE_HOME/conf   metastore.stats.autogather hive.stats.autogather true Whether to automatically gather basic statistics during insert commands.    RDBMS Option 1: Embedding Derby The metastore can be run with Apache Derby embedded. This is the default configuration. However, it is not intended for use beyond simple testing. In this configuration only one client can use the Metastore and any changes are not durable beyond the life of the client (since it uses an in memory version of Derby).\nOption 2: External RDBMS For any durable, multi-user installation, an external RDBMS should be used to store Metastore objects. The Metastore connects to an external RDBMS via JDBC. Any jars required by the JDBC driver for your RDBMS should be placed in METASTORE_HOME/lib or explicilty passed on the command line. The following values need to be configured to connect the Metastore to an RDBMS. (Note: these configuration parameters did not change between Hive 2 and 3.)\n   Configuration Parameter Comment     javax.jdo.option.ConnectionURL Connection URL for the JDBC driver   javax.jdo.option.ConnectionDriverName JDBC driver class   javax.jdo.option.ConnectionUserName User name to connect to the RDBMS with   javax.jdo.option.ConnectionPassword Password to connect to the RDBMS with. The Metastore uses Hadoop\u0026rsquo;s CredentialProvider API so this does not have to be stored in clear text in your configuration file.    Supported RDBMSs As the Metastore uses DataNucleus to communicate with the RDBMS, theoretically any storage option supported by DataNucleus would work with the Metastore. However, we only test and recommend the following:\n   RDBMS Minimum Version javax.jdo.option.ConnectionURL javax.jdo.option.ConnectionDriverName     MS SQL Server 2008 R2 jdbc:sqlserver://\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;;DatabaseName=\u0026lt;SCHEMA\u0026gt; com.microsoft.sqlserver.jdbc.SQLServerDriver   MySQL 5.6.17 jdbc:mysql://\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;/\u0026lt;SCHEMA\u0026gt; com.mysql.jdbc.Driver   MariaDB 5.5 jdbc:mysql://\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;/\u0026lt;SCHEMA\u0026gt; org.mariadb.jdbc.Driver   Oracle* 11g jdbc:oracle:thin:@//\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;/xe oracle.jdbc.OracleDriver   Postgres 9.1.13 jdbc:postgresql://\u0026lt;HOST\u0026gt;:\u0026lt;PORT\u0026gt;/\u0026lt;SCHEMA\u0026gt; org.postgresql.Driver    \u0026lt;HOST\u0026gt; = The host the RDBMS is on.\n\u0026lt;PORT\u0026gt; = Port the RDBMS is listening for JDBC connections on\n\u0026lt;SCHEMA\u0026gt; = The schema (or database) that the Metastore stores its tables in.\n*The Oracle values shown are for Oracle\u0026rsquo;s thin JDBC client. If you are using a different client the ConnectionURL and ConnectionDriverName values will differ.\nSpecial Note: When using Postgres you should set the configuration parameter metastore.try.direct.sql.ddl (previously hive.metastore.try.direct.sql.ddl) to false, to avoid failures in certain operations.\nInstalling and Upgrading the Metastore Schema The Metastore provides the schematool utility to work with the Metastore schema in the RDBMS. For a full list of options see the -help option of the tool. The following summarizes what the tool can do. In most cases schematool can read the configuration from the metastore-site.xml file, though the configuration can also be passed as options on the command line.\n -initSchema: install a new schema. This should be used when first setting up a Metastore. -upgradeSchema: upgrade to the newly installed version. For 3.0, upgrades can be done from 1.2, 2.0, 2.1, 2.2, and 2.3 to 3.0. If you need to upgrade from before 1.2, use an older version of Hive\u0026rsquo;s schematool to first upgrade your schema to 1.2, then use the current Metastore version to upgrade to 3.0. -createUser: create the Metastore user and schema. This does not install the tables, it just creates the database user and schema. This likely will not work in a production environment because you likely will not have permissions to create users and schemas. You will likely need your DBA to do this for you. -validate: check that your Metastore schema is correct for its recorded version  Running the Metastore Embedded Mode The Metastore can be embedded directly into a process as a library. This is often done with HiveServer2 to avoid an additional network hop for metadata operations. It can also be done when using the Hive CLI or any other process. This mode is the default and will be used anytime the configuration parameter metastore.uris is not set.\nExcept in the case of HiveServer2, using this mode raises a few concerns. First, having many clients will put a burden on the backing RDBMS since each client will have its own set of connections. Second, every client must have read/write access to the RDBMS. This makes it hard to properly secure the RDBMS. Therefore embedded mode is not recommended in production with the exception of HiveServer2.\nMetastore Server To run the Metastore as a service, you must first configure it with a URL.\n   Configured On Parameter Hive 2 Parameter Format Default Value Comment     Client metastore.thrift.uris hive.metastore.uris thrift://:[, thrift://:\u0026hellip;] none HOST = hostname, PORT = should be set to match metastore.thrift.port on the server (which defaults to 9083. You can provide multiple servers in a comma separate list.   Server metastore.thrift.port hive.metastore.port integer 9083 Port Thrift will listen on.    Once you have configured your clients, you can start the Metastore on a server using the start-metastore utility. See the -help option of that utility for available options. There is no stop-metastore script. You must locate the process id for the metastore and kill that process.\nHigh Availability The Metastore service is stateless. This allows you to start multiple instances of the service to provide for high availability. It also allows you to configure some clients to embed the metastore (e.g. HiveServer2) while still running a Metastore service for other clients. If you are running multiple Metastore services you can put all their URIs into your client\u0026rsquo;s metastore.thrift.uris value and then set metastore.thrift.uri.selection ( in Hive 2 hive.metastore.uri.selection) to RANDOM or SEQUENTIAL. RANDOM will cause your client to randomly select one of the servers in the list, while SEQUENTIAL will cause it to start at the beginning of the list and attempt to connect to each server in order.\nSecuring the Service TODO: Need to fill in details for setting up with Kerberos, SSL, etc.\nCLIENT_KERBEROS_PRINCIPAL, KERBEROS_, SSL, USE_SSL, USE_THRIFT_SASL\nRunning the Metastore Without Hive Beginning in Hive 3.0, the Metastore is released as a separate package and can be run without the rest of Hive. This is referred to as standalone mode. By default the Metastore is configured for use with Hive, so a few configuration parameters have to be changed in this configuration.\n   Configuration Parameter Set to for Standalone Mode     metastore.task.threads.always org.apache.hadoop.hive.metastore.events.EventCleanerTask,org.apache.hadoop.hive.metastore.MaterializationsCacheCleanerTask   metastore.expression.proxy org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy    Currently the following features have not been tested or are known not to work with the Metastore in standalone mode:\n The compactor (for use with ACID tables) cannot be run without Hive. ACID tables can be read and written to, but they cannot compacted. Replication has not been tested outside of Hive.  Performance Optimizations CachedStore Prior to Hive 3.0 there was only a single implementation of the MetaStore API (called ObjectStore). HIVE-16520 introduced a second implementation that can cache objects from the database in memory. This can save a significant amount of time for round trips to the database. It can be used by changing the parameter metastore.rawstore.impl to org.apache.hadoop.hive.metastore.cache.CachedStore.\nThe cache is automatically updated with new data when changes are made through this MetaStore. In a scenario where there are multiple MetaStore servers the caches can be out of date on some of them. To prevent this the CachedStore automatically refreshes the cache in a configurable frequency (default: 1 minute).\nDetails about all properties for the CachedStore can be found on Configuration Properties (Prefix: metastore.cached). Less Commonly Changed Configuration Parameters BATCHED_RETRIEVE_, CLIENT_CONNECT_RETRY_DELAY, FILTER_HOOK, SERDES_USING_METASTORE_FOR_SCHEMA, SERVER__THREADS, THREAD_POOL_SIZE Security: EXECUTE_SET_UGI, metastore.authorization.storage.checks\nSetting up Caching: CACHED*, CATALOGS_TO_CACHE \u0026amp; AGGREGATE_STATS_CACHE*\nTransactions: MAX_OPEN_TXNS, TXNS_*\n  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/adminmanual-metastore-3-0-administration_75978150/","tags":null,"title":"Apache Hive : AdminManual Metastore 3.0 Administration"},{"categories":null,"contents":"Apache Hive : AdminManual Metastore Administration Hive Metastore Administration  Hive Metastore Administration  Introduction  Basic Configuration Parameters Additional Configuration Parameters Data Nucleus Auto Start Default Configuration   Local/Embedded Metastore Database (Derby) Remote Metastore Database Local/Embedded Metastore Server Remote Metastore Server  Client Configuration Parameters   Supported Backend Databases for Metastore Metastore Schema Consistency and Upgrades    This page only documents the MetaStore in Hive 2.x and earlier. For 3.x and later releases please see AdminManual Metastore 3.0 Administration\nIntroduction All the metadata for Hive tables and partitions are accessed through the Hive Metastore. Metadata is persisted using JPOX ORM solution (Data Nucleus) so any database that is supported by it can be used by Hive. Most of the commercial relational databases and many open source databases are supported. See the list of supported databases in section below.\nYou can find an E/R diagram for the metastore here.\nThere are 2 different ways to setup the metastore server and metastore database using different Hive configurations:\nConfiguration options for metastore database where metadata is persisted:\n Local/Embedded Metastore Database (Derby) Remote Metastore Database  Configuration options for metastore server:\n Local/Embedded Metastore Server Remote Metastore Server  Basic Configuration Parameters The relevant configuration parameters are shown here. (Non-metastore parameters are described in Configuring Hive. Also see the Language Manual\u0026rsquo;s Hive Configuration Properties, including Metastore and Hive Metastore Security.)\nAlso see hivemetastore-site.xml documentation under Configuring Hive.\n   Configuration Parameter Description     javax.jdo.option.ConnectionURL JDBC connection string for the data store which contains metadata   javax.jdo.option.ConnectionDriverName JDBC Driver class name for the data store which contains metadata   hive.metastore.uris Hive connects to one of these URIs to make metadata requests to a remote Metastore (comma separated list of URIs)   hive.metastore.local local or remote metastore (removed as of Hive 0.10: If hive.metastore.uris is empty local mode is assumed, remote otherwise)   hive.metastore.warehouse.dir URI of the default location for native tables    The Hive metastore is stateless and thus there can be multiple instances to achieve High Availability. Using hive.metastore.uris it is possible to specify multiple remote metastores. Hive will use the first one from the list by default but will pick a random one on connection failure and will try to reconnect.\nAdditional Configuration Parameters The following metastore configuration parameters were carried over from old documentation without a guarantee that they all still exist. See the HiveConf Java class for current Hive configuration options, and see the Metastore and Hive Metastore Security sections of the Language Manual\u0026rsquo;s Hive Configuration Properties for user-friendly descriptions of the metastore parameters.\n   Configuration Parameter Description Default Value     hive.metastore.metadb.dir The location of filestore metadata base directory. (Functionality removed in 0.4.0 with HIVE-143.)     hive.metastore.rawstore.impl Name of the class that implements the org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database. (Hive 0.8.1 and later.)     org.jpox.autoCreateSchema Creates necessary schema on startup if one doesn\u0026rsquo;t exist. (The schema includes tables, columns, and so on.) Set to false after creating it once.     org.jpox.fixedDatastore Whether the datastore schema is fixed.     datanucleus.autoStartMechanism Whether to initialize on startup.     hive.metastore.ds.connection.url.hook Name of the hook to use for retriving the JDO connection URL. If empty, the value in javax.jdo.option.ConnectionURL is used as the connection URL. (Hive 0.6 and later.)     hive.metastore.ds.retry.attempts The number of times to retry a call to the backing datastore if there were a connection error.(Hive 0.6 through 0.12; removed in 0.13.0 – use hive.hmshandler.retry.attempts instead.) 1   hive.metastore.ds.retry.interval The number of miliseconds between datastore retry attempts.(Hive 0.6 through 0.12; removed in 0.13.0 – use hive.hmshandler.retry.interval instead.) 1000   hive.metastore.server.min.threads Minimum number of worker threads in the Thrift server\u0026rsquo;s pool.(Hive 0.6 and later.) 200   hive.metastore.server.max.threads Maximum number of worker threads in the Thrift server\u0026rsquo;s pool.(Hive 0.6 and later.) 100000 since Hive 0.8.1   hive.metastore.filter.hook Metastore hook class for further filtering the metadata read results on client side.(Hive 1.1.0 and later.) org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl   hive.metastore.port Hive metastore listener port.(Hive 1.3.0 and later.) 9083    Data Nucleus Auto Start Configuring datanucleus.autoStartMechanism is highly recommended\nConfiguring auto start for data nucleus is highly recommended. See HIVE-4762 for more details.\n \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;datanucleus.autoStartMechanism\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;SchemaTable\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Default Configuration The default configuration sets up an embedded metastore which is used in unit tests and is described in the next section. More practical options are described in the subsequent sections.\nLocal/Embedded Metastore Database (Derby) An embedded metastore database is mainly used for unit tests. Only one process can connect to the metastore database at a time, so it is not really a practical solution but works well for unit tests.\nFor unit tests Local/Embedded Metastore Server configuration for the metastore server is used in conjunction with embedded database.\nDerby is the default database for the embedded metastore.\n   Config Param Config Value Comment     javax.jdo.option.ConnectionURL jdbc:derby:;databaseName=../build/test/junit_metastore_db;create=true Derby database located at hive/trunk/build\u0026hellip;   javax.jdo.option.ConnectionDriverName org.apache.derby.jdbc.EmbeddedDriver Derby embeded JDBC driver class.   hive.metastore.warehouse.dir file://${user.dir}/../build/ql/test/data/warehouse Unit test data goes in here on your local filesystem.    If you want to run Derby as a network server so the metastore can be accessed from multiple nodes, see Hive Using Derby in Server Mode.\nRemote Metastore Database In this configuration, you would use a traditional standalone RDBMS server. The following example configuration will set up a metastore in a MySQL server. This configuration of metastore database is recommended for any real use.\n   Config Param Config Value Comment     javax.jdo.option.ConnectionURL jdbc:mysql://\u0026lt;host name\u0026gt;/\u0026lt;database name\u0026gt;?createDatabaseIfNotExist=true metadata is stored in a MySQL server   javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver MySQL JDBC driver class   javax.jdo.option.ConnectionUserName \u0026lt;user name\u0026gt; user name for connecting to MySQL server   javax.jdo.option.ConnectionPassword \u0026lt;password\u0026gt; password for connecting to MySQL server    Local/Embedded Metastore Server In local/embedded metastore setup, the metastore server component is used like a library within the Hive Client. Each Hive Client will open a connection to the database and make SQL queries against it. Make sure that the database is accessible from the machines where Hive queries are executed since this is a local store. Also make sure the JDBC client library is in the classpath of Hive Client. This configuration is often used with HiveServer2 (to use embedded metastore only with HiveServer2 add \u0026ldquo;\u0026ndash;hiveconf hive.metastore.uris=' \u0026lsquo;\u0026rdquo; in command line parameters of the hiveserver2 start command or use hiveserver2-site.xml (available in Hive 0.14)).\n   Config Param Config Value Comment     hive.metastore.uris not needed because this is local store     hive.metastore.local true this is local store (removed in Hive 0.10, see configuration description section)   hive.metastore.warehouse.dir \u0026lt;base hdfs path\u0026gt; Points to default location of non-external Hive tables in HDFS.    Remote Metastore Server In remote metastore setup, all Hive Clients will make a connection to a metastore server which in turn queries the datastore (MySQL in this example) for metadata. Metastore server and client communicate using Thrift Protocol. Starting with Hive 0.5.0, you can start a Thrift server by executing the following command:\nhive --service metastore In versions of Hive earlier than 0.5.0, it\u0026rsquo;s instead necessary to run the Thrift server via direct execution of Java:\n$JAVA_HOME/bin/java -Xmx1024m -Dlog4j.configuration=file://$HIVE_HOME/conf/hms-log4j.properties -Djava.library.path=$HADOOP_HOME/lib/native/Linux-amd64-64/ -cp $CLASSPATH org.apache.hadoop.hive.metastore.HiveMetaStore If you execute Java directly, then JAVA_HOME, HIVE_HOME, HADOOP_HOME must be correctly set; CLASSPATH should contain Hadoop, Hive (lib and auxlib), and Java jars.\nServer Configuration Parameters\nThe following example uses aRemote Metastore Database.\n   Config Param Config Value Comment     javax.jdo.option.ConnectionURL jdbc:mysql://\u0026lt;host name\u0026gt;/\u0026lt;database name\u0026gt;?createDatabaseIfNotExist=true metadata is stored in a MySQL server   javax.jdo.option.ConnectionDriverName com.mysql.jdbc.Driver MySQL JDBC driver class   javax.jdo.option.ConnectionUserName \u0026lt;user name\u0026gt; user name for connecting to MySQL server   javax.jdo.option.ConnectionPassword \u0026lt;password\u0026gt; password for connecting to MySQL server   hive.metastore.warehouse.dir \u0026lt;base hdfs path\u0026gt; default location for Hive tables.   hive.metastore.thrift.bind.host \u0026lt;host_name\u0026gt; Host name to bind the metastore service to. When empty, \u0026ldquo;localhost\u0026rdquo; is used. This configuration is available Hive 4.0.0 onwards.    From Hive 3.0.0 (HIVE-16452) onwards the metastore database stores a GUID which can be queried using the Thrift API get_metastore_db_uuid by metastore clients in order to identify the backend database instance. This API can be accessed by the HiveMetaStoreClient using the method getMetastoreDbUuid().\nClient Configuration Parameters    Config Param Config Value Comment     hive.metastore.uris thrift://\u0026lt;host_name\u0026gt;:\u0026lt;port\u0026gt; host and port for the Thrift metastore server. If hive.metastore.thrift.bind.host is specified, host should be same as that configuration. Read more about this in dynamic service discovery configuration parameters.   hive.metastore.local false Metastore is remote. Note: This is no longer needed as of Hive 0.10. Setting hive.metastore.uri is sufficient.   hive.metastore.warehouse.dir \u0026lt;base hdfs path\u0026gt; Points to default location of non-external Hive tables in HDFS.    Dynamic Service Discovery Configuration Parameters\nFrom Hive 4.0.0 (HIVE-20794) onwards, similar to HiveServer2, a ZooKeeper service can be used for dynamic service discovery of a remote metastore server. Following parameters are used by both metastore server and client.\n   Config Param Config Value Comment     hive.metastore.service.discovery.mode service discovery mode When it is set to \u0026ldquo;zookeeper\u0026rdquo;, ZooKeeper is used for dynamic service discovery of a remote metastore. In that case, a metastore adds itself to the ZooKeeper when it is started and removes itself when it shuts down. By default it is empty. Both the client and server should have same value for this parameter.   hive.metastore.uris \u0026lt;host_name\u0026gt;:, \u0026lt;host_name\u0026gt;:, \u0026hellip; One or more host and port pairs of ZooKeeper servers forming a ZooKeeper ensemble. Used when hive.metastore.service.discovery.mode is set to \u0026ldquo;zookeeper\u0026rdquo;. The configuration is not used by server otherwise. If all the servers are using the same port you may specify the port using hive.metastore.zookeeper.client.port instead of specifying it with every server separately. Both the client and server should have same value for this parameter.   hive.metastore.zookeeper.client.port  Port number when same port number is used by all the ZooKeeper servers in the ensemble. Both the client and server should have same value for this parameter.   hive.metastore.zookeeper.namespace  The parent node under which all ZooKeeper nodes for metastores are created.   hive.metastore.zookeeper.session.timeout  ZooKeeper client\u0026rsquo;s session timeout (in milliseconds). The client is disconnected if a heartbeat is not sent in the timeout.   hive.metastore.zookeeper.connection.timeout  ZooKeeper client\u0026rsquo;s connection timeout in seconds. Connection timeout * hive.metastore.zookeeper.connection.max.retries with exponential backoff is when curator client deems connection is lost to zookeeper.   hive.metastore.zookeeper.connection.max.retries  Max number of times to retry when connecting to the ZooKeeper server.   hive.metastore.zookeeper.connection.basesleeptime  Initial amount of time (in milliseconds) to wait between retries when connecting to the ZooKeeper server when using ExponentialBackoffRetry policy.    If you are using MySQL as the datastore for metadata, put MySQL jdbc libraries in HIVE_HOME/lib before starting Hive Client or HiveMetastore Server.\nTo change the metastore port, use this hive command:\nhive --service metastore -p \u0026lt;port_num\u0026gt; Supported Backend Databases for Metastore    Database Minimum Supported Version Name for Parameter Values See Also     MySQL 5.6.17 mysql    Postgres 9.1.13 postgres    Oracle 11g oracle hive.metastore.orm.retrieveMapNullsAsEmptyStrings   MS SQL Server 2008 R2 mssql     Metastore Schema Consistency and Upgrades Version\nIntroduced in Hive 0.12.0. See HIVE-3764.\nHive now records the schema version in the metastore database and verifies that the metastore schema version is compatible with Hive binaries that are going to accesss the metastore. Note that the Hive properties to implicitly create or alter the existing schema are disabled by default. Hive will not attempt to change the metastore schema implicitly. When you execute a Hive query against an old schema, it will fail to access the metastore.\nTo suppress the schema check and allow the metastore to implicitly modify the schema, you need to set a configuration property hive.metastore.schema.verification to false in hive-site.xml.\nStarting in release 0.12, Hive also includes an off-line schema tool to initialize and upgrade the metastore schema. Please refer to the details here.\nSave\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/adminmanual-metastore-administration_27362076/","tags":null,"title":"Apache Hive : AdminManual Metastore Administration"},{"categories":null,"contents":"Apache Hive : AdminManual SettingUpHiveServer Setting Up Hive Server  Setting Up HiveServer2 Setting Up Thrift Hive Server Setting Up Hive JDBC Server Setting Up Hive ODBC Server  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/adminmanual-settinguphiveserver_27362079/","tags":null,"title":"Apache Hive : AdminManual SettingUpHiveServer"},{"categories":null,"contents":"Apache Hive : Apache Hive 4.0.X  Overview of Major Changes ChangeLog Introduction to Apache Hive Building from Source Installing Hive  From Docker Manual Installation   HiveServer2 Overview HiveServer2 Clients AdminManual Metastore 3.0+ Administration  SchemaTool   Supported Features: Apache Hive 3.1+ Hive Metrics HiveCounters Hive APIs Overview UDFs Operators Hive-Iceberg Integration Accumulo Integration Druid Integration Kudu Integration Hive Transactions (HIVE ACID) JDBC Storage Handler Materialized Views Hive Replication Streaming Data Ingest V2 Configuration Defaults HiveDeveloperFAQ Books, Blogs \u0026amp; Talks  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/apache-hive-4-0-x_282102245/","tags":null,"title":"Apache Hive : Apache Hive 4.0.X"},{"categories":null,"contents":"Apache Hive : Apache Hive SQL Conformance This page documents which parts of the SQL standard are supported by Apache Hive. The information here is not a full statement of conformance but provides users detail sufficient to generally understand Hive\u0026rsquo;s SQL conformance.\nThis information is versioned by Hive release version, allowing a user to quickly identify features available to them.\nThe formal name of the current SQL standard is ISO/IEC 9075 \u0026ldquo;Database Language SQL\u0026rdquo;. A revised version of the standard is released from time to time; the most recent update appearing in 2016. The 2016 version is referred to as ISO/IEC 9075:2016, or simply as SQL:2016. Hive\u0026rsquo;s SQL Conformance pages reference SQL features by the Feature ID values of the SQL:2016 Standard.\n   Version Supported SQL Features     Apache Hive 2.1 Supported SQL Features   Apache Hive 2.3 Supported SQL Features   Apache Hive 3.1 Supported SQL Features    Information in these pages is not guaranteed to be accurate. Corrections can be submitted to the Apache Hive mailing list at user@hive.apache.org.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/apache-hive-sql-conformance_67641449/","tags":null,"title":"Apache Hive : Apache Hive SQL Conformance"},{"categories":null,"contents":"Apache Hive : AuthDev Index\n 1. Privilege  1.1 Access Privilege   2. Hive Operations 3. Metadata  3.1 user, group, and roles  3.1.1 Role management 3.1.2 role metadata 3.1.3 hive role user membership table   3.2 Privileges to be supported by Hive  3.2.1 metadata     4. grant/revoke access privilege  4.1 Privilege names/types: 4.2 show grant 4.3 grant/revoke statement   5. Authorization verification  5.1 USER/GROUP/ROLE 5.2 The verification steps 5.3 Examples   6. Where to add authorization in Hive 7. Implementation  7.1 Authenticator interface 7.2 Authorization   8. Metastore upgrade script for mysql HDFS Permission  Authorization modes\nThis is the design document for the original Hive authorization mode. See Authorization for an overview of authorization modes, which include storage based authorization and SQL standards based authorization.\n1. Privilege 1.1 Access Privilege Admin privilege, DB privilege, Table level privilege, column level privilege\n1.1.1 Admin privileges are global privileges, and are used to perform administration.\n1.1.2 DB privileges are database specific, and apply to all objects inside that database.\n1.1.3 Table privileges apply to table/view/index in a given database\n1.1.4 Column privileges apply to column level.\nAll DB/Table/Column privilege differentiate read and write privileges even though now hive does not support column level overwrite. And there is no partition level privilege.\n2. Hive Operations create index/drop index\ncreate database/drop database\ncreate table/drop table\ncreate view/drop view\nalter table\nshow databases\nlock table/unlock table/show lock\nadd partition\narchive\nSelect\ninsert overwrite directory\ninsert overwrite table\nothers include \u0026ldquo;create table as \u0026ldquo;, \u0026ldquo;create table like\u0026rdquo; etc\n3. Metadata Store the privilege information in the new metastore tables \u0026lsquo;user\u0026rsquo;, \u0026lsquo;db\u0026rsquo;, \u0026lsquo;tables_priv\u0026rsquo;, \u0026lsquo;columns_priv\u0026rsquo;.\nThe user table indicates user\u0026rsquo;s global privileges, which apply to all databases.\nThe db table determine database level access privileges, which apply to all objects inside that database.\n3.1 user, group, and roles User can belong to some groups. The group information is provided by authenticator.\nAnd each user or group can have some privileges and roles. A role can be a member of another role, but not in a circular manner.\nSo hive metadata needs to store:\n  roles -\u0026gt; privileges, roles mapping\n  Hive user/group -\u0026gt; privileges, role mapping\n  3.1.1 Role management create role\ndrop role\ngrant a role to a user\nrevoke a role from a user\n3.1.2 role metadata role_name - string\ncreate_time - int\n3.1.3 hive role user membership table role_name - string\nuser_name - string\nis_group – is the user name a group name\nis_role – is the user name a role name\n3.2 Privileges to be supported by Hive 3.2.1 metadata The below shows how we store the grant information in metastore. The deny information is stored in a same matter (just in different tables).\nSo for each grant table, there will also be a deny table. The metastore tables are\nuser, deny_user, db, deny_db, tables_priv, deny_tables_priv, columns_priv, deny_columns_priv\nAnother way to do it is to add a column in the grant table to record this row is grant or deny.\nWe store privileges in one column, and use comma to separate different privileges.\nhive\u0026gt; desc user;\nField\n     User\nisRole\nisGroup\nisSuper\ndb_priv – set (Select_priv, Insert_priv, Create_priv, Drop_priv, Reload_priv,\nGrant_priv, Index_priv, Alter_priv, Show_db_priv,\nLock_tables_priv, Create_view_priv, Show_view_priv)\nhive\u0026gt; desc db;\nField\n     Db\nUser\nisRole\nisGroup\nTable_priv – set (Select_priv, Insert_priv, Create_priv, Drop_priv, Grant_priv,\nIndex_priv, Reload_priv, Alter_priv, Create_tmp_table_priv,\nLock_tables_priv, Create_view_priv, Show_view_priv)\nhive\u0026gt; desc tables_priv;\nField\n     Db\nUser\nisRole\nisGroup\nTable_name\nGrantor\nTimestamp\nTable_priv – set(\u0026lsquo;Select\u0026rsquo;,\u0026lsquo;Insert\u0026rsquo;,,\u0026lsquo;Create\u0026rsquo;,\u0026lsquo;Drop\u0026rsquo;,\u0026lsquo;Grant\u0026rsquo;,\u0026lsquo;Index\u0026rsquo;,\u0026lsquo;Alter\u0026rsquo;,\u0026lsquo;Create View\u0026rsquo;,\u0026lsquo;Show view\u0026rsquo;)\nColumn_priv – set(\u0026lsquo;Select\u0026rsquo;,\u0026lsquo;Insert\u0026rsquo;,)\nmysql\u0026gt; desc columns_priv;\nField\n     Db\nUser\nisRole\nisGroup\nTable_name\nColumn_name\nTimestamp\nColumn_priv – set(\u0026lsquo;Select\u0026rsquo;,\u0026lsquo;Insert\u0026rsquo;,\u0026lsquo;Update\u0026rsquo;)\n4. grant/revoke access privilege 4.1 Privilege names/types: ALL Privileges\nALTER\nCreate\nCreate view\nDelete\nDrop\nIndex\nInsert\nLock Tables\nSelect\nShow databases\nSuper\n4.2 show grant 4.3 grant/revoke statement GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level TO user [, user] ... WITH ADMIN OPTION object_type: TABLE priv_level: * | *.* | db_name.* | db_name.tbl_name | tbl_name REVOKE priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level FROM user [, user] ... REVOKE ALL PRIVILEGES, GRANT OPTION FROM user [, user] ... DENY priv_type [(column_list)] [, priv_type [(column_list)]] ... ON [object_type] priv_level FROM user [, user] ... 5. Authorization verification 5.1 USER/GROUP/ROLE USER\nGROUP\nROLE\nGROUP is very similar to a role. And we support Group is because we may need to pass the group information to HDFS/Map-reduce.\nA role can also contain other roles and privileges - and they can be granted to users and groups.\nRole can be nested but not circular.\n5.2 The verification steps When a user logins to the system, he has a user name, one or few groups that he belongs to.\nSo it is\n[\nusername, list of group names, list of privileges and roles that has been directly granted, list of privileges and roles that been directly granted to groups that users belongs to ].\n Steps to authorize one access: *  First try user name: # If there is an entry in 'user' that accept this access, return ACCEPT 2. If there is an entry in 'db' that accept this access, return ACCEPT 3. If there is an entry in 'table' that accept this access, return ACCEPT 4. If there is an entry in 'column' that accept this access, return ACCEPT Second try the user's group/role names one by one until we get an ACCEPT. For each role/group, we do the same routine as we did for user name. 5.3 Examples 5.3.1 I want to grant everyone (new people may join at anytime) to\ndb_name.*, and then later i want to protect one table db_name.T from ALL\nusers but a few\n  Add all users to a group \u0026lsquo;users\u0026rsquo;. (assumption: new users will\nautomatically join this group). And grant \u0026lsquo;users\u0026rsquo; ALL privileges to db_name.*\n  Add those few users to a new group \u0026lsquo;users2\u0026rsquo;. AND REMOVE them from \u0026lsquo;users\u0026rsquo;\n  DENY \u0026lsquo;users\u0026rsquo; to db_name.T\n  Grant ALL on db_name.T to users2\n  5.3.2 I want to protect one table db_name.T from one/few users, but all\nother people can access it\n  Add all users to a group \u0026lsquo;users\u0026rsquo;. (assumption: new users will automatically\njoin this group). And grant \u0026lsquo;users\u0026rsquo; ALL privileges to db_name.*.\n  Add those few users to a new group \u0026lsquo;users2\u0026rsquo;. (Note: those few users will now\nbelong to 2 groups: users and user2)\n  DENY \u0026lsquo;users2\u0026rsquo; to db_name.T\n  6. Where to add authorization in Hive CliDriver and HiveServer. Basically they share the same code. If HiveServer invokes CliDriver, we can just add it into CliDriver. And we also need to make HiveServer be able to support multiple user/connections.\nThis still does not solve the problem if someone accesses the metastore directly (without going through Hive).\n7. Implementation 7.1 Authenticator interface We only get the user\u0026rsquo;s user name, group names from the authenticator. The authenticator implementations need to provide these information. This is the only interface between authenticator and authorization.\n7.2 Authorization Authorization decision manager manages a set of authorization provider, and each provider can decide to accept or deny. And it is the decision manager to do the final decision. Can be vote based, or one -1 then deny, or one +1 then accept. Authorization provider decides whether to accept or deny an access based on his own information.\n8. Metastore upgrade script for mysql -- -- Table structure for table {{ROLES}} -- DROP TABLE IF EXISTS {{ROLES}}; CREATE TABLE {{ROLES}} ( {{ROLE_ID}} bigint(20) NOT NULL, {{CREATE_TIME}} int(11) NOT NULL, {{OWNER_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{ROLE_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, PRIMARY KEY ({{ROLE_ID}}), UNIQUE KEY {{ROLEENTITYINDEX}} ({{ROLE_NAME}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{ROLE_MAP}} -- DROP TABLE IF EXISTS {{ROLE_MAP}}; CREATE TABLE {{ROLE_MAP}} ( {{ROLE_GRANT_ID}} bigint(20) NOT NULL, {{ADD_TIME}} int(11) NOT NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{ROLE_ID}} bigint(20) default NULL, PRIMARY KEY ({{ROLE_GRANT_ID}}), UNIQUE KEY {{USERROLEMAPINDEX}} ({{PRINCIPAL_NAME}},{{ROLE_ID}},{{GRANTOR}},{{GRANTOR_TYPE}}), KEY {{ROLE_MAP_N49}} ({{ROLE_ID}}), CONSTRAINT {{ROLE_MAP_FK1}} FOREIGN KEY ({{ROLE_ID}}) REFERENCES {{ROLES}} ({{ROLE_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{GLOBAL_PRIVS}} -- DROP TABLE IF EXISTS {{GLOBAL_PRIVS}}; CREATE TABLE {{GLOBAL_PRIVS}} ( {{USER_GRANT_ID}} bigint(20) NOT NULL, {{CREATE_TIME}} int(11) NOT NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{USER_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL, PRIMARY KEY ({{USER_GRANT_ID}}), UNIQUE KEY {{GLOBALPRIVILEGEINDEX}} ({{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{USER_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{DB_PRIVS}} -- DROP TABLE IF EXISTS {{DB_PRIVS}}; CREATE TABLE {{DB_PRIVS}} ( {{DB_GRANT_ID}} bigint(20) NOT NULL, {{CREATE_TIME}} int(11) NOT NULL, {{DB_ID}} bigint(20) default NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{DB_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL, PRIMARY KEY ({{DB_GRANT_ID}}), UNIQUE KEY {{DBPRIVILEGEINDEX}} ({{DB_ID}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{DB_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}), KEY {{DB_PRIVS_N49}} ({{DB_ID}}), CONSTRAINT {{DB_PRIVS_FK1}} FOREIGN KEY ({{DB_ID}}) REFERENCES {{DBS}} ({{DB_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{TBL_PRIVS}} -- DROP TABLE IF EXISTS {{TBL_PRIVS}}; CREATE TABLE {{TBL_PRIVS}} ( {{TBL_GRANT_ID}} bigint(20) NOT NULL, {{CREATE_TIME}} int(11) NOT NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{TBL_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL, {{TBL_ID}} bigint(20) default NULL, PRIMARY KEY ({{TBL_GRANT_ID}}), KEY {{TBL_PRIVS_N49}} ({{TBL_ID}}), KEY {{TABLEPRIVILEGEINDEX}} ({{TBL_ID}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{TBL_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}), CONSTRAINT {{TBL_PRIVS_FK1}} FOREIGN KEY ({{TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{PART_PRIVS}} -- DROP TABLE IF EXISTS {{PART_PRIVS}}; CREATE TABLE {{PART_PRIVS}} ( {{PART_GRANT_ID}} bigint(20) NOT NULL, {{CREATE_TIME}} int(11) NOT NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PART_ID}} bigint(20) default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PART_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL, PRIMARY KEY ({{PART_GRANT_ID}}), KEY {{PARTPRIVILEGEINDEX}} ({{PART_ID}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{PART_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}), KEY {{PART_PRIVS_N49}} ({{PART_ID}}), CONSTRAINT {{PART_PRIVS_FK1}} FOREIGN KEY ({{PART_ID}}) REFERENCES {{PARTITIONS}} ({{PART_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{TBL_COL_PRIVS}} -- DROP TABLE IF EXISTS {{TBL_COL_PRIVS}}; CREATE TABLE {{TBL_COL_PRIVS}} ( {{TBL_COLUMN_GRANT_ID}} bigint(20) NOT NULL, {{COLUMN_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{CREATE_TIME}} int(11) NOT NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{TBL_COL_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL, {{TBL_ID}} bigint(20) default NULL, PRIMARY KEY ({{TBL_COLUMN_GRANT_ID}}), KEY {{TABLECOLUMNPRIVILEGEINDEX}} ({{TBL_ID}},{{COLUMN_NAME}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{TBL_COL_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}), KEY {{TBL_COL_PRIVS_N49}} ({{TBL_ID}}), CONSTRAINT {{TBL_COL_PRIVS_FK1}} FOREIGN KEY ({{TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; DROP TABLE IF EXISTS {{PART_COL_PRIVS}}; CREATE TABLE {{PART_COL_PRIVS}} ( {{PART_COLUMN_GRANT_ID}} bigint(20) NOT NULL, {{COLUMN_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{CREATE_TIME}} int(11) NOT NULL, {{GRANT_OPTION}} smallint(6) NOT NULL, {{GRANTOR}} varchar(128) character set latin1 collate latin1_bin default NULL, {{GRANTOR_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PART_ID}} bigint(20) default NULL, {{PRINCIPAL_NAME}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PRINCIPAL_TYPE}} varchar(128) character set latin1 collate latin1_bin default NULL, {{PART_COL_PRIV}} varchar(128) character set latin1 collate latin1_bin default NULL, PRIMARY KEY ({{PART_COLUMN_GRANT_ID}}), KEY {{PART_COL_PRIVS_N49}} ({{PART_ID}}), KEY {{PARTITIONCOLUMNPRIVILEGEINDEX}} ({{PART_ID}},{{COLUMN_NAME}},{{PRINCIPAL_NAME}},{{PRINCIPAL_TYPE}},{{PART_COL_PRIV}},{{GRANTOR}},{{GRANTOR_TYPE}}), CONSTRAINT {{PART_COL_PRIVS_FK1}} FOREIGN KEY ({{PART_ID}}) REFERENCES {{PARTITIONS}} ({{PART_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1;  HDFS Permission The above has a STRONG assumption on the file layer security. Users can easily by-pass the security if the hdfs file permission is open to him. We hope we can easily plug in external authorizations (like HDFS permission/Howl permission) to alter the authorization result or even the rule.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/authdev_27362078/","tags":null,"title":"Apache Hive : AuthDev"},{"categories":null,"contents":"Apache Hive : AvroSerDe   Availability\n  Overview – Working with Avro from Hive\n  Requirements\n  Avro to Hive type conversion\n  Creating Avro-backed Hive tables\n All Hive versions Hive 0.14 and later versions    Writing tables to Avro files\n - [Example](#example)+ [All Hive versions](#all-hive-versions)   Hive 0.14 and later Avro file extension    Specifying the Avro schema for a table\n Use avro.schema.url Use schema.literal and embed the schema in the create statement Use avro.schema.literal and pass the schema into the script Use none to ignore either avro.schema.literal or avro.schema.url    HBase Integration\n  If something goes wrong\n  FAQ\n  Availability Earliest version AvroSerde is available\nThe AvroSerde is available in Hive 0.9.1 and greater.\nOverview – Working with Avro from Hive The AvroSerde allows users to read or write Avro data as Hive tables. The AvroSerde\u0026rsquo;s bullet points:\n Infers the schema of the Hive table from the Avro schema. Starting in Hive 0.14, the Avro schema can be inferred from the Hive table schema. Reads all Avro files within a table against a specified schema, taking advantage of Avro\u0026rsquo;s backwards compatibility abilities Supports arbitrarily nested schemas. Translates all Avro data types into equivalent Hive types. Most types map exactly, but some Avro types don\u0026rsquo;t exist in Hive and are automatically converted by the AvroSerde. Understands compressed Avro files. Transparently converts the Avro idiom of handling nullable types as Union[T, null] into just T and returns null when appropriate. Writes any Hive table to Avro files. Has worked reliably against our most convoluted Avro schemas in our ETL process. Starting in Hive 0.14, columns can be added to an Avro backed Hive table using the Alter Table statement.  For general information about SerDes, see Hive SerDe in the Developer Guide. Also see SerDe for details about input and output processing.\nRequirements The AvroSerde has been built and tested against Hive 0.9.1 and later, and uses Avro 1.7.5 as of Hive 0.13 and 0.14.\n   Hive Versions Avro Version     Hive 0.9.1 Avro 1.5.3   Hive 0.10, 0.11, and 0.12 Avro 1.7.1   Hive 0.13 and 0.14 Avro 1.7.5    Avro to Hive type conversion While most Avro types convert directly to equivalent Hive types, there are some which do not exist in Hive and are converted to reasonable equivalents. Also, the AvroSerde special cases unions of null and another type, as described below:\n   Avro type Becomes Hive type Note     null void    boolean boolean    int int    long bigint    float float    double double    bytes binary Bytes are converted to Array[smallint] prior to Hive 0.12.0.   string string    record struct    map map    list array    union union Unions of [T, null] transparently convert to nullable T, other types translate directly to Hive\u0026rsquo;s unions of those types. However, unions were introduced in Hive 7 and are not currently able to be used in where/group-by statements. They are essentially look-at-only. Because the AvroSerde transparently converts [T,null], to nullable T, this limitation only applies to unions of multiple types or unions not of a single type and null.   enum string Hive has no concept of enums.   fixed binary Fixeds are converted to Array[smallint] prior to Hive 0.12.0.    Creating Avro-backed Hive tables Avro-backed tables can be created in Hive using AvroSerDe.\nAll Hive versions To create an Avro-backed table, specify the serde as org.apache.hadoop.hive.serde2.avro.AvroSerDe, specify the inputformat as org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat, and the outputformat as org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat. Also provide a location from which the AvroSerde will pull the most current schema for the table. For example:\nCREATE TABLE kst PARTITIONED BY (ds string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' TBLPROPERTIES ( 'avro.schema.url'='http://schema_provider/kst.avsc'); In this example we\u0026rsquo;re pulling the source-of-truth reader schema from a webserver. Other options for providing the schema are described below.\nAdd the Avro files to the database (or create an external table) using standard Hive operations (http://wiki.apache.org/hadoop/Hive/LanguageManual/DML).\nThis table might result in a description as below:\nhive\u0026gt; describe kst; OK string1 string from deserializer string2 string from deserializer int1 int from deserializer boolean1 boolean from deserializer long1 bigint from deserializer float1 float from deserializer double1 double from deserializer inner_record1 struct\u0026lt;int_in_inner_record1:int,string_in_inner_record1:string\u0026gt; from deserializer enum1 string from deserializer array1 array\u0026lt;string\u0026gt; from deserializer map1 map\u0026lt;string,string\u0026gt; from deserializer union1 uniontype\u0026lt;float,boolean,string\u0026gt; from deserializer fixed1 binary from deserializer null1 void from deserializer unionnullint int from deserializer bytes1 binary from deserializer At this point, the Avro-backed table can be worked with in Hive like any other table.\nHive 0.14 and later versions Starting in Hive 0.14, Avro-backed tables can simply be created by using \u0026ldquo;STORED AS AVRO\u0026rdquo; in a DDL statement. AvroSerDe takes care of creating the appropriate Avro schema from the Hive table schema, a big win in terms of Avro usability in Hive.\nFor example:\nCREATE TABLE kst ( string1 string, string2 string, int1 int, boolean1 boolean, long1 bigint, float1 float, double1 double, inner_record1 struct\u0026lt;int_in_inner_record1:int,string_in_inner_record1:string\u0026gt;, enum1 string, array1 array\u0026lt;string\u0026gt;, map1 map\u0026lt;string,string\u0026gt;, union1 uniontype\u0026lt;float,boolean,string\u0026gt;, fixed1 binary, null1 void, unionnullint int, bytes1 binary) PARTITIONED BY (ds string) STORED AS AVRO; This table might result in a description as below:\nhive\u0026gt; describe kst; OK string1 string from deserializer string2 string from deserializer int1 int from deserializer boolean1 boolean from deserializer long1 bigint from deserializer float1 float from deserializer double1 double from deserializer inner_record1 struct\u0026lt;int_in_inner_record1:int,string_in_inner_record1:string\u0026gt; from deserializer enum1 string from deserializer array1 array\u0026lt;string\u0026gt; from deserializer map1 map\u0026lt;string,string\u0026gt; from deserializer union1 uniontype\u0026lt;float,boolean,string\u0026gt; from deserializer fixed1 binary from deserializer null1 void from deserializer unionnullint int from deserializer bytes1 binary from deserializer Writing tables to Avro files The AvroSerde can serialize any Hive table to Avro files. This makes it effectively an any-Hive-type to Avro converter. In order to write a table to an Avro file, you must first create an appropriate Avro schema (except in Hive 0.14.0 and later, as described below). Create as select type statements are not currently supported.\nTypes translate as detailed in the table above. For types that do not translate directly, there are a few items to keep in mind:\n Types that may be null must be defined as a union of that type and Null within Avro. A null in a field that is not so defined will result in an exception during the save. No changes need be made to the Hive schema to support this, as all fields in Hive can be null. Avro Bytes type should be defined in Hive as lists of tiny ints. The AvroSerde will convert these to Bytes during the saving process. Avro Fixed type should be defined in Hive as lists of tiny ints. The AvroSerde will convert these to Fixed during the saving process. Avro Enum type should be defined in Hive as strings, since Hive doesn\u0026rsquo;t have a concept of enums. Ensure that only valid enum values are present in the table – trying to save a non-defined enum will result in an exception.  Hive is very forgiving about types: it will attempt to store whatever value matches the provided column in the equivalent column position in the new table. No matching is done on column names, for instance. Therefore, it is incumbent on the query writer to make sure the target column types are correct. If they are not, Avro may accept the type or it may throw an exception; this is dependent on the particular combination of types.\nExample Consider the following Hive table, which covers all types of Hive data types, making it a good example:\nCREATE TABLE test_serializer(string1 STRING, int1 INT, tinyint1 TINYINT, smallint1 SMALLINT, bigint1 BIGINT, boolean1 BOOLEAN, float1 FLOAT, double1 DOUBLE, list1 ARRAY\u0026lt;STRING\u0026gt;, map1 MAP\u0026lt;STRING,INT\u0026gt;, struct1 STRUCT\u0026lt;sint:INT,sboolean:BOOLEAN,sstring:STRING\u0026gt;, union1 uniontype\u0026lt;FLOAT, BOOLEAN, STRING\u0026gt;, enum1 STRING, nullableint INT, bytes1 BINARY, fixed1 BINARY) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' COLLECTION ITEMS TERMINATED BY ':' MAP KEYS TERMINATED BY '#' LINES TERMINATED BY '\\n' STORED AS TEXTFILE; If the table were backed by a csv file such as:\n| why hello there | 42 | 3 | 100 | 1412341 | true | 42.43 | 85.23423424 | alpha:beta:gamma | Earth#42:Control#86:Bob#31 | 17:true:Abe Linkedin | 0:3.141459 | BLUE | 72 | ^A^B^C | ^A^B^C | | another record | 98 | 4 | 101 | 9999999 | false | 99.89 | 0.00000009 | beta | Earth#101 | 1134:false:wazzup | 1:true | RED | NULL | ^D^E^F^G | ^D^E^F | | third record | 45 | 5 | 102 | 999999999 | true | 89.99 | 0.00000000000009 | alpha:gamma | Earth#237:Bob#723 | 102:false:BNL | 2:Time to go home | GREEN | NULL | ^H | ^G^H^I |\nthen you could write it out to Avro as described below.\nAll Hive versions To save this table as an Avro file, create an equivalent Avro schema (the namespace and actual name of the record are not important):\n{ \u0026quot;namespace\u0026quot;: \u0026quot;com.linkedin.haivvreo\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;test_serializer\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;record\u0026quot;, \u0026quot;fields\u0026quot;: [ { \u0026quot;name\u0026quot;:\u0026quot;string1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;int1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;tinyint1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;smallint1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;bigint1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;long\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;boolean1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;boolean\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;float1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;float\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;double1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;double\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;list1\u0026quot;, \u0026quot;type\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;array\u0026quot;, \u0026quot;items\u0026quot;:\u0026quot;string\u0026quot;} }, { \u0026quot;name\u0026quot;:\u0026quot;map1\u0026quot;, \u0026quot;type\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;map\u0026quot;, \u0026quot;values\u0026quot;:\u0026quot;int\u0026quot;} }, { \u0026quot;name\u0026quot;:\u0026quot;struct1\u0026quot;, \u0026quot;type\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;record\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;struct1_name\u0026quot;, \u0026quot;fields\u0026quot;: [ { \u0026quot;name\u0026quot;:\u0026quot;sInt\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;sBoolean\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;boolean\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;sString\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;string\u0026quot; } ] } }, { \u0026quot;name\u0026quot;:\u0026quot;union1\u0026quot;, \u0026quot;type\u0026quot;:[\u0026quot;float\u0026quot;, \u0026quot;boolean\u0026quot;, \u0026quot;string\u0026quot;] }, { \u0026quot;name\u0026quot;:\u0026quot;enum1\u0026quot;, \u0026quot;type\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;enum\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;enum1_values\u0026quot;, \u0026quot;symbols\u0026quot;:[\u0026quot;BLUE\u0026quot;,\u0026quot;RED\u0026quot;, \u0026quot;GREEN\u0026quot;]} }, { \u0026quot;name\u0026quot;:\u0026quot;nullableint\u0026quot;, \u0026quot;type\u0026quot;:[\u0026quot;int\u0026quot;, \u0026quot;null\u0026quot;] }, { \u0026quot;name\u0026quot;:\u0026quot;bytes1\u0026quot;, \u0026quot;type\u0026quot;:\u0026quot;bytes\u0026quot; }, { \u0026quot;name\u0026quot;:\u0026quot;fixed1\u0026quot;, \u0026quot;type\u0026quot;:{\u0026quot;type\u0026quot;:\u0026quot;fixed\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;threebytes\u0026quot;, \u0026quot;size\u0026quot;:3} } ] } Then you can write it out to Avro with:\nCREATE TABLE as_avro ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED as INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' TBLPROPERTIES ( 'avro.schema.url'='file:///path/to/the/schema/test_serializer.avsc'); INSERT OVERWRITE TABLE as_avro SELECT * FROM test_serializer; Hive 0.14 and later In Hive versions 0.14 and later, you do not need to create the Avro schema manually. The procedure shown above to save a table as an Avro file reduces to just a DDL statement followed by an insert into the table.\nCREATE TABLE as_avro(string1 STRING, int1 INT, tinyint1 TINYINT, smallint1 SMALLINT, bigint1 BIGINT, boolean1 BOOLEAN, float1 FLOAT, double1 DOUBLE, list1 ARRAY\u0026lt;STRING\u0026gt;, map1 MAP\u0026lt;STRING,INT\u0026gt;, struct1 STRUCT\u0026lt;sint:INT,sboolean:BOOLEAN,sstring:STRING\u0026gt;, union1 uniontype\u0026lt;FLOAT, BOOLEAN, STRING\u0026gt;, enum1 STRING, nullableint INT, bytes1 BINARY, fixed1 BINARY) STORED AS AVRO; INSERT OVERWRITE TABLE as_avro SELECT * FROM test_serializer; Avro file extension The files that are written by the Hive job are valid Avro files, however, MapReduce doesn\u0026rsquo;t add the standard .avro extension. If you copy these files out, you\u0026rsquo;ll likely want to rename them with .avro.\nSpecifying the Avro schema for a table There are three ways to provide the reader schema for an Avro table, all of which involve parameters to the serde. As the schema evolves, you can update these values by updating the parameters in the table.\nUse avro.schema.url Specifies a URL to access the schema from. For http schemas, this works for testing and small-scale clusters, but as the schema will be accessed at least once from each task in the job, this can quickly turn the job into a DDOS attack against the URL provider (a web server, for instance). Use caution when using this parameter for anything other than testing.\nThe schema can also point to a location on HDFS, for instance: hdfs://your-nn:9000/path/to/avsc/file. The AvroSerde will then read the file from HDFS, which should provide resiliency against many reads at once. Note that the serde will read this file from every mapper, so it\u0026rsquo;s a good idea to turn the replication of the schema file to a high value to provide good locality for the readers. The schema file itself should be relatively small, so this does not add a significant amount of overhead to the process.\nUse schema.literal and embed the schema in the create statement You can embed the schema directly into the create statement. This works if the schema doesn\u0026rsquo;t have any single quotes (or they are appropriately escaped), as Hive uses this to define the parameter value. For instance:\nCREATE TABLE embedded COMMENT \u0026quot;just drop the schema right into the HQL\u0026quot; ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' TBLPROPERTIES ( 'avro.schema.literal'='{ \u0026quot;namespace\u0026quot;: \u0026quot;com.howdy\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;some_schema\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;record\u0026quot;, \u0026quot;fields\u0026quot;: [ { \u0026quot;name\u0026quot;:\u0026quot;string1\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;string\u0026quot;}] }'); Note that the value is enclosed in single quotes and just pasted into the create statement.\nUse avro.schema.literal and pass the schema into the script Hive can do simple variable substitution and you can pass the schema embedded in a variable to the script. Note that to do this, the schema must be completely escaped (carriage returns converted to \\n, tabs to \\t, quotes escaped, etc). An example:\nset hiveconf:schema; DROP TABLE example; CREATE TABLE example ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat' TBLPROPERTIES ( 'avro.schema.literal'='${hiveconf:schema}'); To execute this script file, assuming $SCHEMA has been defined to be the escaped schema value:\nhive --hiveconf schema=\u0026quot;${SCHEMA}\u0026quot; -f your_script_file.sql Note that $SCHEMA is interpolated into the quotes to correctly handle spaces within the schema.\nUse none to ignore either avro.schema.literal or avro.schema.url Hive does not provide an easy way to unset or remove a property. If you wish to switch from using URL or schema to the other, set the to-be-ignored value to none and the AvroSerde will treat it as if it were not set.\nHBase Integration Hive 0.14.0 onward supports storing and querying Avro objects in HBase columns by making them visible as structs to Hive. This allows Hive to perform ad hoc analysis of HBase data which can be deeply structured. Prior to 0.14.0, the HBase Hive integration only supported querying primitive data types in columns. See Avro Data Stored in HBase Columns for details.\nIf something goes wrong Hive tends to swallow exceptions from the AvroSerde that occur before job submission. To force Hive to be more verbose, it can be started with hive \u0026ndash;hiveconf hive.root.logger=INFO,console, which will spit orders of magnitude more information to the console and will likely include any information the AvroSerde is trying to get you about what went wrong. If the AvroSerde encounters an error during MapReduce, the stack trace will be provided in the failed task log, which can be examined from the JobTracker\u0026rsquo;s web interface. The AvroSerde only emits the AvroSerdeException; look for these. Please include these in any bug reports. The most common is expected to be exceptions while attempting to serializing an incompatible type from what Avro is expecting.\nFAQ  Why do I get error-error-error-error-error-error-error and a message to check avro.schema.literal and avro.schema.url when describing a table or running a query against a table?   The AvroSerde returns this message when it has trouble finding or parsing the schema provided by either the avro.schema.literal or avro.avro.schema.url value. It is unable to be more specific because Hive expects all calls to the serde config methods to be successful, meaning we are unable to return an actual exception. By signaling an error via this message, the table is left in a good state and the incorrect value can be corrected with a call to alter table T set TBLPROPERTIES.\n  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/avroserde_27850707/","tags":null,"title":"Apache Hive : AvroSerDe"},{"categories":null,"contents":"Apache Hive : BecomingACommitter Becoming A Hive Committer The Apache Software Foundation defines generic guidelines for what it means to be a committer. However, it leaves the question of whether a particular contributor is ready to become a committer on a project up to the judgement of that project\u0026rsquo;s PMC. This wiki page attempts to explain what that means for the Hive project.\nCommitter Zen Contributors often ask Hive PMC members the question, \u0026ldquo;What do I need to do in order to become a committer?\u0026rdquo; The simple (though frustrating) answer to this question is, \u0026ldquo;If you want to become a committer, behave like a committer.\u0026rdquo; If you follow this advice, then rest assured that the PMC will notice, and committership will seek you out rather than the other way around. So besides continuing to contribute high-quality code and tests, there are many other things that you should naturally be undertaking as part of getting deeper into the project\u0026rsquo;s life:\n help out users and other developers on the mailing lists, in JIRA, and in IRC review and test the patches submitted by others; this can help to offload the burden on existing committers, who will definitely appreciate your efforts participate in discussions about releases, roadmaps, architecture, and long-term plans help improve the website and the wiki participate in (or even initiate) real-world events such as user/developer meetups, papers/talks at conferences, etc improve project infrastructure in order to increase the efficiency of committers and other contributors help raise the project\u0026rsquo;s quality bar (e.g. by setting up code coverage analysis) as much as possible, keep your activity sustained rather than sporadic  Of course, before becoming a committer, there are certain things you can\u0026rsquo;t actually do (e.g. commit a patch to source control; cast a binding vote), but the more you participate in the activities which surround these actions, the more ready you will be to eventually carry them out yourself.\nVisualize The graph below shows monthly patch authorship and commit activity for an actual Hive committer. The blue shows all commits going into Hive from all committers. The orange shows patches authored by this committer, whereas the green shows patches reviewed and committed by him after they were authored by others. (Not shown are reviews he participated in before becoming a committer.)\n Important points to notice:\n it took a while for him to become a committer: we\u0026rsquo;d like to make sure that all committers are truly dedicated to the role after becoming a committer, he began fulfilling the role by actively reviewing and committing many patches from others (even more than those he continued to author himself!), and sustained that energy over time we\u0026rsquo;re using a narrow quantitative measure here (patch count) purely for the purpose of visualizing activity level over time; what we\u0026rsquo;re really interested in are quality and value brought to the project across a wide range of activities (for example, the committer in this case also volunteered to serve as release manager for multiple releases of Hive, starting even before becoming a committer)  The Dark Side It should go without saying, but here it is anyway: your participation in the project should be a natural part of your work with Hive; if you find yourself undertaking tasks \u0026ldquo;so that you can become a committer\u0026rdquo;, then you\u0026rsquo;re doing it wrong, young padawan. This is particularly true if your motivations for wanting to become a committer are primarily negative or self-centered, e.g.\n you desire the power of a -1 vote (these should be used only extremely rarely in a healthy project) you want to push your own changes through unreviewed (Hive follows a review-before-commit policy where even committers need to wait for a +1 from another committer) you only want to commit changes from other contributors within a particular affiliation group (e.g. coworkers in the same corporation); the committer role is about furthering a diverse project, not a narrow agenda  Attachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/community/becomingcommitter/","tags":null,"title":"Apache Hive : BecomingACommitter"},{"categories":null,"contents":"Apache Hive : Binary DataType Proposal Binary Type in Hive Motivation: Hive is designed to work with big data. Often in such cases, a row in a data might be very wide with hundreds of columns. Sometimes, user is just interested in few of those columns and doesn\u0026rsquo;t want to bother about exact type information for rest of columns. In such cases, he may just declare the types of those columns as binary and Hive will not try to interpret those columns. One important thing to note is this binary type is not modeled after blob type as it exists in other systems.\nSyntax:  create table binary_table (a string, b binary); How is \u0026lsquo;binary\u0026rsquo; represented internally in Hive Binary type in Hive will map to \u0026lsquo;binary\u0026rsquo; data type in thrift. Primitive java object for \u0026lsquo;binary\u0026rsquo; type is ByteArrayRef\nPrimitiveWritableObject for \u0026lsquo;binary\u0026rsquo; type is BytesWritable\nCasting: Binary type will not be coerced into any other type implicitly. Even explicit casting will not be supported. Serialization: String in Hive is serialized by first extracting the underlying bytes of string and then serializing them. Binary type will just piggyback on it and will reuse the same code.\nTransform Scripts: As with other types, binary data will be sent to transform script in String form. byte[] will be first encoded in Base64 format and then a String will be created and sent to the script. Supported Serde: ColumnarSerde\nBinarySortableSerde\nLazyBinaryColumnarSerde LazyBinarySerde\nLazySimpleSerde\nGroup-by and unions will be supported on columns with \u0026lsquo;binary\u0026rsquo; type\nJIRA: https://issues.apache.org/jira/browse/HIVE-2380\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/binary-datatype-proposal_27826614/","tags":null,"title":"Apache Hive : Binary DataType Proposal"},{"categories":null,"contents":"Apache Hive : Books about Hive These books describe Apache Hive and explain how to use its features. If you know of others that should be listed here, or newer editions, please send a message to the Hive user mailing list or add the information yourself if you have wiki edit privileges. Most links go to the publishers although you can also buy most of these books from bookstores, either online or brick-and-mortar.\n Programming Hive by Edward Capriolo, Dean Wampler, and Jason Rutherglen – O\u0026rsquo;Reilly Media, 2012 Apache Hive Essentials by Dayong Du – Packt Publishing, 2015 and 2018 (second edition) Apache Hive Cookbook by Hanish Bansal, Saurabh Chauhan, and Shrey Mehrotra – Packt Publishing, 2016 Instant Apache Hive Essentials How-to by Darren Lee – Packt Publishing, 2013 Practical Hive by Scott Shaw, Andreas François Vermeulen, Ankur Gupta, and David Kjerrumgaard – Apress, 2016 The Ultimate Guide to Programming Apache Hive by Fru Nde – NextGen Publishing, 2015 Learn Hive in 1 Day by Krishna Rungta – independently published, 2017  Books primarily about Hadoop, with some coverage of Hive:\n Hadoop: The Definitive Guide by Tom White (one chapter on Hive) – O\u0026rsquo;Reilly Media, 2009, 2010, 2012, and 2015 (fourth edition) Hadoop in Action by Chuck Lam (one chapter on Hive) – Manning Publications, 2010 Hadoop in Practice by Alex Holmes (one chapter on Hive) – Manning Publications, 2012  Online book:\n The Free Hive Book by Christian Prokopp – GitHub  Related books:\n Apache Iceberg: The Definitive Guide: Data Lakehouse Functionality, Performance, and Scalability on the Data Lake by Tomer Shiran, Jason Hughes, Alex Merced  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/books-about-hive_61322063/","tags":null,"title":"Apache Hive : Books about Hive"},{"categories":null,"contents":"Apache Hive : Books, Blogs \u0026amp; Talks  Books:   Programming Hive by Edward Capriolo, Dean Wampler, and Jason Rutherglen – O\u0026rsquo;Reilly Media, 2012  Apache Hive Essentials by Dayong Du – Packt Publishing, 2015 and 2018 (second edition) Apache Hive Cookbook by Hanish Bansal, Saurabh Chauhan, and Shrey Mehrotra – Packt Publishing, 2016 Instant Apache Hive Essentials How-to by Darren Lee – Packt Publishing, 2013 Practical Hive by Scott Shaw, Andreas François Vermeulen, Ankur Gupta, and David Kjerrumgaard – Apress, 2016 The Ultimate Guide to Programming Apache Hive by Fru Nde – NextGen Publishing, 2015 Learn Hive in 1 Day by Krishna Rungta – independently published, 2017      Books primarily about Hadoop, with some coverage of Hive:\n   Hadoop: The Definitive Guide by Tom White (one chapter on Hive) – O\u0026rsquo;Reilly Media, 2009, 2010, 2012, and 2015 (fourth edition)  Hadoop in Action by Chuck Lam (one chapter on Hive) – Manning Publications, 2010 Hadoop in Practice by Alex Holmes (one chapter on Hive) – Manning Publications, 2012      Online book:\n   The Free Hive Book by Christian Prokopp – GitHub   Blogs:  Apache Hive 4.x With Apache Iceberg (Part-I) by Ayush Saxena Apache Hive-4.x with Iceberg Branches \u0026amp; Tags by Ayush Saxena Data Federation with Apache Hive by Akshat Mathur Apache Hive: ESRI GeoSpatial Support by Ayush Saxena Open Data Lakehouse powered by Iceberg for all your Data Warehouse needs by Zoltan Borok-Nagy, Ayush Saxena, Tamas Mate, Simhadri Govindappa Optimizing Hive on Tez Performance by Jay Desai How to run queries periodically in Apache Hive by Zoltan Haindrich and Jesus Camacho Rodriguez Why We Need Hive Metastore by Pasha Finkelshteyn   Talks:  Apache Hive Replication V3(ApacheCon) by Pravin Kumar Sinha Transactional SQL in Apache Hive by Eugene Koifman Transactional operations in Apache Hive: present and future by Eugene Koifman    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/282102318/","tags":null,"title":"Apache Hive : Books, Blogs \u0026 Talks"},{"categories":null,"contents":"Apache Hive : Building Hive from Source  Fetching the source code  Using the source tar: Download the source tar from [TODO: Put link post release] and untar From Git tag: Checkout the release tag using git clone \u0026ndash;branch rel/release-4.0.0 https://github.com/apache/hive.git   Building Distribution  Run: mvn clean install -DskipTests -Pdist -Piceberg -Pitests Find the built tar under packaging/target/apache-hive-*   Running Unit Tests  Run: mvn clean install -Piceberg   Running Integration Tests  GoTo itests directory Run: mvn clean test -pl itest -Piceberg    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/building-hive-from-source_282102252/","tags":null,"title":"Apache Hive : Building Hive from Source"},{"categories":null,"contents":"Apache Hive : Bylaws Apache Hive Project Bylaws  Apache Hive Project Bylaws  Roles and Responsibilities  Users Committers Submodule Committers Project Management Committee   Decision Making  Voting Approvals Vetoes Actions      This document defines the bylaws under which the Apache Hive project operates. It defines the roles and responsibilities of the project, who may vote, how voting works, how conflicts are resolved, etc.\nHive is a project of the Apache Software Foundation. The foundation holds the copyright on Apache code including the code in the Hive codebase. The foundation FAQ explains the operation and background of the foundation.\nHive is typical of Apache projects in that it operates under a set of principles, known collectively as the \u0026lsquo;Apache Way\u0026rsquo;. If you are new to Apache development, please refer to the Incubator Project for more information on how Apache projects operate.\nRoles and Responsibilities Apache projects define a set of roles with associated rights and responsibilities. These roles govern what tasks an individual may perform within the project. The roles are defined in the following sections.\nUsers The most important participants in the project are people who use our software. The majority of our contributors start out as users and guide their development efforts from the user\u0026rsquo;s perspective.\nUsers contribute to the Apache projects by providing feedback to contributors in the form of bug reports and feature suggestions. Also, users participate in the Apache community by helping other users on mailing lists and user support forums.\nCommitters The project\u0026rsquo;s Committers are responsible for the project\u0026rsquo;s technical management. Committers have access to and responsibility for all of Hive\u0026rsquo;s source code repository.\nCommitter access is by invitation only and must be approved by lazy consensus of the active PMC members. A Committer is considered emeritus by their own declaration or by not contributing in any form to the project for over six months. An emeritus committer may request reinstatement of commit access from the PMC which will be sufficient to restore him or her to active committer status.\nCommit access can be revoked by an unanimous vote of all the active PMC members (except the committer in question if they are also a PMC member).\nSignificant, pervasive features are often developed in a speculative branch of the repository. The PMC may grant commit rights on the branch to its consistent contributors, while the initiative is active. Branch committers are responsible for shepherding their feature into an active release and do not cast binding votes or vetoes in the project.\nAll Apache committers are required to have a signed Individual Contributor License Agreement (CLA) on file with the Apache Software Foundation. There is a Committer FAQ which provides more details on the requirements for Committers.\nA committer who makes a sustained contribution to the project may be invited to become a member of the PMC. The form of contribution is not limited to code. It can also include code review, helping out users on the mailing lists, documentation, etc.\nSubmodule Committers Submodule committers are committers who are responsible for maintenance of a particular submodule of Hive. Committers on submodules have access to and responsibility for a specified subset of Hive\u0026rsquo;s source code repository. Committers on submodules may cast binding votes on any technical discussion regarding that submodule.\nSubmodule committers are not directly created by the PMC. When Hive adopts new code bases, for example by merging in an existing project, committers on that newly adopted code base become committers on the submodules that correspond to the new code base. The intention is that submodule committers will work towards becoming committers. Submodule committers must be voted on by the PMC in the same way as other Hive contributors to become committers.\nAll rules that apply to committers regarding transitioning to emeritus status, revocation of commit rights, and having a signed Individual Contributor License Agreement apply to submodule committers as well.\nProject Management Committee The PMC is responsible to the board and the ASF for the management and oversight of the Apache Hive codebase. The responsibilities of the PMC include\n Deciding what is distributed as products of the Apache Hive project. In particular all releases must be approved by the PMC. Maintaining the project\u0026rsquo;s shared resources, including the codebase repository, mailing lists, websites. Speaking on behalf of the project. Resolving license disputes regarding products of the project. Nominating new PMC members and committers. Maintaining these bylaws and other guidelines of the project.  Membership of the PMC is by invitation only and must be approved by a lazy consensus of active PMC members. A PMC member is considered emeritus by their own declaration or by not contributing in any form to the project for over six months. An emeritus member may request reinstatement to the PMC, which will be sufficient to restore him or her to active PMC member.\nMembership of the PMC can be revoked by an unanimous vote of all the active PMC members other than the member in question.\nThe chair of the PMC is appointed by the ASF board. The chair is an office holder of the Apache Software Foundation (Vice President, Apache Hive) and has primary responsibility to the board for the management of the projects within the scope of the Hive PMC. The chair reports to the board quarterly on developments within the Hive project.\nWhen the current chair of the PMC resigns, the PMC votes to recommend a new chair using lazy consensus, but the decision must be ratified by the Apache board.\nDecision Making Within the Hive project, different types of decisions require different forms of approval. For example, the previous section describes several decisions which require \u0026lsquo;lazy consensus\u0026rsquo; approval. This section defines how voting is performed, the types of approvals, and which types of decision require which type of approval.\nVoting Decisions regarding the project are made by votes on the primary project development mailing list (user@hive.apache.org). Where necessary, PMC voting may take place on the private Hive PMC mailing list. Votes are clearly indicated by subject line starting with [VOTE]. Votes may contain multiple items for approval and these should be clearly separated. Voting is carried out by replying to the vote mail. Voting may take four flavors\n   Vote Meaning     +1 \u0026lsquo;Yes,\u0026rsquo; \u0026lsquo;Agree,\u0026rsquo; or \u0026lsquo;the action should be performed.\u0026rsquo; In general, this vote also indicates a willingness on the behalf of the voter in \u0026lsquo;making it happen\u0026rsquo;.   +0 This vote indicates a willingness for the action under consideration to go ahead. The voter, however will not be able to help.   -0 This vote indicates that the voter does not, in general, agree with the proposed action but is not concerned enough to prevent the action going ahead.   -1 This is a negative vote. On issues where consensus is required, this vote counts as a veto. All vetoes must contain an explanation of why the veto is appropriate. Vetoes with no explanation are void. It may also be appropriate for a -1 vote to include an alternative course of action.    All participants in the Hive project are encouraged to show their agreement with or against a particular action by voting. For technical decisions, only the votes of active committers are binding. Non binding votes are still useful for those with binding votes to understand the perception of an action in the wider Hive community. For PMC decisions, only the votes of PMC members are binding.\nVoting can also be applied to changes already made to the Hive codebase. These typically take the form of a veto (-1) in reply to the commit message sent when the commit is made. Note that this should be a rare occurrence. All efforts should be made to discuss issues when they are still patches before the code is committed.\nApprovals These are the types of approvals that can be sought. Different actions require different types of approvals.\n   Approval Type Definition     Consensus For this to pass, all voters with binding votes must vote and there can be no binding vetoes (-1). Consensus votes are rarely required due to the impracticality of getting all eligible voters to cast a vote.   Lazy Consensus Lazy consensus requires 3 binding +1 votes and no binding vetoes.   Lazy Majority A lazy majority vote requires 3 binding +1 votes and more binding +1 votes that -1 votes.   Lazy Approval An action with lazy approval is implicitly allowed unless a -1 vote is received, at which time, depending on the type of action, either lazy majority or lazy consensus approval must be obtained.   2/3 Majority Some actions require a 2/3 majority of active committers or PMC members to pass. Such actions typically affect the foundation of the project (e.g. adopting a new codebase to replace an existing product). The higher threshold is designed to ensure such changes are strongly supported. To pass this vote requires at least 2/3 of binding vote holders to vote +1.    Vetoes A valid, binding veto cannot be overruled. If a veto is cast, it must be accompanied by a valid reason explaining the reasons for the veto. The validity of a veto, if challenged, can be confirmed by anyone who has a binding vote. This does not necessarily signify agreement with the veto - merely that the veto is valid.\nIf you disagree with a valid veto, you must lobby the person casting the veto to withdraw their veto. If a veto is not withdrawn, the action that has been vetoed must be reversed in a timely manner.\nActions    Actions Description Approval Binding Votes Minimum Length Mailing List     Code Change A change made to a codebase of the project and committed by a committer. This includes source code, documentation, website content, etc. one +1 from a committer who has not authored the patch followed by a Lazy approval (not counting the vote of the contributor), moving to lazy majority if a -1 is receivedMinor issues (e.g. typos, code style issues, JavaDoc changes. At committer\u0026rsquo;s discretion) can be committed after soliciting feedback/review on the mailing list and not receiving feedback within 2 days. Active committers 1 JIRA (dev@hive.apache.org)   Release Plan Defines the timetable and actions for a release. The plan also nominates a Release Manager. Lazy majority Active committers 3 user@hive.apache.org   Product Release When a release of one of the project\u0026rsquo;s products is ready, a vote is required to accept the release as an official release of the project. Lazy Majority Active PMC members 3 user@hive.apache.org   Adoption of New Codebase When the codebase for an existing, released product is to be replaced with an alternative codebase. If such a vote fails to gain approval, the existing code base will continue. This also covers the creation of new sub-projects and submodules within the project. 2/3 majority Active PMC members 6 dev@hive.apache.org   New Committer When a new committer is proposed for the project. Lazy consensus Active PMC members 3 private@hive.apache.org   New PMC Member When a committer is proposed for the PMC. Lazy consensus Active PMC members 3 private@hive.apache.org   Committer Removal When removal of commit privileges is sought. Note: Such actions will also be referred to the ASF board by the PMC chair. Consensus Active PMC members (excluding the committer in question if a member of the PMC). 6 private@hive.apache.org   PMC Member Removal When removal of a PMC member is sought. Note: Such actions will also be referred to the ASF board by the PMC chair. Consensus Active PMC members (excluding the member in question). 6 private@hive.apache.org   Modifying Bylaws Modifying this document. 2/3 majority Active PMC members 6 user@hive.apache.org   New Branch Committer When a new branch committer is proposed for the project. Lazy Consensus Active PMC members 3 private@hive.apache.org   Removal of Branch Committer When a branch committer is removed from the project. Consensus Active PMC members excluding the committer in question if they are PMC members too. 6 private@hive.apache.org    ","date":"12","image":null,"permalink":"https://hive.apache.org/community/bylaws/","tags":null,"title":"Apache Hive : Bylaws"},{"categories":null,"contents":"Apache Hive : CAST\u0026hellip;FORMAT with SQL:2016 datetime formats Usage CAST(\u0026lt;timestamp/date\u0026gt; AS \u0026lt;varchar/char/string\u0026gt; [FORMAT \u0026lt;template\u0026gt;]) CAST(\u0026lt;varchar/char/string\u0026gt; AS \u0026lt;timestamp/date\u0026gt; [FORMAT \u0026lt;template\u0026gt;]) Example select cast(dt as string format 'DD-MM-YYYY') select cast('01-05-2017' as date format 'DD-MM-YYYY') Template elements, a.k.a. Tokens, a.k.a Patterns a.k.a SQL:2016 Datetime Formats Notes For all tokens:\n Patterns are case-insensitive, except AM/PM and T/Z. See these sections for more details. For string to datetime conversion, no duplicate format tokens are allowed, including tokens\nthat have the same meaning but different lengths (\u0026ldquo;Y\u0026rdquo; and \u0026ldquo;YY\u0026rdquo; conflict) or different\nbehaviors (\u0026ldquo;RR\u0026rdquo; and \u0026ldquo;YY\u0026rdquo; conflict).  For all numeric tokens:\n The \u0026ldquo;expected length\u0026rdquo; of input/output is the number of tokens in the character (e.g. \u0026ldquo;YYY\u0026rdquo;: 3,\n\u0026ldquo;Y\u0026rdquo;: 1, and so on), with some exceptions (see map SPECIAL_LENGTHS). For string to datetime conversion, inputs of fewer digits than expected are accepted if\nfollowed by a delimiter, e.g. format=\u0026ldquo;YYYY-MM-DD\u0026rdquo;, input=\u0026ldquo;19-1-1\u0026rdquo;, output=2019-01-01 00:00:00. For datetime to string conversion, output is left padded with zeros, e.g. format=\u0026ldquo;DD SSSSS\u0026rdquo;,\ninput=2019-01-01 00:00:03, output=\u0026ldquo;01 00003\u0026rdquo;.  Accepted format tokens Note: - \u0026ldquo;|\u0026rdquo; means \u0026ldquo;or\u0026rdquo;.\n \u0026ldquo;Delimiter\u0026rdquo; for numeric tokens means any non-numeric character or end of input. The words token and pattern are used interchangeably.  A.1. Numeric temporal tokens YYYY\n4-digit year\n For string to datetime conversion, prefix digits for 1, 2, and 3-digit inputs are obtained\nfrom current date\nE.g. input=‘9-01-01’, pattern =‘YYYY-MM-DD’, current year=2020, output=2029-01-01 00:00:00  YYY\nLast 3 digits of a year\n Gets the prefix digit from current date. Can accept fewer digits than 3, similarly to YYYY.  YY\nLast 2 digits of a year\n Gets the 2 prefix digits from current date. Can accept fewer digits than 2, similarly to YYYY.  Y\nLast digit of a year\n Gets the 3 prefix digits from current date.  RRRR\n4-digit rounded year\n String to datetime conversion: If 2 digits are provided then acts like RR. If 1,3 or 4 digits provided then acts like YYYY. For datetime to string conversion, acts like YYYY.  RR\n2-digit rounded year\n-String to datetime conversion:\n Semantics:\nInput: Last 2 digits of current year: First 2 digits of output:\n0 to 49 00 to 49 First 2 digits of current year\n0 to 49 50 to 99 First 2 digits of current year + 1\n50 to 99 00 to 49 First 2 digits of current year - 1\n50 to 99 50 to 99 First 2 digits of current year If 1-digit year is provided followed by a delimiter, falls back to YYYY with 1-digit year\ninput. For datetime to string conversion, acts like YY.  MM\nMonth (1-12)\n For string to datetime conversion, conflicts with DDD, MONTH, MON.  DD\nDay of month (1-31)\n For string to datetime conversion, conflicts with DDD.  DDD\nDay of year (1-366)\n For string to datetime conversion, conflicts with DD and MM.  HH\nHour of day (1-12)\n If no AM/PM provided then defaults to AM. In string to datetime conversion, conflicts with SSSSS and HH24.  HH12\nHour of day (1-12)\nSee HH.\nHH24\nHour of day (0-23)\n In string to datetime conversion, conflicts with SSSSS, HH12 and AM/PM.  MI\nMinute of hour (0-59)\n In string to datetime conversion, conflicts with SSSSS.  SS\nSecond of minute (0-59)\n In string to datetime conversion, conflicts with SSSSS.  SSSSS\nSecond of Day (0-86399)\n In string to datetime conversion, conflicts with SS, HH, HH12, HH24, MI, AM/PM.  FF[1..9]\nFraction of second\n 1..9 indicates the number of decimal digits. \u0026ldquo;FF\u0026rdquo; (no number of digits specified) is also\naccepted. In datetime to string conversion, \u0026ldquo;FF\u0026rdquo; will omit trailing zeros, or output \u0026ldquo;0\u0026rdquo; if subsecond\nvalue is 0. In string to datetime conversion, fewer digits than expected are accepted if followed by a\ndelimiter. \u0026ldquo;FF\u0026rdquo; acts like \u0026ldquo;FF9\u0026rdquo;.  AM|A.M.|PM|P.M.\nMeridiem indicator (or AM/PM)\n Datetime to string conversion: AM and PM mean the exact same thing in the pattern.\ne.g. input=2019-01-01 20:00, format=“AM”, output=“PM”. Retains the exact format (capitalization and length) provided in the pattern string. If p.m.\nis in the pattern, we expect a.m. or p.m. in the output; if AM is in the pattern, we expect\nAM or PM in the output. If the case is mixed (Am or aM) then the output case will match the\ncase of the pattern\u0026rsquo;s first character (Am =\u0026gt; AM, aM =\u0026gt; am). String to datetime conversion: Conflicts with HH24 and SSSSS. It doesn\u0026rsquo;t matter which meridian indicator is in the pattern.\nE.g. input=\u0026ldquo;2019-01-01 11:00 p.m.\u0026rdquo;, pattern=\u0026ldquo;YYYY-MM-DD HH12:MI AM\u0026rdquo;,\noutput=2019-01-01 23:00:00 If FX is enabled, input length has to match the pattern\u0026rsquo;s length. e.g. pattern=AM input=A.M.\nis not accepted, but input=pm is. Not listed as a character temporal because of special status: does not get padded with spaces\nupon formatting, and case is handled differently at datetime to string conversion.  D\nDay of week (1-7)\n 1 means Sunday, 2 means Monday, and so on. Not allowed in string to datetime conversion.  Q\nQuarter of year (1-4)\n Not allowed in string to datetime conversion.  WW\nAligned week of year (1-53)\n 1st week begins on January 1st and ends on January 7th, and so on. Not allowed in string to datetime conversion.  W\nAligned week of month (1-5)\n 1st week starts on the 1st of the month and ends on the 7th, and so on. Not allowed in string to datetime conversion.  IYYY\n4-digit ISO 8601 week-numbering year\n Returns the year relating to the ISO week number (IW), which is the full week (Monday to\nSunday) which contains January 4 of the Gregorian year. Behaves similarly to YYYY in that for datetime to string conversion, prefix digits for 1, 2,\nand 3-digit inputs are obtained from current week-numbering year. For string to datetime conversion, requires IW and ID|DAY|DY. Conflicts with all other date\npatterns (see \u0026ldquo;List of Date-Based Patterns\u0026rdquo;).  IYY\nLast 3 digits of ISO 8601 week-numbering year\n See IYYY. Behaves similarly to YYY in that for datetime to string conversion, prefix digit is obtained\nfrom current week-numbering year and can accept 1 or 2-digit input.  IY\nLast 2 digits of ISO 8601 week-numbering year\n See IYYY. Behaves similarly to YY in that for datetime to string conversion, prefix digits are obtained\nfrom current week-numbering year and can accept 1-digit input.  I\nLast digit of ISO 8601 week-numbering year\n See IYYY. Behaves similarly to Y in that for datetime to string conversion, prefix digits are obtained\nfrom current week-numbering year.  IW\nISO 8601 week of year (1-53)\n Begins on the Monday closest to January 1 of the year. For string to datetime conversion, if the input week does not exist in the input year, an\nerror will be thrown. e.g. the 2019 week-year has 52 weeks; with pattern=\u0026ldquo;iyyy-iw-id\u0026rdquo;\ninput=\u0026ldquo;2019-53-2\u0026rdquo; is not accepted. For string to datetime conversion, requires IYYY|IYY|IY|I and ID|DAY|DY. Conflicts with all other\ndate patterns (see \u0026ldquo;List of Date-Based Patterns\u0026rdquo;).  ID\nISO 8601 day of week (1-7)\n 1 is Monday, and so on. For string to datetime conversion, requires IYYY|IYY|IY|I and IW. Conflicts with all other\ndate patterns (see \u0026ldquo;List of Date-Based Patterns\u0026rdquo;).  A.2. Character temporals Temporal elements, but spelled out.\n For datetime to string conversion, the pattern\u0026rsquo;s case must match one of the listed formats\n(e.g. mOnTh is not accepted) to avoid ambiguity. Output is right padded with trailing spaces\nunless the pattern is marked with the fill mode modifier (FM). For string to datetime conversion, the case of the pattern does not matter.  MONTH|Month|month\nName of month of year\n For datetime to string conversion, will include trailing spaces up to length 9 (length of\nlongest month of year name: \u0026ldquo;September\u0026rdquo;). Case is taken into account according to the\nfollowing example (pattern =\u0026gt; output): MONTH =\u0026gt; JANUARY Month =\u0026gt; January month =\u0026gt; january For string to datetime conversion, neither the case of the pattern nor the case of the input\nare taken into account. For string to datetime conversion, conflicts with MM and MON.  MON|Mon|mon\nAbbreviated name of month of year\n For datetime to string conversion, case is taken into account according to the following\nexample (pattern =\u0026gt; output): MON =\u0026gt; JAN Mon =\u0026gt; Jan mon =\u0026gt; jan For string to datetime conversion, neither the case of the pattern nor the case of the input\nare taken into account. For string to datetime conversion, conflicts with MM and MONTH.  DAY|Day|day\nName of day of week\n For datetime to string conversion, will include trailing spaces until length is 9 (length of\nlongest day of week name: \u0026ldquo;Wednesday\u0026rdquo;). Case is taken into account according to the following\nexample (pattern =\u0026gt; output): DAY = SUNDAY Day = Sunday day = sunday For string to datetime conversion, neither the case of the pattern nor the case of the input\nare taken into account. Not allowed in string to datetime conversion except with IYYY|IYY|IY|I and IW.  DY|Dy|dy\nAbbreviated name of day of week\n For datetime to string conversion, case is taken into account according to the following\nexample (pattern =\u0026gt; output): DY = SUN Dy = Sun dy = sun For string to datetime conversion, neither the case of the pattern nor the case of the input\nare taken into account. Not allowed in string to datetime conversion except with IYYY|IYY|IY|I and IW.  B. Time zone tokens TZH\nTime zone offset hour (-15 to +15)\n 3-character-long input is expected: 1 character for the sign and 2 digits for the value.\ne.g. “+10”, “-05” 2-digit input is accepted without the sign, e.g. “04”. Both these 2 and 3-digit versions are accepted even if not followed by separators. Disabled for timestamp to string and date to string conversion, as timestamp and date are time\nzone agnostic.  TZM\nTime zone offset minute (0-59)\n For string to datetime conversion: TZH token is required. Unsigned; sign comes from TZH. Therefore time zone offsets like “-30” minutes should be expressed thus: input=“-00:30”\npattern=“TZH:TZM”. Disabled for timestamp to string and date to string conversion, as timestamp and date are time\nzone agnostic.  C. Separators -|.|/|,|'|;|:|Separator\n Uses loose matching. Existence of a sequence of separators in the format should match the\nexistence of a sequence of separators in the input regardless of the types of the separator or\nthe length of the sequence where length \u0026gt; 1. E.g. input=“2019-. ;10/10”, pattern=“YYYY-MM-DD”\nis valid; input=“20191010”, pattern=“YYYY-MM-DD” is not valid. If the last separator character in the separator substring is \u0026ldquo;-\u0026rdquo; and is immediately followed\nby a time zone hour (tzh) token, it\u0026rsquo;s a negative sign and not counted as a separator, UNLESS\nthis is the only possible separator character in the separator substring (in which case it is\nnot counted as the tzh\u0026rsquo;s negative sign). If the whole pattern string is delimited by single quotes (''), then the apostrophe separator\n(') must be escaped with a single backslash: (').  D. ISO 8601 delimiters T|Z\nISO 8601 delimiter\n Serves as a delimiter. Function is to support formats like “YYYY-MM-DDTHH24:MI:SS.FF9Z”, “YYYY-MM-DD-HH24:MI:SSZ” For datetime to string conversion, output is always capitalized (\u0026ldquo;T\u0026rdquo;), even if lowercase (\u0026ldquo;t\u0026rdquo;)\nis provided in the pattern. For string to datetime conversion, case of input and pattern may differ.  E. Nested strings (Text) – Surround with double quotes (\u0026quot;) in the pattern. Note, if the whole pattern string is delimited\nby double quotes, then the double quotes must be escaped with a single backslash: (\u0026quot;).\n In order to include a literal double quote character within the nested string, the double\nquote character must be escaped with a double backslash: (\\”). If the whole pattern string is\ndelimited by double quotes, then escape with a triple backslash: (\\\u0026quot;) If the whole pattern string is delimited by single quotes, literal single\nquotes/apostrophes (') in the nested string must be escaped with a single backslash: (') For datetime to string conversion, we simply include the string in the output, preserving the\ncharacters' case. For string to datetime conversion, the information is lost as the nested string won’t be part\nof the resulting datetime object. However, the nested string has to match the related part of\nthe input string, except case may differ.  F. Format modifier tokens FM\nFill mode modifier\n Default for string to datetime conversion. Inputs of fewer digits than expected are accepted\nif followed by a delimiter:\ne.g. format=\u0026ldquo;YYYY-MM-DD\u0026rdquo;, input=\u0026ldquo;19-1-1\u0026rdquo;, output=2019-01-01 00:00:00 For datetime to string conversion, padding (trailing spaces for text data and leading zeroes\nfor numeric data) is omitted for the temporal element immediately following an \u0026ldquo;FM\u0026rdquo; in the\npattern string. If the element following is not a temporal element (for example, if \u0026ldquo;FM\u0026rdquo;\nprecedes a separator), an error will be thrown.\ne.g. pattern=FMHH12:MI:FMSS, input=2019-01-01 01:01:01, output=1:01:1 Modifies FX so that lack of leading zeroes are accepted for the element immediately following\nan \u0026ldquo;FM\u0026rdquo; in the pattern string.  FX\nFormat exact modifier\n Default for datetime to string conversion. Numeric output is left padded with zeros, and\nnon-numeric output except for AM/PM is right padded with spaces up to expected length. Applies to the whole pattern. Rules applied at string to datetime conversion: Separators must match exactly, down to the character. Numeric input can\u0026rsquo;t omit leading zeroes. This rule does not apply to elements (tokens)\nimmediately preceded by an \u0026ldquo;FM.\u0026rdquo; AM/PM input length has to match the pattern\u0026rsquo;s length. e.g. pattern=AM input=A.M. is not\naccepted, but input=pm is.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/122917025/","tags":null,"title":"Apache Hive : CAST...FORMAT with SQL:2016 datetime formats"},{"categories":null,"contents":"Apache Hive : ChangeLog Release 4.0.0 - 2024-03-29 NEW FEATURES:    JIRA Summary Priority Component Reporter Contributor     HIVE-27850 Iceberg: Major QB Compaction Major Iceberg integration Dmitriy Fingerman Dmitriy Fingerman   HIVE-26222 Native GeoSpatial Support in Hive Major Hive, HiveServer2 mahesh kumar behera Ayush Saxena   HIVE-27980 Hive Iceberg Compaction: add support for OPTIMIZE TABLE syntax Major . Dmitriy Fingerman Dmitriy Fingerman   HIVE-26435 Add method for collecting HMS meta summary Major . Ruyi Zheng Hongdan Zhu    IMPROVEMENTS:    JIRA Summary Priority Component Reporter Contributor     HIVE-26666 Filter out compactions by id to minimise expense of db operations Major . KIRTI RUGE KIRTI RUGE   HIVE-27563 Add typeof UDF Minor UDF John Sherman John Sherman   HIVE-27575 MASK_HASH UDF generate a log per row Major UDF Shohei Okumiya Shohei Okumiya   HIVE-27396 Use -strict argument for Thrift code generation to prevent compatibility issues Major Build Infrastructure Joe McDonnell Joe McDonnell   HIVE-27554 Validate URL used by SSO workflow for JDBC connection Minor JDBC Henri Biestro Henri Biestro   HIVE-27516 Add programatically added DAG scoped properties to DAG Configuration Major . László Bodor László Bodor   HIVE-27586 Parse dates from strings ignoring trailing (potentialy) invalid chars Major HiveServer2 Stamatis Zampetakis Stamatis Zampetakis   HIVE-27578 Refactor genJoinRelNode to use genAllRexNode instead of genAllExprNodeDesc Major . Soumyakanti Das Soumyakanti Das   HIVE-27600 Reduce filesystem calls in OrcFileMergeOperator Minor Hive Yi Zhang Yi Zhang   HIVE-25576 Configurable datetime formatter for unix_timestamp, from_unixtime Major . Ashish Sharma Stamatis Zampetakis   HIVE-27585 Upgrade kryo serialization lib to latest version Minor Serializers/Deserializers Suprith Chandrashekharachar Suprith Chandrashekharachar   HIVE-27595 Improve efficiency in the filtering hooks Minor HiveServer2 Naveen Gangam Henri Biestro   HIVE-27645 Clean test cases by refactoring assertFalse(equals()) using assertNotEquals \u0026amp; @Test(excepted) using assertThrows Minor . Taher Ghaleb Taher Ghaleb   HIVE-27599 ConvertAstToSearchArg improvement with output and typo in comments Major Server Infrastructure xy xy   HIVE-27627 Iceberg: Insert into/overwrite partition support Major . Sourabh Badhya Sourabh Badhya   HIVE-27525 Ease the write permissions on external table during create table operation Major Standalone Metastore Sai Hemanth Gantasala Riju Trivedi   HIVE-27665 Change Filter Parser on HMS to allow backticks Major Standalone Metastore Steve Carlin Steve Carlin   HIVE-27461 HiveMetaStoreAuthorizer should log the root cause of checkPrivileges Major Hive Wechar Wechar   HIVE-27661 Auth mode inferred from the Authorization header Major HiveServer2 Gergely Farkas Gergely Farkas   HIVE-27309 Large number of partitions and small files causes OOM in query coordinator Major Iceberg integration Rajesh Balamohan Dmitriy Fingerman   HIVE-27656 Upgrade jansi.version to 2.4.0 Major . Simhadri Govindappa Simhadri Govindappa   HIVE-27673 Configurable datetime formatter for date_format Major HiveServer2 Stamatis Zampetakis Stamatis Zampetakis   HIVE-27664 AlterTableSetLocationAnalyzer threw a confusing exception \u0026ldquo;Cannot connect to namenode\u0026rdquo; Major . xiongyinke xiongyinke   HIVE-27564 Add log for ZooKeeperTokenStore Major . lvyanquan lvyanquan   HIVE-27694 Include HiveIcebergSerDe in default list of serdes using HMS Minor Standalone Metastore Naveen Gangam Naveen Gangam   HIVE-27696 Docker build from source should include iceberg profile Major . Ayush Saxena Ayush Saxena   HIVE-21100 Allow flattening of table subdirectories resulted when using TEZ engine and UNION clause Major . George Pachitariu Ayush Saxena   HIVE-27646 Iceberg: Retry query when concurrent write queries fail due to conflicting writes Major . Simhadri Govindappa Simhadri Govindappa   HIVE-27672 Iceberg: Truncate partition support Major . Sourabh Badhya Sourabh Badhya   HIVE-27729 Iceberg: Check Iceberg type in AlterTableExecuteAnalyzer Major Iceberg integration Butao Zhang Butao Zhang   HIVE-27406 CompactionTxnHandler cleanup Major Hive László Végh László Végh   HIVE-27752 Remove DagUtils duplicate class Minor . László Bodor Akshat Mathur   HIVE-27757 Upgrade hadoop to 3.3.6 Major . Ayush Saxena Ayush Saxena   HIVE-27723 Prevent localizing the same original file more than once if symlinks are present Major . László Bodor László Bodor   HIVE-27771 Iceberg: Allow expire snapshot by time range Major . Ayush Saxena Ayush Saxena   HIVE-27755 Quote identifiers in SQL emitted by SchemaTool for MySQL Major Standalone Metastore Stamatis Zampetakis Stamatis Zampetakis   HIVE-27793 Iceberg: Support setting current snapshot with SnapshotRef Major Iceberg integration Butao Zhang Butao Zhang   HIVE-27799 Bump org.apache.zookeeper:zookeeper from 3.7.1 to 3.7.2 Major . Ayush Saxena    HIVE-27182 tez_union_with_udf.q with TestMiniTezCliDriver is flaky Major . Ayush Saxena Stamatis Zampetakis   HIVE-27686 Use ORC 1.8.5. Major . Zoltán Rátkai Zoltán Rátkai   HIVE-27802 Simplify TestTezSessionState.testSymlinkedLocalFilesAreLocalizedOnce Major . László Bodor László Bodor   HIVE-27786 Iceberg: Eliminate engine.hive.enabled table property Major . Ayush Saxena Ayush Saxena   HIVE-27346 Getting exception for wildcard (*) search for database and table name Major . Hongdan Zhu Hongdan Zhu   HIVE-27731 Perform metadata delete when only static filters are present Major . Sourabh Badhya Sourabh Badhya   HIVE-27826 Upgrade to Parquet 1.13.1 Major Parquet Butao Zhang Butao Zhang   HIVE-27780 Implement direct SQL for get_all_functions Major Standalone Metastore Butao Zhang Butao Zhang   HIVE-27825 Better error message for an empty quoted identifier Minor Parser Shohei Okumiya Shohei Okumiya   HIVE-24815 Remove \u0026ldquo;IDXS\u0026rdquo; Table from Metastore Schema Major Metastore, Standalone Metastore Hunter Logan Riju Trivedi   HIVE-27779 Iceberg: Drop partition support Major . Sourabh Badhya Sourabh Badhya   HIVE-27789 Iceberg: Add a way to expire snapshots with retain last Major . Ayush Saxena Ayush Saxena   HIVE-27593 Iceberg: Keep iceberg properties in sync with hms properties Major Iceberg integration Butao Zhang Butao Zhang   HIVE-27864 Update plugin for SBOM generation to 2.7.10 Major . Vinod Anandan Vinod Anandan   HIVE-27762 Don\u0026rsquo;t fall back to jdo query in ObjectStore if direct sql throws unrecoverable exception Major Hive Wechar Wechar   HIVE-27819 Iceberg: Upgrade iceberg version to 1.4.2 Major Iceberg integration Butao Zhang Butao Zhang   HIVE-27869 Iceberg: Select on HadoopTable fails at HiveIcebergStorageHandler#canProvideColStats Major Iceberg integration Butao Zhang Butao Zhang   HIVE-27828 Iceberg integration: enable copy on write update when split update is on Major Iceberg integration Krisztian Kasa Krisztian Kasa   HIVE-27877 Bump org.apache.avro:avro from 1.11.1 to 1.11.3 Major . Ayush Saxena    HIVE-27889 Bump org.apache.santuario:xmlsec from 2.3.0 to 2.3.4 Major . Ayush Saxena    HIVE-27871 Fix some formatting problems is YarnQueueHelper Major . László Bodor Mahesh Raju Somalaraju   HIVE-27896 Remove common-lang usage Major . Cheng Pan Cheng Pan   HIVE-27906 Iceberg: Implement Delete Orphan Files Major . Ayush Saxena Ayush Saxena   HIVE-27912 Include Iceberg module in nightly builds Major . Butao Zhang Butao Zhang   HIVE-27903 Iceberg: Implement Expire Snapshot with default table properties Major Hive JK Pasimuthu Ayush Saxena   HIVE-27633 HMS: MTable to Table process reduces view related SQL Minor Metastore dzcxzl dzcxzl   HIVE-26630 Introduce copy-on-write DML implementations for Iceberg tables Major . Ádám Szita Denys Kuzmenko   HIVE-27918 Iceberg: Push transforms for clustering during table writes Major . Sourabh Badhya Sourabh Badhya   HIVE-27894 Enhance HMS Handler Logs for all \u0026lsquo;get_partition\u0026rsquo; functions. Major . Shivangi Jha Shivangi Jha   HIVE-27905 Some GenericUDFs wrongly cast ObjectInspectors Major . Shohei Okumiya Shohei Okumiya   HIVE-27935 Add qtest for Avro invalid schema and field names Major . Akshat Mathur Akshat Mathur   HIVE-27856 Change the default value of hive.optimize.cte.materialize.threshold to -1 Major . Seonggon Namgung Seonggon Namgung   HIVE-27936 Disable flaky test testBootstrapAcidTablesDuringIncrementalWithConcurrentWrites Major Testing Infrastructure Butao Zhang Butao Zhang   HIVE-27803 Bump org.apache.avro:avro from 1.11.1 to 1.11.3 Major . Ayush Saxena    HIVE-27934 Fix incorrect description about the execution framework in README.md Minor . Butao Zhang Butao Zhang   HIVE-27690 Handle casting NULL literal to complex type Major . Krisztian Kasa Krisztian Kasa   HIVE-27919 Constant reduction in CBO does not work for FROM_UNIXTIME, DATE_ADD, DATE_SUB, TO_UNIX_TIMESTAMP Major CBO Stamatis Zampetakis Stamatis Zampetakis   HIVE-23558 Remove compute_stats UDAF Major Statistics Jesús Camacho Rodríguez Butao Zhang   HIVE-27961 Beeline will print duplicate stats info when hive.tez.exec.print.summary is true Minor . Butao Zhang Butao Zhang   HIVE-27530 Implement direct SQL for alter partitions to improve performance Major . Wechar Wechar   HIVE-27925 HiveConf: unify ConfVars enum and use underscore for better readability Major . László Bodor Kokila N   HIVE-27969 Add verbose logging for schematool and metastore service for Docker container Major . Akshat Mathur Akshat Mathur   HIVE-27978 Tests in hive-unit module are not running again Major . László Bodor László Bodor   HIVE-27911 Drop database query failing with Invalid ACL Exception Major . KIRTI RUGE KIRTI RUGE   HIVE-27937 Clarifying comments and xml configs around tez container size Major . László Bodor László Bodor   HIVE-27977 Fix ordering flakiness in TestHplSqlViaBeeLine Major . László Bodor László Bodor   HIVE-27023 Add setting to disable hive session being started during Hive Cli startup Minor . Alagappan Maruthappan Alagappan Maruthappan   HIVE-27827 Improve performance of direct SQL implement for getPartitionsByFilter Major . Wechar Wechar   HIVE-27994 Optimize renaming the partitioned table Major . Zhihua Deng Zhihua Deng   HIVE-27979 HMS alter_partitions log adds table name Minor Standalone Metastore dzcxzl dzcxzl   HIVE-28008 ParquetFileReader is not closed in ParquetHiveSerDe.readSchema Minor Hive Michal Lorek Michal Lorek   HIVE-27991 Utilise FanoutWriters when inserting records in an Iceberg table when the records are unsorted Major . Sourabh Badhya Sourabh Badhya   HIVE-27984 Support backward compatibility of hms thrift struct about column stats Major Standalone Metastore Butao Zhang Butao Zhang   HIVE-27481 TxnHandler cleanup Major Hive László Végh László Végh   HIVE-28038 Disable fallback to jdo for DeadlineException Major Hive Wechar Wechar   HIVE-27958 Refactor DirectSqlUpdatePart class Major Hive Wechar Wechar   HIVE-27992 Upgrade to tez 0.10.3 Major . László Bodor László Bodor   HIVE-26445 Use tez.local.mode.without.network for qtests Major . László Bodor László Bodor   HIVE-27022 Split removeDuplicateCompletedTxnComponents away from AcidHouseKeeper and onto a separate timer Major . Jason Phelps Taraka Rama Rao Lethavadla   HIVE-28071 Sync jetty version across modules Minor . Raghav Aggarwal Raghav Aggarwal   HIVE-28015 Iceberg: Add identifier-field-ids support in Hive Major Iceberg integration Denys Kuzmenko Butao Zhang   HIVE-28081 Code refine on ClearDanglingScratchDir::removeLocalTmpFiles Trivial . Butao Zhang Butao Zhang   HIVE-28064 Add cause to ParseException for diagnosability purposes Major Parser Stamatis Zampetakis Stamatis Zampetakis   HIVE-28056 Bump com.jayway.jsonpath:json-path from 2.8.0 to 2.9.0 Major . László Bodor László Bodor   HIVE-28078 TestTxnDbUtil should generate csv files when we query the metastore database Major Standalone Metastore Zsolt Miskolczi Zsolt Miskolczi   HIVE-27692 Explore removing the always task from embedded HMS Major Standalone Metastore Zhihua Deng Zhihua Deng   HIVE-27405 Throw out the detail error Invalid partition name to the clients Major HiveServer2 Taraka Rama Rao Lethavadla Taraka Rama Rao Lethavadla   HIVE-27845 Upgrade protobuf to 3.24.4 to fix CVEs Major . Akshat Mathur tanishqchugh   HIVE-28080 Propagate statistics from a source table to the materialized CTE Major Query Planning Shohei Okumiya Shohei Okumiya   HIVE-28090 correct desc of hive.metastore.disallow.incompatible.col.type.changes Major . zzzzming95 zzzzming95   HIVE-28083 Enable HMS client/query cache for EXPLAIN queries Minor Hive Soumyakanti Das Soumyakanti Das   HIVE-25972 HIVE_VECTORIZATION_USE_ROW_DESERIALIZE in hiveconf.java imply default value is false，in fact the default value is \u0026lsquo;true\u0026rsquo; Major Configuration, Hive lkl Kokila N   HIVE-28093 Re-execute DAG in case of NoCurrentDAGException Major . László Bodor László Bodor   HIVE-28051 LLAP: cleanup local folders on startup and periodically Major . László Bodor László Bodor    BUG FIXES:    JIRA Summary Priority Component Reporter Contributor     HIVE-26837 CTLT with hive.create.as.external.legacy as true creates managed table instead of external table Major HiveServer2 Ramesh Kumar Thangarajan Ramesh Kumar Thangarajan   HIVE-27562 Iceberg: Fetching virtual columns failing Major . Ayush Saxena Ayush Saxena   HIVE-27487 NPE in Hive JDBC storage handler Major JDBC storage handler Zhihua Deng Zhihua Deng   HIVE-27582 Do not cache HBase table input format in FetchOperator Major . Ganesha Shreedhara Ganesha Shreedhara   HIVE-27304 Exclude CTAS condition while forming storage handler url permissions in HS2 authorizer. Major Hive Sai Hemanth Gantasala Sai Hemanth Gantasala   HIVE-24771 Fix hang of TransactionalKafkaWriterTest Major . Zoltan Haindrich Kokila N   HIVE-27631 Fix CCE when set fs.hdfs.impl other than DistributedFileSystem Major Hive Baolong Mao Baolong Mao   HIVE-22961 Drop function in Hive should not send request for drop database to Ranger plugin. Major Hive Sam An Riju Trivedi   HIVE-27463 Non-daemon thread prevents HMS from exiting when failed to start thrift server Minor . Zhihua Deng Zhihua Deng   HIVE-27536 Merge task must be invoked after optimisation for external CTAS queries Major . Sourabh Badhya Sourabh Badhya   HIVE-27566 Fix some yarn cluster options for tests Major . László Bodor László Bodor   HIVE-27632 ClassCast Exception in Vectorization converting decimal64 to decimal Major Vectorization Riju Trivedi Stephen Carlin   HIVE-27539 Drop renamed external table fails when hive.metastore.try.direct.sql.ddl is disabled Major . Venugopal Reddy K Venugopal Reddy K   HIVE-21213 Acid table bootstrap replication needs to handle directory created by compaction with txn id Major Hive, HiveServer2, repl mahesh kumar behera mahesh kumar behera   HIVE-27667 Fix get partitions with max_parts Major Metastore Yuming Wang Yuming Wang   HIVE-27657 Change hive.fetch.task.conversion.threshold default value Major . Mayank Kunwar Mayank Kunwar   HIVE-26961 Fix improper replication metric count when hive.repl.filter.transactions is set to true. Major . Rakshith C Rakshith C   HIVE-27642 StartMiniHS2Cluster fails to run due to missing JDBC driver with Postgres Major HiveServer2 Zoltán Rátkai Zoltán Rátkai   HIVE-27643 Exclude compaction queries from ranger policies Critical . László Végh László Végh   HIVE-17350 metrics errors when retrying HS2 startup Major . Sergey Shelukhin Mayank Kunwar   HIVE-27648 CREATE TABLE with CHECK constraint fails with SemanticException Major Hive Soumyakanti Das Krisztian Kasa   HIVE-24606 Multi-stage materialized CTEs can lose intermediate data Major Query Planning Shohei Okumiya Shohei Okumiya   HIVE-27138 MapJoinOperator throws NPE when computing OuterJoin with filter expressions on small table Blocker . Seonggon Namgung Seonggon Namgung   HIVE-27675 Support keystore/truststore types for hive to zookeeper integration points Major HiveServer2, JDBC, Standalone Metastore Naveen Gangam Naveen Gangam   HIVE-27730 Bump org.xerial.snappy:snappy-java from 1.1.10.1 to 1.1.10.4 Major . Ayush Saxena    HIVE-27738 SchemaTool picks incorrect schema script after 4.0.0-beta-1 release Major . KIRTI RUGE KIRTI RUGE   HIVE-27649 Support ORDER BY clause in subqueries with set operators Major Parser Nicolas Richard Nicolas Richard   HIVE-27760 WHERE condition on DATE type partitioning column leads to wrong results Major HiveServer2 Dayakar M Dayakar M   HIVE-27728 Changed behavior for alter table rename partition from legacy tables Major . Naveen Gangam Zhihua Deng   HIVE-27733 Intermittent ConcurrentModificationException in HiveServer2 Major HiveServer2 Henri Biestro Henri Biestro   HIVE-27764 Authentication does not work behind Knox gateway because the \u0026ldquo;WWW-Authenticate: Negotiate\u0026rdquo; response header is missing Major HiveServer2 Gergely Farkas Gergely Farkas   HIVE-27695 Intermittent OOM when running TestMiniTezCliDriver Major Test Stamatis Zampetakis Stamatis Zampetakis   HIVE-26828 Fix OOM for hybridgrace_hashjoin_2.q Major Test, Tez Alessandro Solimando Stamatis Zampetakis   HIVE-27798 Correct configuration item in hive-site.xml in docker. Major . 易霖威 易霖威   HIVE-27772 UNIX_TIMESTAMP should return NULL when date fields are out of bounds Major . Simhadri Govindappa Simhadri Govindappa   HIVE-27682 AlterTableAlterPartitionOperation cannot change the type if the column has default partition Minor HiveServer2 Zhihua Deng Zhihua Deng   HIVE-27777 CBO fails on multi insert overwrites with common group expression Major HiveServer2 Steve Carlin Steve Carlin   HIVE-27651 Upgrade hbase version Major . Ayush Saxena Butao Zhang   HIVE-27324 Hive query with NOT IN condition is giving incorrect results when the sub query table contains the null value. Major Hive Shobika Selvaraj Diksha   HIVE-27114 Provide a configurable filter for removing useless properties in Partition objects from listPartitions HMS Calls Major . Naresh P R Zhihua Deng   HIVE-27113 Increasing default for hive.thrift.client.max.message.size to 2 GB Major Hive Riju Trivedi Riju Trivedi   HIVE-27846 Tests under hive-unit module are not running Major Testing Infrastructure Stamatis Zampetakis Stamatis Zampetakis   HIVE-27849 Replication tests using ivy fail after the upgrade to 2.5.2 Major . Stamatis Zampetakis Stamatis Zampetakis   HIVE-27866 JDBC: HttpRequestInterceptorBase should not add an empty \u0026ldquo;Cookie:\u0026rdquo; header to the request if no custom cookies have been specified Major JDBC Gergely Farkas Gergely Farkas   HIVE-27862 Map propertyContent to a wrong column in package.jdo Major . Zhihua Deng Zhihua Deng   HIVE-27679 Ranger Yarn Queue policies are not applying correctly, rework done for HIVE-26352 Major . Mahesh Raju Somalaraju Mahesh Raju Somalaraju   HIVE-27865 HMS in http mode shuts down silently with no errors Major . Zhihua Deng Zhihua Deng   HIVE-27885 Cast decimal from string with space without digits before dot returns NULL Major . Naresh P R Naresh P R   HIVE-27867 Incremental materialized view throws NPE whew Iceberg source table is empty Major . Krisztian Kasa Krisztian Kasa   HIVE-27662 Incorrect parsing of nested complex types containing map during vectorized text processing Major Vectorization Raghav Aggarwal Raghav Aggarwal   HIVE-27890 Tez Progress bar is not displayed in Beeline upon setting session level execution engine to Tez Major Beeline Shivangi Jha Shivangi Jha   HIVE-27713 Iceberg: metadata location overrides can cause data breach Major Authorization, Iceberg integration Janos Kovacs Ayush Saxena   HIVE-27797 Transactions that got timed out are not getting logged as \u0026lsquo;ABORTED\u0026rsquo; in NOTIFICATION_LOG Major repl, Transactions Taraka Rama Rao Lethavadla Taraka Rama Rao Lethavadla   HIVE-27093 Fix NPE in initialize() of Partition class Critical . Wechar Wechar   HIVE-27893 Add a range validator in hive.metastore.batch.retrieve.max to only have values greater than 0 Major . Vikram Ahuja Vikram Ahuja   HIVE-27240 NPE on Hive Hook Proto Log Writer Critical Hive Shubham Sharma Shubham Sharma   HIVE-27658 Error resolving join keys during conversion to dynamic partition hashjoin Major Query Planning xiaojunxiang Stamatis Zampetakis   HIVE-27555 Upgrade issues with Kudu table on backend db Critical . Zhihua Deng Zhihua Deng   HIVE-24730 Shims classes override values from hive-site.xml and tez-site.xml silently Major . László Bodor László Bodor   HIVE-27446 Exception when rebuild materialized view incrementally in presence of delete operations Major CBO, Materialized views Krisztian Kasa Krisztian Kasa   HIVE-27801 Exists subquery rewrite results in a wrong plan Critical . Denys Kuzmenko Denys Kuzmenko   HIVE-27943 NPE in VectorMapJoinCommonOperator.setUpHashTable when running query with join on date Major Query Processor Stamatis Zampetakis Stamatis Zampetakis   HIVE-27930 Insert/Load overwrite table partition does not clean up directory before overwriting Major . Kiran Velumuri Kiran Velumuri   HIVE-27892 Hive \u0026ldquo;insert overwrite table\u0026rdquo; for multiple partition table issue Major . Mayank Kunwar Mayank Kunwar   HIVE-24219 Disable flaky TestStreaming Major . Peter Varga Stamatis Zampetakis   HIVE-27952 Hive fails to create SslContextFactory when KeyStore has multiple certificates Major . Seonggon Namgung Seonggon Namgung   HIVE-27876 Incorrect query results on tables with ClusterBy \u0026amp; SortBy Major . Naresh P R Ramesh Kumar Thangarajan   HIVE-27963 Build failure when license-maven-plugin downloads bsd-license.php Major Hive Akshat Mathur Akshat Mathur   HIVE-27161 MetaException when executing CTAS query in Druid storage handler Critical Druid integration Stamatis Zampetakis Krisztian Kasa   HIVE-25803 URL Mapping appends default Fs scheme even for LOCAL DIRECTORY ops Critical Authorization, HiveServer2 Soumitra Sulav Ayush Saxena   HIVE-27804 Implement batching in getPartition calls which returns partition list along with auth info Major . Vikram Ahuja Vikram Ahuja   HIVE-27967 Iceberg: Fix dynamic runtime filtering Major Iceberg integration Denys Kuzmenko Denys Kuzmenko   HIVE-27966 Disable flaky testFetchResultsOfLogWithOrientation Major Hive Wechar Wechar   HIVE-27916 Increase tez.am.resource.memory.mb for TestIcebergCliDrver to 512MB Major Test László Bodor László Bodor   HIVE-27948 Wrong results when using materialized views with non-deterministic/dynamic functions Critical CBO, Materialized views Stamatis Zampetakis Krisztian Kasa   HIVE-27988 Do not convert FullOuterJoin with filter to MapJoin Critical HiveServer2 Denys Kuzmenko Seonggon Namgung   HIVE-27951 hcatalog dynamic partitioning fails with partition already exist error when exist parent partitions path Critical HCatalog Yi Zhang Yi Zhang   HIVE-27974 Fix flaky test - TestReplicationMetricCollector.testSuccessStageFailure Major HiveServer2 Zsolt Miskolczi Zsolt Miskolczi   HIVE-27989 Wrong database name in MetaException from MetastoreDefaultTransformer.java Minor . Riza Suminto Butao Zhang   HIVE-27857 Do not check write permission while dropping external table or partition Major Hive Wechar Wechar   HIVE-27492 HPL/SQL built-in functions like sysdate not working Major hpl/sql Dayakar M Dayakar M   HIVE-27999 Run Sonar analysis using Java 17 Major . Wechar Wechar   HIVE-21520 Query \u0026ldquo;Submit plan\u0026rdquo; time reported is incorrect Trivial . Rajesh Balamohan Butao Zhang   HIVE-27749 SchemaTool initSchema fails on Mariadb 10.2 Critical Standalone Metastore Stamatis Zampetakis Sourabh Badhya   HIVE-28013 No space left on device when running precommit tests Blocker Testing Infrastructure Stamatis Zampetakis Stamatis Zampetakis   HIVE-27960 Invalid function error when using custom udaf Major Hive gaoxiong gaoxiong   HIVE-27489 HPL/SQL does not support table aliases on column names in loops Major hpl/sql Dayakar M Dayakar M   HIVE-28009 Shared work optimizer ignores schema merge setting in case of virtual column difference Major Query Planning Krisztian Kasa Krisztian Kasa   HIVE-26713 StringExpr ArrayIndexOutOfBoundsException with LIKE \u0026lsquo;%xxx%\u0026rsquo; Major storage-api Ryu Kobayashi Ryu Kobayashi   HIVE-27942 Missing aux jar errors during LLAP launch Minor Hive, llap Shubham Sharma Shubham Sharma   HIVE-27993 Netty4 ShuffleHandler: should use 1 boss thread Major llap Anmol Sundaram Anmol Sundaram   HIVE-27914 Fix the missing partitions judgement in drop_partitions_req Major . Wechar Wechar   HIVE-28017 Add generated protobuf code Major . Ayush Saxena Ayush Saxena   HIVE-28000 Hive QL : \u0026ldquo;not in\u0026rdquo; clause gives incorrect results when type coercion cannot take place. Major Hive Anmol Sundaram Anmol Sundaram   HIVE-27938 Iceberg: Fix java.lang.ClassCastException during vectorized reads on partition columns Major . Simhadri Govindappa Simhadri Govindappa   HIVE-28004 DELETE on ACID table failed with NoClassDefFoundError: com/sun/tools/javac/util/List Blocker Transactions Butao Zhang László Végh   HIVE-28053 Incorrect shading configuration for beeline jar-with-dependencies Major Beeline Stamatis Zampetakis Stamatis Zampetakis   HIVE-26818 Beeline module misses transitive dependencies due to shading Major Beeline Stamatis Zampetakis Stamatis Zampetakis   HIVE-28052 Iceberg: Major QB Compaction fails with ClassNotFoundException: org.springframework.core.ErrorCoded Major . Butao Zhang Ayush Saxena   HIVE-28054 SemanticException for join condition in subquery Major Hive Soumyakanti Das Soumyakanti Das   HIVE-28057 Iceberg: Branches with non-lowercase characters can\u0026rsquo;t be accessed Major . Attila Turoczy Ayush Saxena   HIVE-28050 Disable Incremental non aggregated materialized view rebuild in presence of delete operations Major Materialized views Krisztian Kasa Krisztian Kasa   HIVE-28065 Upgrade Bouncy castle to bcprov-jdk18on 1.77 Major . Araika Singh Araika Singh   HIVE-27950 STACK UDTF returns wrong results when # of argument is not a multiple of N Major . Shohei Okumiya Shohei Okumiya   HIVE-27778 Alter table command gives error after computer stats is run with Impala Major . Kokila N Zhihua Deng   HIVE-27924 Incremental rebuild goes wrong when inserts and deletes overlap between the source tables Critical Materialized views Wenhao Li Krisztian Kasa   HIVE-28021 Iceberg: Attempting to create a table with a percent symbol fails Minor Iceberg integration Tim Thorpe Tim Thorpe   HIVE-28084 Iceberg: COW fix for Merge operation Major . Denys Kuzmenko Denys Kuzmenko   HIVE-27490 HPL/SQL says it support default value for parameters but not considering them when no value is passed Major hpl/sql Dayakar M Dayakar M   HIVE-27775 DirectSQL and JDO results are different when fetching partitions by timestamp in DST shift Critical Standalone Metastore Stamatis Zampetakis Zhihua Deng   HIVE-28073 Upgrade jackson version to 2.16.1 Major . Araika Singh Araika Singh   HIVE-28102 Iceberg: Invoke validateDataFilesExist for RowDelta operations Major . Zoltán Borók-Nagy Ayush Saxena   HIVE-28076 Selecting data from a bucketed table with decimal column type throwing NPE. Major HiveServer2 Dayakar M Dayakar M   HIVE-28123 Add Generated Protobuf code for 3.24.4 Upgrade Major . Indhumathi Muthumurugesh Indhumathi Muthumurugesh   HIVE-28034 HiveConf: unify ConfVars enum and use underscore for better readability - pt2 leftovers Major . László Bodor Kokila N    TESTS:    JIRA Summary Priority Component Reporter Contributor     HIVE-27431 Clean invalid properties in test module Minor Test Butao Zhang Butao Zhang   HIVE-27747 Generalize TestSchemaToolForMetastore to run on every supported DBMS Major Tests Stamatis Zampetakis Stamatis Zampetakis   HIVE-27745 Add unit test to ensure short version is inline with full version Major . KIRTI RUGE KIRTI RUGE   HIVE-28001 Fix the flaky test TestLeaderElection Major . Zhihua Deng Zhihua Deng   HIVE-27556 Add Unit Test for KafkaStorageHandlerInfo Major kafka integration, StorageHandler Kokila N Kokila N    SUB-TASKS:    JIRA Summary Priority Component Reporter Contributor     HIVE-27031 Iceberg: Implement Copy-On-Write for Delete queries Major . Ayush Saxena Ayush Saxena   HIVE-27523 Implement array_union UDF in Hive Major . Taraka Rama Rao Lethavadla Taraka Rama Rao Lethavadla   HIVE-27277 Set up github actions workflow to build and push docker image to docker hub Major . Simhadri Govindappa Simhadri Govindappa   HIVE-27630 Iceberg: Fast forward branch Major . Denys Kuzmenko Ayush Saxena   HIVE-27654 Iceberg: Cherry-Pick commit to a branch Major . Ayush Saxena Ayush Saxena   HIVE-27670 Failed to build the image locally on Apple silicon Major . Zhihua Deng Zhihua Deng   HIVE-27322 Iceberg: metadata location overrides can cause data breach - custom location to AuthZ Blocker Iceberg integration Janos Kovacs Ayush Saxena   HIVE-27716 Precommit: Save log files for first 10 failures Major . László Bodor László Bodor   HIVE-27711 Allow creating a branch from tag name Major . Ayush Saxena Butao Zhang   HIVE-27702 Remove PowerMock from beeline and upgrade mockito to 4.11 Major HiveServer2 Zsolt Miskolczi Mayank Kunwar   HIVE-27736 Remove PowerMock from itests-jmh and upgrade mockito Major . Ayush Saxena Zsolt Miskolczi   HIVE-27705 Remove PowerMock from service (hive-service) and upgrade mockito to 4.11 Major HiveServer2 Zsolt Miskolczi KIRTI RUGE   HIVE-27701 Remove PowerMock from llap-client and upgrade mockito to 4.11 Major HiveServer2 Zsolt Miskolczi Zsolt Miskolczi   HIVE-27704 Remove PowerMock from jdbc-handler and upgrade mockito to 4.11 Major HiveServer2 Zsolt Miskolczi KIRTI RUGE   HIVE-26455 Remove PowerMockito from hive-exec Minor Hive Zsolt Miskolczi Zsolt Miskolczi   HIVE-27399 Add lateral view support for CBO Major HiveServer2 Steve Carlin Steve Carlin   HIVE-27597 Implement JDBC Connector for HiveServer Major Hive Naveen Gangam Naveen Gangam   HIVE-27783 Iceberg: Implement Copy-On-Write for Update queries Major . Denys Kuzmenko Denys Kuzmenko   HIVE-27006 ParallelEdgeFixer inserts misconfigured operator and does not connect it in Tez DAG Major . Seonggon Namgung Seonggon Namgung   HIVE-26621 TPC-DS query 2 fails with java.lang.RuntimeException: cannot find field _col0 from [] Major . Sungwoo Park Seonggon Namgung   HIVE-27794 Iceberg: Implement Copy-On-Write for Merge queries Major . Denys Kuzmenko Denys Kuzmenko   HIVE-27269 VectorizedMapJoin returns wrong result for TPC-DS query 97 Critical . Seonggon Namgung Seonggon Namgung   HIVE-27714 Iceberg: metadata location overrides can cause data breach - handling default locations Critical Authorization, Iceberg integration Janos Kovacs Ayush Saxena   HIVE-27926 Iceberg: Allow restricting Iceberg data file reads to table location Blocker Iceberg integration Janos Kovacs Ayush Saxena   HIVE-27955 Missing Postgres driver when start services from Docker compose Major . Zhihua Deng Zhihua Deng   HIVE-28016 Iceberg: NULL column values handling in COW mode Major . Denys Kuzmenko Denys Kuzmenko   HIVE-27880 Iceberg: Support creating a branch on an empty table Major Iceberg integration Butao Zhang Butao Zhang   HIVE-27929 Run TPC-DS queries and validate results correctness Major . Denys Kuzmenko Simhadri Govindappa    OTHER:    JIRA Summary Priority Component Reporter Contributor     HIVE-27589 Iceberg: Branches of Merge/Update statements should be committed atomically Major . Denys Kuzmenko Simhadri Govindappa   HIVE-27638 Preparing for 4.0.0-beta-2 development Major . Stamatis Zampetakis Stamatis Zampetakis   HIVE-22618 Fix checkstyle violations for ParseUtils Minor Query Processor Shohei Okumiya Shohei Okumiya   HIVE-27558 HBase table query does not push BETWEEN predicate to storage layer Major . Denys Kuzmenko Dayakar M   HIVE-27526 Cleaner should honor compaction writeIdHwm Major . Denys Kuzmenko Denys Kuzmenko   HIVE-27687 Logger variable should be static final as its creation takes more time in query compilation Major Hive Ramesh Kumar Thangarajan Ramesh Kumar Thangarajan   HIVE-27843 Add QueryOperation to Hive proto logger for post execution hook information Major . Ramesh Kumar Thangarajan Ramesh Kumar Thangarajan   HIVE-27907 Upgrade aws-java-sdk version in HIVE Minor . Devaspati Krishnatri Devaspati Krishnatri   HIVE-27824 Upgrade ivy to 2.5.2 and htmlunit to 2.70.0 Major Hive Devaspati Krishnatri Devaspati Krishnatri   HIVE-28005 Remove upgrade-acid module Major . Ayush Saxena Butao Zhang   HIVE-28030 LLAP util code refactor Major . Denys Kuzmenko Denys Kuzmenko   HIVE-28020 Iceberg: Upgrade iceberg version to 1.4.3 Major . Simhadri Govindappa Simhadri Govindappa   HIVE-28043 Upgrade ZooKeeper to 3.8.3 Major . Anmol Sundaram Anmol Sundaram   HIVE-28063 Drop PerfLogger#setPerfLogger method and unused fields/methods Major Hive, Standalone Metastore Stamatis Zampetakis Stamatis Zampetakis    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/changelog_283118275/","tags":null,"title":"Apache Hive : ChangeLog"},{"categories":null,"contents":"Apache Hive : Column Statistics in Hive *** Introduction\n HiveQL changes Metastore Schema Metastore Thrift API**  Introduction This document describes changes to a) HiveQL, b) metastore schema, and c) metastore Thrift API to support column level statistics in Hive. Please note that the document doesn’t describe the changes needed to persist histograms in the metastore yet.\nVersion information\nColumn statistics are introduced in Hive 0.10.0 by HIVE-1362. This is the design document.\nColumn statistics auto gather is introduced in Hive 2.3 by HIVE-11160. This is also the design document.\nFor general information about Hive statistics, see Statistics in Hive. For information about top K statistics, see Column Level Top K Statistics.\nHiveQL changes HiveQL currently supports the analyze command to compute statistics on tables and partitions. HiveQL’s analyze command will be extended to trigger statistics computation on one or more column in a Hive table/partition. The necessary changes to HiveQL are as below,\nanalyze table t [partition p] compute statistics for [columns c,...];\nPlease note that table and column aliases are not supported in the analyze statement.\nTo view column stats :\ndescribe formatted [table_name] [column_name]; Metastore Schema To persist column level statistics, we propose to add the following new tables,\nCREATE TABLE TAB_COL_STATS\n(\nCS_ID NUMBER NOT NULL,\nTBL_ID NUMBER NOT NULL,\nCOLUMN_NAME VARCHAR(128) NOT NULL,\nCOLUMN_TYPE VARCHAR(128) NOT NULL,\nTABLE_NAME VARCHAR(128) NOT NULL,\nDB_NAME VARCHAR(128) NOT NULL,\nLOW_VALUE RAW,\nHIGH_VALUE RAW,\nNUM_NULLS BIGINT,\nNUM_DISTINCTS BIGINT,\nBIT_VECTOR, BLOB, /* introduced in HIVE-16997 in Hive 3.0.0 */\nAVG_COL_LEN DOUBLE,\nMAX_COL_LEN BIGINT,\nNUM_TRUES BIGINT,\nNUM_FALSES BIGINT,\nLAST_ANALYZED BIGINT NOT NULL)\nALTER TABLE COLUMN_STATISTICS ADD CONSTRAINT COLUMN_STATISTICS_PK PRIMARY KEY (CS_ID);\nALTER TABLE COLUMN_STATISTICS ADD CONSTRAINT COLUMN_STATISTICS_FK1 FOREIGN KEY (TBL_ID) REFERENCES TBLS (TBL_ID) INITIALLY DEFERRED ;\nCREATE TABLE PART_COL_STATS\n(\nCS_ID NUMBER NOT NULL,\nPART_ID NUMBER NOT NULL,\nDB_NAME VARCHAR(128) NOT NULL,\nCOLUMN_NAME VARCHAR(128) NOT NULL,\nCOLUMN_TYPE VARCHAR(128) NOT NULL,\nTABLE_NAME VARCHAR(128) NOT NULL,\nPART_NAME VARCHAR(128) NOT NULL,\nLOW_VALUE RAW,\nHIGH_VALUE RAW,\nNUM_NULLS BIGINT,\nNUM_DISTINCTS BIGINT,\nBIT_VECTOR, BLOB, /* introduced in HIVE-16997 in Hive 3.0.0 */\nAVG_COL_LEN DOUBLE,\nMAX_COL_LEN BIGINT,\nNUM_TRUES BIGINT,\nNUM_FALSES BIGINT,\nLAST_ANALYZED BIGINT NOT NULL)\nALTER TABLE COLUMN_STATISTICS ADD CONSTRAINT COLUMN_STATISTICS_PK PRIMARY KEY (CS_ID);\nALTER TABLE COLUMN_STATISTICS ADD CONSTRAINT COLUMN_STATISTICS_FK1 FOREIGN KEY (PART_ID) REFERENCES PARTITIONS (PART_ID) INITIALLY DEFERRED;\nMetastore Thrift API We propose to add the following Thrift structs to transport column statistics:\nstruct BooleanColumnStatsData {\n1: required i64 numTrues,\n2: required i64 numFalses,\n3: required i64 numNulls\n}\nstruct DoubleColumnStatsData {\n1: required double lowValue,\n2: required double highValue,\n3: required i64 numNulls,\n4: required i64 numDVs,\n5: optional string bitVectors\n}\nstruct LongColumnStatsData {\n1: required i64 lowValue,\n2: required i64 highValue,\n3: required i64 numNulls,\n4: required i64 numDVs,\n5: optional string bitVectors\n}\nstruct StringColumnStatsData {\n1: required i64 maxColLen,\n2: required double avgColLen,\n3: required i64 numNulls,\n4: required i64 numDVs,\n5: optional string bitVectors\n}\nstruct BinaryColumnStatsData {\n1: required i64 maxColLen,\n2: required double avgColLen,\n3: required i64 numNulls\n}\nstruct Decimal {\n1: required binary unscaled,\n3: required i16 scale\n}\nstruct DecimalColumnStatsData {\n1: optional Decimal lowValue,\n2: optional Decimal highValue,\n3: required i64 numNulls,\n4: required i64 numDVs,\n5: optional string bitVectors\n}\nstruct Date {\n1: required i64 daysSinceEpoch\n}\nstruct DateColumnStatsData {\n1: optional Date lowValue,\n2: optional Date highValue,\n3: required i64 numNulls,\n4: required i64 numDVs,\n5: optional string bitVectors\n}\nunion ColumnStatisticsData {\n1: BooleanColumnStatsData booleanStats,\n2: LongColumnStatsData longStats,\n3: DoubleColumnStatsData doubleStats,\n4: StringColumnStatsData stringStats,\n5: BinaryColumnStatsData binaryStats,\n6: DecimalColumnStatsData decimalStats,\n7: DateColumnStatsData dateStats\n}\nstruct ColumnStatisticsObj {\n1: required string colName,\n2: required string colType,\n3: required ColumnStatisticsData statsData\n}\nstruct ColumnStatisticsDesc {\n1: required bool isTblLevel,\n2: required string dbName,\n3: required string tableName,\n4: optional string partName,\n5: optional i64 lastAnalyzed\n}\nstruct ColumnStatistics {\n1: required ColumnStatisticsDesc statsDesc,\n2: required liststatsObj;\n}\nWe propose to add the following Thrift APIs to persist, retrieve and delete column statistics:\nbool update_table_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,\n2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)\nbool update_partition_column_statistics(1:ColumnStatistics stats_obj) throws (1:NoSuchObjectException o1,\n2:InvalidObjectException o2, 3:MetaException o3, 4:InvalidInputException o4)\nColumnStatistics get_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws\n(1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidInputException o3, 4:InvalidObjectException o4)\nColumnStatistics get_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name,\n4:string col_name) throws (1:NoSuchObjectException o1, 2:MetaException o2,\n3:InvalidInputException o3, 4:InvalidObjectException o4)\nbool delete_partition_column_statistics(1:string db_name, 2:string tbl_name, 3:string part_name, 4:string col_name) throws\n(1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,\n4:InvalidInputException o4)\nbool delete_table_column_statistics(1:string db_name, 2:string tbl_name, 3:string col_name) throws\n(1:NoSuchObjectException o1, 2:MetaException o2, 3:InvalidObjectException o3,\n4:InvalidInputException o4)\nNote that delete_column_statistics is needed to remove the entries from the metastore when a table is dropped. Also note that currently Hive doesn’t support drop column.\nNote that in V1 of the project, we will support only scalar statistics. Furthermore, we will support only static partitions, i.e., both the partition key and partition value should be specified in the analyze command. In a following version, we will add support for height balanced histograms as well as support for dynamic partitions in the analyze command for column level statistics.\nComments:            Shreepadma, is there a jira for this ? Is this ready for review, or is it a initial design ?   Also, can you go over https://issues.apache.org/jira/browse/HIVE-3421 and see how the two are related ?    Posted by namit.jain at Sep 14, 2012 00:51 | | Namit, This patch is ready for review. There is already a JIRA for this - HIVE-1362. I\u0026rsquo;ve the patch on both JIRA and reviewboard. Please note that this goes beyond HIVE-3421 - this patch adds the stats specified on both this wiki and the JIRA page. Thanks.\nPosted by shreepadma at Oct 03, 2012 00:46 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/column-statistics-in-hive_29131019/","tags":null,"title":"Apache Hive : Column Statistics in Hive"},{"categories":null,"contents":"Apache Hive : Common Table Expression A Common Table Expression (CTE) is a temporary result set derived from a simple query specified in a WITH clause, which immediately precedes a SELECT or INSERT keyword. The CTE is defined only within the execution scope of a single statement. One or more CTEs can be used in a Hive SELECT, INSERT, CREATE TABLE AS SELECT, or CREATE VIEW AS SELECT statement.\nVersion\nCommon Table Expressions are added in Hive 0.13.0 with HIVE-1180.\nCommon Table Expression Syntax withClause: cteClause (, cteClause)* cteClause: cte_name AS (select statment) Additional Grammar Rules  The WITH clause is not supported within SubQuery Blocks CTEs are supported in Views, CTAS and INSERT statements. Recursive Queries are not supported.  Examples CTE in Select Statements with q1 as ( select key from src where key = '5') select * from q1; -- from style with q1 as (select * from src where key= '5') from q1 select *; -- chaining CTEs with q1 as ( select key from q2 where key = '5'), q2 as ( select key from src where key = '5') select * from (select key from q1) a; -- union example with q1 as (select * from src where key= '5'), q2 as (select * from src s2 where key = '4') select * from q1 union all select * from q2; CTE in Views, CTAS, and Insert Statements -- insert example create table s1 like src; with q1 as ( select key, value from src where key = '5') from q1 insert overwrite table s1 select *; -- ctas example create table s2 as with q1 as ( select key from src where key = '4') select * from q1; -- view example create view v1 as with q1 as ( select key from src where key = '5') select * from q1; select * from v1; -- view example, name collision create view v1 as with q1 as ( select key from src where key = '5') select * from q1; with q1 as ( select key from src where key = '4') select * from v1; In the second View example, a query\u0026rsquo;s CTE is different from the CTE used when creating the view. The result will contain rows with key = \u0026lsquo;5\u0026rsquo; because in the view\u0026rsquo;s query statement the CTE defined in the view definition takes effect.\nAlso see this JIRA:\n HIVE-1180 Support Common Table Expressions (CTEs) in Hive  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/common-table-expression_38572242/","tags":null,"title":"Apache Hive : Common Table Expression"},{"categories":null,"contents":"Apache Hive : Compaction pooling Concept: Compaction requests and workers can be assigned to pools. A worker assigned to a specific pool will only process compaction requests in that pool. Workers and compaction requests without pool assignment are implicitly belong to the default pool. The pooling concept allows fine tuning of processing compaction requests. For example it is possible to create a pool name \u0026lsquo;high priority compaction\u0026rsquo;, assign some frequently modified tables to it, and dedicate a set of workers to this pool. As a result, the compaction requests for these tables will be immediately picked up by the dedicated workers, even if there are several other compaction requests (enqueued earlier) in the default queue.\nPool assignment Compaction requests can be assigned to pools in three different ways.\nAutomatic pool assignment Databases, tables and partitions can be assigned to compaction pools through the\nhive.compactor.worker.pool={pool_name} Database/Table property. If the property is set on Database level, it applies to all tables and partitions. The pool also can be assigned on a table/partition level, in this case it overrides the Database level value (if set). If any of the above is set, it is used by the Initiator during the creation of the compaction requests.\nManual pool assignment The compaction request also can be assigned to a pool by using the ALTER TABLE COMPACT command (E.g. manual compaction). If provided, this value overrides the hive.compactor.worker.pool value on any level.\nALTER TABLE COMPACT table_name POOL 'pool_name'; Implicit pool assignment Tables, partitions and manual compaction requests without specified pool name are implicitly assigned to the default pool.\nPool timeout If a compaction request is not processed by any of the labeled pools within a predefined period, it falls back to the default pool. The timeout can be set through the hive.compactor.worker.pool.timeout configuration property. This approach is to cover the following use cases:\n The request is accidentally assigned to a non-existent pool. (E.g.: Typo in the pool name when issuing an ALTER TABLE COMPACT command. Typo in the DB or table property used by the initiator to create compaction requests. A HS2 instance is stopped due to a scaledown or schedule, and its pending compaction requests still should be processed.  The timeout can be disabled by setting the configuration property to 0.\nLabeled worker pools The labeled worker pools can be defined through the hive.compactor.worker.{poolname}.threads={thread_count} configuration setting. Please note that in this case the configuration key is also dynamic.\nDefault pool The default pool is responsible for processing the non-labeled and timed-out compaction requests. On a cluster-wide level, at least 1 worker thread on at least one node should be assigned to the default pool, otherwise compaction requests may never be processed.\nWorker allocation The already existing hive.compactor.worker.threads configuration value holds the maximum number of worker threads. The worker allocation happens as follows:\n Labeled pools are initialized in a sequential manner with random ordering. Each pool decreases the number of available workers by its own worker count. If the number of assignable workers is less than the configured one, the pool size will be adjusted (In other words: if the requested pool size is 5 but there are only 3 workers remaining, than the pool size will be decreased to 3). If the number of assignable workers is 0, the pool won’t be initialized. All remaining workers not used up by the labeled pools, are assigned to the default pool.  The worker allocation can be configured per HS2 instance.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/compaction-pooling_240884493/","tags":null,"title":"Apache Hive : Compaction pooling"},{"categories":null,"contents":"Apache Hive : CompressedStorage Compressed Data Storage Keeping data compressed in Hive tables has, in some cases, been known to give better performance than uncompressed storage; both in terms of disk usage and query performance.\nYou can import text files compressed with Gzip or Bzip2 directly into a table stored as TextFile. The compression will be detected automatically and the file will be decompressed on-the-fly during query execution. For example:\nCREATE TABLE raw (line STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'; LOAD DATA LOCAL INPATH '/tmp/weblogs/20090603-access.log.gz' INTO TABLE raw; The table \u0026lsquo;raw\u0026rsquo; is stored as a TextFile, which is the default storage. However, in this case Hadoop will not be able to split your file into chunks/blocks and run multiple maps in parallel. This can cause underutilization of your cluster\u0026rsquo;s \u0026lsquo;mapping\u0026rsquo; power.\nThe recommended practice is to insert data into another table, which is stored as a SequenceFile. A SequenceFile can be split by Hadoop and distributed across map jobs whereas a GZIP file cannot be. For example:\nCREATE TABLE raw (line STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' LINES TERMINATED BY '\\n'; CREATE TABLE raw_sequence (line STRING) STORED AS SEQUENCEFILE; LOAD DATA LOCAL INPATH '/tmp/weblogs/20090603-access.log.gz' INTO TABLE raw; SET hive.exec.compress.output=true; SET io.seqfile.compression.type=BLOCK; -- NONE/RECORD/BLOCK (see below) INSERT OVERWRITE TABLE raw_sequence SELECT * FROM raw; The value for io.seqfile.compression.type determines how the compression is performed. Record compresses each value individually while BLOCK buffers up 1MB (default) before doing compression.\nLZO Compression See LZO Compression for information about using LZO with Hive.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/compressedstorage_27362073/","tags":null,"title":"Apache Hive : CompressedStorage"},{"categories":null,"contents":"Apache Hive : Configuration Properties Hive Configuration Properties  Hive Configuration Properties  Query and DDL Execution  Datetime SerDes and I/O  SerDes I/O   File Formats  RCFile Format ORC File Format Parquet Avro   Vectorization   MetaStore  Hive Metastore Connection Pooling Configuration Hive Metastore HBase   HiveServer2  HiveServer2 Web UI   Spark  Remote Spark Driver   Tez LLAP  LLAP Client LLAP Web Services LLAP Cache LLAP I/O LLAP CBO LLAP Metrics LLAP UDF Security LLAP Security   Transactions and Compactor  Transactions Compactor Compaction History   Indexing Statistics  Runtime Filtering   Authentication and Authorization  Restricted/Hidden/Internal List and Whitelist  Whitelist for SQL Standard Based Hive Authorization   Hive Client Security Hive Metastore Security SQL Standard Based Authorization   Archiving Locking Metrics Clustering Regions Command Line Interface HBase StorageHandler Hive Web Interface (HWI) (component removed as of Hive 2.2.0) Replication Blobstore (i.e. Amazon S3) Test Properties   HCatalog Configuration Properties WebHCat Configuration Properties  This document describes the Hive user configuration properties (sometimes called parameters, variables, or options), and notes which releases introduced new properties.\nThe canonical list of configuration properties is managed in the HiveConf Java class, so refer to the HiveConf.java file for a complete list of configuration properties available in your Hive release.\nFor information about how to use these configuration properties, see Configuring Hive. That document also describes administrative configuration properties for setting up Hive in the Configuration Variables section. Hive Metastore Administration describes additional configuration properties for the metastore.\nVersion information\nAs of Hive 0.14.0 ( HIVE-7211 ), a configuration name that starts with \u0026ldquo;hive.\u0026rdquo; is regarded as a Hive system property. With the hive.conf.validation option true (default), any attempts to set a configuration property that starts with \u0026ldquo;hive.\u0026rdquo; which is not registered to the Hive system will throw an exception.\nQuery and DDL Execution hive.execution.engine  Default Value: mr (deprecated in Hive 2.0.0 – see below) Added In: Hive 0.13.0 with HIVE-6103 and HIVE-6098  Chooses execution engine. Options are: mr (Map Reduce, default), tez (Tez execution, for Hadoop 2 only), or spark (Spark execution, for Hive 1.1.0 onward).\nWhile mr remains the default engine for historical reasons, it is itself a historical engine and is deprecated in the Hive 2 line (HIVE-12300). It may be removed without further warning.\nSee Hive on Tez and Hive on Spark for more information, and see the Tez section and the Spark section below for their configuration properties.\nhive.execution.mode  Chooses whether query fragments will run in container or in llap Default Value: container Valid settings  container: launch containers llap: utilize llap nodes during execution of tasks   Added In: Hive 2.0 with HIVE-9460  mapred.reduce.tasks  Default Value: -1 Added In: Hive 0.1.0  The default number of reduce tasks per job. Typically set to a prime close to the number of available hosts. Ignored when mapred.job.tracker is \u0026ldquo;local\u0026rdquo;. Hadoop set this to 1 by default, whereas Hive uses -1 as its default value. By setting this property to -1, Hive will automatically figure out what should be the number of reducers.\nhive.exec.reducers.bytes.per.reducer  Default Value: 1,000,000,000 prior to Hive 0.14.0; 256 MB (256,000,000 ) in Hive 0.14.0 and later Added In: Hive 0.2.0; default changed in 0.14.0 with HIVE-7158 (and HIVE-7917)  Size per reducer. The default in Hive 0.14.0 and earlier is 1 GB, that is, if the input size is 10 GB then 10 reducers will be used. In Hive 0.14.0 and later the default is 256 MB, that is, if the input size is 1 GB then 4 reducers will be used.\nhive.exec.reducers.max  Default Value: 999 prior to Hive 0.14.0; 1009 in Hive 0.14.0 and later Added In: Hive 0.2.0; default changed in 0.14.0 with HIVE-7158 (and HIVE-7917)  Maximum number of reducers that will be used. If the one specified in the configuration property mapred.reduce.tasks is negative, Hive will use this as the maximum number of reducers when automatically determining the number of reducers.\nhive.jar.path  Default Value: (empty) Added In: Hive 0.2.0 or earlier  The location of hive_cli.jar that is used when submitting jobs in a separate jvm.\nhive.aux.jars.path  Default Value: (empty) Added In: Hive 0.2.0 or earlier  The location of the plugin jars that contain implementations of user defined functions (UDFs) and SerDes.\nhive.reloadable.aux.jars.path  Default Value: (empty) Added In: Hive 0.14.0 with HIVE-7553  The locations of the plugin jars, which can be comma-separated folders or jars. They can be renewed (added, removed, or updated) by executing the Beeline reload command without having to restart HiveServer2. These jars can be used just like the auxiliary classes in hive.aux.jars.path for creating UDFs or SerDes.\nhive.exec.scratchdir  Default Value: /tmp/${user.name} in Hive 0.2.0 through 0.8.0; /tmp/hive-${user.name} in Hive 0.8.1 through 0.14.0; or /tmp/hive in Hive 0.14.0 and later Added In: Hive 0.2.0; default changed in 0.8.1 and in 0.14.0 with HIVE-6847 and HIVE-8143  Scratch space for Hive jobs. This directory is used by Hive to store the plans for different map/reduce stages for the query as well as to stored the intermediate outputs of these stages.\nHive 0.14.0 and later: HDFS root scratch directory for Hive jobs, which gets created with write all (733) permission. For each connecting user, an HDFS scratch directory ${hive.exec.scratchdir}/is created with ${ hive.scratch.dir.permission }.\nAlso see hive.start.cleanup.scratchdir and hive.scratchdir.lock . When running Hive in local mode, see hive.exec.local.scratchdir.\nhive.scratch.dir.permission  Default Value: 700 Added In: Hive 0.12.0 with HIVE-4487  The permission for the user-specific scratch directories that get created in the root scratch directory. (See hive.exec.scratchdir.)\nhive.exec.local.scratchdir  Default Value: /tmp/${user.name} Added In: Hive 0.10.0 with HIVE-1577  Scratch space for Hive jobs when Hive runs in local mode. Also see hive.exec.scratchdir.\nhive.hadoop.supports.splittable.combineinputformat  Default Value: false Added In: Hive 0.6.0 with HIVE-1280 Removed In: Hive 2.0.0 with HIVE-11376  Whether to combine small input files so that fewer mappers are spawned.\nhive.map.aggr  Default Value: true in Hive 0.3 and later; false in Hive 0.2 Added In: Hive 0.2.0  Whether to use map-side aggregation in Hive Group By queries.\nhive.groupby.skewindata  Default Value: false Added In: Hive 0.3.0  Whether there is skew in data to optimize group by queries.\nhive.groupby.mapaggr.checkinterval  Default Value: 100000 Added In: Hive 0.3.0  Number of rows after which size of the grouping keys/aggregation classes is performed.\nhive.new.job.grouping.set.cardinality  Default Value: 30 Added In: Hive 0.11.0 with HIVE-3552  Whether a new map-reduce job should be launched for grouping sets/rollups/cubes.\nFor a query like \u0026ldquo;select a, b, c, count(1) from T group by a, b, c with rollup;\u0026rdquo; four rows are created per row: (a, b, c), (a, b, null), (a, null, null), (null, null, null). This can lead to explosion across the map-reduce boundary if the cardinality of T is very high, and map-side aggregation does not do a very good job.\nThis parameter decides if Hive should add an additional map-reduce job. If the grouping set cardinality (4 in the example above) is more than this value, a new MR job is added under the assumption that the orginal \u0026ldquo;group by\u0026rdquo; will reduce the data size.\nhive.mapred.local.mem  Default Value: 0 Added In: Hive 0.3.0  For local mode, memory of the mappers/reducers.\nhive.map.aggr.hash.force.flush.memory.threshold  Default Value: 0.9 Added In: Hive 0.7.0 with HIVE-1830  The maximum memory to be used by map-side group aggregation hash table. If the memory usage is higher than this number, force to flush data.\nhive.map.aggr.hash.percentmemory  Default Value: 0.5 Added In: Hive 0.2.0  Portion of total memory to be used by map-side group aggregation hash table.\nhive.map.aggr.hash.min.reduction  Default Value: 0.5 Added In: Hive 0.4.0  Hash aggregation will be turned off if the ratio between hash table size and input rows is bigger than this number. Set to 1 to make sure hash aggregation is never turned off.\nhive.optimize.groupby  Default Value: true Added In: Hive 0.5.0  Whether to enable the bucketed group by from bucketed partitions/tables.\nhive.optimize.countdistinct  Default Value: true Added In: Hive 3.0.0 with HIVE-16654  Whether to rewrite count distinct into 2 stages, i.e., the first stage uses multiple reducers with the count distinct key and the second stage uses a single reducer without key.\nhive.optimize.remove.sq_count_check  Default Value: false Added In: Hive 3.0.0 with HIVE-16793  Whether to remove an extra join with sq_count_check UDF for scalar subqueries with constant group by keys. hive.multigroupby.singlemr  Default Value: false Added In: Hive 0.8.0 with HIVE-2056 Removed In: Hive 0.9.0 by HIVE-2621 (see hive.multigroupby.singlereducer )  Whether to optimize multi group by query to generate a single M/R job plan. If the multi group by query has common group by keys, it will be optimized to generate a single M/R job. (This configuration property was removed in release 0.9.0.)\nhive.multigroupby.singlereducer  Default Value: true Added In: Hive 0.9.0 with HIVE-2621  Whether to optimize multi group by query to generate a single M/R job plan. If the multi group by query has common group by keys, it will be optimized to generate a single M/R job.\nhive.optimize.cp  Default Value: true Added In: Hive 0.4.0 with HIVE-626 Removed In: Hive 0.13.0 with HIVE-4113  Whether to enable column pruner. (This configuration property was removed in release 0.13.0.)\nhive.optimize.index.filter  Default Value: false Added In: Hive 0.8.0 with HIVE-1644  Whether to enable automatic use of indexes.\nNote: See Indexing for more configuration properties related to Hive indexes.\nhive.optimize.ppd  Default Value: true Added In: Hive 0.4.0 with HIVE-279, default changed to true in Hive 0.4.0 with HIVE-626  Whether to enable predicate pushdown (PPD). Note: Turn on hive.optimize.index.filter as well to use file format specific indexes with PPD.\nhive.optimize.ppd.storage  Default Value: true Added In: Hive 0.7.0  Whether to push predicates down into storage handlers. Ignored when hive.optimize.ppd is false.\nhive.ppd.remove.duplicatefilters  Default Value: true Added In: Hive 0.8.0  During query optimization, filters may be pushed down in the operator tree. If this config is true, only pushed down filters remain in the operator tree, and the original filter is removed. If this config is false, the original filter is also left in the operator tree at the original place.\nhive.ppd.recognizetransivity  Default Value: true Added In: Hive 0.8.1  Whether to transitively replicate predicate filters over equijoin conditions.\nhive.join.emit.interval  Default Value: 1000 Added In: Hive 0.2.0  How many rows in the right-most join operand Hive should buffer before\nemitting the join result.\nhive.join.cache.size  Default Value: 25000 Added In: Hive 0.5.0  How many rows in the joining tables (except the streaming table)\nshould be cached in memory.\nhive.mapjoin.bucket.cache.size  Default Value: 100 Added In: Hive 0.5.0 (replaced by hive.smbjoin.cache.rows in Hive 0.12.0)  How many values in each key in the map-joined table should be cached in memory.\nhive.mapjoin.followby.map.aggr.hash.percentmemory  Default Value: 0.3 Added In: Hive 0.7.0 with HIVE-1830  Portion of total memory to be used by map-side group aggregation hash table, when this group by is followed by map join.\nhive.smalltable.filesize orhive.mapjoin.smalltable.filesize  Default Value: 25000000 Added In: Hive 0.7.0 with HIVE-1642: hive.smalltable.filesize (replaced by hive.mapjoin.smalltable.filesize in Hive 0.8.1) Added In: Hive 0.8.1 with HIVE-2499: hive.mapjoin.smalltable.filesize  The threshold (in bytes) for the input file size of the small tables; if the file size is smaller than this threshold, it will try to convert the common join into map join.\nhive.mapjoin.localtask.max.memory.usage  Default Value: 0.90 Added In: Hive 0.7.0 with HIVE-1808 and HIVE-1642  This number means how much memory the local task can take to hold the key/value into an in-memory hash table. If the local task\u0026rsquo;s memory usage is more than this number, the local task will be aborted. It means the data of small table is too large to be held in memory.\nhive.mapjoin.followby.gby.localtask.max.memory.usage  Default Value: 0.55 Added In: Hive 0.7.0 with HIVE-1830  This number means how much memory the local task can take to hold the key/value into an in-memory hash table when this map join is followed by a group by. If the local task\u0026rsquo;s memory usage is more than this number, the local task will abort by itself. It means the data of the small table is too large to be held in memory.\nhive.mapjoin.check.memory.rows  Default Value: 100000 Added In: Hive 0.7.0 with HIVE-1808 and HIVE-1642  The number means after how many rows processed it needs to check the memory usage.\nhive.ignore.mapjoin.hint  Default Value: true Added In: Hive 0.11.0 with HIVE-4042  Whether Hive ignores the mapjoin hint.\nhive.smbjoin.cache.rows  Default Value: 10000 Added In: Hive 0.12.0 with HIVE-4440 (replaces hive.mapjoin.bucket.cache.size )  How many rows with the same key value should be cached in memory per sort-merge-bucket joined table.\nhive.mapjoin.optimized.keys  Default Value: true Added In: Hive 0.13.0 with HIVE-6429 and HIVE-6188 Removed In: Hive 1.1.0 with HIVE-9331  Whether a MapJoin hashtable should use optimized (size-wise) keys, allowing the table to take less memory. Depending on the key, memory savings for the entire table can be 5-15% or so.\nhive.mapjoin.optimized.hashtable  Default Value: true Added In: Hive 0.14.0 with HIVE-6430  Whether Hive should use a memory-optimized hash table for MapJoin. Only works on Tez and Spark, because memory-optimized hash table cannot be serialized. (Spark is supported starting from Hive 1.3.0, with HIVE-11180.)\nhive.mapjoin.optimized.hashtable.wbsize  Default Value: 10485760 (10 * 1024 * 1024) Added In: Hive 0.14.0 with HIVE-6430  Optimized hashtable (see hive.mapjoin.optimized.hashtable ) uses a chain of buffers to store data. This is one buffer size. Hashtable may be slightly faster if this is larger, but for small joins unnecessary memory will be allocated and then trimmed.\nhive.mapjoin.lazy.hashtable  Default Value: true Added In: Hive 0.13.0 with HIVE-6418 and HIVE-6188 Removed In: Hive 1.1.0 with HIVE-9331  Whether a MapJoin hashtable should deserialize values on demand. Depending on how many values in the table the join will actually touch, it can save a lot of memory by not creating objects for rows that are not needed. If all rows are needed, obviously there\u0026rsquo;s no gain.\nhive.hashtable.initialCapacity  Default Value: 100000 Added In: Hive 0.7.0 with HIVE-1642  Initial capacity of mapjoin hashtable if statistics are absent, or if hive.hashtable.key.count.adjustment is set to 0.\nhive.hashtable.key.count.adjustment  Default Value: 1.0 Added In: Hive 0.14.0 with HIVE-7616  Adjustment to mapjoin hashtable size derived from table and column statistics; the estimate of the number of keys is divided by this value. If the value is 0, statistics are not used and hive.hashtable.initialCapacity is used instead.\nhive.hashtable.loadfactor  Default Value: 0.75 Added In: Hive 0.7.0 with HIVE-1642  In the process of Mapjoin, the key/value will be held in the hashtable. This value means the load factor for the in-memory hashtable.\nhive.debug.localtask  Default Value: false Added In: Hive 0.7.0 with HIVE-1642  hive.outerjoin.supports.filters  Default Value: true Added In: Hive 0.7.0 with HIVE-1534 Removed In: Hive 2.2.0 with HIVE-14522  hive.optimize.skewjoin  Default Value: false Added In: Hive 0.6.0  Whether to enable skew join optimization. (Also see hive.optimize.skewjoin.compiletime .)\nhive.skewjoin.key  Default Value: 100000 Added In: Hive 0.6.0  Determine if we get a skew key in join. If we see more than the specified number of rows with the same key in join operator, we think the key as a skew join key.\nhive.skewjoin.mapjoin.map.tasks  Default Value: 10000 Added In: Hive 0.6.0  Determine the number of map task used in the follow up map join job for a skew join. It should be used together with hive.skewjoin.mapjoin.min.split to perform a fine grained control.\nhive.skewjoin.mapjoin.min.split  Default Value: 33554432 Added In: Hive 0.6.0  Determine the number of map task at most used in the follow up map join job for a skew join by specifying the minimum split size. It should be used together with hive.skewjoin.mapjoin.map.tasks to perform a fine grained control.\nhive.optimize.skewjoin.compiletime  Default Value: false Added In: Hive 0.10.0  Whether to create a separate plan for skewed keys for the tables in the join. This is based on the skewed keys stored in the metadata. At compile time, the plan is broken into different joins: one for the skewed keys, and the other for the remaining keys. And then, a union is performed for the two joins generated above. So unless the same skewed key is present in both the joined tables, the join for the skewed key will be performed as a map-side join.\nThe main difference between this paramater and hive.optimize.skewjoin is that this parameter uses the skew information stored in the metastore to optimize the plan at compile time itself. If there is no skew information in the metadata, this parameter will not have any effect.\nBoth hive.optimize.skewjoin.compiletime and hive.optimize.skewjoin should be set to true. (Ideally, hive.optimize.skewjoin should be renamed as hive.optimize.skewjoin.runtime , but for backward compatibility that has not been done.)\nIf the skew information is correctly stored in the metadata, hive.optimize.skewjoin.compiletime will change the query plan to take care of it, and hive.optimize.skewjoin will be a no-op.\nhive.optimize.union.remove  Default Value: false Added In: Hive 0.10.0 with HIVE-3276  Whether to remove the union and push the operators between union and the filesink above union. This avoids an extra scan of the output by union. This is independently useful for union queries, and especially useful when hive.optimize.skewjoin.compiletime is set to true, since an extra union is inserted.\nThe merge is triggered if either of hive.merge.mapfiles or hive.merge.mapredfiles is set to true. If the user has set hive.merge.mapfiles to true and hive.merge.mapredfiles to false, the idea was that the number of reducers are few, so the number of files anyway is small. However, with this optimization, we are increasing the number of files possibly by a big margin. So, we merge aggresively.\nhive.mapred.supports.subdirectories  Default Value: false Added In: Hive 0.10.0 with HIVE-3276  Whether the version of Hadoop which is running supports sub-directories for tables/partitions. Many Hive optimizations can be applied if the Hadoop version supports sub-directories for tables/partitions. This support was added by MAPREDUCE-1501.\nhive.mapred.mode  Default Value:  Hive 0.x: nonstrict Hive 1.x: nonstrict Hive 2.x: strict (HIVE-12413)   Added In: Hive 0.3.0  The mode in which the Hive operations are being performed. In strict mode, some risky queries are not allowed to run. For example, full table scans are prevented (see HIVE-10454) and ORDER BY requires a LIMIT clause.\nhive.exec.script.maxerrsize  Default Value: 100000 Added In: Hive 0.2.0  Maximum number of bytes a script is allowed to emit to standard error (per map-reduce task). This prevents runaway scripts from filling logs partitions to capacity.\nhive.script.auto.progress  Default Value: false Added In: Hive 0.4.0  Whether Hive Tranform/Map/Reduce Clause should automatically send progress information to TaskTracker to avoid the task getting killed because of inactivity. Hive sends progress information when the script is outputting to stderr. This option removes the need of periodically producing stderr messages, but users should be cautious because this may prevent infinite loops in the scripts to be killed by TaskTracker.\nhive.exec.script.allow.partial.consumption  Default Value: false Added In: Hive 0.5.0  When enabled, this option allows a user script to exit successfully without consuming all the data from the standard input.\nhive.script.operator.id.env.var  Default Value: HIVE_SCRIPT_OPERATOR_ID Added In: Hive 0.5.0  Name of the environment variable that holds the unique script operator ID in the user\u0026rsquo;s transform function (the custom mapper/reducer that the user has specified in the query).\nhive.script.operator.env.blacklist  Default Value: hive.txn.valid.txns,hive.script.operator.env.blacklist Added In: Hive 0.14.0 with HIVE-8341  By default all values in the HiveConf object are converted to environment variables of the same name as the key (with \u0026lsquo;.\u0026rsquo; (dot) converted to \u0026lsquo;_\u0026rsquo; (underscore)) and set as part of the script operator\u0026rsquo;s environment. However, some values can grow large or are not amenable to translation to environment variables. This value gives a comma separated list of configuration values that will not be set in the environment when calling a script operator. By default the valid transaction list is excluded, as it can grow large and is sometimes compressed, which does not translate well to an environment variable.\nAlso see:  SerDes for more hive.script.* configuration properties  hive.exec.compress.output  Default Value: false Added In: Hive 0.2.0  This controls whether the final outputs of a query (to a local/hdfs file or a Hive table) is compressed. The compression codec and other options are determined from Hadoop configuration variables mapred.output.compress* .\nhive.exec.compress.intermediate  Default Value: false Added In: Hive 0.2.0  This controls whether intermediate files produced by Hive between multiple map-reduce jobs are compressed. The compression codec and other options are determined from Hadoop configuration variables mapred.output.compress*.\nhive.exec.parallel  Default Value: false Added In: Hive 0.5.0  Whether to execute jobs in parallel. Applies to MapReduce jobs that can run in parallel, for example jobs processing different source tables before a join. As of Hive 0.14 , also applies to move tasks that can run in parallel, for example moving files to insert targets during multi-insert.\nhive.exec.parallel.thread.number  Default Value: 8 Added In: Hive 0.6.0  How many jobs at most can be executed in parallel.\nhive.exec.rowoffset  Default Value: false Added In: Hive 0.8.0  Whether to provide the row offset virtual column.\nhive.task.progress  Default Value: false Added In: Hive 0.5.0 Removed In: Hive 0.13.0 with HIVE-4518  Whether Hive should periodically update task progress counters during execution. Enabling this allows task progress to be monitored more closely in the job tracker, but may impose a performance penalty. This flag is automatically set to true for jobs with hive.exec.dynamic.partition set to true. (This configuration property was removed in release 0.13.0.)\nhive.counters.group.name  Default Value: HIVE Added In: Hive 0.13.0 with HIVE-4518  Counter group name for counters used during query execution. The counter group is used for internal Hive variables (CREATED_FILE, FATAL_ERROR, and so on).\nhive.exec.pre.hooks  Default Value: (empty) Added In: Hive 0.4.0  Comma-separated list of pre-execution hooks to be invoked for each statement. A pre-execution hook is specified as the name of a Java class which implements the org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface.\nhive.exec.post.hooks  Default Value: (empty) Added In: Hive 0.5.0  Comma-separated list of post-execution hooks to be invoked for each statement. A post-execution hook is specified as the name of a Java class which implements the org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface.\nhive.exec.failure.hooks  Default Value: (empty) Added In: Hive 0.8.0  Comma-separated list of on-failure hooks to be invoked for each statement. An on-failure hook is specified as the name of Java class which implements the org.apache.hadoop.hive.ql.hooks.ExecuteWithHookContext interface.\nhive.merge.mapfiles  Default Value: true Added In: Hive 0.4.0  Merge small files at the end of a map-only job.\nhive.merge.mapredfiles  Default Value: false Added In: Hive 0.4.0  Merge small files at the end of a map-reduce job.\nhive.mergejob.maponly  Default Value: true Added In: Hive 0.6.0 Removed In: Hive 0.11.0  Try to generate a map-only job for merging files if CombineHiveInputFormat is supported. (This configuration property was removed in release 0.11.0.)\nhive.merge.size.per.task  Default Value: 256000000 Added In: Hive 0.4.0  Size of merged files at the end of the job.\nhive.merge.smallfiles.avgsize  Default Value: 16000000 Added In: Hive 0.5.0  When the average output file size of a job is less than this number, Hive will start an additional map-reduce job to merge the output files into bigger files. This is only done for map-only jobs if hive.merge.mapfiles is true, and for map-reduce jobs if hive.merge.mapredfiles is true.\nhive.heartbeat.interval  Default Value: 1000 Added In: Hive 0.4.0  Send a heartbeat after this interval – used by mapjoin and filter operators.\nhive.auto.convert.join  Default Value: false in 0.7.0 to 0.10.0; true in 0.11.0 and later (HIVE-3297) Added In: 0.7.0 with HIVE-1642  Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. (Note that hive-default.xml.template incorrectly gives the default as false in Hive 0.11.0 through 0.13.1.)\nhive.auto.convert.join.noconditionaltask  Default Value: true Added In: 0.11.0 with HIVE-3784 (default changed to true with HIVE-4146)  Whether Hive enables the optimization about converting common join into mapjoin based on the input file size. If this parameter is on, and the sum of size for n-1 of the tables/partitions for an n-way join is smaller than the size specified by hive.auto.convert.join.noconditionaltask.size, the join is directly converted to a mapjoin (there is no conditional task).\nhive.auto.convert.join.noconditionaltask.size  Default Value: 10000000 Added In: 0.11.0 with HIVE-3784  If hive.auto.convert.join.noconditionaltask is off, this parameter does not take effect. However, if it is on, and the sum of size for n-1 of the tables/partitions for an n-way join is smaller than this size, the join is directly converted to a mapjoin (there is no conditional task). The default is 10MB.\nhive.auto.convert.join.use.nonstaged  Default Value: false Added In: 0.13.0 with HIVE-6144 (default originally true, but changed to false with HIVE-6749 also in 0.13.0)  For conditional joins, if input stream from a small alias can be directly applied to the join operator without filtering or projection, the alias need not be pre-staged in the distributed cache via a mapred local task. Currently, this is not working with vectorization or Tez execution engine.\nhive.merge.nway.joins  Default Value: true Added In: 2.2.0 with HIVE-15655  For multiple joins on the same condition, merge joins together into a single join operator. This is useful in the case of large shuffle joins to avoid a reshuffle phase. Disabling this in Tez will often provide a faster join algorithm in case of left outer joins or a general Snowflake schema.\nhive.udtf.auto.progress  Default Value: false Added In: Hive 0.5.0  Whether Hive should automatically send progress information to TaskTracker when using UDTF\u0026rsquo;s to prevent the task getting killed because of inactivity. Users should be cautious because this may prevent TaskTracker from killing tasks with infinite loops.\nhive.mapred.reduce.tasks.speculative.execution  Default Value: true Added In: Hive 0.5.0  Whether speculative execution for reducers should be turned on.\nhive.exec.counters.pull.interval  Default Value: 1000 Added In: Hive 0.6.0  The interval with which to poll the JobTracker for the counters the running job. The smaller it is the more load there will be on the jobtracker, the higher it is the less granular the caught will be.\nhive.enforce.bucketing  Default Value:  Hive 0.x: false Hive 1.x: false Hive 2.x: removed, which effectively makes it always true (HIVE-12331)   Added In: Hive 0.6.0  Whether bucketing is enforced. If true, while inserting into the table, bucketing is enforced.\nSet to true to support INSERT \u0026hellip; VALUES, UPDATE, and DELETE transactions in Hive 0.14.0 and 1.x.x. For a complete list of parameters required for turning on Hive transactions, see hive.txn.manager .\nhive.enforce.sorting  Default Value:  Hive 0.x: false Hive 1.x: false Hive 2.x: removed, which effectively makes it always true (HIVE-12331)   Added In: Hive 0.6.0  Whether sorting is enforced. If true, while inserting into the table, sorting is enforced.\nhive.optimize.bucketingsorting  Default Value: true Added In: Hive 0.11.0 with HIVE-4240  If hive.enforce.bucketing or hive.enforce.sorting is true, don\u0026rsquo;t create a reducer for enforcing bucketing/sorting for queries of the form:\ninsert overwrite table T2 select * from T1;\nwhere T1 and T2 are bucketed/sorted by the same keys into the same number of buckets. (In Hive 2.0.0 and later, this parameter does not depend on hive.enforce.bucketing or hive.enforce.sorting .)\nhive.optimize.reducededuplication  Default Value: true Added In: Hive 0.6.0  Remove extra map-reduce jobs if the data is already clustered by the same key which needs to be used again. This should always be set to true. Since it is a new feature, it has been made configurable.\nhive.optimize.reducededuplication.min.reducer  Default Value: 4 Added In: Hive 0.11.0 with HIVE-2340  Reduce deduplication merges two RSs (reduce sink operators) by moving key/parts/reducer-num of the child RS to parent RS. That means if reducer-num of the child RS is fixed (order by or forced bucketing) and small, it can make very slow, single MR. The optimization will be disabled if number of reducers is less than specified value.\nhive.optimize.correlation  Default Value: false Added In: Hive 0.12.0 with HIVE-2206  Exploit intra-query correlations. For details see the Correlation Optimizer design document.\nhive.optimize.limittranspose  Default Value: false Added In: Hive 2.0.0 with HIVE-11684, modified by HIVE-11775  Whether to push a limit through left/right outer join or union. If the value is true and the size of the outer input is reduced enough (as specified in hive.optimize.limittranspose.reductionpercentage and hive.optimize.limittranspose.reductiontuples), the limit is pushed to the outer input or union; to remain semantically correct, the limit is kept on top of the join or the union too.\nhive.optimize.limittranspose.reductionpercentage  Default Value: 1.0 Added In: Hive 2.0.0 with HIVE-11684, modified by HIVE-11775  When hive.optimize.limittranspose is true, this variable specifies the minimal percentage (fractional) reduction of the size of the outer input of the join or input of the union that the optimizer should get in order to apply the rule.\nhive.optimize.limittranspose.reductiontuples  Default Value: 0 Added In: Hive 2.0.0 with HIVE-11684, modified by HIVE-11775  When hive.optimize.limittranspose is true, this variable specifies the minimal reduction in the number of tuples of the outer input of the join or input of the union that the optimizer should get in order to apply the rule.\nhive.optimize.filter.stats.reduction  Default Value: false Added In: Hive 2.1.0 with HIVE-13269  Whether to simplify comparison expressions in filter operators using column stats.\nhive.optimize.sort.dynamic.partition  Default Value: true in Hive 0.13.0 and 0.13.1; false in Hive 0.14.0 and later (HIVE-8151) Added In: Hive 0.13.0 with HIVE-6455 Deprecated: replaced with hive.optimize.sort.dynamic.partition.threshold Removed in Hive 4.0 with HIVE-25320  When enabled, dynamic partitioning column will be globally sorted. This way we can keep only one record writer open for each partition value in the reducer thereby reducing the memory pressure on reducers.\nhive.cbo.enable  Default Value: false in Hive 0.14.*; true in Hive 1.1.0 and later ( HIVE-8395 ) Added In: Hive 0.14.0 with HIVE-5775 and HIVE-7946  When true, the cost based optimizer, which uses the Calcite framework, will be enabled.\nhive.cbo.fallback.strategy  Default Value: CONSERVATIVE Possible Values:  \u0026ldquo;NEVER\u0026rdquo;, never use the legacy optimizer (all CBO errors are fatal). \u0026ldquo;ALWAYS\u0026rdquo;, always use the legacy optimizer (CBO errors are not fatal). \u0026ldquo;CONSERVATIVE\u0026rdquo;, use the legacy optimizer only when the CBO error is not related to subqueries and views. \u0026ldquo;TEST\u0026rdquo;, specific behavior only for tests, do not use in production.   Added In: HIVE-24601  Options are \u0026ldquo;NEVER\u0026rdquo;, \u0026ldquo;CONSERVATIVE\u0026rdquo;, \u0026ldquo;ALWAYS\u0026rdquo;, \u0026ldquo;TEST\u0026rdquo;. The strategy defines when Hive fallbacks to legacy optimizer when CBO fails: hive.cbo.returnpath.hiveop  Default Value: false Added In: Hive 1.2.0 with HIVE-9581 and HIVE-9795  When true, this optimization to CBO Logical plan will add rule to introduce not null filtering on join keys. Controls Calcite plan to Hive operator conversion. Overrides hive.optimize.remove.identity.project when set to false.\nhive.cbo.cnf.maxnodes  Default Value: -1 Added In: Hive 2.1.1 with HIVE-14021  When converting to conjunctive normal form (CNF), fail if the expression exceeds the specified threshold; the threshold is expressed in terms of the number of nodes (leaves and interior nodes). The default, -1, does not set up a threshold.\nhive.optimize.null.scan  Default Value: true Added In: Hive 0.14.0 with HIVE-7385  When true, this optimization will try to not scan any rows from tables which can be determined at query compile time to not generate any rows (e.g., where 1 = 2, where false, limit 0 etc.).\nhive.exec.dynamic.partition  Default Value: false prior to Hive 0.9.0; true in Hive 0.9.0 and later (HIVE-2835) Added In: Hive 0.6.0  Whether or not to allow dynamic partitions in DML/DDL.\nhive.exec.dynamic.partition.mode  Default Value: strict Added In: Hive 0.6.0  In strict mode, the user must specify at least one static partition in case the user accidentally overwrites all partitions. In nonstrict mode all partitions are allowed to be dynamic.\nSet to nonstrict to support INSERT \u0026hellip; VALUES, UPDATE, and DELETE transactions (Hive 0.14.0 and later). For a complete list of parameters required for turning on Hive transactions, see hive.txn.manager .\nhive.exec.max.dynamic.partitions  Default Value: 1000 Added In: Hive 0.6.0  Maximum number of dynamic partitions allowed to be created in total.\nhive.exec.max.dynamic.partitions.pernode  Default Value: 100 Added In: Hive 0.6.0  Maximum number of dynamic partitions allowed to be created in each mapper/reducer node.\nhive.exec.max.created.files  Default Value: 100000 Added In: Hive 0.7.0  Maximum number of HDFS files created by all mappers/reducers in a MapReduce job.\nhive.exec.default.partition.name  Default Value: __HIVE_DEFAULT_PARTITION__ Added In: Hive 0.6.0  The default partition name in case the dynamic partition column value is null/empty string or any other values that cannot be escaped. This value must not contain any special character used in HDFS URI (e.g., \u0026lsquo;:\u0026rsquo;, \u0026lsquo;%\u0026rsquo;, \u0026lsquo;/\u0026rsquo; etc). The user has to be aware that the dynamic partition value should not contain this value to avoid confusions.\nhive.fetch.output.serde  Default Value: org.apache.hadoop.hive.serde2.DelimitedJSONSerDe Added In: Hive 0.7.0  The SerDe used by FetchTask to serialize the fetch output.\nhive.exec.mode.local.auto  Default Value: false Added In: Hive 0.7.0 with HIVE-1408  Lets Hive determine whether to run in local mode automatically.\nhive.exec.mode.local.auto.inputbytes.max  Default Value: 134217728 Added In: Hive 0.7.0 with HIVE-1408  When hive.exec.mode.local.auto is true, input bytes should be less than this for local mode.\nhive.exec.mode.local.auto.tasks.max  Default Value: 4 Added In: Hive 0.7.0 with HIVE-1408 Removed In: Hive 0.9.0 with HIVE-2651  When hive.exec.mode.local.auto is true, the number of tasks should be less than this for local mode. Replaced in Hive 0.9.0 by hive.exec.mode.local.auto.input.files.max.\nhive.exec.mode.local .auto.input.files.max  Default Value: 4 Added In: Hive 0.9.0 with HIVE-2651  When hive.exec.mode.local.auto is true, the number of tasks should be less than this for local mode.\nhive.exec.drop.ignorenonexistent  Default Value: true Added In: Hive 0.7.0 with HIVE-1856 and HIVE-1858  Do not report an error if DROP TABLE/VIEW/PARTITION/INDEX/TEMPORARY FUNCTION specifies a non-existent table/view. Also applies to permanent functions as of Hive 0.13.0.\nhive.exec.show.job.failure.debug.info  Default Value: true Added In: Hive 0.7.0  If a job fails, whether to provide a link in the CLI to the task with the most failures, along with debugging hints if applicable.\nhive.auto.progress.timeout  Default Value: 0 Added In: Hive 0.7.0  How long to run autoprogressor for the script/UDTF operators (in seconds). Set to 0 for forever.\nhive.table.parameters.default  Default Value: (empty) Added In: Hive 0.7.0  Default property values for newly created tables.\nhive.variable.substitute  Default Value: true Added In: Hive 0.7.0  This enables substitution using syntax like ${var} ${system:var} and ${env:var}.\nhive.error.on.empty.partition  Default Value: false Added In: Hive 0.7.0  Whether to throw an exception if dynamic partition insert generates empty results.\nhive.exim.uri.scheme.whitelist  Default Value: hdfs,pfile prior to Hive 2.2.0; hdfs,pfile,file in Hive 2.2.0 and later Added In: Hive 0.8.0 with HIVE-1918; default changed in Hive 2.2.0 with HIVE-15151  A comma separated list of acceptable URI schemes for import and export.\nhive.limit.row.max.size  Default Value: 100000 Added In: Hive 0.8.0  When trying a smaller subset of data for simple LIMIT, how much size we need to guarantee each row to have at least.\nhive.limit.optimize.limit.file  Default Value: 10 Added In: Hive 0.8.0  When trying a smaller subset of data for simple LIMIT, maximum number of files we can sample.\nhive.limit.optimize.enable  Default Value: false Added In: Hive 0.8.0  Whether to enable to optimization to trying a smaller subset of data for simple LIMIT first.\nhive.limit.optimize.fetch.max  Default Value: 50000 Added In: Hive 0.8.0  Maximum number of rows allowed for a smaller subset of data for simple LIMIT, if it is a fetch query. Insert queries are not restricted by this limit.\nhive.rework.mapredwork  Default Value: false Added In: Hive 0.8.0  Should rework the mapred work or not. This is first introduced by SymlinkTextInputFormat to replace symlink files with real paths at compile time.\nhive.sample.seednumber  Default Value: 0 Added In: Hive 0.8.0  A number used to percentage sampling. By changing this number, user will change the subsets of data sampled.\nhive.autogen.columnalias.prefix.label  Default Value: _c Added In: Hive 0.8.0  String used as a prefix when auto generating column alias. By default the prefix label will be appended with a column position number to form the column alias. Auto generation would happen if an aggregate function is used in a select clause without an explicit alias.\nhive.autogen.columnalias.prefix.includefuncname  Default Value: false Added In: Hive 0.8.0  Whether to include function name in the column alias auto generated by Hive.\nhive.exec.perf.logger  Default Value: org.apache.hadoop.hive.ql.log.PerfLogger Added In: Hive 0.8.0  The class responsible logging client side performance metrics. Must be a subclass of org.apache.hadoop.hive.ql.log.PerfLogger.\nhive.start.cleanup.scratchdir  Default Value: false Added In: Hive 0.8.1 with HIVE-2181 Fixed In: Hive 1.3.0 with HIVE-10415  To clean up the Hive scratch directory while starting the Hive server (or HiveServer2). This is not an option for a multi-user environment since it will accidentally remove the scratch directory in use.\nhive.scratchdir.lock  Default Value: false Added In: Hive 1.3.0 and 2.1.0 (but not 2.0.x) with HIVE-13429  When true, holds a lock file in the scratch directory. If a Hive process dies and accidentally leaves a dangling scratchdir behind, the cleardanglingscratchdir tool will remove it.\nWhen false, does not create a lock file and therefore the cleardanglingscratchdir tool cannot remove any dangling scratch directories.\nhive.output.file.extension  Default Value: (empty) Added In: Hive 0.8.1  String used as a file extension for output files. If not set, defaults to the codec extension for text files (e.g. \u0026ldquo;.gz\u0026rdquo;), or no extension otherwise.\nhive.insert.into.multilevel.dirs  Default Value: false Added In: Hive 0.8.1  Whether to insert into multilevel nested directories like \u0026ldquo;insert directory \u0026lsquo;/HIVEFT25686/chinna/\u0026rsquo; from table\u0026rdquo;.\nThe following error may be shown when inserting into a nested directory that does not exist:\nERROR org.apache.hadoop.hive.ql.exec.Task: Failed with exception Unable to rename: \nTo enable automatic subdirectory generation set \u0026lsquo;hive.insert.into.multilevel.dirs=true\u0026rsquo;\nhive.conf.validation  Default Value: true Added In: Hive 0.10.0 with HIVE-2848  Enables type checking for registered Hive configurations.\nAs of Hive 0.14.0 ( HIVE-7211 ), a configuration name that starts with \u0026ldquo;hive.\u0026rdquo; is regarded as a Hive system property. With hive.conf.validation true (default), any attempts to set a configuration property that starts with \u0026ldquo;hive.\u0026rdquo; which is not registered to the Hive system will throw an exception.\nhive.fetch.task.conversion  Default Value: minimal in Hive 0.10.0 through 0.13.1, more in Hive 0.14.0 and later Added In: Hive 0.10.0 with HIVE-2925; default changed in Hive 0.14.0 with HIVE-7397  Some select queries can be converted to a single FETCH task, minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incur RS – ReduceSinkOperator, requiring a MapReduce task), lateral views and joins.\nSupported values are none, minimal and more.\nnone: Disable hive.fetch.task.conversion (value added in Hive 0.14.0 with HIVE-8389)\n1. minimal: SELECT *, FILTER on partition columns (WHERE and HAVING clauses), LIMIT only more: SELECT, FILTER, LIMIT only (including TABLESAMPLE, virtual columns)  \u0026ldquo;more\u0026rdquo; can take any kind of expressions in the SELECT clause, including UDFs.\n(UDTFs and lateral views are not yet supported – see HIVE-5718.)\nhive.map.groupby.sorted  Default Value:  Hive 0.x and 1.x: false Hive 2.0 and later: true (HIVE-12325)   Added In: Hive 0.10.0 with HIVE-3432  If the bucketing/sorting properties of the table exactly match the grouping key, whether to perform the group by in the mapper by using BucketizedHiveInputFormat. The only downside to this is that it limits the number of mappers to the number of files.\nhive.map.groupby.sorted.testmode  Default Value: false Added In: Hive 0.11.0 with HIVE-4281 Removed In: Hive 2.0.0 with HIVE-12325  If the bucketing/sorting properties of the table exactly match the grouping key, whether to perform the group by in the mapper by using BucketizedHiveInputFormat. If the test mode is set, the plan is not converted, but a query property is set to denote the same. (This configuration property was removed in release 2.0.0.)\nhive.groupby.orderby.position.alias  Default Value: false Added In: Hive 0.11.0 with HIVE-581 Deprecated In: Hive 2.2.0 with HIVE-15797  Whether to enable using Column Position Alias in GROUP BY and ORDER BY clauses of queries (deprecated as of Hive 2.2.0; use hive.groupby.position.alias and hive.orderby.position.alias instead).\nhive.groupby.position.alias  Default Value: false Added In: Hive 2.2.0 with HIVE-15797  Whether to enable using Column Position Alias in GROUP BY.\nhive.orderby.position.alias  Default Value: true Added In: Hive 2.2.0 with HIVE-15797  Whether to enable using Column Position Alias in ORDER BY.\nhive.fetch.task.aggr  Default Value: false Added In: Hive 0.12.0 with HIVE-4002 (description added in Hive 0.13.0 with HIVE-5793)  Aggregation queries with no group-by clause (for example, select count(*) from src) execute final aggregations in a single reduce task. If this parameter is set to true, Hive delegates the final aggregation stage to a fetch task, possibly decreasing the query time.\nhive.fetch.task.conversion.threshold  Default Value: -1 in Hive 0.13.0 and 0.13.1, 1073741824 (1 GB) in Hive 0.14.0 and later Added In: Hive 0.13.0 with HIVE-3990; default changed in Hive 0.14.0 with HIVE-7397  Input threshold (in bytes) for applying hive.fetch.task.conversion. If target table is native, input length is calculated by summation of file lengths. If it\u0026rsquo;s not native, the storage handler for the table can optionally implement the org.apache.hadoop.hive.ql.metadata.InputEstimator interface. A negative threshold means hive.fetch.task.conversion is applied without any input length threshold.\nhive.limit.pushdown.memory.usage  Default Value: -1 Added In: Hive 0.12.0 with HIVE-3562  The maximum memory to be used for hash in RS operator for top K selection. The default value \u0026ldquo;-1\u0026rdquo; means no limit.\nhive.cache.expr.evaluation  Default Value: true Added In: Hive 0.12.0 with HIVE-4209 Bug Fix: Hive 0.14.0 with HIVE-7314 (expression caching doesn\u0026rsquo;t work when using UDF inside another UDF or a Hive function)  If true, the evaluation result of a deterministic expression referenced twice or more will be cached. For example, in a filter condition like \u0026ldquo;\u0026hellip; where key + 10 \u0026gt; 10 or key + 10 = 0\u0026rdquo; the expression \u0026ldquo;key + 10\u0026rdquo; will be evaluated/cached once and reused for the following expression (\u0026ldquo;key + 10 = 0\u0026rdquo;). Currently, this is applied only to expressions in select or filter operators.\nhive.resultset.use.unique.column.names  Default Value: true Added In: Hive 0.13.0 with HIVE-6687  Make column names unique in the result set by qualifying column names with table alias if needed. Table alias will be added to column names for queries of type \u0026ldquo;select *\u0026rdquo; or if query explicitly uses table alias \u0026ldquo;select r1.x..\u0026rdquo;.\nhive.support.quoted.identifiers  Default Value: column Added In: Hive 0.13.0 with HIVE-6013  Whether to use quoted identifiers. Value can be \u0026ldquo;none\u0026rdquo; or \u0026ldquo;column\u0026rdquo;.\ncolumn: Column names can contain any Unicode character. Any column name that is specified within backticks (```) is treated literally. Within a backtick string, use double backticks (````) to represent a backtick character.\nnone: Only alphanumeric and underscore characters are valid in identifiers. Backticked names are interpreted as regular expressions. This is also the behavior in releases prior to 0.13.0.\nhive.plan.serialization.format  Default Value: kryo Added In: Hive 0.13.0 with HIVE-1511 Removed a Value In: Hive 2.0.0 with HIVE-12609 javaXML is no longer supported  Query plan format serialization between client and task nodes. Two supported values are kryo and javaXML (prior to Hive 2.0.0). Kryo is the default (and starting from Hive 2.0.0 Kryo is the only supported value).\nhive.exec.check.crossproducts  Default Value: true Added In: Hive 0.13.0 with HIVE-6643  Check if a query plan contains a cross product. If there is one, output a warning to the session\u0026rsquo;s console.\nhive.display.partition.cols.separately  Default Value: true Added In: Hive 0.13.0 with HIVE-6689  In older Hive versions (0.10 and earlier) no distinction was made between partition columns or non-partition columns while displaying columns in DESCRIBE TABLE. From version 0.12 onwards, they are displayed separately. This flag will let you get the old behavior, if desired. See test-case in patch for HIVE-6689.\nhive.limit.query.max.table.partition  Default Value: -1 Added In: Hive 0.13.0 with HIVE-6492 Deprecated In: Hive 2.2.0 with HIVE-13884 (See hive.metastore.limit.partition.request .) Removed In: Hive 3.0.0 with HIVE-17965  To protect the cluster, this controls how many partitions can be scanned for each partitioned table. The default value \u0026ldquo;-1\u0026rdquo; means no limit. The limit on partitions does not affect metadata-only queries.\nhive.files.umask.value  Default Value: 0002 Added In: (none, but temporarily in patches for HIVE-2504 before release 0.9.0) Removed In: Hive 0.9.0 (HIVE-2504-1.patch), replaced by hive.warehouse.subdir.inherit.perms  Obsolete: The dfs.umask value for the Hive-created folders.\nhive.optimize.sampling.orderby  Default Value: false Added In: Hive 0.12.0 with HIVE-1402  Uses sampling on order-by clause for parallel execution.\nhive.optimize.sampling.orderby .number  Default Value: 1000 Added In: Hive 0.12.0 with HIVE-1402  With hive.optimize.sampling.orderby=true, total number of samples to be obtained to calculate partition keys.\nhive.optimize.sampling.orderby .percent  Default Value: 0.1 Added In: Hive 0.12.0 with HIVE-1402  With hive.optimize.sampling.orderby=true, probability with which a row will be chosen.\nhive. compat  Default Value: 0.12 Added In: Hive 0.13.0 with HIVE-6012  Enable (configurable) deprecated behaviors of arithmetic operations by setting the desired level of backward compatibility. The default value gives backward-compatible return types for numeric operations. Other supported release numbers give newer behavior for numeric operations, for example 0.13 gives the more SQL compliant return types introduced in HIVE-5356.\nThe value \u0026ldquo;latest\u0026rdquo; specifies the latest supported level. Currently, this only affects division of integers.\nSetting to 0.12 (default) maintains division behavior in Hive 0.12 and earlier releases: int / int = double.\nSetting to 0.13 gives division behavior in Hive 0.13 and later releases: int / int = decimal.\nAn invalid setting will cause an error message, and the default support level will be used.\nhive.optimize.constant.propagation  Default Value: true Added In: Hive 0.14.0 with HIVE-5771  Whether to enable the constant propagation optimizer.\nhive.entity.capture.transform  Default Value: false Added In: Hive 1.1.0 with HIVE-8938  Enable capturing compiler read entity of transform URI which can be introspected in the semantic and exec hooks.\nhive.support.sql11.reserved.keywords  Default Value: true Added In: Hive 1.2.0 with HIVE-6617  Whether to enable support for SQL2011 reserved keywords. When enabled, will support (part of) SQL2011 reserved keywords.\nhive.log.explain.output  Default Value: false Added In: 1.1.0 with HIVE-8600  When enabled, will log EXPLAIN EXTENDED output for the query at log4j INFO level and in HiveServer2 WebUI / Drilldown / Query Plan.\nFrom Hive 3.1.0 onwards, this configuration property only logs to the log4j INFO. T o log the EXPLAIN EXTENDED output in WebUI / Drilldown / Query Plan from Hive 3.1.0 onwards, use hive.server2.webui.explain.output . hive.explain.user  Default Value: false Added In: Hive 1.2.0 with HIVE-9780  Whether to show explain result at user level. When enabled, will log EXPLAIN output for the query at user level. (Tez only. For Spark, see hive.spark.explain.user.)\nhive.typecheck.on.insert  Default Value: true Added In: Hive 0.12.0 with HIVE-5297 for insert partition Extended In: Hive 1.2 with HIVE-10307 for alter, describe partition, etc.  Whether to check, convert, and normalize partition value specified in partition specification to conform to the partition column type.\nhive.exec.temporary.table.storage  Default Value: default Added In: Hive 1.1.0 with HIVE-7313  Expects one of [memory, ssd, default].\nDefine the storage policy for temporary tables. Choices between memory, ssd and default. See HDFS Storage Types and Storage Policies.\nhive.optimize.distinct.rewrite  Default Value: true Added In: Hive 1.2.0 with HIVE-10568  When applicable, this optimization rewrites distinct aggregates from a single-stage to multi-stage aggregation. This may not be optimal in all cases. Ideally, whether to trigger it or not should be a cost-based decision. Until Hive formalizes the cost model for this, this is config driven.\nhive.optimize.point.lookup  Default Value: true Added In: Hive 2.0.0 with HIVE-11461  Whether to transform OR clauses in Filter operators into IN clauses.\nhive.optimize.point.lookup.min  Default Value: 31 Added In: Hive 2.0.0 with HIVE-11573  Minimum number of OR clauses needed to transform into IN clauses.\nhive.allow.udf.load.on.demand  Default Value: false Added In: Hive 2.1.0 with HIVE-13596  Whether enable loading UDFs from metastore on demand; this is mostly relevant for HS2 and was the default behavior before Hive 1.2.\nhive.async.log.enabled  Default Value: true Added In: Hive 2.1.0 with HIVE-13027  Whether to enable Log4j2\u0026rsquo;s asynchronous logging. Asynchronous logging can give significant performance improvement as logging will be handled in a separate thread that uses the LMAX disruptor queue for buffering log messages.\nRefer to https://logging.apache.org/log4j/2.x/manual/async.html for benefits and drawbacks.\nhive.msck.repair.batch.size  Default Value: 0 Added In: Hive 2.2.0 with HIVE-12077  To run the MSCK REPAIR TABLE command batch-wise. If there is a large number of untracked partitions, by configuring a value to the property it will execute in batches internally. The default value of the property is zero, which means it will execute all the partitions at once.\nhive.exec.copyfile.maxnumfiles  Default Value: 1 Added In: Hive 2.3.0 with HIVE-14864  Maximum number of files Hive uses to do sequential HDFS copies between directories. Distributed copies (distcp) will be used instead for larger numbers of files so that copies can be done faster.\nhive.exec.copyfile.maxsize  Default Value: 32 megabytes Added In: Hive 1.1.0 with HIVE-8750  Maximum file size (in bytes) that Hive uses to do single HDFS copies between directories. Distributed copies (distcp) will be used instead for bigger files so that copies can be done faster.\nhive.exec.stagingdir  Default Value: .hive-staging Added in: Hive 1.1.0 with HIVE-8750  Directory name that will be created inside table locations in order to support HDFS encryption. This is replaces hive.exec.scratchdir for query results with the exception of read-only tables. In all cases hive.exec.scratchdir is still used for other temporary files, such as job plans.\nhive.query.lifetime.hooks  Default Value: (empty) Added In: Hive 2.3.0 with HIVE-14340  A comma separated list of hooks which implement QueryLifeTimeHook. These will be triggered before/after query compilation and before/after query execution, in the order specified. As of Hive 3.0.0 (HIVE-16363), this config can be used to specify implementations of QueryLifeTimeHookWithParseHooks. If they are specified then they will be invoked in the same places as QueryLifeTimeHooks and will be invoked during pre and post query parsing.\nhive.remove.orderby.in.subquery  Default Value: true Added In: Hive 3.0.0 with HIVE-6348  If set to true, order/sort by without limit in subqueries and views will be removed.\nDatetime hive.datetime.formatter  Default Value: DATETIME Added In: Hive 4.0.0 with  HIVE-25576 Configurable datetime formatter for unix_timestamp, from_unixtime Closed\n,\nHIVE-27673 Configurable datetime formatter for date_format Closed\nThe formatter to use for handling datetime values. The possible values are:\n DATETIME: For using java.time.format.DateTimeFormatter SIMPLE: For using java.text.SimpleDateFormat (known bugs: HIVE-25458, HIVE-25403, HIVE-25268)  hive.datetime.formatter**.**resolver.style  Default Value: SMART Added in: Hive 4.0.0 with HIVE-27772  The style used by the hive.datetime.formatter (only applicable to DATETIME) to resolve dates amd times. The possible values are:\n SMART:  Using smart resolution will perform the sensible default for each field, which may be the same as strict, the same as lenient, or a third behavior. Individual fields will interpret this differently. For example, resolving year-month and day-of-month in the ISO calendar system using smart mode will ensure that the day-of-month is from 1 to 31, converting any value beyond the last valid day-of-month to be the last valid day-of-month.   STRICT:  Using strict resolution will ensure that all parsed values are within the outer range of valid values for the field. Individual fields may be further processed for strictness. For example, resolving year-month and day-of-month in the ISO calendar system using strict mode will ensure that the day-of-month is valid for the year-month, rejecting invalid values. When using Strict as the hive.datetime.formatter.resolver.style we should use the pattern \u0026ldquo;u\u0026rdquo; to represent year. For more details, please refer: https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html   LENIENT:  Lenient mode allows the month in the ISO calendar system to be outside the range 1 to 12. For example, month 15 is treated as being 3 months after month 12.    Currently these configuration only affects the behavior of the following SQL functions:\n unix_timestamp(string,[string]) from_unixtime date_format  The SIMPLE formatter exists purely for compatibility purposes with previous versions of Hive thus its use is discouraged. It suffers from known bugs that are unlikely to be fixed in subsequent versions of the product. Furthermore, using SIMPLE formatter may lead to strange behavior, and unexpected results when combined with SQL functions/operators that are using the new DATETIME formatter.\nSerDes and I/O SerDes hive.script.serde  Default Value: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Added In: Hive 0.4.0  The default SerDe for transmitting input data to and reading output data from the user scripts.\nhive.script.recordreader  Default Value: org.apache.hadoop.hive.ql.exec.TextRecordReader Added In: Hive 0.4.0  The default record reader for reading data from the user scripts.\nhive.script.recordwriter  Default Value: org.apache.hadoop.hive.ql.exec.TextRecordWriter Added In: Hive 0.5.0  The default record writer for writing data to the user scripts.\nhive.default.serde  Default Value: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Added in: Hive 0.14 with HIVE-5976  The default SerDe Hive will use for storage formats that do not specify a SerDe. Storage formats that currently do not specify a SerDe include \u0026lsquo;TextFile, RcFile\u0026rsquo;. See Registration of Native SerDes for more information for storage formats and SerDes.\nhive.lazysimple.extended_boolean_literal  Default Value: false Added in: Hive 0.14 with HIVE-3635  LazySimpleSerDe uses this property to determine if it treats \u0026lsquo;T\u0026rsquo;, \u0026rsquo;t', \u0026lsquo;F\u0026rsquo;, \u0026lsquo;f\u0026rsquo;, \u0026lsquo;1\u0026rsquo;, and \u0026lsquo;0\u0026rsquo; as extended, legal boolean literals, in addition to \u0026lsquo;TRUE\u0026rsquo; and \u0026lsquo;FALSE\u0026rsquo;. The default is false, which means only \u0026lsquo;TRUE\u0026rsquo; and \u0026lsquo;FALSE\u0026rsquo; are treated as legal boolean literals.\nI/O hive.io.exception.handlers  Default Value: (empty) Added In: Hive 0.8.1  A list of I/O exception handler class names. This is used to construct a list of exception handlers to handle exceptions thrown by record readers.\nhive.input.format  Default Value: [org.apache.hadoop.hive.ql.io](http://org.apache.hadoop.hive.ql.io).CombineHiveInputFormat Added In: Hive 0.5.0  The default input format. Set this to HiveInputFormat if you encounter problems with CombineHiveInputFormat.\nAlso see:  hive.tez.input.format  File Formats hive.default.fileformat  Default Value: TextFile Added In: Hive 0.2.0  Default file format for CREATE TABLE statement. Options are TextFile, SequenceFile, RCfile, ORC, and Parquet.\nUsers can explicitly say CREATE TABLE\u0026hellip; STORED AS TEXTFILE|SEQUENCEFILE|RCFILE|ORC|AVRO|INPUTFORMAT\u0026hellip;OUTPUTFORMAT\u0026hellip; to override. (RCFILE was added in Hive 0.6.0, ORC in 0.11.0, AVRO in 0.14.0, and Parquet in 2.3.0) See Row Format, Storage Format, and SerDe for details.\nhive.default.fileformat.managed  Default Value: none Added In: Hive 1.2.0 with HIVE-9915  Default file format for CREATE TABLE statement applied to managed tables only. External tables will be created with format specified by hive.default.fileformat. Options are none, TextFile, SequenceFile, RCfile, ORC, and Parquet (as of Hive 2.3.0). Leaving this null will result in using hive.default.fileformat for all native tables. For non-native tables the file format is determined by the storage handler, as shown below (see the StorageHandlers section for more information on managed/external and native/non-native terminology).\n    Native Non-Native     Managed hive.default.fileformat.managed (or fall back to hive.default.fileformat) Not covered by default file-formats   External hive.default.fileformat Not covered by default file-formats    hive.fileformat.check  Default Value: true Added In: Hive 0.5.0  Whether to check file format or not when loading data files.\nhive.query.result.fileformat  Default Value:  Hive 0.x, 1.x, and 2.0: TextFile Hive 2.1 onward: SequenceFile   Added In: Hive 0.7.0 with HIVE-1598  File format to use for a query\u0026rsquo;s intermediate results. Options are TextFile, SequenceFile, and RCfile. Default value is changed to SequenceFile since Hive 2.1.0 (HIVE-1608).\nRCFile Format hive.io.rcfile.record.interval  Default Value: 2147483647 Added In: Hive 0.4.0 with HIVE-352; added to HiveConf.java in Hive 0.14.0 with HIVE-7211  hive.io.rcfile.column.number.conf  Default Value: 0 Added In: Hive 0.4.0 with HIVE-352; added to HiveConf.java in Hive 0.14.0 with HIVE-7211  hive.io.rcfile.tolerate.corruptions  Default Value: false Added In: Hive 0.4.0 with HIVE-352; added to HiveConf.java in Hive 0.14.0 with HIVE-7211  hive.io.rcfile.record.buffer.size  Default Value: 4194304 Added In: Hive 0.4.0 with HIVE-352; added to HiveConf.java in Hive 0.14.0 with HIVE-7211  ORC File Format The ORC file format was introduced in Hive 0.11.0. See ORC Files for details.\nBesides the configuration properties listed in this section, some properties in other sections are also related to ORC:\n hive.default.fileformat hive.stats.gather.num.threads  hive.exec.orc.memory.pool  Default Value: 0.5 Added In: Hive 0.11.0 with HIVE-4248  Maximum fraction of heap that can be used by ORC file writers.\nhive.exec.orc.write.format  Default Value: (empty) Added In: Hive 0.12.0 with HIVE-4123; default changed from 0.11 to null with HIVE-5091 (also in Hive 0.12.0)  Define the version of the file to write. Possible values are 0.11 and 0.12. If this parameter is not defined, ORC will use the run length encoding (RLE) introduced in Hive 0.12. Any value other than 0.11 results in the 0.12 encoding.\nAdditional values may be introduced in the future (see HIVE-6002).\nhive.exec.orc.base.delta.ratio  Default Value: 8 Added In: Hive 1.3.0 and 2.1.0 with HIVE-13563  Define the ratio of base writer and delta writer in terms of STRIPE_SIZE and BUFFER_SIZE.\nhive.exec.orc.default.stripe.size  Default Value: 256*1024*1024 (268,435,456) in 0.13.0;\n64*1024*1024 (67,108,864) in 0.14.0 Added In: Hive 0.13.0 with HIVE-5425; default changed in 0.14.0 with HIVE-7231 and HIVE-7490  Define the default ORC stripe size, in bytes.\nhive.exec.orc.default.block.size  Default Value: 256*1024*1024 (268,435,456) Added In: Hive 0.14.0 with HIVE-7231  Define the default file system block size for ORC files.\nhive.exec.orc.dictionary.key.size.threshold  Default Value: 0.8 Added In: Hive 0.12.0 with HIVE-4324  If the number of keys in a dictionary is greater than this fraction of the total number of non-null rows, turn off dictionary encoding. Use 1 to always use dictionary encoding.\nhive.exec.orc.default.row.index.stride  Default Value: 10000 Added In: Hive 0.13.0 with HIVE-5728  Define the default ORC index stride in number of rows. (Stride is the number of rows an index entry represents.)\nhive.exec.orc.default.buffer.size  Default Value: 256*1024 (262,144) Added In: Hive 0.13.0 with HIVE-5728  Define the default ORC buffer size, in bytes.\nhive.exec.orc.default.block.padding  Default Value: true Added In: Hive 0.13.0 with HIVE-5728  Define the default block padding. Block padding was added in Hive 0.12.0 (HIVE-5091, \u0026ldquo;ORC files should have an option to pad stripes to the HDFS block boundaries\u0026rdquo;).\nhive.exec.orc.block.padding.tolerance  Default Value: 0.05 Added In: Hive 0.14.0 with HIVE-7231  Define the tolerance for block padding as a decimal fraction of stripe size (for example, the default value 0.05 is 5% of the stripe size). For the defaults of 64Mb ORC stripe and 256Mb HDFS blocks, a maximum of 3.2Mb will be reserved for padding within the 256Mb block with the default hive.exec.orc.block.padding.tolerance. In that case, if the available size within the block is more than 3.2Mb, a new smaller stripe will be inserted to fit within that space. This will make sure that no stripe written will cross block boundaries and cause remote reads within a node local task.\nhive.exec.orc.default.compress  Default Value: ZLIB Added In: Hive 0.13.0 with HIVE-5728  Define the default compression codec for ORC file.\nhive.exec.orc.encoding.strategy  Default Value: SPEED Added In: Hive 0.14.0 with HIVE-7219  Define the encoding strategy to use while writing data. Changing this will only affect the light weight encoding for integers. This flag will not change the compression level of higher level compression codec (like ZLIB). Possible options are SPEED and COMPRESSION.\nhive.orc.splits.include.file.footer  Default Value: false Added In: Hive 0.13.0 with HIVE-6125 and HIVE-6128  If turned on, splits generated by ORC will include metadata about the stripes in the file. This data is read remotely (from the client or HiveServer2 machine) and sent to all the tasks.\nhive.orc.cache.stripe.details.size  Default Value: 10000 Added In: Hive 0.13.0 with HIVE-6125 and HIVE-6128  Cache size for keeping meta information about ORC splits cached in the client.\nhive.orc.cache.use.soft.references  Default Value: false Added In: Hive 1.3.0, Hive 2.1.1, Hive 2.2.0 with HIVE-13985  By default, the cache that ORC input format uses to store the ORC file footer uses hard references for the cached object. Setting this to true can help avoid out-of-memory issues under memory pressure (in some cases) at the cost of slight unpredictability in overall query performance.\nhive.io.sarg.cache.max.weight.mb  Default Value: 10 Added In: Hive 2.2.1, Hive 2.3.1, Hive 2.4.0, Hive 3.0.0 with HIVE-17669  The maximum weight allowed for the SearchArgument Cache, in megabytes. By default, the cache allows a max-weight of 10MB, after which entries will be evicted. Set to 0, to disable SearchArgument caching entirely.\nhive.orc.compute.splits.num.threads  Default Value: 10 Added In: Hive 0.13.0 with HIVE-6125 and HIVE-6128  How many threads ORC should use to create splits in parallel.\nhive.exec.orc.split.strategy  Default Value: HYBRID Added In: Hive 1.2.0 with HIVE-10114  What strategy ORC should use to create splits for execution. The available options are \u0026ldquo;BI\u0026rdquo;, \u0026ldquo;ETL\u0026rdquo; and \u0026ldquo;HYBRID\u0026rdquo;.\nThe HYBRID mode reads the footers for all files if there are fewer files than expected mapper count, switching over to generating 1 split per file if the average file sizes are smaller than the default HDFS blocksize. ETL strategy always reads the ORC footers before generating splits, while the BI strategy generates per-file splits fast without reading any data from HDFS.\nhive.exec.orc.skip.corrupt.data  Default Value: false Added In: Hive 0.13.0 with HIVE-6382  If ORC reader encounters corrupt data, this value will be used to determine whether to skip the corrupt data or throw an exception. The default behavior is to throw an exception.\nhive.exec.orc.zerocopy  Default Value: false Added In: Hive 0.13.0 with HIVE-6347 and HIVE-6360  Use zerocopy reads with ORC. (This requires Hadoop 2.3 or later.)\nhive.merge.orcfile.stripe.level  Default Value: true Added In: Hive 0.14.0 with HIVE-7509  When hive.merge.mapfiles , hive.merge.mapredfiles or hive.merge.tezfiles is enabled while writing a table with ORC file format, enabling this configuration property will do stripe-level fast merge for small ORC files. Note that enabling this configuration property will not honor the padding tolerance configuration ( hive.exec.orc.block.padding.tolerance ).\nhive.orc.row.index.stride.dictionary.check  Default Value: true Added In: Hive 0.14.0 with HIVE-7832  If enabled dictionary check will happen after first row index stride (default 10000 rows) else dictionary check will happen before writing first stripe. In both cases, the decision to use dictionary or not will be retained thereafter.\nhive.exec.orc.compression.strategy  Default Value: SPEED Added In: Hive 0.14.0 with HIVE-7859  Define the compression strategy to use while writing data. This changes the compression level of higher level compression codec (like ZLIB).\nValue can be SPEED or COMPRESSION.\nParquet Parquet is supported by a plugin in Hive 0.10, 0.11, and 0.12 and natively in Hive 0.13 and later. See Parquet for details.\nhive.parquet.timestamp.skip.conversion  Default Value: true Added In: Hive 1.2.0 with HIVE-9482  Pre-3.1.2 Hive implementation of Parquet stores timestamps in UTC on-file, this flag allows skipping of the conversion on reading Parquet files created from other tools that may not have done so.\nAvro See AvroSerDe for details.\nhive.avro.timestamp.skip.conversion  Default Value: false Added In: Hive 3.1.2 with HIVE-21291  Some older Hive implementations (pre-3.1.2) wrote Avro timestamps in a UTC-normalized manner, while from version 3.1.0 until 3.1.2 Hive wrote time zone agnostic timestamps.\nSetting this flag to true will treat legacy timestamps as time zone agnostic. Setting it to false will treat legacy timestamps as UTC-normalized.\nThis flag does not affect timestamps written starting with Hive 3.1.2, which are effectively time zone agnostic (see HIVE-21002 for details).\nNOTE: This property will influence how HBase files using the AvroSerDe and timestamps in Kafka tables (in the payload/Avro file, this is not about Kafka timestamps) are deserialized – keep in mind that timestamps serialized using the AvroSerDe will be UTC-normalized during serialization. So keep this property false if using HBase or Kafka.\nVectorization Hive added vectorized query execution in release 0.13.0 (HIVE-4160, HIVE-5283). For more information see the design document Vectorized Query Execution .\nhive.vectorized.execution.enabled  Default Value: false Added In: Hive 0.13.0 with HIVE-5283  This flag should be set to true to enable vectorized mode of query execution. The default value is false.\nhive.vectorized.execution.reduce.enabled  Default Value: true Added In: Hive 0.14.0 with HIVE-7405  This flag should be set to true to enable vectorized mode of the reduce-side of query execution. The default value is true.\nhive.vectorized.execution.reduce.groupby.enabled  Default Value: true Added In: Hive 0.14.0 with HIVE-8052  This flag should be set to true to enable vectorized mode of the reduce-side GROUP BY query execution. The default value is true.\nhive.vectorized.execution.reducesink.new.enabled  Default Value: true Added In: Hive 2.0.0 with HIVE-12290  This flag should be set to true to enable the new vectorization of queries using ReduceSink.\nhive.vectorized.execution.mapjoin.native.enabled  Default Value: true Added In: Hive 1.2.0 with HIVE-9824  This flag should be set to true to enable native (i.e. non-pass through) vectorization of queries using MapJoin.\nhive.vectorized.execution.mapjoin.native.multikey.only.enabled  Default Value: false Added In: Hive 1.2.0 with HIVE-9824  This flag should be set to true to restrict use of native vector map join hash tables to the MultiKey in queries using MapJoin.\nhive.vectorized.execution.mapjoin.minmax.enabled  Default Value: false Added In: Hive 1.2.0 with HIVE-9824  This flag should be set to true to enable vector map join hash tables to use max / max filtering for integer join queries using MapJoin.\nhive.vectorized.execution.mapjoin.overflow.repeated.threshold  Default Value: -1 Added In: Hive 1.2.0 with HIVE-9824  The number of small table rows for a match in vector map join hash tables where we use the repeated field optimization in overflow vectorized row batch for join queries using MapJoin. A value of -1 means do use the join result optimization. Otherwise, threshold value can be 0 to maximum integer.\nhive.vectorized.execution.mapjoin.native.fast.hashtable.enabled  Default Value: false Added In: Hive 1.2.0 with HIVE-9824  This flag should be set to true to enable use of native fast vector map join hash tables in queries using MapJoin.\nhive.vectorized.groupby.checkinterval  Default Value: 100000 Added In: Hive 0.13.0 with HIVE-5692  Number of entries added to the GROUP BY aggregation hash before a recomputation of average entry size is performed.\nhive.vectorized.groupby.maxentries  Default Value: 1000000 Added In: Hive 0.13.0 with HIVE-5692  Maximum number of entries in the vector GROUP BY aggregation hashtables. Exceeding this will trigger a flush regardless of memory pressure condition.\nhive.vectorized.use.vectorized.input.format  Default Value: true Added In: Hive 2.1.0 with HIVE-12878  This flag should be set to true to allow Hive to take advantage of input formats that support vectorization. The default value is true.\nhive.vectorized.use.vector.serde.deserialize  Default Value: false Added In: Hive 2.1.0 with HIVE-12878  This flag should be set to true to enable vectorizing rows using vector deserialize. The default value is false.\nhive.vectorized.use.row.serde.deserialize  Default Value: false Added In: Hive 2.1.0 with HIVE-12878  This flag should be set to true to enable vectorizing using row deserialize. The default value is false.\nhive.vectorized.input.format.excludes  Default Value: (empty) Added in: Hive 2.4.0 with HIVE-17534  This flag should be used to provide a comma separated list of fully qualified classnames to exclude certain FileInputFormats from vectorized execution using the vectorized file inputformat. Note that vectorized execution could still occur for that input format based on whether hive.vectorized.use.vector.serde.deserialize or hive.vectorized.use.row.serde.deserialize is enabled or not. MetaStore In addition to the Hive metastore properties listed in this section, some properties are listed in other sections:\n Hive Metastore Security  hive.metastore.pre.event.listeners hive.security.metastore.authorization.manager hive.security.metastore.authenticator.manager hive.security.metastore.authorization.auth.reads   Metrics  hive.metastore.metrics.enabled    hive.metastore.local  Default Value: true Added In: Hive 0.8.1 Removed In: Hive 0.10 with HIVE-2585  Controls whether to connect to remote metastore server or open a new metastore server in Hive Client JVM. As of Hive 0.10 this is no longer used. Instead if [hive.metastore.uris](#hive-metastore-uris) is set then remote mode is assumed otherwise local.\nhive.metastore.uri.selection  Default Value: RANDOM Added In: Hive 3.0.0  Determines the selection mechanism used by metastore client to connect to remote metastore. SEQUENTIAL implies that the first valid metastore from the URIs specified as part of hive.metastore.uris will be picked. RANDOM implies that the metastore will be picked randomly.\njavax.jdo.option.ConnectionURL  Default Value: jdbc:derby:;databaseName=metastore_db;create=true Added In: Hive 0.6.0  JDBC connect string for a JDBC metastore.\njavax.jdo.option.ConnectionDriverName  Default Value: org.apache.derby.jdbc.EmbeddedDriver Added In: Hive 0.8.1  Driver class name for a JDBC metastore.\njavax.jdo.PersistenceManagerFactoryClass  Default Value: org.datanucleus.jdo.JDOPersistenceManagerFactory Added In: Hive 0.8.1  Class implementing the JDO PersistenceManagerFactory.\njavax.jdo.option.DetachAllOnCommit  Default Value: true Added In: Hive 0.8.1  Detaches all objects from session so that they can be used after transaction is committed.\njavax.jdo.option.NonTransactionalRead  Default Value: true Added In: Hive 0.8.1  Reads outside of transactions.\njavax.jdo.option.ConnectionUserName  Default Value: APP Added In: Hive 0.8.1  Username to use against metastore database.\njavax.jdo.option.ConnectionPassword  Default Value: mine Added In: Hive 0.3.0  Password to use against metastore database.\nFor an alternative configuration, see Removing Hive Metastore Password from Hive Configuration.\njavax.jdo.option.Multithreaded  Default Value: true Added In: Hive 0.8.0  Set this to true if multiple threads access metastore through JDO concurrently.\ndatanucleus.connectionPoolingType  Default Value: DBCP in Hive 0.7 to 0.11; BoneCP in 0.12 to 2.3; HikariCP in 3.0 and later Added In: Hive 0.7.0  Uses a HikariCP connection pool for JDBC metastore from 3.0 release onwards (HIVE-16383).\nUses a BoneCP connection pool for JDBC metastore in release 0.12 to 2.3 (HIVE-4807), or a DBCP connection pool in releases 0.7 to 0.11.\nAs of Hive 2.2.0 (HIVE-13159), this parameter can also be set to none.\ndatanucleus.connectionPool.maxPoolSize  Default Value: 10 Added In: Hive 3.0.0  Specify the maximum number of connections in the connection pool.\nNote: The configured size will be used by 2 connection pools (TxnHandler and ObjectStore).\nWhen configuring the max connection pool size, it is recommended to take into account the number of metastore instances and the number of HiveServer2 instances\nconfigured with embedded metastore. To get optimal performance, set config to meet the following condition\n(2 * pool_size * metastore_instances + 2 * pool_size * HS2_instances_with_embedded_metastore) = (2 * physical_core_count + hard_disk_count).\ndatanucleus.validateTables  Default Value: false Added In: Hive 0.7.0 Removed In: Hive 2.0.0 with HIVE-6113, replaced by datanucleus.schema.validateTables  Validates existing schema against code. Turn this on if you want to verify existing schema.\ndatanucleus.schema.validateTables  Default Value: false Added In: Hive 2.0.0 with HIVE-6113, replaces datanucleus.validateTables  Validates existing schema against code. Turn this on if you want to verify existing schema.\ndatanucleus.validateColumns  Default Value: false Added In: Hive 0.7.0 Removed In: Hive 2.0.0 with HIVE-6113, replaced by datanucleus.schema.validateColumns  Validates existing schema against code. Turn this on if you want to verify existing schema.\ndatanucleus.schema.validateColumns  Default Value: false Added In: Hive 2.0.0 with HIVE-6113, replaces datanucleus.validateColumns  Validates existing schema against code. Turn this on if you want to verify existing schema.\ndatanucleus.validateConstraints  Default Value: false Added In: Hive 0.7.0 Removed In: Hive 2.0.0 with HIVE-6113, replaced by datanucleus.schema.validateConstraints  Validates existing schema against code. Turn this on if you want to verify existing schema.\ndatanucleus.schema.validateConstraints  Default Value: false Added In: Hive 2.0.0 with HIVE-6113, replaces datanucleus.validateConstraints  Validates existing schema against code. Turn this on if you want to verify existing schema.\ndatanucleus.storeManagerType  Default Value: rdbms Added In: Hive 0.7.0  Metadata store type.\ndatanucleus.fixedDatastore  Default Value:  Hive 0.x: false Hive 1.x: false   Added In: Hive 0.12.0 with HIVE-3764 Removed In: Hive 2.0.0 with HIVE-6113  Dictates whether to allow updates to schema or not.\ndatanucleus.autoCreateSchema  Default Value: true Added In: Hive 0.7.0 Removed In: Hive 2.0.0 with HIVE-6113, replaced by datanucleus.schema.autoCreateAll  Creates necessary schema on a startup if one does not exist. Set this to false, after creating it once.\nIn Hive 0.12.0 and later releases, datanucleus.autoCreateSchema is disabled if hive.metastore.schema.verification is true.\ndatanucleus.schema.autoCreateAll  Default Value: false Added In: Hive 2.0.0 with HIVE-6113, replaces datanucleus.autoCreateSchema (with different default value)  Creates necessary schema on a startup if one does not exist. Reset this to false, after creating it once.\ndatanucleus.schema.autoCreateAll is disabled if hive.metastore.schema.verification is true.\ndatanucleus.autoStartMechanismMode  Default Value: checked Added In: Hive 0.7.0  Throw exception if metadata tables are incorrect.\ndatanucleus.transactionIsolation  Default Value: read-committed Added In: Hive 0.7.0  Default transaction isolation level for identity generation.\ndatanucleus.cache.level2  Default Value: false Added In: Hive 0.7.0  This parameter does nothing.\nWarning note: For most installations, Hive should not enable the DataNucleus L2 cache, since this can cause correctness issues. Thus, some people set this parameter to false assuming that this disables the cache – unfortunately, it does not. To actually disable the cache, set datanucleus.cache.level2.type to \u0026ldquo;none\u0026rdquo;.\ndatanucleus.cache.level2.type  Default Value: none in Hive 0.9 and later; SOFT in Hive 0.7 to 0.8.1 Added In: Hive 0.7.0  NONE = disable the datanucleus level 2 cache, SOFT = soft reference based cache, WEAK = weak reference based cache.\nWarning note: For most Hive installations, enabling the datanucleus cache can lead to correctness issues, and is dangerous. This should be left as \u0026ldquo;none\u0026rdquo;.\ndatanucleus.identifierFactory  Default Value: datanucleus Added In: Hive 0.7.0  Name of the identifier factory to use when generating table/column names etc. \u0026lsquo;datanucleus\u0026rsquo; is used for backward compatibility.\ndatanucleus.plugin.pluginRegistryBundleCheck  Default Value: LOG Added In: Hive 0.7.0  Defines what happens when plugin bundles are found and are duplicated: EXCEPTION, LOG, or NONE.\nhive.metastore.warehouse.dir  Default Value: /user/hive/warehouse Added In: Hive 0.2.0  Location of default database for the warehouse.\nhive.warehouse.subdir.inherit.perms  Default Value: false Added In: Hive 0.9.0 with HIVE-2504. Removed In: Hive 3.0.0 with HIVE-16392  Set this to true if table directories should inherit the permissions of the warehouse or database directory instead of being created with permissions derived from dfs umask. (This configuration property replaced hive.files.umask.value before Hive 0.9.0 was released) (This configuration property was removed in release 3.0.0, more details in Permission Inheritance in Hive)\nBehavior of the flag is changed with Hive-0.14.0 in HIVE-6892 and sub-JIRA\u0026rsquo;s. More details in Permission Inheritance in Hive.\nhive.metastore.execute.setugi  Default Value: false in Hive 0.8.1 through 0.13.0, true starting in Hive 0.14.0 Added In: Hive 0.8.1 with HIVE-2616, default changed in Hive 0.14.0 with HIVE-6903  In unsecure mode, true will cause the metastore to execute DFS operations using the client\u0026rsquo;s reported user and group permissions. Note that this property must be set on both the client and server sides. Further note that it\u0026rsquo;s best effort. If client sets it to true and server sets it to false, the client setting will be ignored.\nhive.metastore.event.listeners  Default Value: (empty) Added In: Hive 0.8.0 with HIVE-2038  List of comma-separated listeners for metastore events.\nhive.metastore.partition.inherit.table.properties  Default Value: (empty) Added In: Hive 0.8.1  List of comma-separated keys occurring in table properties which will get inherited to newly created partitions. * implies all the keys will get inherited.\nhive.metastore.end.function.listeners  Default Value: (empty) Added In: Hive 0.8.1  List of comma-separated listeners for the end of metastore functions.\nhive.metastore.event.expiry.duration  Default Value: 0 Added In: Hive 0.8.0  Duration after which events expire from events table (in seconds).\nhive.metastore.event.clean.freq  Default Value: 0 Added In: Hive 0.8.0  Frequency at which timer task runs to purge expired events in metastore(in seconds).\nhive.metastore.connect.retries  Default Value: 3 Added In: Hive 0.6.0  Number of retries while opening a connection to metastore.\nhive.metastore.client.connect.retry.delay  Default Value: 1 Added In: Hive 0.7.0  Number of seconds for the client to wait between consecutive connection attempts.\nhive.metastore.client.socket.timeout  Default Value: 20 in Hive 0.7 through 0.13.1; 600 in Hive 0.14.0 and later Added In: Hive 0.7.0; default changed in Hive 0.14.0 with HIVE-7140  MetaStore Client socket timeout in seconds.\nhive.metastore.rawstore.impl  Default Value: org.apache.hadoop.hive.metastore.ObjectStore Added In: Hive 0.8.1  Name of the class that implements org.apache.hadoop.hive.metastore.rawstore interface. This class is used to store and retrieval of raw metadata objects such as table, database.\nAs of Hive 3.0 there are two implementations. The default implementation (ObjectStore) queries the database directly. HIVE-16520 introduced a new CachedStore (full class name is org.apache.hadoop.hive.metastore.cache.CachedStore) that caches retrieved objects in memory on the Metastore.\nmetastore.cached.rawstore.impl  Default Value:org.apache.hadoop.hive.metastore.ObjectStore Added in: Hive 3.0  If you\u0026rsquo;re using the CachedStore this is the name of the wrapped RawStore class to use.\nmetastore.cached.rawstore.cache.update.frequency  Default Value: 60 Added in: Hive 3.0.0  The time - in seconds - after which the metastore cache is updated from the metastore DB.\nmetastore.cached.rawstore.cached.object.whitelist  Default Value: .* Added in: Hive 3.0.0  Comma separated list of regular expressions to select the tables (and its partitions, stats etc) that will be cached by CachedStore. This can be used in conjunction with hive.metastore.cached.rawstore.cached.object.blacklist.\nExample: .*, db1.*, db2\\.tbl.*. The last item can potentially override patterns specified before.\nmetastore.cached.rawstore.cached.object.blacklist  Default Value: (empty) Added in: Hive 3.0.0  Comma separated list of regular expressions to filter out the tables (and its partitions, stats etc) that will be cached by CachedStore. This can be used in conjunction with hive.metastore.cached.rawstore.cached.object.whitelist.\nExample: db2.*, db3\\.tbl1, db3\\..*. The last item can potentially override patterns specified before.\nmetastore.cached.rawstore.max.cache.memory  Default Value: 1gb Added in: Hive 3.0.0  The maximum memory in bytes that the cached objects can use. Memory used is calculated based on estimated size of tables and partitions in the cache. Setting it to a negative value disables memory estimation.\nhive.metastore.batch.retrieve.max  Default Value: 300 Added In: Hive 0.8.0  Maximum number of objects (tables/partitions) can be retrieved from metastore in one batch. The higher the number, the less the number of round trips is needed to the Hive metastore server, but it may also cause higher memory requirement at the client side.\nhive.metastore.ds.connection.url.hook  Default Value: (empty) Added In: Hive 0.6.0  Name of the hook to use for retrieving the JDO connection URL. If empty, the value in javax.jdo.option.ConnectionURL is used.\nhive.metastore.ds.retry.attempts  Default Value: 1 Added In: Hive 0.6.0  The number of times to retry a metastore call if there were a connection error.\nhive.metastore.ds.retry.interval  Default Value: 1000 Added In: Hive 0.6.0  The number of milliseconds between metastore retry attempts.\nhive.metastore.server.min.threads  Default Value: 200 Added In: Hive 0.6.0 with HIVE-1270  Minimum number of worker threads in the Thrift server\u0026rsquo;s pool.\nhive.metastore.server.max.threads  Default Value:  Hive 0.x and 1.0.x: 100000 Hive 1.1.0 and later: 1000 (HIVE-8666)   Added In: Hive 0.6.0 with HIVE-1270  Maximum number of worker threads in the Thrift server\u0026rsquo;s pool.\nhive.metastore.server.max.message.size  Default Value: 100*1024*1024 Added In: Hive 1.1.0 (backported to Hive 1.0.2) with HIVE-8680  Maximum message size in bytes a Hive metastore will accept.\nhive.metastore.server.tcp.keepalive  Default Value: true Added In: Hive 0.6.0  Whether to enable TCP keepalive for the metastore server. Keepalive will prevent accumulation of half-open connections.\nhive.metastore.sasl.enabled  Default Value: false Added In: Hive 0.7.0  If true, the metastore thrift interface will be secured with SASL. Clients must authenticate with Kerberos.\nhive.metastore.kerberos.keytab.file  Default Value: (empty) Added In: Hive 0.7.0  The path to the Kerberos Keytab file containing the metastore thrift server\u0026rsquo;s service principal.\nhive.metastore.kerberos.principal  Default Value: hive-metastore/_HOST@EXAMPLE.COM Added In: Hive 0.7.0  The service principal for the metastore thrift server. The special string _HOST will be replaced automatically with the correct host name.\nNote: This principal is used by the metastore process for authentication with other services (e.g. for HDFS operations).\nhive.metastore.client.kerberos.principal  Default Value: (empty) Added In: Hive 2.2.1, 2.4.0 (HIVE-17489)  The client-facing Kerberos service principal for the Hive metastore. If unset, it defaults to the value set for hive.metastore.kerberos.principal, for backward compatibility.\nAlso see hive.server2.authentication.client.kerberos.principal.\nhive.metastore.client.cache.v2.enabled  Default Value: true Added In: (HIVE-23949)  This property enables a Caffeiene LoadingCache for Metastore client.\nhive.metastore.client.cache.v2.maxSize  Default Value: 1Gb Added In: (HIVE-23949)  Set the maximum size (number of bytes) of the metastore client cache (DEFAULT: 1GB). Only in effect when the cache is enabled.\nhive.metastore.client.cache.v2.recordStats  Default Value: false Added In: (HIVE-23949)  This property enables recording metastore client cache stats in DEBUG logs.\nhive.metastore.cache.pinobjtypes  Default Value: Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order Added In: Hive 0.7.0  List of comma-separated metastore object types that should be pinned in the cache.\nhive.metastore.authorization.storage.checks  Default Value: false Added In: Hive 0.8.0  Should the metastore do authorization checks against the underlying storage for operations like drop-partition (disallow the drop-partition if the user in question doesn\u0026rsquo;t have permissions to delete the corresponding directory on the storage).\nhive.metastore.thrift.framed.transport.enabled  Default Value: false Added In: Hive 0.10.0 with HIVE-2767  If true, the metastore Thrift interface will use TFramedTransport. When false (default) a standard TTransport is used.\nhive.metastore.schema.verification  Default Value: false Added In: Hive 0.12.0 with HIVE-3764  Enforce metastore schema version consistency.\nTrue: Verify that version information stored in metastore matches with one from Hive jars. Also disable automatic schema migration attempt (see datanucleus.autoCreateSchema and datanucleus.schema.autoCreateAll ). Users are required to manually migrate schema after Hive upgrade which ensures proper metastore schema migration.\nFalse: Warn if the version information stored in metastore doesn\u0026rsquo;t match with one from Hive jars.\nFor more information, see Metastore Schema Consistency and Upgrades.\nhive.metastore.disallow.incompatible.col.type.changes  Default Value:  Hive 0.x and 1.x: false Hive 2.x and later: true (HIVE-12320)   Added In: Hive 0.12.0 with HIVE-4409  If true, ALTER TABLE operations which change the type of a column (say STRING) to an incompatible type (say MAP\u0026lt;STRING, STRING\u0026gt;) are disallowed. RCFile default SerDe (ColumnarSerDe) serializes the values in such a way that the datatypes can be converted from string to any type. The map is also serialized as a string, which can be read as a string as well. However, with any binary serialization, this is not true. Blocking the ALTER TABLE prevents ClassCastExceptions when subsequently trying to access old partitions.\nPrimitive types like INT, STRING, BIGINT, etc. are compatible with each other and are not blocked.\nSee HIVE-4409 for more details.\nhive.metastore.integral.jdo.pushdown  Default Value: false Added In: Hive 0.13.0 with HIVE-6052  Allow JDO query pushdown for integral partition columns in metastore. Off by default. This improves metastore performance for integral columns, especially if there\u0026rsquo;s a large number of partitions. However, it doesn\u0026rsquo;t work correctly with integral values that are not normalized (for example, if they have leading zeroes like 0012). If metastore direct SQL is enabled and works ( hive.metastore.try.direct.sql ), this optimization is also irrelevant.\nhive.metastore.try.direct.sql  Default Value: true Added In: Hive 0.12.0 with HIVE-4051  Whether the Hive metastore should try to use direct SQL queries instead of the DataNucleus for certain read paths. This can improve metastore performance when fetching many partitions or column statistics by orders of magnitude; however, it is not guaranteed to work on all RDBMS-es and all versions. In case of SQL failures, the metastore will fall back to the DataNucleus, so it\u0026rsquo;s safe even if SQL doesn\u0026rsquo;t work for all queries on your datastore. If all SQL queries fail (for example, your metastore is backed by MongoDB), you might want to disable this to save the try-and-fall-back cost.\nThis can be configured on a per client basis by using the \u0026ldquo;set metaconf:hive.metastore.try.direct.sql=\u0026rdquo; command, starting with Hive 0.14.0 ( HIVE-7532).\nhive.metastore.try.direct.sql.ddl  Default Value: true Added In: Hive 0.13.0 with HIVE-5626  Same as hive.metastore.try.direct.sql , for read statements within a transaction that modifies metastore data. Due to non-standard behavior in Postgres, if a direct SQL select query has incorrect syntax or something similar inside a transaction, the entire transaction will fail and fall-back to DataNucleus will not be possible. You should disable the usage of direct SQL inside transactions if that happens in your case.\nThis can be configured on a per client basis by using the \u0026ldquo;set metaconf:hive.metastore.try.direct.sql.ddl=\u0026rdquo; command, starting with Hive 0.14.0 ( HIVE-7532).\nhive.metastore.orm.retrieveMapNullsAsEmptyStrings  Default Value: false Added In: Hive 1.0.0 with HIVE-8485  Thrift does not support nulls in maps, so any nulls present in maps retrieved from object-relational mapping (ORM) must be either pruned or converted to empty strings. Some backing databases such as Oracle persist empty strings as nulls, and therefore will need to have this parameter set to true in order to reverse that behavior. For others, the default pruning behavior is correct.\nhive.direct.sql.max.query.length  Default Value: 100 Added In: Hive 1.3.0 and 2.1.0 (but not 2.0.x) with HIVE-12349  The maximum size of a query string (in KB) as generated by direct SQL.\nhive.direct.sql.max.elements.in.clause  Default Value: 1000 Added In: Hive 1.3.0 and 2.1.0 (but not 2.0.x) with HIVE-12349  The maximum number of values in an IN clause as generated by direct SQL. Once exceeded, it will be broken into multiple OR separated IN clauses.\nhive.direct.sql.max.elements.values.clause  Default Value: 1000 Added In: Hive 1.3.0 and 2.1.0 (but not 2.0.x) with HIVE-12349  The maximum number of values in a VALUES clause for an INSERT statement as generated by direct SQL.\nhive.metastore.port  Default Value: 9083 Added In: Hive 1.3.0 with HIVE-9365  Hive metastore listener port.\nhive.metastore.initial.metadata.count.enabled  Default Value: true Added In: Hive 2.1.0 with HIVE-12628  Enable a metadata count at metastore startup for metrics.\nhive.metastore.limit.partition.request  Default value: -1 Added In: Hive 2.2.0 with HIVE-13884  This limits the number of partitions that can be requested from the Metastore for a given table. A query will not be executed if it attempts to fetch more partitions per table than the limit configured. A value of \u0026ldquo;-1\u0026rdquo; means unlimited. This parameter is preferred over hive.limit.query.max.table.partition (deprecated; removed in 3.0.0).\nhive.metastore.fastpath  Default Value: false Added In: Hive 2.0.0 with HIVE-9453  Used to avoid all of the proxies and object copies in the metastore. Note, if this is set, you MUST use a local metastore (hive.metastore.uris must be empty) otherwise undefined and most likely undesired behavior will result.\nhive.metastore.jdbc.max.batch.size  Default Value: 1000 Added In: Hive 4.0.0 with HIVE-23093  This controls the maximum number of update/delete/insert queries in a single JDBC batch statement.\nHive Metastore Connection Pooling Configuration The Hive Metastore supports several connection pooling implementations (e.g. hikaricp, bonecp, dbcp). Configuration properties prefixed by \u0026lsquo;dbcp\u0026rsquo; in versions prior to Hive 4.0.0-alpha-1 will be propagated as is to the connection pool implementation by Hive. Starting in release 4.0.0-alpha-1, when using hikaricp, properties prefixed by \u0026lsquo;hikaricp\u0026rsquo; will be propagated to the underlying connection pool. Jdbc connection url, username, password and connection pool maximum connections are exceptions which must be configured with their special Hive Metastore configuration properties.\nAdded in: Hive 3.0.0 with HIVE-17318 and HIVE-17319.\nHive Metastore HBase Development of an HBase metastore for Hive started in release 2.0.0 (HIVE-9452) but the work has been stopped and the code was removed from Hive in release 3.0.0 (HIVE-17234).\nMany more configuration properties were created for the HBase metastore in releases 2.x.x – they are not documented here. For a full list, see the doc note on HIVE-17234.\nhive.metastore.hbase.cache.size (This configuration property should never have been documented here, because it was removed before the initial release by HIVE-9693.)\nhive.metastore.hbase.cache.ttl  Default Value: 600s Added In: Hive 2.0.0 with HIVE-9453 Removed In: Hive 3.0.0 with HIVE-17234  Number of seconds for stats items to live in the cache.\nhive.metastore.hbase.file.metadata.threads  Default Value: 1 Added In: Hive 2.1.0 with HIVE-12075  Number of threads to use to read file metadata in background to cache it.\nHiveServer2 HiveServer2 was added in Hive 0.11.0 with HIVE-2935. For more information see HiveServer2 Overview, Setting Up HiveServer2, and HiveServer2 Clients.\nBesides the configuration properties listed in this section, some HiveServer2 properties are listed in other sections:\n hive.server2.builtin.udf.whitelist hive.server2.builtin.udf.blacklist hive.server2.metrics.enabled  hive.server2.support.dynamic.service.discovery  Default Value: false Added In: Hive 0.14.0 with HIVE-7935  Whether HiveServer2 supports dynamic service discovery for its clients. To support this, each instance of HiveServer2 currently uses ZooKeeper to register itself, when it is brought up. JDBC/ODBC clients should use the ZooKeeper ensemble: hive.zookeeper.quorum in their connection string.\nhive.server2.thrift.port  Default Value: 10000 Added In: Hive 0.11.0 with HIVE-2935  Port number of HiveServer2 Thrift interface. Can be overridden by setting $HIVE_SERVER2_THRIFT_PORT.\nhive.server2.thrift.bind.host  Default Value: localhost Added In: Hive 0.11.0 with HIVE-2935  Bind host on which to run the HiveServer2 Thrift interface. Can be overridden by setting $HIVE_SERVER2_THRIFT_BIND_HOST.\nhive.server2.thrift.min.worker.threads  Default Value: 5 Added In: Hive 0.11.0 with HIVE-2935  Minimum number of Thrift worker threads.\nhive.server2.thrift.max.worker.threads  Default Value: 100 in Hive 0.11.0, 500 in Hive 0.12.0 and later Added In: Hive 0.11.0 with HIVE-2935, default value changed in HIVE 0.12.0 with HIVE-4617  Maximum number of Thrift worker threads.\nhive.server2.thrift.worker.keepalive.time  Default Value: 60 Added in: Hive 0.14.0 with HIVE-7353  Keepalive time (in seconds) for an idle worker thread. When number of workers \u0026gt; min workers, excess threads are killed after this time interval.\nhive.server2.thrift.max.message.size  Default Value: 100*1024*1024 Added in: Hive 1.1.0 (backported to Hive 1.0.2) with HIVE-8680  Maximum message size in bytes a HiveServer2 server will accept.\nhive.server2.authentication  Default Value: NONE Added In: Hive 0.11.0 with HIVE-2935  Client authentication types.\nNONE: no authentication check – plain SASL transport\nLDAP: LDAP/AD based authentication\nKERBEROS: Kerberos/GSSAPI authentication\nCUSTOM: Custom authentication provider (use with property hive.server2.custom.authentication.class )\nPAM: Pluggable authentication module (added in Hive 0.13.0 with HIVE-6466)\nNOSASL: Raw transport (added in Hive 0.13.0) hive.server2.authentication.kerberos.keytab  Default Value: (empty) Added In: Hive 0.11.0 with HIVE-2935  Kerberos keytab file for server principal.\nhive.server2.authentication.kerberos.principal  Default Value: (empty) Added In: Hive 0.11.0 with HIVE-2935  Kerberos server principal.\nhive.server2.authentication.client.kerberos.principal  Default Value: (empty) Added In: Hive 2.1.1, 2.4.0 with HIVE-17489  Kerberos server principal used by the HA HiveServer2. Also see hive.metastore.client.kerberos.principal.\nhive.server2.custom.authentication.class  Default Value: (empty) Added In: Hive 0.11.0 with HIVE-2935  Custom authentication class. Used when property hive.server2.authentication is set to \u0026lsquo;CUSTOM\u0026rsquo;. Provided class must be a proper implementation of the interface org.apache.hive.service.auth.PasswdAuthenticationProvider. HiveServer2 will call its Authenticate(user, passed) method to authenticate requests. The implementation may optionally extend Hadoop\u0026rsquo;s org.apache.hadoop.conf.Configured class to grab Hive\u0026rsquo;s Configuration object.\nhive.server2.enable.doAs  Default Value: true Added In: Hive 0.11.0 with HIVE-2935 and HIVE-4356  Setting this property to true will have HiveServer2 execute Hive operations as the user making the calls to it.\nhive.server2.authentication.ldap.url  Default Value: (empty) Added In: Hive 0.11.0 with HIVE-2935  LDAP connection URL(s), value could be a SPACE separated list of URLs to multiple LDAP servers for resiliency. URLs are tried in the order specified until the connection is successful.\nhive.server2.authentication.ldap.baseDN  Default Value: (empty) Added In: Hive 0.11.0 with HIVE-2935  LDAP base DN (distinguished name).\nhive.server2.authentication.ldap.guidKey  Default Value: uid Added In: Hive 2.1.0 with HIVE-13295  This property is to indicate what prefix to use when building the bindDN for LDAP connection (when using just baseDN). So bindDN will be \u0026ldquo;=\u0026lt;user/group\u0026gt;,\u0026rdquo;. If userDNPattern and/or groupDNPattern is used in the configuration, the guidKey is not needed. Primarily required when just baseDN is being used.\nhive.server2.authentication.ldap.Domain  Default Value: (empty) Added In: Hive 0.12.0 with HIVE-4707  LDAP domain.\nhive.server2.authentication.ldap.groupDNPattern  Default Value: (empty) Added In: Hive 1.3 with HIVE-7193  A COLON-separated list of string patterns to represent the base DNs for LDAP Groups. Use \u0026ldquo;%s\u0026rdquo; where the actual group name is to be plugged in. See Group Membership for details.\nExample of one string pattern: uid=%s,OU=Groups,DC=apache,DC=org\nhive.server2.authentication.ldap.groupFilter  Default Value: (empty) Added In: Hive 1.3 with HIVE-7193  A COMMA-separated list of group names that the users should belong to (at least one of the groups) for authentication to succeed. See Group Membership for details.\nhive.server2.authentication.ldap.groupMembershipKey  Default Value: member Added In: Hive 2.1.0 with HIVE-13295 Extended In: Hive 2.1.1 with HIVE-14513  LDAP attribute name on the group object that contains the list of distinguished names for the user, group, and contact objects that are members of the group. For example: member, uniqueMember, or memberUid.\nThis property is used in LDAP search queries when finding LDAP group names that a particular user belongs to. The value of the LDAP attribute, indicated by this property, should be a full DN for the user or the short username or userid. For example, a group entry for \u0026ldquo;fooGroup\u0026rdquo; containing \u0026ldquo;member : uid=fooUser,ou=Users,dc=domain,dc=com\u0026rdquo; will help determine that \u0026ldquo;fooUser\u0026rdquo; belongs to LDAP group \u0026ldquo;fooGroup\u0026rdquo;.\nSee Group Membership for a detailed example.\nThis property can also be used to find the users if a custom-configured LDAP query returns a group instead of a user (as of Hive 2.1.1). For details, see Support for Groups in Custom LDAP Query.\nhive.server2.authentication.ldap.userMembershipKey  Default Value: null Added In: Hive 2.2.0 with HIVE-15076  LDAP attribute name on the user object that contains groups of which the user is a direct member, except for the primary group, which is represented by the primaryGroupId. For example: memberOf.\nhive.server2.authentication.ldap.groupClassKey  Default Value: groupOfNames Added In: Hive 1.3 with HIVE-13295  This property is used in LDAP search queries for finding LDAP group names a user belongs to. The value of this property is used to construct LDAP group search query and is used to indicate what a group\u0026rsquo;s objectClass is. Every LDAP group has certain objectClass. For example: group, groupOfNames, groupOfUniqueNames etc.\nSee Group Membership for a detailed example.\nhive.server2.authentication.ldap.userDNPattern  Default Value: (empty) Added In: Hive 1.3 with HIVE-7193  A COLON-separated list of string patterns to represent the base DNs for LDAP Users. Use \u0026ldquo;%s\u0026rdquo; where the actual username is to be plugged in. See User Search List for details.\nExample of one string pattern: uid=%s,OU=Users,DC=apache,DC=org\nhive.server2.authentication.ldap.userFilter  Default Value: (empty) Added In: Hive 1.3 with HIVE-7193  A COMMA-separated list of usernames for whom authentication will succeed if the user is found in LDAP. See User Search List for details.\nhive.server2.authentication.ldap.customLDAPQuery  Default Value: (empty) Added In: Hive 1.3 with HIVE-7193  A user-specified custom LDAP query that will be used to grant/deny an authentication request. If the user is part of the query\u0026rsquo;s result set, authentication succeeds. See Custom Query String for details.\nhive.server2.authentication.ldap.binddn  Default Value: (empty) Added In: Hive 4.0 with HIVE-21009  Specifies a fully qualified domain user to use when binding to LDAP for authentication, instead of using the user itself. This allows for scenarios where all users don\u0026rsquo;t have search permissions on LDAP, instead requiring only the bind user to have search permissions.\nExample of possible value: uid=binduser,OU=Users,DC=apache,DC=org\nhive.server2.authentication.ldap.bindpw  Default Value: (empty) Added In: Hive 4.0 with HIVE-21009  The password for the bind domain name. This password may be specified in the configuration file directly, or in a credentials provider to the cluster. This setting must be set somewhere if hive.server2.authentication.ldap.binddn is set.\nhive.server2.global.init.file.location  Default Value: $HIVE_CONF_DIR (typically \u0026lt;hive_root\u0026gt;/conf) Added in Hive 0.14.0 with HIVE-5160, HIVE-7497, and HIVE-8138  Either the location of a HiveServer2 global init file or a directory containing a .hiverc file. If the property is set, the value must be a valid path to an init file or directory where the init file is located.\nhive.server2.transport.mode  Default Value: binary Added In: Hive 0.12.0 Deprecated In: Hive 0.14.0 with HIVE-6972 (see Connection URL When HiveServer2 Is Running in HTTP Mode) but only for clients. This setting is still in use and not deprecated for the HiveServer2 itself.  Server transport mode. Value can be \u0026ldquo;binary\u0026rdquo; or \u0026ldquo;http\u0026rdquo;.\nhive.server2.thrift.http.port  Default Value: 10001 Added In: Hive 0.12.0  Port number when in HTTP mode.\nhive.server2.thrift.http.path  Default Value: cliservice Added In: Hive 0.12.0 Deprecated In: Hive 0.14.0 with HIVE-6972 (see Connection URL When HiveServer2 Is Running in HTTP Mode)  Path component of URL endpoint when in HTTP mode.\nhive.server2.thrift.http.min.worker.threads  Default Value: 5 Added In: Hive 0.12.0  Minimum number of worker threads when in HTTP mode.\nhive.server2.thrift.http.max.worker.threads  Default Value: 500 Added In: Hive 0.12.0  Maximum number of worker threads when in HTTP mode.\nhive.server2.thrift.http.max.idle.time  Default Value: 1800s (ie, 1800 seconds) Added In: Hive 0.14.0 in HIVE-7169  Maximum idle time for a connection on the server when in HTTP mode.\nhive.server2.thrift.http.worker.keepalive.time  Default Value: 60 Added In: Hive 0.14.0 in HIVE-7353  Keepalive time (in seconds) for an idle http worker thread. When number of workers \u0026gt; min workers, excess threads are killed after this time interval.\nhive.server2.thrift.sasl.qop  Default Value: auth Added In: Hive 0.12.0  Sasl QOP value; set it to one of the following values to enable higher levels of protection for HiveServer2 communication with clients.\n\u0026ldquo;auth\u0026rdquo; – authentication only (default)\n\u0026ldquo;auth-int\u0026rdquo; – authentication plus integrity protection\n\u0026ldquo;auth-conf\u0026rdquo; – authentication plus integrity and confidentiality protection\nNote that hadoop.rpc.protection being set to a higher level than HiveServer2 does not make sense in most situations. HiveServer2 ignores hadoop.rpc.protection in favor of hive.server2.thrift.sasl.qop.\nThis is applicable only if HiveServer2 is configured to use Kerberos authentication.\nhive.server2.async.exec.threads  Default Value: 50 in Hive 0.12.0, 100 in Hive 0.13.0 and later Added In: Hive 0.12.0 with HIVE-4617, default value changed in Hive 0.13.0 with HIVE-5229  Number of threads in the async thread pool for HiveServer2.\nhive.server2.async.exec.shutdown.timeout  Default Value: 10 Added In: Hive 0.12.0 with HIVE-4617  Time (in seconds) for which HiveServer2 shutdown will wait for async threads to terminate.\nhive.server2.table.type.mapping  Default Value: CLASSIC Added In: Hive 0.12.0 with HIVE-4573 and HIVE-4998  This setting reflects how HiveServer2 will report the table types for JDBC and other client implementations that retrieve the available tables and supported table types.\nHIVE: Exposes Hive\u0026rsquo;s native table types like MANAGED_TABLE, EXTERNAL_TABLE, VIRTUAL_VIEW\nCLASSIC: More generic types like TABLE and VIEW\nhive.server2.session.hook  Default Value: (empty) Added In: Hive 0.12.0 with HIVE-4588  Session-level hook for HiveServer2.\nhive.server2.max.start.attempts  Default Value: 30 Added In: Hive 0.13.0 with HIVE-5794  The number of times HiveServer2 will attempt to start before exiting, sleeping 60 seconds between retries. The default of 30 will keep trying for 30 minutes.\nhive.server2.async.exec.wait.queue.size  Default Value: 100 Added In: Hive 0.13.0 with HIVE-5229  Size of the wait queue for async thread pool in HiveServer2. After hitting this limit, the async thread pool will reject new requests.\nhive.server2.async.exec.keepalive.time  Default Value: 10 Added In: Hive 0.13.0 with HIVE-5229  Time (in seconds) that an idle HiveServer2 async thread (from the thread pool) will wait for a new task to arrive before terminating.\nhive.server2.long.polling.timeout  Default Value: 5000L Added In: Hive 0.13.0 with HIVE-5217  Time in milliseconds that HiveServer2 will wait, before responding to asynchronous calls that use long polling.\nhive.server2.allow.user.substitution  Default Value: true Added In: Hive 0.13.0  Allow alternate user to be specified as part of HiveServer2 open connection request.\nhive.server2.authentication.spnego.keytab  Default Value: (empty) Added In: Hive 0.13.0  Keytab file for SPNEGO principal, optional. A typical value would look like /etc/security/keytabs/spnego.service.keytab. This keytab would be used by HiveServer2 when Kerberos security is enabled and HTTP transport mode is used. This needs to be set only if SPNEGO is to be used in authentication.\nSPNEGO authentication would be honored only if valid hive.server2.authentication.spnego.principal and hive.server2.authentication.spnego.keytab are specified.\nhive.server2.authentication.spnego.principal  Default Value: (empty) Added In: Hive 0.13.0  SPNEGO service principal, optional. A typical value would look like HTTP/_HOST@EXAMPLE.COM. The SPNEGO service principal would be used by HiveServer2 when Kerberos security is enabled and HTTP transport mode is used. This needs to be set only if SPNEGO is to be used in authentication.\nhive.server2.authentication.pam.services  Default Value: (empty) Added In: Hive 0.13.0 with HIVE-6466  List of the underlying PAM services that should be used when hive.server2.authentication type is PAM. A file with the same name must exist in /etc/pam.d.\nhive.server2.use.SSL  Default Value: false Added In: Hive 0.13.0 with HIVE-5351  Set this to true for using SSL encryption in HiveServer2.\nhive.server2.keystore.path  Default Value: (empty) Added In: Hive 0.13.0 with HIVE-5351  SSL certificate keystore location.\nhive.server2.keystore.password  Default Value: (empty) Added In: Hive 0.13.0 with HIVE-5351  SSL certificate keystore password.\nhive.server2.tez.default.queues  Default Value: (empty) Added In: Hive 0.13.0 with HIVE-6325  A list of comma separated values corresponding to YARN queues of the same name. When HiveServer2 is launched in Tez mode, this configuration needs to be set for multiple Tez sessions to run in parallel on the cluster.\nhive.server2.tez.sessions.per.default.queue  Default Value: 1 Added In: Hive 0.13.0 with HIVE-6325  A positive integer that determines the number of Tez sessions that should be launched on each of the queues specified by hive.server2.tez.default.queues . Determines the parallelism on each queue.\nhive.server2.tez.initialize.default.sessions  Default Value: false Added In: Hive 0.13.0 with HIVE-6325  This flag is used in HiveServer 2 to enable a user to use HiveServer 2 without turning on Tez for HiveServer 2. The user could potentially want to run queries over Tez without the pool of sessions.\nhive.server2.session.check.interval  Default Value:  Hive 0.x, 1.0.x, 1.1.x, 1.2.0: 0ms Hive 1.2.1+, 1.3+, 2.x+: 6h (HIVE-9842)   Added In: Hive 0.14.0 with HIVE-5799  The check interval for session/operation timeout, which can be disabled by setting to zero or negative value.\nhive.server2.idle.session.timeout   Default Value:\n Hive 0.x, 1.0.x, 1.1.x, 1.2.0: 0ms Hive 1.2.1+, 1.3+, 2.x+: 7d (HIVE-9842)    Added In: Hive 0.14.0 with HIVE-5799\n  With hive.server2.session.check.interval set to a positive time value, session will be closed when it\u0026rsquo;s not accessed for this duration of time, which can be disabled by setting to zero or negative value.\nhive.server2.idle.operation.timeout  Default Value: 0ms Added In: Hive 0.14.0 with HIVE-5799  With hive.server2.session.check.interval set to a positive time value, operation will be closed when it\u0026rsquo;s not accessed for this duration of time, which can be disabled by setting to zero value.\nWith positive value, it\u0026rsquo;s checked for operations in terminal state only (FINISHED, CANCELED, CLOSED, ERROR).\nWith negative value, it\u0026rsquo;s checked for all of the operations regardless of state.\nhive.server2.logging.operation.enabled  Default Value: true Added In: Hive 0.14.0 with HIVE-4629 and HIVE-8785  When true, HiveServer2 will save operation logs and make them available for clients.\nhive.server2.logging.operation.log.location  Default Value: ${[java.io](http://java.io).tmpdir}/${[user.name](http://user.name)}/operation_logs Added In: Hive 0.14.0 with HIVE-4629  Top level directory where operation logs are stored if logging functionality is enabled.\nhive.server2.logging.operation.verbose  Default Value: false Added In: Hive 0.14.0 with HIVE-8785 Removed In: Hive 1.2.0 with HIVE-10119  When true, HiveServer2 operation logs available for clients will be verbose. Replaced in Hive 1.2.0 by hive.server2.logging.operation.level.\nhive.server2.logging.operation.level  Default Value: EXECUTION Added In: Hive 1.2.0 with HIVE-10119  HiveServer2 operation logging mode available to clients to be set at session level.\nFor this to work, hive.server2.logging.operation.enabled should be set to true. The allowed values are:\n NONE: Ignore any logging. EXECUTION: Log completion of tasks. PERFORMANCE: Execution + Performance logs. VERBOSE: All logs.  hive.server2.thrift.http.cookie.auth.enabled  Default Value: true Added In: Hive 1.2.0 with HIVE-9710  When true, HiveServer2 in HTTP transport mode will use cookie based authentication mechanism.\nhive.server2.thrift.http.cookie.max.age  Default Value: 86400s (1 day) Added In: Hive 1.2.0 with HIVE-9710  Maximum age in seconds for server side cookie used by HiveServer2 in HTTP mode.\nhive.server2.thrift.http.cookie.path  Default Value: (empty) Added In: Hive 1.2.0 with HIVE-9710  Path for the HiveServer2 generated cookies.\nhive.server2.thrift.http.cookie.domain  Default Value: (empty) Added In: Hive 1.2.0 with HIVE-9710  Domain for the HiveServer2 generated cookies.\nhive.server2.thrift.http.cookie.is.secure  Default Value: true Added In: Hive 1.2.0 with HIVE-9710  Secure attribute of the HiveServer2 generated cookie.\nhive.server2.thrift.http.cookie.is.httponly  Default Value: true Added In: Hive 1.2.0 with HIVE-9710  HttpOnly attribute of the HiveServer2 generated cookie.\nhive.server2.close.session.on.disconnect  Default Value: true Added In: Hive 2.1.0 with HIVE-13415  Session will be closed when connection is closed. Set this to false to have session outlive its parent connection.\nhive.server2.xsrf.filter.enabled  Default Value: false Added In: Hive 2.2.0 with HIVE-13853  If enabled, HiveServer2 will block any requests made to it over HTTP if an X-XSRF-HEADER header is not present.\nhive.server2.job.credential.provider.path  Default Value: (empty) Added In: Hive 2.2.0 with HIVE-14822  This configuration property enables the user to provide a comma-separated list of URLs which provide the type and location of Hadoop credential providers. These credential providers are used by HiveServer2 for providing job-specific credentials launched using MR or Spark execution engines. This functionality has not been tested against Tez.\nhive.server2.in.place.progress  Default Value: true Added In: Hive 2.2.0 with HIVE-15473  Allows HiveServer2 to send progress bar update information. This is currently available only if the execution engine is tez.\nhive.hadoop.classpath  Default Value: (empty) Added In: Hive 0.14.0 with HIVE-8340  For the Windows operating system, Hive needs to pass the HIVE_HADOOP_CLASSPATH Java parameter while starting HiveServer2 using \u0026ldquo;-hiveconf hive.hadoop.classpath=%HIVE_LIB%\u0026quot;. Users can set this parameter in hiveserver2.xml.\nHiveServer2 Web UI A web interface for HiveServer2 is introduced in release 2.0.0 (see Web UI for HiveServer2).\nhive.server2.webui.host  Default Value: 0.0.0.0 Added In: Hive 2.0.0 with HIVE-12338  The host address the HiveServer2 Web UI will listen on. The Web UI can be used to access the HiveServer2 configuration, local logs, and metrics. It can also be used to check some information about active sessions and queries being executed.\nhive.server2.webui.port  Default Value: 10002 Added In: Hive 2.0.0 with HIVE-12338 and HIVE-12711  The port the HiveServer2 Web UI will listen on. Set to 0 or a negative number to disable the HiveServer2 Web UI feature.\nhive.server2.webui.max.threads  Default Value: 50 Added In: Hive 2.0.0 with HIVE-12338  The maximum number of HiveServer2 Web UI threads.\nhive.server2.webui.max.historic.queries  Default Value: 25 Added In: Hive 2.1.0 with HIVE-12550  The maximum number of past queries to show in HiveServer2 Web UI.\nhive.server2.webui.use.ssl  Default Value: false Added In: Hive 2.0.0 with HIVE-12471 and HIVE-12485  Set this to true for using SSL encryption for HiveServer2 WebUI.\nhive.server2.webui.keystore.path  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12471  SSL certificate keystore location for HiveServer2 WebUI.\nhive.server2.webui.keystore.password  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12471  SSL certificate keystore password for HiveServer2 WebUI.\nhive.server2.webui.use.spnego  Default Value: false Added In: Hive 2.0.0 with HIVE-12485  SSL certificate keystore password for HiveServer2 WebUI.\nhive.server2.webui.spnego.keytab  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12485  The path to the Kerberos Keytab file containing the HiveServer2 WebUI SPNEGO service principal.\nhive.server2.webui.spnego.principal  Default Value: HTTP/_HOST@EXAMPLE.COM Added In: Hive 2.0.0 with HIVE-12485  The HiveServer2 WebUI SPNEGO service principal. The special string _HOST will be replaced automatically with the value of hive.server2.webui.host or the correct host name.\nhive.server2.webui. explain.out put  Default Value: false Added in: Hive 3.1.0 with HIVE-18469  The EXPLAIN EXTENDED output for the query will be shown in the WebUI / Drilldown / Query Plan tab when this configuration property is set to true.\nPrior to Hive 3.1.0, you can use hive.log.explain.output instead of this configuration property.\nhive.server2.webui.show.graph  Default Value: false Added in: Hive 4.0.0 with HIVE-17300  Set this to true to to display query plan as a graph instead of text in the WebUI. Only works with hive.server2.webui.explain.output set to true.\nhive.server2.webui.max.graph.size  Default Value: 25 Added in: Hive 4.0.0 with HIVE-17300  Max number of stages graph can display. If number of stages exceeds this, no query plan will be shown. Only works when hive.server2.webui.show.graph and hive.server2.webui.explain.output set to true.\nhive.server2.webui.show.stats  Default Value: false Added in: Hive 4.0.0 with HIVE-17300  Set this to true to to display statistics and log file for MapReduce tasks in the WebUI. Only works when hive.server2.webui.show.graph and hive.server2.webui.explain.output set to true.\nSpark Apache Spark was added in Hive 1.1.0 (HIVE-7292 and the merge-to-trunk JIRA\u0026rsquo;s HIVE-9257, 9352, 9448). For information see the design document Hive on Spark and Hive on Spark: Getting Started.\nTo configure Hive execution to Spark, set the following property to \u0026ldquo;spark\u0026quot;:\n hive.execution.engine  Besides the configuration properties listed in this section, some properties in other sections are also related to Spark:\n hive.exec.reducers.max hive.exec.reducers.bytes.per.reducer hive.mapjoin.optimized.hashtable hive.mapjoin.optimized.hashtable.wbsize  hive.spark.job.monitor.timeout\n Default Value: 60 seconds Added In: Hive 1.1.0 with HIVE-9337  Timeout for job monitor to get Spark job state.\nhive.spark.dynamic.partition.pruning  Default Value: false Added In: Hive 1.3.0 with HIVE-9152  When true, this turns on dynamic partition pruning for the Spark engine, so that joins on partition keys will be processed by writing to a temporary HDFS file, and read later for removing unnecessary partitions.\nhive.spark.dynamic.partition.pruning.map.join.only  Default Value: false Added In: Hive 3.0.0 with HIVE-16998  Similar to hive.spark.dynamic.partition.pruning, but only enables DPP if the join on the partitioned table can be converted to a map-join.\nhive.spark.dynamic.partition.pruning.max.data.size  Default Value: 100MB Added In: Hive 1.3.0 with HIVE-9152  The maximum data size for the dimension table that generates partition pruning information. If reaches this limit, the optimization will be turned off.\nhive.spark.exec.inplace.progress  Default Value: true Added In: Hive 2.2.0 with HIVE-15039  Updates Spark job execution progress in-place in the terminal.\nhive.spark.use.file.size.for.mapjoin  Default Value: false Added In: Hive 2.3.0 with HIVE-15489 Removed In: Hive 3.0.0 with HIVE-16336, replaced by hive.spark.use.ts.stats.for.mapjoin  If this is set to true, mapjoin optimization in Hive/Spark will use source file sizes associated with the TableScan operator on the root of the operator tree, instead of using operator statistics.\nhive.spark.use.ts.stats.for.mapjoin  Default Value: false Added In: Hive 3.0.0 with HIVE-16336, replaces hive.spark.use.file.size.for.mapjoin  If this is set to true, mapjoin optimization in Hive/Spark will use statistics from TableScan operators at the root of the operator tree, instead of parent ReduceSink operators of the Join operator.\nhive.spark.explain.user  Default Value: false Added In: Hive 3.0.0 with HIVE-11133  Whether to show explain result at user level for Hive-on-Spark queries. When enabled, will log EXPLAIN output for the query at user level.\nhive.prewarm.spark.timeout  Default Value: 5000ms Added In: Hive 3.0.0 with HIVE-17362  Time to wait to finish prewarming Spark executors when hive.prewarm.enabled is true.\nNote: These configuration properties for Hive on Spark are documented in the Tez section because they can also affect Tez:\n hive.prewarm.enabled hive.prewarm.numcontainers  hive.spark.optimize.shuffle.serde  Default Value: false Added In: Hive 3.0.0 with HIVE-15104  If this is set to true, Hive on Spark will register custom serializers for data types in shuffle. This should result in less shuffled data.\nhive.merge.sparkfiles  Default Value: false Added In: Hive 1.1.0 with HIVE-7810  Merge small files at the end of a Spark DAG Transformation.\nhive.spark.session.timeout.period  Default Value: 30 minutes Added In: Hive 4.0.0 with HIVE-14162  Amount of time the Spark Remote Driver should wait for a Spark job to be submitted before shutting down. If a Spark job is not launched after this amount of time, the Spark Remote Driver will shutdown, thus releasing any resources it has been holding onto. The tradeoff is that any new Hive-on-Spark queries that run in the same session will have to wait for a new Spark Remote Driver to startup. The benefit is that for long running Hive sessions, the Spark Remote Driver doesn\u0026rsquo;t unnecessarily hold onto resources. Minimum value is 30 minutes.\nhive.spark.session.timeout.period  Default Value: 60 seconds Added In: Hive 4.0.0 with HIVE-14162  How frequently to check for idle Spark sessions. Minimum value is 60 seconds.\nhive.spark.use.op.stats  Default Value: true Added in: Hive 2.3.0 with HIVE-15796  Whether to use operator stats to determine reducer parallelism for Hive on Spark. If this is false, Hive will use source table stats to determine reducer parallelism for all first level reduce tasks, and the maximum reducer parallelism from all parents for all the rest (second level and onward) reducer tasks.\nSetting this to false triggers an alternative algorithm for calculating the number of partitions per Spark shuffle. This new algorithm typically results in an increased number of partitions per shuffle.\nhive.spark.use.ts.stats.for.mapjoin  Default Value: false Added in: Hive 2.3.0 with HIVE-15489  If this is set to true, mapjoin optimization in Hive/Spark will use statistics from TableScan operators at the root of operator tree, instead of parent ReduceSink operators of the Join operator. Setting this to true is useful when the operator statistics used for a common join → map join conversion are inaccurate.\nhive.spark.use.groupby.shuffle  Default Value: true Added in: Hive 2.3.0 with HIVE-15580  When set to true, use Spark\u0026rsquo;s RDD#groupByKey to perform group bys. When set to false, use Spark\u0026rsquo;s RDD#repartitionAndSortWithinPartitions to perform group bys. While #groupByKey has better performance when running group bys, it can use an excessive amount of memory. Setting this to false may reduce memory usage, but will hurt performance.\nmapreduce.job.reduces  Default Value: -1 (disabled) Added in: Hive 1.1.0 with HIVE-7567  Sets the number of reduce tasks for each Spark shuffle stage (e.g. the number of partitions when performing a Spark shuffle). This is set to -1 by default (disabled); instead the number of reduce tasks is dynamically calculated based on Hive data statistics. Setting this to a constant value sets the same number of partitions for all Spark shuffle stages.\nRemote Spark Driver The remote Spark driver is the application launched in the Spark cluster, that submits the actual Spark job. It was introduced in HIVE-8528. It is a long-lived application initialized upon the first query of the current user, running until the user\u0026rsquo;s session is closed. The following properties control the remote communication between the remote Spark driver and the Hive client that spawns it.\nhive.spark.client.future.timeout  Default Value: 60 seconds Added In: Hive 1.1.0 with HIVE-9337  Timeout for requests from Hive client to remote Spark driver.\nhive.spark.client.connect.timeout  Default Value: 1000 miliseconds Added In: Hive 1.1.0 with HIVE-9337  Timeout for remote Spark driver in connecting back to Hive client.\nhive.spark.client.server.connect.timeout  Default Value: 90000 miliseconds Added In: Hive 1.1.0 with HIVE-9337, default changed in same release with HIVE-9519  Timeout for handshake between Hive client and remote Spark driver. Checked by both processes.\nhive.spark.client.secret.bits  Default Value: 256 Added In: Hive 1.1.0 with HIVE-9337  Number of bits of randomness in the generated secret for communication between Hive client and remote Spark driver. Rounded down to nearest multiple of 8.\nhive.spark.client.rpc.server.address  Default Value: hive.spark.client.rpc.server.address, localhost if unavailable. Added In: Hive 2.1.0 with HIVE- 12568  The server address of HiverServer2 host to be used for communication between Hive client and remote Spark driver.\nhive.spark.client.rpc.threads  Default Value: 8 Added In: Hive 1.1.0 with HIVE-9337  Maximum number of threads for remote Spark driver\u0026rsquo;s RPC event loop.\nhive.spark.client.rpc.max.size  Default Value: 52,428,800(50 * 1024 * 1024, or 50 MB) Added In: Hive 1.1.0 with HIVE-9337  Maximum message size in bytes for communication between Hive client and remote Spark driver. Default is 50 MB.\nhive.spark.client.channel.log.level  Default Value: (empty) Added In: Hive 1.1.0 with HIVE-9337  Channel logging level for remote Spark driver. One of DEBUG, ERROR, INFO, TRACE, WARN. If unset, TRACE is chosen.\nTez Apache Tez was added in Hive 0.13.0 (HIVE-4660 and HIVE-6098). For information see the design document Hive on Tez, especially the Installation and Configuration section.\nBesides the configuration properties listed in this section, some properties in other sections are also related to Tez:\n hive.execution.engine  hive.mapjoin.optimized.hashtable   hive.mapjoin.optimized.hashtable.wbsize  hive.server2.tez.default.queues hive.server2.tez.sessions.per.default.queue hive.server2.tez.initialize.default.sessions hive.stats.max.variable.length hive.stats.list.num.entries hive.stats.map.num.entries hive.stats.map.parallelism (Hive 0.13 only; removed in Hive 0.14) hive.stats.join.factor hive.stats.deserialization.factor hive.tez.dynamic.semijoin.reduction hive.tez.min.bloom.filter.entries hive.tez.max.bloom.filter.entries hive.tez.bloom.filter.factor hive.tez.bigtable.minsize.semijoin.reduction hive.explain.user  hive.jar.directory  Default Value: null Added In: Hive 0.13.0 with HIVE-5003 and HIVE-6098, default changed in HIVE-6636  This is the location that Hive in Tez mode will look for to find a site-wide installed Hive instance. See hive.user.install.directory for the default behavior.\nhive.user.install.directory  Default Value: hdfs:///user/ Added In: Hive 0.13.0 with HIVE-5003 and HIVE-6098  If Hive (in Tez mode only) cannot find a usable Hive jar in hive.jar.directory , it will upload the Hive jar to \u0026lt;hive.user.install.directory\u0026gt;/\u0026lt;user_name\u0026gt; and use it to run queries.\nhive.compute.splits.in.am  Default Value: true Added In: Hive 0.13.0 with HIVE-5522 and HIVE-6098  Whether to generate the splits locally or in the ApplicationMaster (Tez only).\nhive.rpc.query.plan  Default Value: false Added In: Hive 0.13.0 with HIVE-5522 and HIVE-6098  Whether to send the query plan via local resource or RPC.\nhive.prewarm.enabled  Default Value: false Added In: Hive 0.13.0 with HIVE-6391 and HIVE-6360  Enables container prewarm for Tez (0.13.0 to 1.2.x) or Tez/Spark (1.3.0+). This is for Hadoop 2 only.\nhive.prewarm.numcontainers  Default Value: 10 Added In: Hive 0.13.0 with HIVE-6391 and HIVE-6360  Controls the number of containers to prewarm for Tez (0.13.0 to 1.2.x) or Tez/Spark (1.3.0+). This is for Hadoop 2 only.\nhive.merge.tezfiles  Default Value: false Added In: Hive 0.13.0 with HIVE-6498 and HIVE-6360  Merge small files at the end of a Tez DAG.\nhive.tez.input.format  Default Value: [org.apache.hadoop.hive.ql.io](http://org.apache.hadoop.hive.ql.io).HiveInputFormat Added In: Hive 0.13.0 with HIVE-6498 and HIVE-6360  The default input format for Tez. Tez groups splits in the AM (ApplicationMaster).\nhive.tez.input.generate.consistent.splits  Default Value: true Added In: Hive 2.1.0 with HIVE-9850, HIVE-10104 and HIVE-12078  Whether to generate consistent split locations when generating splits in the AM. Setting to false randomizes the location and order of splits depending on how threads generate.\nRelates to LLAP.\nhive.tez.container.size  Default Value: -1 Added In: Hive 0.13.0 with HIVE-6498 and HIVE-6360  By default Tez will spawn containers of the size of a mapper. This can be used to overwrite the default.\nhive.tez.java.opts  Default Value: (empty) Added In: Hive 0.13.0 with HIVE-6498 and HIVE-6360  By default Tez will use the Java options from map tasks. This can be used to overwrite the default.\nhive.convert.join.bucket.mapjoin.tez  Default Value: false Added In: Hive 0.13.0 with HIVE-6447  Whether joins can be automatically converted to bucket map joins in Hive when Tez is used as the execution engine ( hive.execution.engine is set to \u0026ldquo;tez\u0026quot;).\nhive.tez.log.level  Default Value: INFO Added In: Hive 0.13.0 with HIVE-6743  The log level to use for tasks executing as part of the DAG. Used only if hive.tez.java.opts is used to configure Java options.\nhive.localize.resource.wait.interval  Default Value: 5000 Added In: Hive 0.13.0 with HIVE-6782  Time in milliseconds to wait for another thread to localize the same resource for Hive-Tez.\nhive.localize.resource.num.wait.attempts  Default Value: 5 Added In: Hive 0.13.0 with HIVE-6782  The number of attempts waiting for localizing a resource in Hive-Tez.\nhive.tez.smb.number.waves  Default Value: 0.5 Added In: Hive 0.14.0 with HIVE-8409  The number of waves in which to run the SMB (sort-merge-bucket) join. Account for cluster being occupied. Ideally should be 1 wave.\nhive.tez.cpu.vcores  Default Value: -1 Added In: Hive 0.14.0 with HIVE-8452  By default Tez will ask for however many CPUs MapReduce is configured to use per container. This can be used to overwrite the default.\nhive.tez.auto.reducer.parallelism  Default Value: false Added In: Hive 0.14.0 with HIVE-7158  Turn on Tez' auto reducer parallelism feature. When enabled, Hive will still estimate data sizes and set parallelism estimates. Tez will sample source vertices' output sizes and adjust the estimates at runtime as necessary.\nhive.tez.max.partition.factor  Default Value: 2 Added In: Hive 0.14.0 with HIVE-7158  When auto reducer parallelism is enabled this factor will be used to over-partition data in shuffle edges.\nhive.tez.min.partition.factor  Default Value: 0.25 Added In: Hive 0.14.0 with HIVE-7158  When auto reducer parallelism is enabled this factor will be used to put a lower limit to the number of reducers that Tez specifies.\nhive.tez.exec.print.summary  Default Value: false Added In: Hive 0.14.0 with HIVE-8495  If true, displays breakdown of execution steps for every query executed on Hive CLI or Beeline client.\nhive.tez.exec.inplace.progress  Default Value: true Added In: Hive 0.14.0 with HIVE-8495  Updates Tez job execution progress in-place in the terminal when Hive CLI is used.\nLLAP Live Long and Process (LLAP) functionality was added in Hive 2.0 (HIVE-7926 and associated tasks). For details see LLAP in Hive.\nLLAP adds the following configuration properties. hive.llap.execution.mode  Default Value: none Possible Values:  none: not tried map: only map operators are considered for llap all: every operator is tried; but falls back to no-llap in case of problems only: same as \u0026quot;all\u0026quot; but stops with an exception if execution is not possible (as of 2.2.0 with HIVE-15135) auto: conversion is controlled by hive   Added In: Hive 2.0.0 with HIVE-9635  Chooses whether query fragments will run in a container or in LLAP. When set to \u0026quot; all \u0026quot; everything runs in LLAP if possible; \u0026quot; only \u0026quot; is like \u0026ldquo;all\u0026rdquo; but disables fallback to containers, so that the query fails if it cannot run in LLAP.\nhive.server2.llap.concurrent.queries  Default Value: -1 Added In: Hive 2.0.0 with HIVE-10647  The number of queries allowed in parallel via llap. Negative number implies \u0026lsquo;infinite\u0026rsquo;.\nLLAP Client hive.llap.client.consistent.splits  Default Value: false Added In: Hive 2.0.0 with HIVE-12470  Whether to setup split locations to match nodes on which LLAP daemons are running, instead of using the locations provided by the split itself.\nLLAP Web Services hive.llap.daemon.web.port  Default Value: 15002 Added In: Hive 2.0.0 with HIVE-11358  LLAP daemon web UI port.\nhive.llap.daemon.web.ssl  Default Value: false Added In: Hive 2.0.0 with HIVE-11358  Whether LLAP daemon web UI should use SSL\nhive.llap.auto.auth  Default Value: true Added In: Hive 2.0.0 with ..  Whether or not to set Hadoop configs to enable auth in LLAP web app.\nhive.llap.daemon.service.principal  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12341  The name of the LLAP daemon\u0026rsquo;s service principal.\nhive.llap.daemon.service.hosts  Default Value: null Added In: Hive 2.0.0 with HIVE-11358 and HIVE-12470  Explicitly specified hosts to use for LLAP scheduling. Useful for testing. By default, YARN registry is used.\nhive.llap.daemon.task.preemption.metrics.intervals  Default Value:30,60,300 Added In: Hive 2.1.0 with HIVE-13536  Comma-delimited set of integers denoting the desired rollover intervals (in seconds) for percentile latency metrics.\nUsed by LLAP daemon task scheduler metrics for time taken to kill task (due to pre-emption) and useful time wasted by the task that is about to be preempted.\nLLAP Cache hive.llap.object.cache.enabled  Default Value: true Added In: Hive 2.0.0 with HIVE-9849  Cache objects (plans, hashtables, etc) in LLAP\nhive.llap.io.use.lrfu  Default Value: false Added In: Hive 2.0.0 with ..  Whether ORC low-level cache should use Least Frequently / Frequently Used (LRFU) cache policy instead of default First-In-First-Out (FIFO).\nhive.llap.io.lrfu.lambda  Default Value: 0.01f Possible Values: Between 0 and 1 Added In: Hive 2.0.0 with ..  Lambda for ORC low-level cache LRFU cache policy. Must be in [0, 1].\n0 makes LRFU behave like LFU, 1 makes it behave like LRU, values in between balance accordingly.\nLLAP I/O hive.llap.io.enabled  Default Value: null Added In: Hive 2.0.0 with updates in HIVE-12078  Whether the LLAP I/O layer is enabled. Remove property or set to false to disable LLAP I/O.\nhive.llap.io.cache.orc.size  Default Value: 1Gb Added In: Hive 2.0.0 with ..  Maximum size for IO allocator or ORC low-level cache.\nhive.llap.io.threadpool.size  Default Value: 10 Added In: Hive 2.0.0 with HIVE-10081  Specify the number of threads to use for low-level IO thread pool.\nhive.llap.io.orc.time.counters  Default Value: true Added In: Hive 2.0.0 with HIVE-10777  Whether to enable time counters for LLAP IO layer (time spent in HDFS, etc.)\nhive.llap.io.memory.mode  Default Value: cache Possible Values: cache, allocator, none Added In: Hive 2.0.0 with HIVE-12597  LLAP IO memory usage;\n\u0026lsquo;cache\u0026rsquo; (the default) uses data and metadata cache with a custom off-heap allocator,\n\u0026lsquo;allocator\u0026rsquo; uses the custom allocator without the caches,\n\u0026lsquo;none\u0026rsquo; doesn\u0026rsquo;t use either (this mode may result in significant performance degradation)\nhive.llap.io.allocator.alloc.min  Default Value: 128Kb Added In: Hive 2.0.0 with HIVE-12597  Minimum allocation possible from LLAP buddy allocator. Allocations below that are padded to minimum allocation.\nFor ORC, should generally be the same as the expected compression buffer size, or next lowest power of 2. Must be a power of 2.\nhive.llap.io.allocator.alloc.max  Default Value: 16Mb Added In: Hive 2.0.0 with HIVE-12597  Maximum allocation possible from LLAP buddy allocator. For ORC, should be as large as the largest expected ORC compression buffer size. Must be a power of 2.\nhive.llap.io.allocator.arena.count  Default Value: 8 Added In: Hive 2.0.0 with HIVE-12597  Arena count for LLAP low-level cache; cache will be allocated in the steps of (size/arena_count) bytes. This size must be \u0026lt;= 1Gb and \u0026gt;= max allocation; if it is not the case, an adjusted size will be used. Using powers of 2 is recommended.\nhive.llap.io.memory.size  Default Value: 1Gb Added In: Hive 2.0.0 with HIVE-12597  Maximum size for IO allocator or ORC low-level cache.\nhive.llap.io.allocator.direct  Default Value: true Added In: Hive 2.0.0 with HIVE-12597  Whether ORC low-level cache should use direct allocation.\nhive.llap.io.allocator.nmap  Default value: false Added In: Hive 2.1.0 with HIVE-13029  Whether ORC low-level cache should use memory mapped allocation (direct I/O)\nhive.llap.io.allocator.nmap.path  Default value: /tmp Added In: Hive 2.1.0 with HIVE-13029  The directory location for mapping NVDIMM/NVMe flash storage into the ORC low-level cache.\nLLAP CBO hive.llap.auto.allow.uber  Default Value: true Added In: Hive 2.0.0 with HIVE-9777  Whether or not to allow the planner to run vertices in the AM.\nhive.llap.auto.enforce.tree  Default Value: true Added In: Hive 2.0.0 with HIVE-9635  Enforce that all parents are in llap, before considering vertex\nhive.llap.auto.enforce.vectorized  Default Value: true Added In: Hive 2.0.0 with HIVE-9635  Enforce that inputs are vectorized, before considering vertex\nhive.llap.auto.enforce.stats  Default Value: true Added In: Hive 2.0.0 with HIVE-9635  Enforce that column stats are available, before considering vertex.\nhive.llap.auto.max.input.size  Default Value: 1010241024*1024L Added In: Hive 2.0.0 with HIVE-9635  Check input size, before considering vertex (-1 disables check)\nhive.llap.auto.max.output.size  Default Value: 110241024*1024L Added In: Hive 2.0.0 with HIVE-9635  Check output size, before considering vertex (-1 disables check)\nLLAP Metrics hive.llap.queue.metrics.percentiles.intervals  Default Value: blank Added In: Hive 2.0.0 with ..  Comma-delimited set of integers denoting the desired rollover intervals (in seconds) for percentile latency metrics on the LLAP daemon producer-consumer queue.\nBy default, percentile latency metrics are disabled.\nhive**.llap.management.rpc.port**  Default Value: 15004 Added In: 2.0.0 with HIVE-12341  RPC port for LLAP daemon management service.\nLLAP UDF Security Whitelist based UDF support (HIVE-12852).\nhive.llap.allow.permanent.fns  Default Value: true Added In: 2.1.0 with HIVE-13307  Whether LLAP decider should allow permanent UDFs.\nhive.llap.daemon.download.permanent.fns  Default Value: false Added In: Hive 2.1.0 with HIVE-12853, HIVE-13054, and HIVE-13307  Whether LLAP daemon should localize the resources for permanent UDFs.\nLLAP Security hive.llap.daemon.keytab.file  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12341  The path to the Kerberos Keytab file containing the LLAP daemon\u0026rsquo;s service principal.\nhive.llap.zk.sm.principal  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12341  The name of the principal to use to talk to ZooKeeper for ZooKeeper SecretManager.\nhive.llap.zk.sm.keytab.file  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12341  The path to the Kerberos Keytab file containing the principal to use to talk to ZooKeeper for ZooKeeper SecretManager.\nhive.llap.zk.sm.connectionString  Default Value: (empty) Added In: Hive 2.0.0 with HIVE-12341  ZooKeeper connection string for ZooKeeper SecretManager.\nhive.llap.daemon.acl  Default Value: * Added In: Hive 2.0.0 with HIVE-12341 and HIVE-12813  The ACL for LLAP daemon.\nhive.llap.management.acl  Default Value: * Added In: Hive 2.0.0 with HIVE-12341 and HIVE-12813  The ACL for LLAP daemon management.\nhive.llap.daemon.delegation.token.lifetime  Default Value: 14d Added In: Hive 2.0.0 with HIVE-12341  LLAP delegation token lifetime, in seconds if specified without a unit.\nTransactions and Compactor Hive transactions with row-level ACID functionality were added in Hive 0.13.0 (HIVE-5317 and its subtasks). For details see ACID and Transactions in Hive.\nTo turn on Hive transactions, change the values of these parameters from their defaults, as described below:\n hive.txn.manager hive.compactor.initiator.on hive.compactor.cleaner.on hive.compactor.worker.threads  These parameters must also have non-default values to turn on Hive transactions:\n hive.support.concurrency hive.enforce.bucketing (Hive 0.x and 1.x only) hive.exec.dynamic.partition.mode  Transactions hive.txn.manager  Default Value: org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager Hive Transactions Value: org.apache.hadoop.hive.ql.lockmgr.DbTxnManager Added In: Hive 0.13.0 with HIVE-5843  Set this to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager as part of turning on Hive transactions. The default DummyTxnManager replicates pre-Hive-0.13 behavior and provides no transactions.\nTurning on Hive transactions also requires appropriate settings for hive.compactor.initiator.on , hive.compactor.cleaner.on, hive.compactor.worker.threads , hive.support.concurrency , hive.enforce.bucketing (Hive 0.x and 1.x only), and hive.exec.dynamic.partition.mode .\nhive.txn.strict.locking.mode  Default Value: true Added In: Hive 2.2.0 with HIVE-15774  In strict mode non-ACID resources use standard R/W lock semantics, e.g. INSERT will acquire exclusive lock. In non-strict mode, for non-ACID resources, INSERT will only acquire shared lock, which allows two concurrent writes to the same partition but still lets lock manager prevent DROP TABLE etc. when the table is being written to. Only apples when hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.\nhive.txn.timeout  Default Value: 300 Added In: Hive 0.13.0 with HIVE-5843  Time after which transactions are declared aborted if the client has not sent a heartbeat, in seconds.\nhive.txn.heartbeat.threadpool.size  Default Value: 5 Added In: Hive 1.3.0 and 2.0.0 with HIVE-12366  The number of threads to use for heartbeating. For the Hive CLI one thread is enough, but HiveServer2 needs a few threads.\nhive.timedout.txn.reaper.start  Default Value: 100s Added In: Hive 1.3.0 with HIVE-11317  Time delay of first reaper (the process which aborts timed-out transactions) run after the metastore start.\nhive.timedout.txn.reaper.interval  Default Value: 180s Added In: Hive 1.3.0 with HIVE-11317  Time interval describing how often the reaper (the process which aborts timed-out transactions) runs.\nhive.writeset.reaper.interval  Default Value: 60s Added In: Hive 1.3.0 and 2.1.0 with HIVE-13395  Frequency of WriteSet reaper runs.\nhive.txn.max.open.batch  Default Value: 1000 Added In: Hive 0.13.0 with HIVE-5843  Maximum number of transactions that can be fetched in one call to open_txns().\nThis controls how many transactions streaming agents such as Flume or Storm open simultaneously. The streaming agent then writes that number of entries into a single file (per Flume agent or Storm bolt). Thus increasing this value decreases the number of delta files created by streaming agents. But it also increases the number of open transactions that Hive has to track at any given time, which may negatively affect read performance.\nhive.max.open.txns  Default Value: 100000 Added In: Hive 1.3.0 and 2.1.0 with HIVE-13249  Maximum number of open transactions. If current open transactions reach this limit, future open transaction requests will be rejected, until the number goes below the limit.\nhive.count.open.txns.interval  Default Value: 1s Added In: Hive 1.3.0 and 2.1.0 with HIVE-13249  Time in seconds between checks to count open transactions.\nhive.txn.retryable.sqlex.regex  Default Value: (empty) Added In: Hive 1.3.0 and 2.1.0 with HIVE-12637  Comma separated list of regular expression patterns for SQL state, error code, and error message of retryable SQLExceptions, that\u0026rsquo;s suitable for the Hive metastore database.\nFor example: Can\u0026rsquo;t serialize.*,40001$,^Deadlock,.ORA-08176.\nThe string that the regex will be matched against is of the following form, where ex is a SQLException:\nex.getMessage() + \u0026quot; (SQLState=\u0026rdquo; + ex.getSQLState() + \u0026ldquo;, ErrorCode=\u0026rdquo; + ex.getErrorCode() + \u0026ldquo;)\u0026rdquo;\nCompactor hive.compactor.initiator.on  Default Value: false Hive Transactions Value: true (for exactly one instance of the Thrift metastore service) Added In: Hive 0.13.0 with HIVE-5843  Whether to run the initiator thread on this metastore instance. Set this to true on one instance of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager .\nBefore Hive 1.3.0 it\u0026rsquo;s critical that this is enabled on exactly one metastore service instance. As of Hive 1.3.0 this property may be enabled on any number of standalone metastore instances.\nhive.compactor.cleaner.on  Default Value: false Hive Transactions Value: true (for exactly one instance of the Thrift metastore service) Added In: Hive 4.0.0 with HIVE-26908  Whether to run the Cleaner thread on this metastore instance. Set this to true on one instance of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager .\nBefore Hive 4.0.0 Cleaner thread can be started/stopped with config hive.compactor.initiator.on. This config helps to enable/disable initiator/cleaner threads independently\nhive.compactor.worker.threads  Default Value: 0 Hive Transactions Value: greater than 0 on at least one instance of the Thrift metastore service Added In: Hive 0.13.0 with HIVE-5843  How many compactor worker threads to run on this metastore instance. Set this to a positive number on one or more instances of the Thrift metastore service as part of turning on Hive transactions. For a complete list of parameters required for turning on transactions, see hive.txn.manager .\nWorker threads spawn MapReduce jobs to do compactions. They do not do the compactions themselves. Increasing the number of worker threads will decrease the time it takes tables or partitions to be compacted once they are determined to need compaction. It will also increase the background load on the Hadoop cluster as more MapReduce jobs will be running in the background.\nhive.compactor.worker.timeout  Default Value: 86400 Added In: Hive 0.13.0 with HIVE-5843  Time in seconds after which a compaction job will be declared failed and the compaction re-queued.\nhive.compactor.check.interval  Default Value: 300 Added In: Hive 0.13.0 with HIVE-5843  Time in seconds between checks to see if any tables or partitions need to be compacted. This should be kept high because each check for compaction requires many calls against the NameNode.\nDecreasing this value will reduce the time it takes for compaction to be started for a table or partition that requires compaction. However, checking if compaction is needed requires several calls to the NameNode for each table or partition that has had a transaction done on it since the last major compaction. So decreasing this value will increase the load on the NameNode.\nhive.compactor.cleaner.run.interval  Default Value: 5000 Added In: Hive 0.14 with HIVE-8258  Time in milliseconds between runs of the cleaner thread. Increasing this value will lengthen the time it takes to clean up old, no longer used versions of data and lower the load on the metastore server. Decreasing this value will shorten the time it takes to clean up old, no longer used versions of the data and increase the load on the metastore server.\nhive.compactor.delta.num.threshold  Default Value: 10 Added In: Hive 0.13.0 with HIVE-5843  Number of delta directories in a table or partition that will trigger a minor compaction.\nhive.compactor.delta.pct.threshold  Default Value: 0.1 Added In: Hive 0.13.0 with HIVE-5843  Percentage (fractional) size of the delta files relative to the base that will trigger a major compaction. (1.0 = 100%, so the default 0.1 = 10%.)\nhive.compactor.abortedtxn.threshold  Default Value: 1000 Added In: Hive 0.13.0 with HIVE-5843  Number of aborted transactions involving a given table or partition that will trigger a major compaction.\nhive.compactor.aborted.txn.time.threshold  Default Value: 12h Added In: Hive 4.0.0 with HIVE-23280  Age of table/partition\u0026rsquo;s oldest aborted transaction when compaction will be triggered.\nDefault time unit is: hours. Set to a negative number to disable.\nCompaction History hive.compactor.history.retention.succeeded  Default Value: 3 Added In: Hive 1.3.0 and 2.0.0 with HIVE-12353  Number of successful compaction entries to retain in history (per partition).\nhive.compactor.history.retention.failed  Default Value: 3 Added In: Hive 1.3.0 and 2.0.0 with HIVE-12353  Number of failed compaction entries to retain in history (per partition).\nmetastore.compactor.history.retention.did.not.initiate   Default Value: 2 Added In: Hive 1.3.0 and 2.0.0 with HIVE-12353 Deprecated name: hive.compactor.history.retention.attempted  Determines how many compaction records in state \u0026lsquo;did not initiate\u0026rsquo; will be retained in compaction history for a given table/partition.\nhive.compactor.history.reaper.interval  Default Value: 2m Added In: Hive 1.3.0 and 2.0.0 with HIVE-12353  Controls how often the process to purge historical record of compactions runs.\nhive.compactor.initiator.failed.compacts.threshold  Default Value: 2 Added In: Hive 1.3.0 and 2.0.0 with HIVE-12353  Number of consecutive failed compactions for a given partition after which the Initiator will stop attempting to schedule compactions automatically. It is still possible to use ALTER TABLE to initiate compaction. Once a manually-initiated compaction succeeds, auto-initiated compactions will resume. Note that this must be less than hive.compactor.history.retention.failed .\nIndexing Indexing was added in Hive 0.7.0 with HIVE-417, and bitmap indexing was added in Hive 0.8.0 with HIVE-1803. For more information see Indexing.\nhive.index.compact.file.ignore.hdfs  Default Value: false Added In: Hive 0.7.0 with HIVE-1889  When true the HDFS location stored in the index file will be ignored at runtime. If the data got moved or the name of the cluster got changed, the index data should still be usable.\nhive.optimize.index.filter  Default Value: false Added In: Hive 0.8.0 with HIVE-1644  Whether to enable automatic use of indexes.\nhive.optimize.index.filter.compact.minsize  Default Value: 5368709120 Added In: Hive 0.8.0 with HIVE-1644  Minimum size (in bytes) of the inputs on which a compact index is automatically used.\nhive.optimize.index.filter.compact.maxsize  Default Value: -1 Added In: Hive 0.8.0 with HIVE-1644  Maximum size (in bytes) of the inputs on which a compact index is automatically used. A negative number is equivalent to infinity.\nhive.index.compact.query.max.size  Default Value: 10737418240 Added In: Hive 0.8.0 with HIVE-2096  The maximum number of bytes that a query using the compact index can read. Negative value is equivalent to infinity.\nhive.index.compact.query.max.entries  Default Value: 10000000 Added In: Hive 0.8.0 with HIVE-2096  The maximum number of index entries to read during a query that uses the compact index. Negative value is equivalent to infinity.\nhive.exec.concatenate.check.index  Default Value: true Added In: Hive 0.8.0 with HIVE-2125  If this sets to true, Hive will throw error when doing ALTER TABLE tbl_name [partSpec] CONCATENATE on a table/partition that has indexes on it. The reason the user want to set this to true is because it can help user to avoid handling all index drop, recreation, rebuild work. This is very helpful for tables with thousands of partitions.\nhive.optimize.index.autoupdate  Default Value: false Added In: Hive 0.8.0 with HIVE-2354  Whether or not to enable automatic rebuilding of indexes when they go stale.\nCaution: Rebuilding indexes can be a lengthy and computationally expensive operation; in many cases it may be best to rebuild indexes manually.\nhive.optimize.index.groupby  Default Value: false Added In: Hive 0.8.1 with HIVE-1694  hive.index.compact.binary.search  Default Value: true Added In: Hive 0.8.1 with HIVE-2535  Whether or not to use a binary search to find the entries in an index table that match the filter, where possible.\nStatistics See Statistics in Hive for information about how to collect and use Hive table, partition, and column statistics.\nhive.stats.dbclass  Default Value: jdbc:derby (Hive 0.7 to 0.12) or fs (Hive 0.13 and later) Added In: Hive 0.7 with HIVE-1361 New Values: counter and custom added in 0.13 with HIVE-4632 and fs added in 0.13 with HIVE-6500  Hive 0.7 to 0.12: The default database that stores temporary Hive statistics. Options are jdbc:derby, jdbc:mysql, and hbase as defined in StatsSetupConst.java.\nHive 0.13 and later: The storage that stores temporary Hive statistics. In filesystem based statistics collection (\u0026quot;fs\u0026quot;), each task writes statistics it has collected in a file on the filesystem, which will be aggregated after the job has finished. Supported values are fs (filesystem), jdbc:*\u0026lt;database\u0026gt;* (where \u0026lt;database\u0026gt; can be derby, mysql, etc.), hbase, counter, and custom as defined in StatsSetupConst.java.\nhive.stats.autogather  Default Value: true Added In: Hive 0.7 with HIVE-1361  This flag enables gathering and updating statistics automatically during Hive DML operations.\nStatistics are not gathered for LOAD DATA statements.\nhive.stats.column.autogather  Default Value: false (Hive 2.1 and later 2.x); true (Hive 3.0 and later) Added In: Hive 2.1  Extends statistics autogathering to also collect column level statistics.\nhive.stats.jdbcdriver  Default Value: org.apache.derby.jdbc.EmbeddedDriver Added In: Hive 0.7 with HIVE-1361  The JDBC driver for the database that stores temporary Hive statistics.\nhive.stats.dbconnectionstring  Default Value: jdbc:derby:;databaseName=TempStatsStore;create=true Added In: Hive 0.7 with HIVE-1361  The default connection string for the database that stores temporary Hive statistics.\nhive.stats.default.publisher  Default Value: (empty) Added In: Hive 0.7 with HIVE-1923  The Java class (implementing the StatsPublisher interface) that is used by default if hive.stats.dbclass is not JDBC or HBase (Hive 0.12.0 and earlier), or if hive.stats.dbclass is a custom type (Hive 0.13.0 and later: HIVE-4632).\nhive.stats.default.aggregator  Default Value: (empty) Added In: Hive 0.7 with HIVE-1923  The Java class (implementing the StatsAggregator interface) that is used by default if hive.stats.dbclass is not JDBC or HBase (Hive 0.12.0 and earlier), or if hive.stats.dbclass is a custom type (Hive 0.13.0 and later: HIVE-4632).\nhive.stats.jdbc.timeout  Default Value: 30 Added In: Hive 0.7 with HIVE-1961  Timeout value (number of seconds) used by JDBC connection and statements.\nhive.stats.atomic  Default Value: false Added In: Hive 0.7 with HIVE-1961  If this is set to true then the metastore statistics will be updated only if all types of statistics (number of rows, number of files, number of bytes, etc.) are available. Otherwise metastore statistics are updated in a best effort fashion with whatever are available.\nhive.stats.retries.max  Default Value: 0 Added In: Hive 0.8 with HIVE-2127  Maximum number of retries when stats publisher/aggregator got an exception updating intermediate database. Default is no tries on failures.\nhive.stats.retries.wait  Default Value: 3000 Added In: Hive 0.8 with HIVE-2127  The base waiting window (in milliseconds) before the next retry. The actual wait time is calculated by baseWindow * failures + baseWindow * (failure + 1) * (random number between 0.0,1.0).\nhive.stats.collect.rawdatasize  Default Value: true Added In: Hive 0.8 with HIVE-2185  If true, the raw data size is collected when analyzing tables.\nhive.client.stats.publishers  Default Value: (empty) Added In: Hive 0.8.1 with HIVE-2446 (patch 2)  Comma-separated list of statistics publishers to be invoked on counters on each job. A client stats publisher is specified as the name of a Java class which implements the org.apache.hadoop.hive.ql.stats.ClientStatsPublisher interface.\nhive.client.stats.counters  Default Value: (empty) Added In: Hive 0.8.1 with HIVE-2446 (patch 2)  Subset of counters that should be of interest for hive.client.stats.publishers (when one wants to limit their publishing). Non-display names should be used.\nhive.stats.reliable  Default Value: false Added In: Hive 0.10.0 with HIVE-1653 New Behavior In: Hive 0.13.0 with HIVE-3777  Whether queries will fail because statistics cannot be collected completely accurately. If this is set to true, reading/writing from/into a partition or unpartitioned table may fail because the statistics could not be computed accurately. If it is set to false, the operation will succeed.\nIn Hive 0.13.0 and later, if hive.stats.reliable is false and statistics could not be computed correctly, the operation can still succeed and update the statistics but it sets a partition property \u0026ldquo;areStatsAccurate\u0026rdquo; to false. If the application needs accurate statistics, they can then be obtained in the background.\nhive.stats.ndv.error  Default Value: 20.0 Added In: Hive 0.10 with HIVE-1362 (patch 10)  Standard error allowed for NDV estimates, expressed in percentage. This provides a tradeoff between accuracy and compute cost. A lower value for the error indicates higher accuracy and a higher compute cost. (NDV means number of distinct values.) It only affects the FM-Sketch (not the HLL algorithm which is the default), where it computes the number of necessary bitvectors to achieve the accuracy.\nhive.stats.collect.tablekeys  Default Value: false Added In: Hive 0.10 with HIVE-3501  Whether join and group by keys on tables are derived and maintained in the QueryPlan. This is useful to identify how tables are accessed and to determine if they should be bucketed.\nhive.stats.collect.scancols  Default Value: false Added In: Hive 0.11 with HIVE-3940  Whether column accesses are tracked in the QueryPlan. This is useful to identify how tables are accessed and to determine if there are wasted columns that can be trimmed.\nhive.stats.key.prefix.max.length  Default Value: 200 (Hive 0.11 and 0.12) or 150 (Hive 0.13 and later) Added In: Hive 0.11 with HIVE-3750  Determines if, when the prefix of the key used for intermediate statistics collection exceeds a certain length, a hash of the key is used instead. If the value \u0026lt; 0 then hashing is never used, if the value \u0026gt;= 0 then hashing is used only when the key prefixes' length exceeds that value. The key prefix is defined as everything preceding the task ID in the key. For counter type statistics, it\u0026rsquo;s maxed by mapreduce.job.counters.group.name.max , which is by default 128.\nhive.stats.key.prefix.reserve.length  Default Value: 24 Added In: Hive 0.13 with HIVE-6229  Reserved length for postfix of statistics key. Currently only meaningful for counter type statistics which should keep the length of the full statistics key smaller than the maximum length configured by hive.stats.key.prefix.max.length . For counter type statistics, it should be bigger than the length of LB spec if exists.\nhive.stats.max.variable.length  Default Value: 100 Added In: Hive 0.13 with HIVE-5369  To estimate the size of data flowing through operators in Hive/Tez (for reducer estimation etc.), average row size is multiplied with the total number of rows coming out of each operator. Average row size is computed from average column size of all columns in the row. In the absence of column statistics, for variable length columns (like string, bytes, etc.) this value will be used. For fixed length columns their corresponding Java equivalent sizes are used (float – 4 bytes, double – 8 bytes, etc.).\nhive.analyze.stmt.collect.partlevel.stats  Default Value: true Added In: Hive 0.14.0 with HIVE-7609  Prior to 0.14, on partitioned table, analyze statement used to collect table level statistics when no partition is specified. That behavior has changed beginning 0.14 to instead collect partition level statistics for all partitions. If old behavior of collecting aggregated table level statistics is desired, change the value of this config to false. This impacts only column statistics. Basic statistics are not impacted by this config.\nhive.stats.list.num.entries  Default Value: 10 Added In: Hive 0.13 with HIVE-5369  To estimate the size of data flowing through operators in Hive/Tez (for reducer estimation etc.), average row size is multiplied with the total number of rows coming out of each operator. Average row size is computed from average column size of all columns in the row. In the absence of column statistics and for variable length complex columns like list, the average number of entries/values can be specified using this configuration property.\nhive.stats.map.num.entries  Default Value: 10 Added In: Hive 0.13 with HIVE-5369  To estimate the size of data flowing through operators in Hive/Tez (for reducer estimation etc.), average row size is multiplied with the total number of rows coming out of each operator. Average row size is computed from average column size of all columns in the row. In the absence of column statistics and for variable length complex columns like map, the average number of entries/values can be specified using this configuration property.\nhive.stats.map.parallelism  Default Value: 1 Added In: Hive 0.13 with HIVE-5369 Removed In: Hive 0.14 with HIVE-7156  The Hive/Tez optimizer estimates the data size flowing through each of the operators. For the GROUPBY operator, to accurately compute the data size map-side parallelism needs to be known. By default, this value is set to 1 since the optimizer is not aware of the number of mappers during compile-time. This Hive configuration property can be used to specify the number of mappers for data size computation of the GROUPBY operator. (This configuration property was removed in release 0.14.0.)\nhive.stats.fetch.partition.stats  Default Value: true Added In: Hive 0.13 with HIVE-6298 Removed In: Hive 3.0.0 with HIVE-17932  Annotation of the operator tree with statistics information requires partition level basic statistics like number of rows, data size and file size. Partition statistics are fetched from the metastore. Fetching partition statistics for each needed partition can be expensive when the number of partitions is high. This flag can be used to disable fetching of partition statistics from the metastore. When this flag is disabled, Hive will make calls to the filesystem to get file sizes and will estimate the number of rows from the row schema.\nhive.stats.fetch.column.stats  Default Value: false Added In: Hive 0.13 with HIVE-5898  Annotation of the operator tree with statistics information requires column statistics. Column statistics are fetched from the metastore. Fetching column statistics for each needed column can be expensive when the number of columns is high. This flag can be used to disable fetching of column statistics from the metastore.\nhive.stats.join.factor  Default Value:(float) 1.1 Added In: Hive 0.13 with HIVE-5921  The Hive/Tez optimizer estimates the data size flowing through each of the operators. The JOIN operator uses column statistics to estimate the number of rows flowing out of it and hence the data size. In the absence of column statistics, this factor determines the amount of rows flowing out of the JOIN operator.\nhive.stats.deserialization.factor  Default Value:  Hive 0.13 to 2.x.x:(float) 1.0 Hive 3.0.0 and later:(float) 10.0   Added In: Hive 0.13 with HIVE-5921 Default value changed from 1.0 to 10.0 in Hive 3.0  The Hive/Tez optimizer estimates the data size flowing through each of the operators. In the absence of basic statistics like number of rows and data size, file size is used to estimate the number of rows and data size. Since files in tables/partitions are serialized (and optionally compressed) the estimates of number of rows and data size cannot be reliably determined. This factor is multiplied with the file size to account for serialization and compression.\nhive.stats.avg.row.size  Default Value: 10000 Added In: Hive 0.13 with HIVE-5921  In the absence of table/partition statistics, average row size will be used to estimate the number of rows/data size.\nhive.compute.query.using.stats  Default Value: false Added In: Hive 0.13.0 with HIVE-5483  When set to true Hive will answer a few queries like min, max, and count(1) purely using statistics stored in the metastore. For basic statistics collection, set the configuration property hive.stats.autogather to true. For more advanced statistics collection, run ANALYZE TABLE queries.\nhive.stats.gather.num.threads  Default Value: 10 Added In: Hive 0.13.0 with HIVE-6578  Number of threads used by partialscan/noscan analyze command for partitioned tables. This is applicable only for file formats that implement the StatsProvidingRecordReader interface (like ORC).\nhive.stats.fetch.bitvector  Default Value: false Added In: Hive 3.0.0 with HIVE-16997  Whether Hive fetches bitvector when computing number of distinct values (ndv). Keep it set to false if you want to use the old schema without bitvectors.\nRuntime Filtering hive.tez.dynamic.semijoin.reduction  Default Value: true Added In: Hive 2.2 with HIVE-15269  hive.tez.min.bloom.filter.entries  Default Value: 1000000 Added In: Hive 2.3 with HIVE-16260  hive.tez.max.bloom.filter.entries  Default Value: 100000000 Added In: Hive 2.2 with HIVE-15269  hive.tez.bloom.filter.factor  Default Value: 2.0 Added In: Hive 2.3 with HIVE-16260  hive.tez.bigtable.minsize.semijoin.reduction  Default Value: 1000000 Added In: Hive 2.3 with HIVE-16260  Authentication and Authorization  Restricted/Hidden/Internal List and Whitelist Hive Client Security Hive Metastore Security SQL Standard Based Authorization  For an overview of authorization modes, see Hive Authorization.\nRestricted/Hidden/Internal List and Whitelist hive.conf.restricted.list   Default Value:\n Hive 0.11.0: (empty, but includes this list implicitly) Hive 0.13.0: hive.security.authenticator.manager, hive.security.authorization.manager (HIVE-5953) Hive 0.14.0: hive.security.authenticator.manager, hive.security.authorization.manager, hive.users.in.admin.role (HIVE-6437) Hive 2.1.0: hive.security.authenticator.manager, hive.security.authorization.manager, hive.users.in.admin.role, hive.server2.xsrf.filter.enabled (HIVE-13853) Hive 2.2.0: hive.security.authenticator.manager, hive.security.authorization.manager, hive.security.metastore.authorization.manager, hive.security.metastore.authenticator.manager, hive.users.in.admin.role, hive.server2.xsrf.filter.enabled, hive.security.authorization.enabled (HIVE-14099), hive.server2.authentication.ldap.baseDN (HIVE-15713), hive.server2.authentication.ldap.url (HIVE-15713), hive.server2.authentication.ldap.Domain (HIVE-15713), hive.server2.authentication.ldap.groupDNPattern (HIVE-15713), hive.server2.authentication.ldap.groupFilter (HIVE-15713), hive.server2.authentication.ldap.userDNPattern (HIVE-15713), hive.server2.authentication.ldap.userFilter (HIVE-15713), hive.server2.authentication.ldap.groupMembershipKey (HIVE-15713), hive.server2.authentication.ldap.userMembershipKey (HIVE-15713), hive.server2.authentication.ldap.groupClassKey (HIVE-15713), hive.server2.authentication.ldap.customLDAPQuery (HIVE-15713) Hive 3.0.0: all of the above, plus these: hive.spark.client.connect.timeout (HIVE-16876), hive.spark.client.server.connect.timeout (HIVE-16876), hive.spark.client.channel.log.level (HIVE-16876), hive.spark.client.rpc.max.size (HIVE-16876), hive.spark.client.rpc.threads (HIVE-16876), hive.spark.client.secret.bits (HIVE-16876), hive.spark.client.rpc.server.address (HIVE-16876), hive.spark.client.rpc.server.port (HIVE-16876), hikari.* (HIVE-17318), dbcp.* (HIVE-17319), hadoop.bin.path (HIVE-18248), yarn.bin.path (HIVE-18248)    Added In: Hive 0.11.0 with HIVE-2935\n  Comma separated list of configuration properties which are immutable at runtime. For example, if hive.security.authorization.enabled is set to true, it should be included in this list to prevent a client from changing it to false at runtime.\nhive.conf.hidden.list   Default Value:\n Hive 1.2.2: j `avax.jdo.option.ConnectionPassword,  hive.server2.keystore.password (` HIVE-9013)\n    Hive 2.3.0: fs.s3.awsAccessKeyId,fs.s3.awsSecretAccessKey,fs.s3n.awsAccessKeyId,fs.s3n.awsSecretAccessKey,fs.s3a.access.key,fs.s3a.secret.key,fs.s3a.proxy.password (HIVE-14588)\n+ ``Hive 3.0.0: dfs.adls.oauth2.credential,fs.adl.oauth2.credential`` ``(`` [HIVE-18228](https://issues.apache.org/jira/browse/HIVE-18228))   Comma separated list of configuration options which should not be read by normal user, such as passwords.\nhive.conf.internal.variable.list  Default Value: hive.added.files.path,hive.added.jars.path,hive.added.archives.path Added In: Hive 1.3.0 with HIVE-12346  Comma separated list of configuration options which are internally used and should not be settable via set command.\nhive.security.command.whitelist  Default Value: set,reset,dfs,add,delete,compile[,list,reload] Added In: Hive 0.13.0 with HIVE-5400 and HIVE-5252 Changed in Hive 0.14.0 to include \u0026ldquo;list\u0026rdquo; and \u0026ldquo;reload\u0026rdquo; with HIVE-7592 (\u0026ldquo;list\u0026rdquo;) and HIVE-7553 (\u0026ldquo;reload\u0026rdquo;)  Comma separated list of non-SQL Hive commands that users are authorized to execute. This can be used to restrict the set of authorized commands. The supported command list is \u0026ldquo;set,reset,dfs,add,delete,compile\u0026rdquo; in Hive 0.13.0 or \u0026ldquo;set,reset,dfs,add,list,delete,reload,compile\u0026rdquo; starting in Hive 0.14.0 and by default all these commands are authorized. To restrict any of these commands, set hive.security.command.whitelist to a value that does not have the command in it.\nWhitelist for SQL Standard Based Hive Authorization See hive.security.authorization.sqlstd.confwhitelist below for information about the whitelist property that authorizes set commands in SQL standard based authorization.\nHive Client Security hive.security.authorization.enabled  Default Value: false Added In: Hive 0.7.0  Enable or disable the Hive client authorization.\nhive.security.authorization.manager  Default Value: org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider Added In: Hive 0.7.0  The Hive client authorization manager class name. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.\nhive.security.authenticator.manager  Default Value: org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator Added In: Hive 0.7.0  Hive client authenticator manager class name. The user-defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.\nhive.security.authorization.createtable.user.grants  Default Value: (empty) Added In: Hive 0.7.0  The privileges automatically granted to some users whenever a table gets created. An example like \u0026ldquo;userX,userY:select;userZ:create\u0026rdquo; will grant select privilege to userX and userY, and grant create privilege to userZ whenever a new table created.\nhive.security.authorization.createtable.group.grants  Default Value: (empty) Added In: Hive 0.7.0  The privileges automatically granted to some groups whenever a table gets created. An example like \u0026ldquo;groupX,groupY:select;groupZ:create\u0026rdquo; will grant select privilege to groupX and groupY, and grant create privilege to groupZ whenever a new table created.\nhive.security.authorization.createtable.role.grants  Default Value: (empty) Added In: Hive 0.7.0  The privileges automatically granted to some roles whenever a table gets created. An example like \u0026ldquo;roleX,roleY:select;roleZ:create\u0026rdquo; will grant select privilege to roleX and roleY, and grant create privilege to roleZ whenever a new table created.\nhive.security.authorization.createtable.owner.grants  Default Value: (empty) Added In: Hive 0.7.0  The privileges automatically granted to the owner whenever a table gets created. An example like \u0026ldquo;select,drop\u0026rdquo; will grant select and drop privilege to the owner of the table. Note that the default gives the creator of a table no access to the table.\nHive Metastore Security Metastore-side security was added in Hive 0.10.0 (HIVE-3705). For more information, see the overview in Authorization and details in Storage Based Authorization in the Metastore Server.\nFor general metastore configuration properties, see MetaStore.\nhive.metastore.pre.event.listeners  Default Value: (empty) Added In: Hive 0.9.0 with HIVE-2853  The pre-event listener classes to be loaded on the metastore side to run code whenever databases, tables, and partitions are created, altered, or dropped. Set this configuration property to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener in hive-site.xml to turn on Hive metastore-side security.\nhive.security.metastore.authorization.manager  Default Value: org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider Added In: Hive 0.10.0 with HIVE-3705; revised in Hive 0.14.0 with HIVE-7209  Hive 0.13 and earlier: The authorization manager class name to be used in the metastore for authorization. The user-defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.\nHive 0.14 and later: Names of authorization manager classes (comma separated) to be used in the metastore for authorization. User-defined authorization classes should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider. All authorization manager classes have to successfully authorize the metastore API call for the command execution to be allowed.\nThe DefaultHiveMetastoreAuthorizationProvider implements the standard Hive grant/revoke model. A storage-based authorization implementation is also provided to use as the value of this configuration property:\n org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider  which uses HDFS permissions to provide authorization instead of using Hive-style grant-based authorization.\nhive.security.metastore.authenticator.manager  Default Value: org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator Added In: Hive 0.10.0 with HIVE-3705  The authenticator manager class name to be used in the metastore for authentication. The user-defined authenticator class should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.\nhive.security.metastore.authorization.auth.reads  Default Value: true Added In: Hive 0.14.0 with HIVE-8221  If this is true, the metastore authorizer authorizes read actions on database and table. See Storage Based Authorization.\nhive.metastore.token.signature  Default Value: \u0026quot;\u0026rdquo; (empty string) Added in Hive 0.7.0, added to HiveConf in Hive 2.1.0  The delegation token service name to match when selecting a token from the current user\u0026rsquo;s tokens.\nSQL Standard Based Authorization Version\nHive 0.13.0 introduces fine-grained authorization based on the SQL standard authorization model. See HIVE-5837 for the functional specification and list of subtasks.\nhive.users.in.admin.role  Default Value: (empty) Added In: Hive 0.13.0 with HIVE-5959  A comma separated list of users which will be added to the ADMIN role when the metastore starts up. More users can still be added later on.\nhive.security.authorization.sqlstd.confwhitelist  Default Value: (empty, but includes list shown below implicitly) Added In: Hive 0.13.0 with HIVE-6846; updated in Hive 0.14.0 with HIVE-8534 and in subsequent releases with several JIRA issues  A Java regex. Configuration properties that match this regex can be modified by user when SQL standard base authorization is used.\nIf this parameter is not set, the default list is added by the SQL standard authorizer. To display the default list for the current release, use the command \u0026lsquo;set hive.security.authorization.sqlstd.confwhitelist\u0026rsquo;.\nIn Hive 0.13.0 the default white list has these properties (see HIVE-6846 for the same list arranged one property per line):\nhive.exec.reducers.bytes.per.reducer, hive.exec.reducers.max, hive.map.aggr, hive.map.aggr.hash.percentmemory, hive.map.aggr.hash.force.flush.memory.threshold, hive.map.aggr.hash.min.reduction, hive.groupby.skewindata, hive.optimize.multigroupby.common.distincts, hive.optimize.index.groupby, hive.optimize.ppd, hive.optimize.ppd.storage, hive.ppd.recognizetransivity, hive.optimize.groupby, hive.optimize.sort.dynamic.partition, hive.optimize.union.remove, hive.multigroupby.singlereducer, hive.map.groupby.sorted, hive.map.groupby.sorted.testmode, hive.optimize.skewjoin, hive.optimize.skewjoin.compiletime, hive.mapred.mode, hive.enforce.bucketmapjoin, hive.exec.compress.output, hive.exec.compress.intermediate, hive.exec.parallel, hive.exec.parallel.thread.number, hive.exec.rowoffset, hive.merge.mapfiles, hive.merge.mapredfiles, hive.merge.tezfiles, hive.ignore.mapjoin.hint, hive.auto.convert.join, hive.auto.convert.join.noconditionaltask, hive.auto.convert.join.noconditionaltask.size, hive.auto.convert.join.use.nonstaged, hive.enforce.bucketing, hive.enforce.sorting, hive.enforce.sortmergebucketmapjoin, hive.auto.convert.sortmerge.join, hive.execution.engine, hive.vectorized.execution.enabled, hive.mapjoin.optimized.keys, hive.mapjoin.lazy.hashtable, hive.exec.check.crossproducts, hive.compat, hive.exec.dynamic.partition.mode, mapred.reduce.tasks, mapred.output.compression.codec, mapred.map.output.compression.codec, mapreduce.job.reduce.slowstart.completedmaps, mapreduce.job.queuename.\nVersion Information\nHive 0.14.0 adds new parameters to the default white list (see HIVE-8534).\nHive 1.1.0 removes some parameters (see HIVE-9331).\nHive 1.2.0 and 1.2.1 add more new parameters (see HIVE-10578, HIVE-10678, and HIVE-10967).\nHive 1.3.0, 2.1.1, and 2.2.0 add further new parameters (see HIVE-14073).\nHive 3.0.0 fixes a parameter added in 1.2.1, changing mapred.job.queuename to mapred.job.queue.name (see HIVE-17584).\nSome parameters are added automatically when they match one of the regex specifications for the white list in HiveConf.java (for example, hive.log.trace.id in Hive 2.0.0 – see HIVE-12419 ).\nNote that the hive.conf.restricted.list checks are still enforced after the white list check.\nhive.security.authorization.sqlstd.confwhitelist.append  Default Value: (empty) Added In: Hive 0.14.0 with HIVE-8534  Second Java regex that the whitelist of configuration properties would match in addition to hive.security.authorization.sqlstd.confwhitelist. Do not include a starting | in the value.\nUsing this regex instead of updating the original regex for hive.security.authorization.sqlstd.confwhitelist means that you can append to the default that is set by SQL standard authorization instead of replacing it entirely.\nhive.server2.builtin.udf.whitelist  Default Value: (empty, treated as not set – all UDFs allowed) Added In: Hive 1.1.0 with HIVE-8893  A comma separated list of builtin UDFs that are allowed to be executed. A UDF that is not included in the list will return an error if invoked from a query. If set to empty, then treated as wildcard character – all UDFs will be allowed. Note that this configuration is read at the startup time by HiveServer2 and changing this using a \u0026lsquo;set\u0026rsquo; command in a session won\u0026rsquo;t change the behavior.\nhive.server2.builtin.udf.blacklist  Default Value: (empty) Added In: Hive 1.1.0 with HIVE-8893  A comma separated list of builtin UDFs that are not allowed to be executed. A UDF that is included in the list will return an error if invoked from a query. Note that this configuration is read at the startup time by HiveServer2 and changing this using a \u0026lsquo;set\u0026rsquo; command in a session won\u0026rsquo;t change the behavior.\nhive.security.authorization.task.factory  Default Value: org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl Added In: Hive 1.1.0 with HIVE-8611  To override the default authorization DDL handling, set hive.security.authorization.task.factory to a class that implements the org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactory interface.\nArchiving See Archiving for File Count Reduction for general information about Hive support for Hadoop archives.\nfs.har.impl  Default Value: org.apache.hadoop.hive.shims.HiveHarFileSystem Added In: Hive 0.8.1  The implementation for accessing Hadoop Archives. Note that this won\u0026rsquo;t be applicable to Hadoop versions less than 0.20.\nhive.archive.enabled  Default Value: false Added In: Hive 0.6.0  Whether archiving operations are permitted.\nhive.archive.har.parentdir.settable  Default Value: false Added In: Hive 0.6.0 Removed In: Hive 0.10.0 with HIVE-3338  In new Hadoop versions, the parent directory must be set while creating a HAR. Because this functionality is hard to detect with just version numbers, this configuration variable needed to be set manually in Hive releases 0.6.0 through 0.9.0. (This configuration property was removed in release 0.10.0.)\nLocking See Hive Concurrency Model for general information about locking.\nhive.support.concurrency  Default Value: false Added In: Hive 0.7.0 with HIVE-1293  Whether Hive supports concurrency or not. A ZooKeeper instance must be up and running for the default Hive lock manager to support read-write locks.\nSet to true to support INSERT \u0026hellip; VALUES, UPDATE, and DELETE transactions (Hive 0.14.0 and later). For a complete list of parameters required for turning on Hive transactions, see hive.txn.manager .\nhive.lock.manager  Default Value: org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager Added In: Hive 0.7.0 with HIVE-1293  The lock manager to use when hive.support.concurrency is set to true.\nhive.lock.mapred.only.operation  Default Value: false Added In: Hive 0.8.0  This configuration property is to control whether or not only do lock on queries that need to execute at least one mapred job.\nhive.lock.query.string.max.length  Default Value: 1000000 Added In: Hive 3.0.0  The maximum length of the query string to store in the lock. The default value is 1000000, since the data limit of a znode is 1MB\nhive.lock.numretries  Default Value: 100 Added In: Hive 0.7.0 with HIVE-1293  The total number of times you want to try to get all the locks.\nhive.unlock.numretries  Default Value: 10 Added In: Hive 0.8.1  The total number of times you want to do one unlock.\nhive.lock.sleep.between.retries  Default Value: 60 Added In: Hive 0.7.0 with HIVE-1293  The sleep time (in seconds) between various retries.\nhive.zookeeper.quorum  Default Value: (empty) Added In: Hive 0.7.0 with HIVE-1293  The list of ZooKeeper servers to talk to. This is only needed for read/write locks.\nhive.zookeeper.client.port  Default Value:  Hive 0.7.0: (empty) Hive 0.8.0 and later: 2181 (HIVE-2196)   Added In: Hive 0.7.0 with HIVE-1293  The port of ZooKeeper servers to talk to. This is only needed for read/write locks.\nhive.zookeeper.session.timeout  Default Value:  Hive 0.7.0 to 1.1.x: 600000ms Hive 1.2.0 and later:1200000ms (HIVE-8890)   Added In: Hive 0.7.0 with HIVE-1293  ZooKeeper client\u0026rsquo;s session timeout (in milliseconds). The client is disconnected, and as a result, all locks released, if a heartbeat is not sent in the timeout.\nhive.zookeeper.namespace  Default Value: hive_zookeeper_namespace Added In: Hive 0.7.0  The parent node under which all ZooKeeper nodes are created.\nhive.zookeeper.clean.extra.nodes  Default Value: false Added In: Hive 0.7.0  Clean extra nodes at the end of the session.\nhive.lockmgr.zookeeper.default.partition.name  Default Value: __HIVE_DEFAULT_ZOOKEEPER_PARTITION__ Added In: Hive 0.7.0 with HIVE-1293  The default partition name when ZooKeeperHiveLockManager is the hive lock manager .\nMetrics The metrics that Hive collects can be viewed in the HiveServer2 Web UI. For more information, see Hive Metrics.\nhive.metastore.metrics.enabled  Default Value: false Added in: Hive 1.3.0 and 2.0.0 with HIVE-10761  Enable metrics on the Hive Metastore Service. (For other metastore configuration properties, see the Metastore and Hive Metastore Security sections.)\nhive.metastore.acidmetrics.thread.on  Default Value: true Added in: Hive 4.0.0 with HIVE-24824  Whether to run acid related metrics collection on this metastore instance.\nhive.server2.metrics.enabled  Default Value: false Added in: Hive 1.3.0 and 2.0.0 with HIVE-10761  Enable metrics on HiveServer2. (For other HiveServer2 configuration properties, see the HiveServer2 section.)\nhive.service.metrics.class  Default Value: org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics Added in: Hive 1.3.0 and 2.0.0 with HIVE-10761  Hive metrics subsystem implementation class. \u0026ldquo;org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics\u0026rdquo; is the new implementation. To revert back to the old implementation before Hive 1.3 and 2.0 along with its built-in JMX reporting capabilities, choose \u0026ldquo;org.apache.hadoop.hive.common.metrics.LegacyMetrics\u0026rdquo;.\nhive.service.metrics.reporter  Default Value: \u0026ldquo;JSON_FILE, JMX\u0026rdquo; Added in: Hive 1.3.0 and 2.0.0 with HIVE-10761 Deprecated in: Hive 3.0.0 with HIVE-16206  Reporter type for metric class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics, a comma separated list of values JMX, CONSOLE, JSON_FILE.\nA new reporter type HADOOP2 has been added in Hive 2.1.0 with HIVE-13480.\nhive.service.metrics.codahale.reporter.classes  Default Value: \u0026ldquo;org.apache.hadoop.hive.common.metrics.metrics2.JsonFileMetricsReporter, org.apache.hadoop.hive.common.metrics.metrics2.JmxMetricsReporter\u0026rdquo; Added in: Hive 3.0.0 with HIVE-16206  Comma separated list of reporter implementation classes for metric class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics. Overrides hive.service.metrics.reporter conf if present.\nhive.service.metrics.file.location  Default Value: \u0026ldquo;/tmp/report.json\u0026rdquo; Added in: Hive 1.3.0 and 2.0.0 with HIVE-10761  For hive.service.metrics.class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics and hive.service.metrics.reporter JSON_FILE, this is the location of the local JSON metrics file dump. This file will get overwritten at every interval of hive.service.metrics.file.frequency.\nhive.service.metrics.file.frequency  Default Value: 5 seconds Added in: Hive 1.3.0 and 2.0.0 with HIVE-10761  For hive.service.metrics.class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics and hive.service.metrics.reporter JSON_FILE, this is the frequency of updating the JSON metrics file.\nhive.service.metrics.hadoop2.component  Default Value: \u0026ldquo;hive\u0026rdquo; Added in: Hive 2.1.0 with HIVE-13480  For hive.service.metrics.class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics and hive.service.metrics.reporter HADOOP2, this is the component name to provide to the HADOOP2 metrics system. Ideally \u0026lsquo;hivemetastore\u0026rsquo; for the MetaStore and \u0026lsquo;hiveserver2\u0026rsquo; for HiveServer2. The metrics will be updated at every interval of hive.service.metrics.hadoop2.frequency.\nhive.service.metrics.hadoop2.frequency  Default Value: 30 seconds Added in: Hive 2.1.0 with HIVE-13480  For hive.service.metrics.class org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics and hive.service.metrics.reporter HADOOP2, this is the frequency of updating the HADOOP2 metrics system.\nClustering hive.cluster.delegation.token.store.class  Default Value: org.apache.hadoop.hive.thrift.MemoryTokenStore Added In: Hive 0.9.0  The delegation token store implementation. Set to org.apache.hadoop.hive.thrift.ZooKeeperTokenStore for load-balanced cluster.\nhive.cluster.delegation.token.store.zookeeper.connectString  Default Value: localhost:2181 Added In: Hive 0.9.0  The ZooKeeper token store connect string.\nhive.cluster.delegation.token.store.zookeeper.znode  Default Value: /hive/cluster/delegation Added In: Hive 0.9.0  The root path for token store data.\nhive.cluster.delegation.token.store.zookeeper.acl  Default Value: sasl:hive/host1@EXAMPLE.COM:cdrwa,sasl:hive/host2@EXAMPLE.COM:cdrwa Added In: Hive 0.9.0  ACL for token store entries. List comma separated all server principals for the cluster.\nRegions Reverted by HIVE-2612 in Hive 0.9.0\nThe configuration properties that used to be documented in this section (hive.use.input.primary.region, hive.default.region.name, and hive.region.properties) existed temporarily in trunk before Hive release 0.9.0 but they were removed before the release. See HIVE-2612 and HIVE-2965.\nApologies for any confusion caused by their former inclusion in this document.\nCommand Line Interface hive.cli.print.header  Default Value: false Added In: Hive 0.7.0  Whether to print the names of the columns in query output.\nhive.cli.print.current.db  Default Value: false Added In: Hive 0.8.1  Whether to include the current database in the Hive prompt.\nHBase StorageHandler hive.hbase.wal.enabled  Default Value: true Added In: Hive 0.6.0 with HIVE-1383  Whether writes to HBase should be forced to the write-ahead log. Disabling this improves HBase write performance at the risk of lost writes in case of a crash.\nhive.hbase.generatehfiles  Default Value: false Added In: Hive 0.14.0 with HIVE-6473 and HIVE-7211  True when HBaseStorageHandler should generate hfiles instead of operate against the online table.\nHive Web Interface (HWI) (component removed as of Hive 2.2.0) hive.hwi.war.file  Default Value: lib/hive-hwi-\u0026lt;version\u0026gt;.war Added In: Hive 0.3.0 with default lib/hive_hwi.war, default changed to lib/hive-hwi-\u0026lt;version\u0026gt;.war in Hive 0.5 (HIVE-978 and HIVE-1183) Removed In: Hive 2.2.0 with HIVE-15622  This sets the path to the HWI war file, relative to ${HIVE_HOME}. (This configuration property was removed in release 2.2.0.)\nhive.hwi.listen.host  Default Value: 0.0.0.0 Added In: Hive 0.3.0 Removed In: Hive 2.2.0 with HIVE-15622  This is the host address the Hive Web Interface will listen on. (This configuration property was removed in release 2.2.0.)\nhive.hwi.listen.port  Default Value: 9999 Added In: Hive 0.3.0 Removed In: Hive 2.2.0 with HIVE-15622  This is the port the Hive Web Interface will listen on. (This configuration property was removed in release 2.2.0.)\nReplication hive.repl.rootdir  Default Value: /usr/hive/repl/ Added in: Hive 2.2.0 with HIVE-15151  This is an HDFS root directory under which Hive\u0026rsquo;s REPL DUMP command will operate, creating dumps to replicate along to other warehouses. hive.repl.replica.functions.root.dir  Default Value: /usr/hive/repl/functions Added in: Hive 3.0.0 with HIVE-16 591  Root directory on the replica warehouse where the repl sub-system will store jars from the primary warehouse.\nhive.repl.partitions.dump.parallelism  Default Value: 100 Added in: Hive 3.0.0 with HIVE-16895; default changed with HIVE-17625 (also in 3.0.0)  Number of threads that will be used to dump partition data information during REPL DUMP.\nhive.repl.approx.max.load.tasks  Default Value: 10000 Added in: Hive 3.0.0 with HIVE-16896  Provide an approximation of the maximum number of tasks that should be executed before dynamically generating the next set of tasks. The number is approximate as Hive will stop at a slightly higher number, the reason being some events might lead to a task increment that would cross the specified limit.\nhive.repl.dump.metadata.only  Default Value: false Added in: Hive 3.0.0 with HIVE-18352  Indicates whether the REPL DUMP command dumps only metadata information (true) or data + metadata (false).\nhive.repl.dump.include.acid.tables  Default Value: false Added in: Hive 3.0.0 with HIVE-18352  Indicates whether replication dump should include information about ACID tables. It should be used in conjunction with hive.repl.dump.metadata.only to enable copying of metadata for ACID tables which do not require the corresponding transaction semantics to be applied on target. This can be removed when ACID table replication is supported.\nhive.repl.add.raw.reserved.namespace  Default Value: false Added in: Hive 3.0.0 with HIVE-18341  For TDE with same encryption keys on source and target, allow Distcp super user to access the raw bytes from filesystem without decrypting on source and then encrypting on target.\nBlobstore (i.e. Amazon S3) Starting in release 2.2.0, a set of configurations was added to enable read/write performance improvements when working with tables stored on blobstore systems, such as Amazon S3.\nhive.blobstore.supported.schemes  Default value: s3,s3a,s3n Added In: Hive 2.2.0 with HIVE-14270  List of supported blobstore schemes that Hive uses to apply special read/write performance improvements.\nhive.blobstore.optimizations.enabled  Default value: true Added In: Hive 2.2.0 with HIVE-15121  This parameter is a global variable that enables a number of optimizations when running on blobstores.\nSome of the optimizations, such as hive.blobstore.use.blobstore.as.scratchdir , won\u0026rsquo;t be used if this variable is set to false.\nhive.blobstore.use.blobstore.as.scratchdir  Default value: false Added In: Hive 2.2.0 with HIVE-14270  Set this to true to enable the use of scratch directories directly on blob storage systems (it may cause performance penalties).\nhive.exec.input.listing.max.threads  Default value: 0 (disabled) Added In: Hive 2.2.0 with HIVE-15881  Set this to a maximum number of threads that Hive will use to list file information from file systems, such as file size and number of files per table (recommended \u0026gt; 1 for blobstore).\nTest Properties Note: This is an incomplete list of configuration properties used by developers when running Hive tests. For other test properties, search for \u0026ldquo;hive.test.\u0026rdquo; in hive-default.xml.template or HiveConf.java. Also see Beeline Query Unit Test.\nhive.test.mode  Default Value: false Added In: Hive 0.4.0  Whether Hive is running in test mode. If yes, it turns on sampling and prefixes the output tablename.\nhive.test.mode.prefix  Default Value: test_ Added In: Hive 0.4.0  If Hive is running in test mode, prefixes the output table by this string.\nhive.test.mode.samplefreq  Default Value: 32 Added In: Hive 0.4.0  If Hive is running in test mode and table is not bucketed, sampling frequency.\nhive.test.mode.nosamplelist  Default Value: (empty) Added In: Hive 0.4.0  If Hive is running in test mode, don\u0026rsquo;t sample the above comma separated list of tables.\nhive.exec.submit.local.task.via.child  Default Value: true Added In: Hive 0.14.0 with HIVE-7271  Determines whether local tasks (typically mapjoin hashtable generation phase) run in a separate JVM (true recommended) or not. Avoids the overhead of spawning new JVMs, but can lead to out-of-memory issues. See HIVE-7271 for details.\nHCatalog Configuration Properties Starting in Hive release 0.11.0, HCatalog is installed and configured with Hive. The HCatalog server is the same as the Hive metastore. See Hive Metastore Administration for metastore configuration properties. For Hive releases prior to 0.11.0, see the \u0026ldquo;Thrift Server Setup\u0026rdquo; section in the HCatalog 0.5.0 document Installation from Tarball for information about setting the Hive metastore configuration properties.\nJobs submitted to HCatalog can specify configuration properties that affect storage, error tolerance, and other kinds of behavior during the job. See HCatalog Configuration Properties for details.\nWebHCat Configuration Properties For WebHCat configuration, see Configuration Variables in the WebHCat manual.\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/configuration-properties_27842758/","tags":null,"title":"Apache Hive : Configuration Properties"},{"categories":null,"contents":"Apache Hive : ContributorDay2011 Apache Hive Contributor Day is a special event hosted as part of Yahoo\u0026rsquo;s Hadoop Summit.\nResources for mini-hackathon:\n PluginDeveloperKit has info on the new pdk; download this snapshot build of Hive which includes it you\u0026rsquo;ll need a Mac or Linux development environment with Hive+Hadoop already installed on it per these instructions; for Hive, use the snapshot you\u0026rsquo;ll also need Apache ant installed. HIVE-1545 has the UDF libraries we\u0026rsquo;d like to get cleaned up for inclusion in Hive or extension libraries (download core.tar.gz and/or ext.tar.gz)  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/contributorday2011_27820725/","tags":null,"title":"Apache Hive : ContributorDay2011"},{"categories":null,"contents":"Apache Hive : ContributorMinutes20110907 Notes from the Hive Meetup at Hortonworks, 9/7/11\nAttendees: http://www.meetup.com/Hive-Contributors-Group/events/30620561/\nThe Binary type proposed by Ashutosh Chauhan, HIVE-2380 was discussed. There was agreement that a design document is needed to explain the proposed changes for this feature. The design document should cover:\n Can columns of binary type be used as a key in group by or join? What native functions exist to manipulate binary types? How does casting between binary and other types work? Are there implicit casts? (There seemed to be general agreement that implicit casting for binary types was a bad idea.) How is binary data encoded when being passed as part of a TRANSFORM? A discussion of what uses the designer envisions for binary fields. This should clarify that this binary type is not a blob (stored out of row) and is not intended for very large objects.  Alan Gates, as an HCatalog developer, asked if the Hive community would be open to moving some authorization related checks into the Thrift metastore server instead of all being done in the client. John Sichi was of the opinion that not all authorization checks can be moved to the server because some need to be done in the client. But DDL ones, which is what HCatalog is interested in, should be movable. The next step is for the HCatalog team to produce a design document clarifying what this change would look like.\nCarl Steinbach led a discussion on the release of Hive 0.8. The release branch was cut last week, and it should be possible to produce an initial release candidate next week. The following blockers remain:\n Publishing Maven artifacts for released versions of Hive Metastore upgrade scripts for changes from 0.7.x Issue with starting the Thrift metastore server when Hadoop is running in secure mode: HIVE-2411  As part of releasing 0.8, the Hive community should publish a proposed date for the next release.\nA question was posted to the meetup page by Amareshwari. She wanted to know if anyone was planning on porting Hive to Hadoop 0.23, once that version was out. Alan said that Hortonworks would like to contribute to this, and is interested in collaborating with others on it.\nAmareshwari posted a second question on moving Hive from the old mapred interface to MapReduce to the newer mapreduce. There was consensus amongst the committers present that it was better to stay on mapred for now since it was guaranteed to be stable even in 0.23, while mapreduce is evolving.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/contributorminutes20110907_27826430/","tags":null,"title":"Apache Hive : ContributorMinutes20110907"},{"categories":null,"contents":"Apache Hive : ContributorMinutes20111205 Notes from the Hive Meetup at Facebook, 12/5/11\nAttendees: http://www.meetup.com/Hive-Contributors-Group/events/41150912/\nJohn gave a demo of the Phabricator instance at http://reviews.facebook.net, and proposed that we push through moving all code review over from Review Board to Phabricator (https://cwiki.apache.org/confluence/display/Hive/PhabricatorCodeReview). There were no objections.\nMarek gave an overview of the new parallel test framework (https://issues.apache.org/jira/browse/HIVE-1487); he\u0026rsquo;ll publish a wiki page explaining how to use it once it gets committed.\nCarl gave an update on the 0.8 release; delays have been due to difficulties with the metastore upgrade scripts, which should now be all sorted out. Carl proposed cutting a new branch from trunk and releasing that as 0.8, with rationale (a) some backports to the old branch are difficult; (b) trunk is stable; (c) trunk contains lots of new high-value work such as the BINARY datatype. There were no objections.\nCarl also proposed that in the future, developers delivering metastore changes should also be responsible for supplying metastore upgrade scripts for all supported DBMS\u0026rsquo;s (currently MySQL and Derby).\nA patch has been committed to make Hive build and run against Hadoop 0.23, and Carl has set up a Jenkins instance for continuous integration. Some tests are still failing, and it is uncertain whether a Hive binary built against 0.20.x will run against Hadoop 0.23. Once tests are all fixed, we\u0026rsquo;ll start requiring committers to keep them working (e.g. if something gets committed which passes tests on 0.20.x, but breaks 0.23, the committer needs to either submit a timely followup to address the breakage, or else back out the original change). There was some discussion about doing the same for Hadoop 0.20.20x, but no resolution.\nAshutosh asked about a registry of available Hive storage handlers, and John referenced the Introduction section in https://cwiki.apache.org/confluence/display/Hive/StorageHandlers.\nCode walkthroughs were carried out for HIVE-2616 and HIVE-2589.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/contributorminutes20111205_27833038/","tags":null,"title":"Apache Hive : ContributorMinutes20111205"},{"categories":null,"contents":"Apache Hive : ContributorMinutes20120418 Notes from the Hive Contributors Meetup at Cloudera, 4/18/12\nAttendees: http://www.meetup.com/Hive-Contributors-Group/events/59148562/\nAshutosh gave a status update on the Hive 0.9.0 release work. RC0 was put up for a vote last week, but it turned out there were several problems. Ashutosh is in the process of fixing those issues, and is also trying to get several other patches resolved and backported before cutting RC1.\nCarl asked for more details about the impact of HIVE-2795 on the upgrade process for 0.9.0. Kevin responded that they have decided to implement regions in a layer above Hive, and do not plan to use the features that were added in HIVE-2612. Since these two features are the only things requiring a metastore upgrade for 0.9.0, it was proposed that we back them out. There were no objections.\nCarl said that he is organizing the Hive BoF session at this year\u0026rsquo;s Hadoop Summit. The meeting will take place on June 12th from 2-5pm. An official announcement will go up on the Hive Meetup group shortly. The current plan is to structure the event like last year: 4-6 fifteen minute long talks, followed by smaller breakout sessions. Please contact Carl if you\u0026rsquo;re interested in giving a talk.\nThe discussion next turned to problems with Arc and Phabricator. Carl expressed concern that bugs have crept in over the past couple of months, and that it\u0026rsquo;s no longer clear who is responsible for making sure Hive works with Arc/Phabricator. John pointed out that the issues which were raised on the dev mailing list last week have already been resolved. There was general consensus that when it works, Arc/Phabricator is an improvement on ReviewBoard. John proposed that we continue using Arc/Phabricator, and raise any problems with it on the dev maligning list. There were no objections.\nHarish gave a short presentation on the SQL Windowing library he wrote for Hive and how it might be integrated into Hive. Everyone agreed that adding this functionality to Hive makes sense. Several people suggested adding the toolkit to the contrib module as-is and using it to generate interest with users, but concerns were raised that this might be painful to support/deprecate in the future. The discussion ended with general agreement that we should start work now to incrementally push this capability into Hive\u0026rsquo;s query compiler.\nCarl explained the motivations and design decisions behind the HiveServer2 API proposal. The main motivations are supporting concurrency and providing a better foundation on which to build ODBC and JDBC drivers. Work on this project has started and is being tracked in HIVE-2935.\nNamit offered to host the next contrib meeting at Facebook.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/contributorminutes20120418_27844528/","tags":null,"title":"Apache Hive : ContributorMinutes20120418"},{"categories":null,"contents":"Apache Hive : ContributorsMinutes110726 Meeting date: July 26, 2011\nLocation: Cloudera (Palo Alto)\nAttendees: http://www.meetup.com/Hive-Contributors-Group/events/26345541/\nCarl proposed end of August as target for 0.8 release, with branch cut in a couple of weeks. Work is still underway for publishing release artifacts in Maven for 0.7.1 and 0.8 (development snapshots are already being published).\nAshutosh gave an update on HCatalog development status; no blocking issues from Hive for the 0.2 release; some ideas are being discussed for the 0.3 release which will need Hive work, but these aren\u0026rsquo;t ready for design proposals yet.\nInterns from Facebook gave an overview of some of the work they\u0026rsquo;ve been doing.\nSohan Jain: add new filtering support for metastore queries, and eliminate duplication of column descriptors across partitions. For the latter, a metastore schema change is needed which cannot be upgraded without a downtime. This is currently being tested out at Facebook; for a 30GB metastore, the downtime required is 10-15 minutes.\nFranklin Hu: adding TIMESTAMP datatype (supports fractional seconds, but not timezones). There was some discussion about storing the internal representation in a canonical timezone such as GMT.\nSyed Albiz: working on automatic usage of indexes for (a) bitmap indexes and (b) queries involving multiple tables; also error message improvements. Bitmap index performance test results to be added to the wiki later.\nCharles Chen: working on improving view support (ALTER VIEW RENAME, CREATE OR REPLACE VIEW, CREATE table LIKE view). Will follow up with support for partitioned join views and simple updatable views.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/contributorsminutes110726_27822784/","tags":null,"title":"Apache Hive : ContributorsMinutes110726"},{"categories":null,"contents":"Apache Hive : Copy of Hive Schema Tool - [TODO: move it under a 4.0 admin manual page, find a proper name]  About Metastore Schema Verification The Hive Schema Tool  The schematool Command Usage Examples    About Schema tool helps to initialise and upgrade metastore database and hive sys schema.\nMetastore Schema Verification Hive records the schema version in the metastore database and verifies that the metastore schema version is compatible with Hive binaries that are going to access the metastore. Note that the Hive properties to implicitly create or alter the existing schema are disabled by default. Hive will not attempt to change the metastore schema implicitly. When you execute a Hive query against an old schema, it will fail to access the metastore:\n$ build/dist/bin/hive -e \u0026quot;show tables\u0026quot; FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient The log will contain an error about version information not found:\n... Caused by: MetaException(message:Version information not found in metastore. ) ... By default the configuration property metastore.schema.verification is true. If we set it false, metastore implicitly writes the schema version if it\u0026rsquo;s not matching. To disable the strict schema verification, you need to set this property to false in hive-site.xml.\nSee Hive Metastore Administration for general information about the metastore.\nThe Hive Schema Tool The Hive distribution now includes an offline tool for Hive metastore schema manipulation. This tool can be used to initialize the metastore schema for the current Hive version. It can also handle upgrading the schema from an older version to current. It tries to find the current schema from the metastore if it is available. This will be applicable to future upgrades like 0.12.0 to 0.13.0. In case of upgrades from older releases like 0.7.0 or 0.10.0, you can specify the schema version of the existing metastore as a command line option to the tool.\nThe schematool figures out the SQL scripts required to initialize or upgrade the schema and then executes those scripts against the backend database. The metastore DB connection information like JDBC URL, JDBC driver and DB credentials are extracted from the Hive configuration. You can provide alternate DB credentials if needed.\nWarning: You should always initialize the metastore schema first and the Hive schema second. The Hive schema initialization checks the metastore database schema and if it is not initialized, it puts some objects into the metastore database as well to make sure hive schema is running. That also means if you want to run the metastore schema initialization after the hive one, it will fail as it finds a database that already contains some objects. The schematool Command The schematool command invokes the Hive schema tool with these options:\n$ usage: schemaTool -alterCatalog \u0026lt;arg\u0026gt; Alter a catalog, requires --catalogLocation and/or --catalogDescription parameter as well -catalogDescription \u0026lt;arg\u0026gt; Description of new catalog -catalogLocation \u0026lt;arg\u0026gt; Location of new catalog, required when adding a catalog -createCatalog \u0026lt;arg\u0026gt; Create a catalog, requires --catalogLocation parameter as well -createLogsTable \u0026lt;arg\u0026gt; Create table for Hive warehouse/compute logs -createUser Create the Hive user, set hiveUser to the db admin user and the hive password to the db admin password with this -dbOpts \u0026lt;databaseOpts\u0026gt; Backend DB specific options -dbType \u0026lt;databaseType\u0026gt; Metastore database type -driver \u0026lt;driver\u0026gt; driver name for connection -dropAllDatabases Drop all Hive databases (with CASCADE). This will remove all managed data! -dryRun list SQL scripts (no execute) -fromCatalog \u0026lt;arg\u0026gt; Catalog a moving database or table is coming from. This is required if you are moving a database or table. -fromDatabase \u0026lt;arg\u0026gt; Database a moving table is coming from. This is required if you are moving a table. -help print this message -hiveDb \u0026lt;arg\u0026gt; Hive database (for use with createUser) -hivePassword \u0026lt;arg\u0026gt; Hive password (for use with createUser) -hiveUser \u0026lt;arg\u0026gt; Hive user (for use with createUser) -ifNotExists If passed then it is not an error to create an existing catalog -info Show config and schema details -initOrUpgradeSchema Initialize or upgrade schema to latest version -initSchema Schema initialization -initSchemaTo \u0026lt;initTo\u0026gt; Schema initialization to a version -mergeCatalog \u0026lt;arg\u0026gt; Merge databases from a catalog into other, Argument is the source catalog name Requires --toCatalog to indicate the destination catalog -metaDbType \u0026lt;metaDatabaseType\u0026gt; Used only if upgrading the system catalog for hive -moveDatabase \u0026lt;arg\u0026gt; Move a database between catalogs. Argument is the database name. Requires --fromCatalog and --toCatalog parameters as well -moveTable \u0026lt;arg\u0026gt; Move a table to a different database. Argument is the table name. Requires --fromCatalog, --toCatalog, --fromDatabase, and --toDatabase parameters as well. -passWord \u0026lt;password\u0026gt; Override config file password -retentionPeriod \u0026lt;arg\u0026gt; Specify logs table retention period -servers \u0026lt;serverList\u0026gt; a comma-separated list of servers used in location validation in the format of scheme://authority (e.g. hdfs://localhost:8000) -toCatalog \u0026lt;arg\u0026gt; Catalog a moving database or table is going to. This is required if you are moving a database or table. -toDatabase \u0026lt;arg\u0026gt; Database a moving table is going to. This is required if you are moving a table. -upgradeSchema Schema upgrade -upgradeSchemaFrom \u0026lt;upgradeFrom\u0026gt; Schema upgrade from a version -url \u0026lt;url\u0026gt; connection url to the database -userName \u0026lt;user\u0026gt; Override config file user name -validate Validate the database -verbose only print SQL statements -yes Don't ask for confirmation when using -dropAllDatabases. The dbType is required and can be one of:\n derby|mysql|postgres|oracle|mssql|hive Note: dbType=hive only can be used on Hive sys schema. The others are metastore db types and in case of dbType=hive, it is mandatory to set metaDbType as well. Usage Examples  Initialize to current schema for a new Hive setup:  $ schematool -dbType derby -initSchema Initializing the schema to: 4.0.0-beta-2 Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting metastore schema initialization to 4.0.0-beta-2 Initialization script hive-schema-4.0.0-beta-2.derby.sql Initialization script completed  Get schema information:  $ schematool -dbType derby -info Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Hive distribution version:\t4.0.0-beta-2 Metastore schema version:\t4.0.0-beta-2  Init schema to for a given version:   $ schematool -dbType derby -initSchemaTo 3.1.0 Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting metastore schema initialization to 3.1.0 Initialization script hive-schema-3.1.0.derby.sql  Upgrade schema from an 3.1.0 release by specifying the \u0026lsquo;from\u0026rsquo; version:  $ schematool -dbType derby -upgradeSchemaFrom 3.1.0 Upgrading from the user input version 3.1.0 Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting upgrade metastore schema from version 3.1.0 to 4.0.0-beta-2 Upgrade script upgrade-3.1.0-to-3.2.0.derby.sql Completed upgrade-3.1.0-to-3.2.0.derby.sql ... Completed upgrade-4.0.0-beta-1-to-4.0.0-beta-2.derby.sql  Upgrade dry run can be used to list the required scripts for the given upgrade.  $ schematool -dbType derby -upgradeSchemaFrom 3.1.0 -dryRun Upgrading from the user input version 3.1.0 Metastore connection URL:\tjdbc:derby:;databaseName=metastore_db;create=true Metastore connection Driver :\torg.apache.derby.jdbc.EmbeddedDriver Metastore connection User:\tAPP Starting upgrade metastore schema from version 3.1.0 to 4.0.0-beta-2 Upgrade script upgrade-3.1.0-to-3.2.0.derby.sql Upgrade script upgrade-3.2.0-to-4.0.0-alpha-1.derby.sql Upgrade script upgrade-4.0.0-alpha-1-to-4.0.0-alpha-2.derby.sql Upgrade script upgrade-4.0.0-alpha-2-to-4.0.0-beta-1.derby.sql Upgrade script upgrade-4.0.0-beta-1-to-4.0.0-beta-2.derby.sql This is useful if you just want to find out all the required scripts for the schema upgrade.\n Initialise Hive sys schema   $ ./schematool -dbType hive -metaDbType derby -initSchema --verbose -url=\u0026quot;jdbc:hive2://localhost:10000\u0026quot; Note 1: As\n Moving a database and tables under it from default Hive catalog to a custom spark catalog  build/dist/bin/schematool -moveDatabase db1 -fromCatalog hive -toCatalog spark  Moving a table from Hive catalog to Spark Catalog  # Create the desired target database in spark catalog if it doesn't already exist. beeline ... -e \u0026quot;create database if not exists newdb\u0026quot;; schematool -moveDatabase newdb -fromCatalog hive -toCatalog spark # Now move the table to target db under the spark catalog. schematool -moveTable table1 -fromCatalog hive -toCatalog spark -fromDatabase db1 -toDatabase newdb ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/284790216/","tags":null,"title":"Apache Hive : Copy of Hive Schema Tool - [TODO: move it under a 4.0 admin manual page, find a proper name]"},{"categories":null,"contents":"Apache Hive : Correlation Optimizer This page documents Correlation Optimizer. It was originally introduced by HIVE-2206 and based on the idea of YSmart [1]. To turn on this optimizer, you can use \u0026hellip;\nset hive.optimize.correlation=true; 1. Overview In Hadoop environments, an SQL query submitted to Hive will be evaluated in distributed systems. Thus, after generating a query operator tree representing the submitted SQL query, Hive needs to determine what operations can be executed in a task which will be evalauted in a single node. Also, since a MapReduce job can shuffle data data once, Hive also needs to cut the tree to multiple MapReduce jobs. It is important to cut an operator tree to multiple MapReduce in a good way, so the generated plan can evaluate the query efficiently.\nWhen generating an operator tree for a given SQL query, Hive identifies when to shuffle the data through operations which may need to shuffle data. For example, a JOIN operation may need to shuffle the input data if input tables have not been distributed by join columns. However, in a complex query, it is possible that the input data of an operation which may need to shuffle the input data has already been partitioned in the desired way. For example, it is possible we can have a query like SELECT t1.key, sum(value) FROM t1 JOIN t2 ON (t1.key = t2.key) GROUP BY t1.key. In this example, both JOIN operation and GROUP BY operation may need to shuffle the input data. However, because the output of JOIN operation is the input of GROUP BY operation and it has been already partitioned by t1.key, we do not need to shuffle the data for GROUP BY operation. However, Hive is not aware this correlation between JOIN operation and GROUP BY operation and thus it will generate two separate MapReduce jobs to evaluate this query. Basically, we unnecessarily shuffle the data for GROUP BY operation. In a more complex query, correlation-unaware query planning can generate a very inefficient execution plan and result in poor performance.\nBefore we integrating Correlation Optimizer into Hive, Hive has ReduceSink Deduplication Optimizer which can figure out if we need to shuffle data for chained operators. However, to support more complex operator trees, we need a more general-purpose optimizer and a mechanism to correctly execute optimized plan. Thus, we have designed and implemented Correlation Optimizer and two operators for evaluating optimized plans. It is worth noting that it is better to use ReduceSink Deduplication Optimizer to handle simple cases first and then use Correlation Optimizer to handle more complex cases.\n2. Examples At first, let\u0026rsquo;s take a look at three examples. For every query, we show the original operator tree generated by Hive and the optimized operator tree. To be concise, we only show the following operators, which are FileSinkOperator (FS), GroupByOperator (AGG), HashTableSinkOperator (HT), JoinOperator (JOIN), MapJoinOperator (MJ), and ReduceSinkOperator (RS). Also, in every query, we add comments (e.g. /JOIN1/) to indicate the node in the operator tree that an operation belongs to.\n2.1 Example 1 SELECT tmp1.key, count(*) FROM (SELECT key, avg(value) AS avg FROM t1 GROUP BY /*AGG1*/ key) tmp1 JOIN /*JOIN1*/ t1 ON (tmp1.key = t2.key) WHERE t1.value \u0026gt; tmp1.avg GROUP BY /*AGG2*/ tmp1.key; The original operator tree generated by Hive is shown below.\nFigure 1: The original operator tree of Example 1 generated by Hive\nThis plan uses three MapReduce jobs to evaluate this query. However, AGG1, JOIN1, and AGG2 all require the column key to be the partitioning column for shuffling the data. Thus, we do not need to shuffle the data in the same way three times. We only need to shuffle the data once, and thus a single MapReduce job is needed. The optimized operator tree is shown below.\nFigure 2: The optimized operator tree of Example 1\nSince the input table of AGG1 and the left table of JOIN1 are both t1, when we use a single MapReduce job to evaluate this query, Hive only needs to scan t1 once. While, in the original plan, t1 is used in two MapReduce jobs, and thus it is scanned twice.\n2.2 Example 2 SELECT tmp1.key, count(*) FROM t1 JOIN /*JOIN1*/ (SELECT key, avg(value) AS avg FROM t1 GROUP BY /*AGG1*/ key) tmp1 ON (t1.key = tmp1.key) JOIN /*JOIN1*/ t2 ON (tmp1.key = t2.key) WHERE t2.value \u0026gt; tmp1.avg GROUP BY /*AGG2*/ t1.key; The original operator tree generated by Hive is shown below.\nFigure 3: The original operator tree of Example 2 generated by Hive\nThis example is similar to Example 1. The optimized operator tree only needs a single MapReduce job, which is shown below.\nFigure 4: The optimized operator tree of Example 2\n2.3 Example 3 SELECT count(distinct ws1.ws_order_number) as order_count, sum(ws1.ws_ext_ship_cost) as total_shipping_cost, sum(ws1.ws_net_profit) as total_net_profit FROM web_sales ws1 JOIN /*MJ1*/ customer_address ca ON (ws1.ws_ship_addr_sk = ca.ca_address_sk) JOIN /*MJ2*/ web_site s ON (ws1.ws_web_site_sk = s.web_site_sk) JOIN /*MJ3*/ date_dim d ON (ws1.ws_ship_date_sk = d.d_date_sk) LEFT SEMI JOIN /*JOIN4*/ (SELECT ws2.ws_order_number as ws_order_number FROM web_sales ws2 JOIN /*JOIN1*/ web_sales ws3 ON (ws2.ws_order_number = ws3.ws_order_number) WHERE ws2.ws_warehouse_sk \u0026lt;\u0026gt; ws3.ws_warehouse_sk) ws_wh1 ON (ws1.ws_order_number = ws_wh1.ws_order_number) LEFT SEMI JOIN /*JOIN4*/ (SELECT wr_order_number FROM web_returns wr JOIN /*JOIN3*/ (SELECT ws4.ws_order_number as ws_order_number FROM web_sales ws4 JOIN /*JOIN2*/ web_sales ws5 ON (ws4.ws_order_number = ws5.ws_order_number) WHERE ws4.ws_warehouse_sk \u0026lt;\u0026gt; ws5.ws_warehouse_sk) ws_wh2 ON (wr.wr_order_number = ws_wh2.ws_order_number)) tmp1 ON (ws1.ws_order_number = tmp1.wr_order_number) WHERE d.d_date \u0026gt;= '2001-05-01' and d.d_date \u0026lt;= '2001-06-30' and ca.ca_state = 'NC' and s.web_company_name = 'pri'; The original operator tree generated by Hive is shown below.\nFigure 5: The original operator tree of Example 3 generated by Hive\nIn this complex query, we will first have several MapJoins (MJ1, MJ2, and MJ3) which can be evaluated in the same Map phase. Since JOIN1, JOIN2, JOIN3, and JOIN4 use the same column as the join key, we can use a single MapReduce job to evaluate all operators before AGG1. The second MapReduce job will generate the final results. The optimized operator tree is shown below.\nFigure 6: The optimized operator tree of Example 3\n3. Intra-query Correlations In Hive, a submitted SQL query needs to be evaluated in a distributed system. When evaluating a query, data may need to shuffled sometimes. Based on the nature of different data operations, operators in Hive can be divided to two categories.\n Operators which do not require data shuffling. Examples are TableScanOperator, SelectOperator and FilterOperator. Operators which require data shuffling. Examples are GroupByOperator and JoinOperator.  For an operator requiring data shuffling, Hive will add one or multiple ReduceSinkOperators as parents of this operator (the number of ReduceSinkOperators depends on the number of inputs of the operator requiring data shuffling). Those ReduceSinkOperators form the boundary between the Map phase and Reduce phase. Then, Hive will cut the operator tree to multiple pieces (MapReduce tasks) and each piece can be executed in a MapReduce job.\nFor a complex query, it is possible that a input table is used by multiple MapReduce tasks. In this case, this table will be loaded multiple times when the original operator tree is used. Also, when generating those ReduceSinkOperators, Hive does not consider if the corresponding operator requiring data shuffling really needs a re-partitioned input data. For example, in the original operator tree of Example 1 (Figure 1), AGG1, JOIN1, and AGG2 require the data been shuffled in the same way because all of them require the column key to be the partitioning column in their corresponding ReduceSinkOperators. But, Hive is not aware this correlation between AGG1, JOIN1, and AGG2, and still generates three MapReduce tasks.\nCorrelation Optimizer aims to exploit two intra-qeury correlations mentioned above.\n Input Correlation: A input table is used by multiple MapReduce tasks in the original operator tree. Job Flow Correlation: Two dependent MapReduce tasks shuffle the data in the same way.  4. Correlation Detection In Hive, every query has one or multiple terminal operators which are the last operators in the operator tree. Those terminal operators are called FileSinkOperatos. To give an easy explanation, if an operator A is on another operator B\u0026rsquo;s path to a FileSinkOperato, A is the downstream of B and B is the upstream of A.\nFor a given operator tree like the one shown in Figure 1, the Correlation Optimizer starts to visit operators in the tree from those FileSinkOperatos in a depth-first way. The tree walker stops at every ReduceSinkOperator. Then, a correlation detector starts to find a correlation from this ReduceSinkOperator and its siblings by finding the furthest correlated upstream ReduceSinkOperators in a recursive way. If we can find any correlated upstream ReduceSinkOperator, we find a correlation. Currently, there are three conditions to determine if a upstream ReduceSinkOperator and an downstream ReduceSinkOperator are correlated, which are\n emitted rows from these two ReduceSinkOperators are sorted in the same way; emitted rows from these two ReduceSinkOperators are partitioned in the same way; and these ReduceSinkOperators do not have any conflict on the number reducers.  Interested readers may refer to our implementation for details.\nDuring the correlation detection, a JoinOperator or a UnionOperator can introduce branches to the searching path. For a JoinOperator, its parents are all ReduceSinkOperators. When the detector reaches a JoinOperator, it will check if all parents of this JoinOperator are correlated to the downstream ReduceSinkOperator. Because a JoinOperator contains one or multiple 2-way Join operations, for a ReduceSinkOperator, we can determine if another ReduceSinkOperator appearing in the same Join operation is correlated based on the Join type and positions of these ReduceSinkOperators in the Join operation with the following two rules.\n If a ReduceSinkOperator represents the left table of a INNER JOIN, a LEFT OUTER JOIN, or a LEFT SEMI JOIN, the ReduceSinkOperator representing the right table is also considered correlated; and If a ReduceSinkOperator represents the right table of a INNER JOIN, or a RIGHT OUTER JOIN, the ReduceSinkOperator representing the left table is also considered correlated.  With these two rules, we start to analyze those parent ReduceSinkOperators of the JoinOperator from every ReduceSinkOperator which has columns appearing in the join clause and then we can find all correlated ReduceSinkOperators recursively. If we can find that all parent ReduceSinkOperators are correlated from every ReduceSinkOperator which has columns appearing in the join clause, we will continue the correlation detection on this branch. Otherwise, we will determine that none of ReduceSinkOperator for the JoinOperator is correlated and stop the correlation detection on this branch.\nFor a UnionOperator, none of its parents will be a ReduceSinkOperator. So, we check if we can find correlated ReduceSinkOperators for every parent branch of this UnionOperator. If any branch does not have a ReduceSinkOperator, we will determine that we do not find any correlated ReduceSinkOperator at parent branches of this UnionOperator.\nDuring the process of correlation detection, it is possible that the detector can visit a JoinOperator which will be converted to a Map Join later. In this case, the detector stops searching the branch containing this Map Join. For example,\nin Figure 5, the detector knows that MJ1, MJ2, and MJ3 will be converted to Map Joins.\n5. Operator Tree Transformation In a correlation, there are two kinds of ReduceSinkOperators. The first kinds of ReduceSinkOperators are at the bottom layer of a query operator tree which are needed to emit rows to the shuffling phase. For example, in Figure 1, RS1 and RS3 are bottom layer ReduceSinkOperators. The second kinds of ReduceSinkOperators are unnecessary ones which can be removed from the optimized operator tree. For example, in Figure 1, RS2 and RS4 are unnecessary ReduceSinkOperators. Because the input rows of the Reduce phase may need to be forwarded to different operators and those input rows are coming from a single stream, we add a new operator called DemuxOperator to dispatch input rows of the Reduce phase to corresponding operators. In the operator tree transformation, we first connect children of those bottom layer ReduceSinkOperators to the DemuxOperator and reassign tags of those bottom layer ReduceSinkOperators (the DemuxOperator is the only child of those bottom layer ReduceSinkOperators). In the DemuxOperator, we record two mappings. The first one is called newTagToOldTag which maps those new tags assigned to those bottom layer ReduceSinkOperators to their original tags. Those original tags are needed to make JoinOperator work correctly. The second mapping is called newTagToChildIndex which maps those new tags to the children indexes. With this mapping, the DemuxOperator can know the correct operator that a row needs to be forwarded based on the tag of this row. The second step of operator tree transformation is to remove those unnecessary ReduceSinkOperators. To make the operator tree in the Reduce phase work correctly, we add a new operator called MuxOperator to the original place of those unnecessary ReduceSinkOperators. It is worth noting that if an operator has multiple unnecessary ReduceSinkOperators as its parents, we only add a single MuxOperator.\n6. Executing Optimized Operator Tree in the Reduce Phase In the Reduce phase, the ExecReducer will forward all reduce input rows to DemuxOperator first. Currently, blocking operators in the reduce phase operator tree share the same keys. Other cases will be supported in future work. Then, DemuxOperator will forward rows to their corresponding operators. Because a Reduce plan optimized Correlation Optimizer can be a tree structure, we need to coordinate operators in this tree to make the Reduce phase work correctly. This coordination mechanism is implemented in ExecDriver, DemuxOperator and MuxOperator.\nWhen a new row is sent to the ExecDriver, it checks if it needs to start a new group of rows by checking values of those key columns. If a new group of rows is coming, it first invokes DemuxOperator.endGroup. Then, the DemuxOperator will ask its children to process their buffered rows and propagate the endGroup call to the operator tree. Finally, DemuxOperator will propagate processGroup call to the operator tree. Usually, the implementation of processGroup in an operator only propagates this call to its children. MuxOperator is the one that overrides processGroup. When a MuxOperator gets the processGroup call, it check if all its parent operators have invoked this call. If so, it will ask its child to generate results and propagate processGroup to its child. Once the processGroup has been propagated to all operators, the DemuxOperator.endGroup will return and ExecDriver will propagate startGroup to the operator tree.\nFor every row sent to the ExecDriver, it also has a tag assigned by a corresponding RediceSinkOperator at the Map phase. In a row group (rows having the same key), rows are also sorted by their tags. When the DemuxOperator sees a new tag coming, it knows all child operators associated with tags smaller than this new coming tag will not have any input within the current row group. Thus, it can call endGroup and processGroup of those operators earlier. With this logic, within a row group, the input rows of every operator in the operator tree are also ordered by tags, which is required by JoinOperator. This logic also makes rows in the buffer of an operator be emitted as quickly as possible, which avoids unnecessary memory footprint contributed from buffering unnecessary rows.\n7. Related Jiras The umbrella jira is HIVE-3667.\n7.1 Resolved Jiras  HIVE-1772 HIVE-2206 HIVE-3430 HIVE-3670 HIVE-3671 HIVE-4952 HIVE-4972  7.2 Unresolved Jiras  HIVE-3668 HIVE-3669 HIVE-3773 HIVE-4751  8. References  Rubao Lee, Tian Luo, Yin Huai, Fusheng Wang, Yongqiang He, Xiaodong Zhang. YSmart: Yet another SQL-to-MapReduce Translator, ICDCS, 2011  Attachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/correlation-optimizer_34019487/","tags":null,"title":"Apache Hive : Correlation Optimizer"},{"categories":null,"contents":"Apache Hive : Cost-based optimization in Hive  Abstract 1. INTRODUCTION 2. RELATED WORK 3. BACKGROUND 4. Implementation details 5. Phase 1 – Work Items Open Issues Reference  Abstract Apache Hadoop is a framework for the distributed processing of large data sets using clusters of computers typically composed of commodity hardware. Over last few years Apache Hadoop has become the de facto platform for distributed data processing using commodity hardware. Apache Hive is a popular SQL interface for data processing using Apache Hadoop.\n User submitted SQL query is converted by Hive to physical operator tree which is optimized and converted to Tez Jobs and is then executed on Hadoop cluster. Distributed SQL query processing in Hadoop differs from conventional relational query engine when it comes to handling of intermediate result sets. Hive query processing often requires sorting and reassembling of intermediate result set; this is called shuffling in Hadoop parlance.\n Most of the existing query optimizations in Hive are about minimizing shuffling cost. Currently user would have to submit an optimized query to Hive with right join order for query to be executed efficiently. Logical optimizations in Hive are limited to filter push down, projection pruning and partition pruning. Cost based logical optimizations can significantly improve Apache Hive’s query latency and ease of use.\n Join reordering and join algorithm selection are few of the optimizations that can benefit from a cost based optimizer. Cost based optimizer would free up user from having to rearrange joins in the right order or from having to specify join algorithm by using query hints and configuration options. This can potentially free up users to model their reporting and ETL needs close to business process without having to worry about query optimizations.\n Calcite is an open source cost based query optimizer and query execution framework. Calcite currently has more than fifty query optimization rules that can rewrite query tree, and an efficient plan pruner that can select cheapest query plan in an optimal manner. In this paper we discuss how Calcite can be used to introduce Cost Based Logical Optimizer (CBO) in to Apache Hive.\n CBO will be introduced in to Hive in a Phased manner. In the first phase, Calcite would be used to reorder joins and to pick right join algorithm so as to reduce query latency. Table cardinality and Boundary statistics will be used for this cost based optimizations.\n1. INTRODUCTION Hive is a data-warehousing infrastructure on top of Apache Hadoop. Hive takes advantage of Hadoop’s massive scale out and fault tolerance capabilities for data storage and processing on commodity hardware. Hive is designed to enable easy data summarization, ad-hoc querying and analysis of large volumes of data. Hive SQL is the declarative query language, which enables users familiar with SQL to do ad-hoc querying, summarization and data analysis easily.\nIn past Hadoop jobs tended to have high latencies and incurred substantial overheads in job submission and scheduling. As a result - latency for Hive queries was generally very high even when data sets involved were very small. As a result Hive was typically used for ETL and not much for interactive queries. With Hadoop2 and Tez the overheads for job submission and job scheduling have gone down significantly. In Hadoop version one, the jobs that could be executed could only be Map-Reduce Jobs. With Hadoop2 and Tez that limitation no longer apply.\nIn Hadoop the output of mapper is sorted and sometimes persisted on the local disk of the mapper. This sorted mapper output is then send to appropriate reducer which then combines sorted results from different mappers. While executing multiple map-reduce jobs where output of one job needs to be consumed by the successive map-reduce job, the output of preceding map-reduce job needs to be persisted into HDFS; this persistence is costly as the data needs to be copied to other nodes based on the replication factor of HDFS.\nHive on top of Hadoop version 1 often had to submit multiple map-reduce jobs to complete query processing. This Map-Reduce job pipeline degraded performance, as the intermediate result set now needs to be persisted to fault tolerant HDFS. Also submitting jobs and scheduling them were relatively costly operations. With Hadoop2 and Tez the cost of job submission and scheduling is minimized. Also Tez does not restrict the job to be only Map followed by Reduce; this implies all of the query execution can be done in a single job without having to cross job boundaries. This would result in a significant cost savings, as the intermediate result sets need not be persisted to HDFS or to even local disk.\nQuery optimizations in a relational query engine can be broadly classified as logical query optimizations and physical query optimizations. Logical query optimizations generally refer to query optimizations that can be derived based on relational algebra independent of the physical layer in which query is executed. Physical query optimizations are query optimizations that are cognizant of physical layer primitives. For Hive, the physical layer implies Map-Reduce and Tez primitives.\nCurrently logical query optimizations in Hive can be broadly categorized as follows:\n Projection Pruning Deducing Transitive Predicates Predicate Push down Merging of Select-Select, Filter-Filter in to single operator Multi-way Join Query Rewrite to accommodate for Join skew on some column values   Physical optimizations in Hive can be broadly classified as follows:\n Partition Pruning Scan pruning based on partitions and bucketing Scan pruning if query is based on sampling Apply Group By on the map side in some cases Perform Join on the Mapper Optimize Union so that union can be performed on map side only Decide which table to stream last, based on user hint, in a multi way join Remove unnecessary reduce sink operators For queries with limit clause, reduce the number of files that needs to be scanned for the table. For queries with Limit clause, restrict the output coming from mapper by restricting what Reduce Sink operator generates. Reduce the number of Tez jobs needed for answering user submitted SQL query Avoid Map-Reduce jobs in case of simple fetch query For simple fetch queries with aggregation, perform aggregation without Map-Reduce tasks Rewrite Group By Query to use index table instead of original table Use Index scans when predicates above table scan is equality predicates and columns in predicate have indexes on it.  In Hive most of the optimizations are not based on the cost of query execution. Most of the optimizations do not rearrange the operator tree except for filter push down and operator merging. Most of the operator tree mutation is for removing reduce-sink and reducer operator. Listed below are some of optimization decisions that can benefit from a CBO:\n How to order Join What algorithm to use for a given Join Should the intermediate result be persisted or should it be recomputed on operator failure. The degree of parallelism at any operator (specifically number of reducers to use). Semi Join selection  Calcite is an open source, Apache Licensed, query planning and execution framework. Many pieces of Calcite are derived from Eigenbase Project. Calcite has optional JDBC server, query parser and validator, query optimizer and pluggable data source adapters. One of the available Calcite optimizer is a cost based optimizer based on volcano paper. Currently different pieces of Calcite is used in following projects/products:\n Apache Drill Cascading (Lingual) Lucid DB Mondrian/Pentaho  Calcite currently has over fifty cost based optimization rules. Some of the prominent cost based optimization rules are listed below:\n Push Join through Union Push Filter past Table Function Join Reordering Semi Join selection Push Aggregate through Union Pull Aggregate through Union Pull Constants through Aggregate Merge Unions  In this document we propose to use Calcite’s cost based optimizer, Volcano, to perform Cost Based Optimizations in Hive. We propose to implement Calcite based CBO in a phased manner. Note here that proposal is to use Calcite’s optimizer only and nothing else. Listed below are the envisioned stages of introducing CBO in to Hive using Calcite:\n Phase1 – Join Reordering \u0026amp; Join algorithm Selection  Table cardinality and boundary statistics will be used to compute operator cardinality. Hive operator tree will be converted to Calcite operator tree. Volcano optimizer in Calcite will be used to rearrange joins and to pick the join algorithm. Optimized Calcite operator tree would be converted back to Hive AST and will be executed as before. So all of the Hive’s existing optimizations would run on top of Calcite optimized SQL.   Phase2 – Add support for Histograms, use other optimizations in Calcite  Introduce space efficient histograms Change operator cardinality computation to use histograms Register additional optimization rules available in Calcite like the ones listed above.   Phase3 – Code restructuring so that Calcite generates optimized Hive Operator tree  Unlike phase1 Hive AST would be directly translated into Calcite operator tree. Optimize Calcite operator tree using Volcano optimizer. Convert optimized Calcite operator tree back into Hive Operator tree. This is unlike phase1 where optimized Calcite operator tree is converted to Hive AST.    2. RELATED WORK STATS  histogram_numeric(): Estimating frequency distributions: https://cwiki.apache.org/confluence/x/CoOhAQ histogram() UDAF for a numerical column https://issues.apache.org/jira/browse/HIVE-1397 Built-in Aggregate Functions (UDAF): https://cwiki.apache.org/confluence/x/-oKhAQ Annotate hive operator tree with statistics from metastore: https://issues.apache.org/jira/browse/HIVE-5369   PAPERS  Query Optimization for Massively Parallel Data Processing  Sai Wu, Feng Li, Sharad Mehrotra, Beng Chin Ooi\nSchool of Computing, National University of Singapore, Singapore, 117590\nSchool of Information and Computer Science, University of California at Irvine\n  Profiling, What-if Analysis, and Cost-based Optimization of MapReduce Programs  Herodotos Herodotou Duke University, Shivnath Babu Duke University\n  Optimizing Joins in a Map-Reduce Environment  Foto N. Afrati, National Technical University of Athens, Greece\nJeffrey D. Ullman, Stanford University USA\n  Efficient and scalable statistics gathering for large databases in Oracle 11g  Sunil Chakkappen, Thierry Cruanes, Benoit Dageville, Linan Jiang, Uri Shaft, Hong Su, Mohamed Zait , Oracle Redwood Shores CA\nhttp://dl.acm.org/citation.cfm?doid=1376616.1376721\n  Estimating Distinct (Postgress SQL)  http://wiki.postgresql.org/wiki/Estimating_Distinct\n  The History of Histograms  Yannis Ioannidis, Department of Informatics and Telecommunications, University of Athens\nhttp://www.vldb.org/conf/2003/papers/S02P01.pdf\n 3. BACKGROUND Hive Query optimization issues Hive converts user specified SQL statement to AST that is then used to produce physical operator tree. All of the query optimizations are performed on the physical operator tree. Hive keeps semantic info separate from query operator tree. Semantic info is extracted during plan generation that is then looked up often during down stream query optimizations. Adding new query optimizations into Hive is often made difficult by the lack of proper logical query plan and due to Semantic info and query tree separation.\nTEZ Apache Tez generalizes the MapReduce paradigm to execute a complex DAG (directed acyclic graph) of tasks. Refer to the following link for more info.\nhttp://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/\nJoin algorithms in Hive Hive only supports equi-Join currently. Hive Join algorithm can be any of the following:\nMulti way Join If multiple joins share the same driving side join key then all of those joins can be done in a single task.\nExample: (R1 PR1.x=R2.a - R2) PR1.x=R3.b - R3) PR1.x=R4.c - R4\nAll of the join can be done in the same reducer, since R1 will already be sorted based on join key x.\nCommon Join Use Mappers to do the parallel sort of the tables on the join keys, which are then passed on to reducers. All of the tuples with same key is given to same reducer. A reducer may get tuples for more than one key. Key for tuple will also include table id, thus sorted output from two different tables with same key can be recognized. Reducers will merge the sorted stream to get join output.\nMap Join Useful for star schema joins, this joining algorithm keeps all of the small tables (dimension tables) in memory in all of the mappers and big table (fact table) is streamed over it in the mapper. This avoids shuffling cost that is inherent in Common-Join. For each of the small table (dimension table) a hash table would be created using join key as the hash table key.\n Bucket Map Join If the joining keys of map-join are bucketed then instead of keeping whole of small table (dimension table) in every mapper, only the matching buckets will be kept. This reduces the memory footprint of the map-join.\nSMB Join This is an optimization on Bucket Map Join; if data to be joined is already sorted on joining keys then hash table creation is avoided and instead a sort merge join algorithm is used.\nSkew Join If the distribution of data is skewed for some specific values, then join performance may suffer since some of the instances of join operators (reducers in map-reduce world) may get over loaded and others may get under utilized. On user hint, hive would rewrite a join query around skew value as union of joins.\n Example R1 PR1.x=R2.a - R2 with most data distributed around x=1 then this join may be rewritten as (R1 PR1.x=R2.a and PR1.x=1 - R2) union all (R1 PR1.x=R2.a and PR1.x\u0026lt;\u0026gt;1 - R2)\n 4. Implementation details CBO will be introduced in to Hive in three different phases. Following provides high-level overview of these phases: Phase 1  Statistics:\n Table Cardinality Column Boundary Stats: min, max, avg, number of distinct values   Cost Based Optimizations:\n Join ordering Join Algorithm   Restrictions:\n Calcite CBO will be used only for select expressions Calcite CBO won’t be used if select expression contains any of the following operators:  Sort By    Hive supports both total ordering (order by) and partial ordering (sort by). Partial ordering cannot be represented in relational algebra and SQL. In future we may represent Sort By as a table function.\n Map/Reduce/Transform  Hive allows users to specify map/reduce/transform operator in the sql; data would be transformed using provided mapper/reducer scripts. There is no direct translation for these operators to relational algebra. We may represent them as table function in future.\n Cluster By/Distribute By  Cluster By and Distribute By are used mainly with the Transform/Map-Reduce Scripts. But, it is sometimes useful in SELECT statements if there is a need to partition and sort the output of a query for subsequent queries. Cluster By is a short-cut for both Distribute By and Sort By. Hive uses the columns in Distribute By to distribute the rows among reducers. All rows with the same Distribute By columns will go to the same reducer. However, Distribute By does not guarantee clustering or sorting properties on the distributed keys.\n Table Sample  The TABLESAMPLE clause allows the users to write queries for samples of the data instead of the whole table. The TABLESAMPLE clause can be added to any table in the FROM clause. In future we may represent Table Sample as table function.\n Lateral Views  Lateral view is used in conjunction with user-defined table generating functions such as explode(). UDTF generates one or more output rows for each input row. A lateral view first applies the UDTF to each row of base table and then joins resulting output rows to the input rows to form a virtual table having the supplied table alias.\n UDTF (Table Functions) PTF (Partitioned Table Functions)   Calcite related enhancements:\n Introduce Operators to represent hive relational operators. Table Scan, Join, Union, Select, Filter, Group By, Distinct, Order By. These operators would implement a calling convention with physical cost for each of these operators. Introduce rules to convert Joins from CommonJoin to MapJoin, MapJoin to BucketJoin, BucketJoin to SMBJoin, CommonJoin to SkewJoin. Introduce rule to merge joins so that a single join operator will represent multi-way join (similar to MergedJoin in Hive). Merged-Join in Hive will be translated to MultiJoinRel in Calcite.   Phase 2 Statistics:\n Histograms   Cost Based Optimizations:\n Join ordering based on histograms Join Algorithm – histograms are used for estimating join selectivity Take advantage of additional optimizations in Calcite. The actual rules to use is TBD.  Phase 3 Configuration The configuration parameter hive.cbo.enable determines whether cost-based optimization is enabled or not.\nProposed Cost Model Hive employs parallel-distributed query execution using Hadoop cluster. This implies for a given query operator tree different operators could be running on different nodes. Also same operator may be running in parallel on different nodes in the cluster, processing different partitions of the original relation. This parallel execution model induces high I/O and CPU costs. Hive query execution cost tends to be more I/O centric due to following reasons.\n Shuffling cost  Data needed by an operator from its child operator in query tree requires assembling data from all instances of child operator. This data then needs to be sorted and chopped up so that a partition of the relation is presented to an instance of the operator.\n This shuffling cost involves cost of writing intermediate result set to local file system, cost of reading data back from local file system, and cost of transferring intermediate result set to the node that is operating child processor. In addition to I/O cost, shuffling also requires sorting of data that should be counted towards CPU cost.\n  HDFS Read/Write is costly  Reading and writing data to HDFS is more costly compared to local FS. In Map-Reduce framework Table Scan would typically read data from HDFS and would write data to HDFS when switching between two Map-Reduce jobs. In Tez all of the operators should work with in a single Tez Job and hence we shouldn’t have to pay the cost of writing intermediate result set to HDFS.\n Cost based optimizations in Hive will keep track of cost in terms of\n CPU usage IO Usage Cardinality Average size of the tuple in the relation.   Average size of the tuple in relation and cardinality of the relation will be used to estimate resources needed to hold a relation in memory. Memory needed to hold a relation is used to decide whether certain join algorithms, like Map/Bucket Join, can be used.\n Volcano optimizer in Calcite compares cost of two equivalent query plans by getting cost of each operator in the tree and summing them up to find cumulative cost. Plan with lowest cumulative cost is picked as the best plan to execute the query. “VolcanoCost” is the Java class representing cost for Calcite’s Volcano optimizer. “VolcanoCost” comparison operators seem to take into consideration only the number of rows to decide if one cost is less than other.\n For Hive we want to consider CPU and IO usage first before comparing cardinality.\nWe propose to introduce a new “RelOptCost” implementation “HiveVolcanoCost” derived from “VolcanoCost”. “HiveVolcanoCost” will keep CPU, I/O, Cardinality, and average size of tuple. Cost comparison algorithm will give importance to CPU and IO cost before paying attention to cardinality. CPU and IO cost will be stored in nano seconds granularity. Following is the pseudo code for “RelOptCost.isLe” function:\n Class HiveVolcanoCost extends VolcanoCost {\nDouble m_sizeOfTuple;\n  @Override\n public boolean isLe(RelOptCost other) {\n VolcanoCost that = (VolcanoCost) other;\n if (((this.dCpu + this.dIo) \u0026lt; (that.dCpu + that.dIo))\n || ((this.dCpu + this.dIo) == (that.dCpu + that.dIo)\n \u0026amp;\u0026amp; this.dRows \u0026lt;= that.dRows)) {\n return true;\n } else {\n return false;\n }\n }\n}\n Design Choice:\nIn the absence of histograms, cardinality can be assumed to follow uniform distribution on the distinct values. With this approach cardinality/distinct computation could always follow same code path (Histograms). Alternative is to use heuristics when histograms are not available (like the one described by Hector Garcia Molina, Jeffrey D. Ullman and Jennifer Widom in “Database Systems”). Currently Hive statistics doesn’t keep track of the distinct values for an attribute (only the number of distinct values is kept); however from min, max,number of distinct values, and table cardinality uniformly distributed histogram can be constructed. Following describes formulas for a uniform histogram construction.\nNumber of buckets = (Max-Min)/No of distinct values.\nBucket width = (Max- Min)/ Number of buckets.\nBucket Height = Table cardinality/ Number of buckets.\n In this paper for the cost formula, in the absence of histogram, we will follow heuristics described by Hector Garcia Molina, Jeffrey D. Ullman and Jennifer Widom in the book “Database Systems”.\n Following are the cost variables that will be used in cost computation:\n Hr - This is the cost of Reading 1 byte from HDFS in nano seconds. Hw - This is the cost of Writing 1 byte to HDFS in nano seconds. Lr - This is the cost of Reading 1 byte from Local FS in nano seconds. Lw - This is the cost of writing 1 byte to Local FS in nano seconds. NEt – This is the average cost of transferring 1 byte over network in the Hadoop cluster from any node to any node; expressed in nano seconds. T(R) - This is the number of tuples in the relation. Tsz – Average size of the tuple in the relation V(R, a) –The number of distinct values for attribute a in relation R CPUc – CPU cost for a comparison in nano seconds   Assumptions:\n Relative cost of Disk, HDFS, and Network read/write with each other is more important than the exact true cost. We assume uniform cost regardless of hardware type, locality, and size of data involved in I/O, type of I/O scatter/gather vs. sequential read/write. This is obviously over simplification, but we are more interested in relative cost of operations. This cost model ignores the number of disk blocks that needs to be read/written from/to and instead look at number of bytes that needs to be read/written. This is an obvious oversimplification of I/O cost. This cost model also ignores storage layout, column store vs. others. We assume all of the tuples to be of uniform size. No colocation of tasks is assumed and hence we consider network transfer cost. For CPU cost, only comparison cost is considered. It is assumed that each comparison will cost 1 nano second. Each vertex in Tez is a different process HDFS read is assumed to be 1.5 times of local disk read and HDFS write is assumed to be 10 times of local disk write.   Following are the assumed values for cost variables:\n CPUc = 1 nano sec NEt = 150 * CPUc nano secs Lw = 4 * NEt Lr = 4 * NEt Hw = 10 * Lw Hr = 1.5 * Lr   Table Scan T(R) = Consult Metadata to get cardinality;\nTsz = Consult Metadata to get average tuple size;\nV(R, a) = Consult Metadata\nCPU Usage = 0;\nIO Usage = Hr * T(R) * Tsz\n Common Join T(R) = Join Cardinality estimation\nTsz = Consult Metadata to get average tuple size based on join schema;\nCPU Usage = Sorting Cost for each of the relation + Merge Cost for sorted stream\n = (T(R1) * log T(R1) * CPUc + T(R2) * log T(R2) * CPUc + … + T(Rm) * log T(Rm) * CPUc) + (T(R1) + T(R2) + …+ T(Rm)) * CPUc nano seconds;\n IO Usage = Cost of writing intermediate result set in to local FS for shuffling + Cost of reading from local FS for transferring to Join operator node + Cost of transferring mapped output to Join operator node\n = Lw * (T(R1) * Tsz1 + T(R2) * Tsz2 + …+ T(Rm) * Tszm) + Lr * (T(R1) * Tsz1 + T(R2) * Tsz2 + …+ T(Rm) * Tszm) + NEt * (T(R1) * Tsz1 + T(R2) * Tsz2 + … + T(Rm) * Tszm)\n R1, R2… Rm is the relations involved in join.\nTsz1, Tsz2… Tszm are the average size of tuple in relations R1, R2…Rm.\n Map Join Number of Rows = Join Cardinality estimation\nSize of tuple = Consult Metadata to get average tuple size based on join schema\n CPU Usage =HashTable Construction cost + Cost of Join\n = (T(R2) + …+ T(Rm)) + (T(R1) + T(R2) + …+ T(Rm)) * CPUc nano seconds\n IO Usage = Cost of transferring small tables to Join Operator Node * Parallelization of the join\n = NEt * (T(R2) * Tsz2 + … + T(Rm) * Tszm) * number of mappers\n R1, R2… Rm is the relations involved in join and R1 is the big table that will be streamed.\nTsz2… Tszm are the average size of tuple in relations R1, R2…Rm.\n Bucket Map Join Number of Rows = Join Cardinality estimation\nSize of tuple = Consult Metadata to get average tuple size based on join schema;\n CPU Usage =Hash Table Construction cost + Cost of Join\n = (T(R2) + …+ T(Rm)) * CPUc + (T(R1) + T(R2) + …+ T(Rm)) * CPUc nano seconds\n IO Usage = Cost of transferring small tables to Join * Parallelization of the join\n = NEt * (T(R2) * Tsz2 + … + T(Rm) * Tszm) * number of mappers\n R1, R2… Rm is the relations involved in join.\nTsz2… Tszm are the average size of tuple in relations R2…Rm.\n SMB Join Number of Rows = Join Cardinality estimation\nSize of tuple = Consult Metadata to get average tuple size based on join schema;\n CPU Usage = Cost of Join\n = (T(R1) + T(R2) + …+ T(Rm)) * CPUc nano seconds\n IO Usage = Cost of transferring small tables to Join * Parallelization of the join\n = NEt * (T(R2) * Tsz2 + … + T(Rm) * Tszm) * number of mappers\n R1, R2… Rm is the relations involved in join.\nTsz2… Tszm are the average size of tuple in relations R2…Rm.\n Skew Join Query will be rewritten as union of two joins. We will have a rule to rewrite query tree for skew join. Rewritten query will use the cost model for the join and union operators.\n Distinct/Group By Number of Rows = Based on Group-By selectivity = V(R, a,b,c..) where a,b,c are the group by keys\nSize of tuple = Consult Metadata to get average tuple size based on schema\n CPU Usage = Cost of Sorting + Cost of categorizing into group\n = (T(R) * log T(R) + T(R)) * CPUc nano seconds;\n IO Usage = Cost of writing intermediate result set in to local FS for shuffling + Cost of reading from local FS for transferring to GB reducer operator node + Cost of transferring data set to GB Node\n = Lw * T(R) * Tsz + Lr * T(R) * Tsz + NEt * T(R) * Tsz\n Union All Number of Rows = Number of Rows from left + Number of Rows from right\nSize of tuple = avg of (avg size of left, avg size of right)\nCPU Usage = 0\nIO Usage = Cost of writing intermediate result set in to HDFS + Cost of reading from HDFS for transferring to UNION mapper node + Cost of transferring data set to Mapper Node\n = (T(R1) * Tsz1 + T(R2) * Tsz2) * Hw + (T(R1) * Tsz1 + T(R2) * Tsz2) * Hr + (T(R1) * Tsz1 + T(R2) * Tsz2) * NEt\n R1, R2 is the relations involved in join.\nTsz1, Tsz2 is the average size of tuple in relations R1, R2.\n Filter/Having Number of Rows = Filter Selectivity * Number of Rows from Child\nSize of tuple = size of tuple from child operator\nCPU Usage = T(R) * CPUc nano seconds\nIO Usage = 0\nSelect Number of Rows = Number of Rows from Child\nSize of tuple = size of tuple from child operator\nCPU Usage = 0\nIO Usage = 0\n Filter Selectivity Without Histogram  Equality Predicates where one side is a literal = 1/V(R, A) Equality Predicate when both sides are not literals = 1/max (V(R, A), V(R, B)) In Equality Predicates (Less/Greater than) = 1/3 Not Equal = (V(R, A) -1)/V(R, A) OR Condition = n*(1 –(1-m1/n)(1-m2/n)) where n is the total number of tuples from child and m1 and m2 is the expected number of tuples from each part of the disjunction predicate. AND condition = product of selectivity of individual leaf predicates in the conjunctive predicate   Join Cardinality (without Histogram)  Inner Join = Product of cardinalities of child relations * Join selectivity One side Outer Join = max (Selectivity of filter rejecting non joined inner tuples * (Product of cardinalities of child relations), Cardinality of Outer side)  Example: C(R1 a=b R2) = max (C(R2.b=R1.a C(R1 X R2)), C(R1))\n Full Outer Join = max (Selectivity of filter rejecting non joined inner tuples * (Product of cardinalities of child relations), sum of cardinalities of left and right relation)  = C(R1 a=b R2) = max (C(R1.a=R1.b C(R1 X R2)), C(R1) + C(R2))\n For multi-way join algorithm, join would be decomposed to number of different joins for cardinality and cost estimation.   Join Selectivity (without Histogram)  Single Attribute = 1/max (V(R1, a), V(R2, a)) where join predicate is R1.a=R2.a Multiple Attributes = 1/(max (V(R1, a), V(R2, a)) * max (V(R1, b), V(R2, b)) ) where join predicate is R1.a=R2.a and R1.b = R2.b   Distinct Estimation Inner Join - Distinct Estimation  V(J, a) = V(R1, a) if attribute a comes only from one side of join; J is the relation representing Join output and R1 one of the relation involved. V(J, a) = max(V(R1, a), V(R2, a)) if attribute a is present in both sides of join; J is the relation representing Join output and R1, R2 are the relations involved in join. V(J, a) = V(R1, a) if attribute a comes only from one side of join; J is the relation representing Join output output and R1 is the relation from which attribute “a” comes from. V(J, a) = V(Ro, a) where Ro is the relation for the outer side; J is the relation representing Join output and Ro is the outer relation of the join. V(J, a) = V(R1, a) if attribute “a” comes only from one side of join; J is the relation representing Join output and R1 is the relation from which attribute a comes from. V(J, a) = max (V(R1, a), V(R2, a)) where J is the relation representing Join output and R1, R2 are the relations involved in join. V(U, a) = max (V(R1, a), V(R2, a)) where U is the relation representing Union All output and R1, R2 are the relations involved in union.  One sided Outer Join - Distinct Estimation Full Outer Join - Distinct Estimation Union All - Distinct Estimation   GB - Distinct Estimation  V(G, a) = V(R, a) where G is the relation representing Group-By output and R is the child relation of Group-By. It is assumed attribute “a” is part of grouping keys. V(G, a,b,c) = max (V(R,a), V(R,b), V(R,c)) where G is the relation representing Group-By output and R is the child relation of Group-By. It is assumed attribute “a”, “b”, “c” is part of grouping keys.   Filter - Distinct Estimation  V(F, a) = V(R, a) where F is the relation representing filter output and R is the child relation of the filter.   5. Phase 1 – Work Items  Walk the Hive OP tree and make sure that OP tree doesn’t contain any op that cannot be translated into Calcite (Lateral Views, PTF, Cubes \u0026amp; Rollups, Multi Table Insert). Walk the Hive OP tree and introduce cast functions to make sure that all of the comparisons (implicit \u0026amp; explicit) are strictly type safe (the type must be same on both sides). Implement Calcite operators specific to Hive that would do cost computation and cloning. Convert the Hive OP tree to Calcite OP tree.  Convert Hive Types to Calcite types Convert Hive Expressions to Calcite expressions Convert Hive Operator to Calcite operator Handle Hidden Columns Handle columns introduced by ReduceSink (for shuffling) Handle Join condition expressions stashed in Reducesink op Handle filters in Join Conditions Convert Hive Semi Join to Calcite Attach cost to operators Alias the top-level query projections to query projections that user expects.   Optimize the Calcite OP tree using Volcano Optimizer.  Implement Rules to convert Joins to Hive Join algorithms.  Common Join -\u0026gt; Map Join Map Join -\u0026gt; Bucket Map Join Common Join -\u0026gt; Bucket Map Join Bucket Map Join -\u0026gt; SMB Join Common Join -\u0026gt; Skew Join     Walk the Optimized Calcite OP tree and introduce derived tables to convert OP tree to SQL.  Generate unique table (including derived table) aliases   Walk the OP tree and convert in to AST.  Stash Join algorithm in AST as query Hints   Modify Plan Generator to pay attention to Calcite query hints Rerun Hive optimizer and generate the execution plan (this second pass would not invoke Calcite optimizer).  Open Issues  CBO needs to differentiate between types of IPC (Durable-Local-FS vs, Durable-HDFS, Memory vs. Streaming)  Reference  Database Systems The Complete Book, Second Edition, Hector Garcia-Molina, Jeffrey D. Ullman, Jennifer Widom Query Optimization for Massively Parallel Data Processing  Sai Wu, Feng Li, Sharad Mehrotra, Beng Chin Ooi\nSchool of Computing, National University of Singapore, Singapore, 117590\nSchool of Information and Computer Science, University of California at Irvine\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/cost-based-optimization-in-hive_42566775/","tags":null,"title":"Apache Hive : Cost-based optimization in Hive"},{"categories":null,"contents":"Apache Hive : CSV Serde  Availability Background Usage Versions  Availability Earliest version CSVSerde is available\nThe CSVSerde is available in Hive 0.14 and greater.\nBackground The CSV SerDe is based on https://github.com/ogrodnek/csv-serde, and was added to the Hive distribution in HIVE-7777.\n Limitation\nThis SerDe treats all columns to be of type String. Even if you create a table with non-string column types using this SerDe, the DESCRIBE TABLE output would show string column type. The type information is retrieved from the SerDe. To convert columns to the desired type in a table, you can create a view over the table that does the CAST to the desired type.\nUsage This SerDe works for most CSV data, but does not handle embedded newlines. To use the SerDe, specify the fully qualified class name org.apache.hadoop.hive.serde2.OpenCSVSerde. Documentation is based on original documentation at https://github.com/ogrodnek/csv-serde.\nCreate table, specify CSV properties\nCREATE TABLE my_table(a string, b string, ...) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde' WITH SERDEPROPERTIES ( \u0026quot;separatorChar\u0026quot; = \u0026quot;\\t\u0026quot;, \u0026quot;quoteChar\u0026quot; = \u0026quot;'\u0026quot;, \u0026quot;escapeChar\u0026quot; = \u0026quot;\\\\\u0026quot; ) STORED AS TEXTFILE; Default separator, quote, and escape characters if unspecified\nDEFAULT_ESCAPE_CHARACTER \\ DEFAULT_QUOTE_CHARACTER \u0026quot; DEFAULT_SEPARATOR , For general information about SerDes, see Hive SerDe in the Developer Guide. Also see SerDe for details about input and output processing.\nVersions The CSVSerde has been built and tested against Hive 0.14 and later, and uses Open-CSV 2.3 which is bundled with the Hive distribution.\n   Hive Versions Open-CSV Version     Hive 0.14 and later 2.3    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/csv-serde_48202659/","tags":null,"title":"Apache Hive : CSV Serde"},{"categories":null,"contents":"Apache Hive : Data Connector for Hive and Hive-like engines What is a Data connector? Data connectors (referred to as \u0026ldquo;connector\u0026rdquo; in Hive Query Language) are top level objects in Hive where users can define a set of properties required to be able to connect to an external datasource from hive. This document illustrates example of the data connector framework can be used to do SQL query federation between two distinct \u0026ldquo;hive\u0026rdquo; clusters/installations or between Hive and another hive-like compute engines (eg: EMR).\nHIVEJDBC type Data connector Apache Hive now has a connector to plugin in multiple hive and hive like sources. HIVE-27597 adds a JDBC based connector of type \u0026ldquo;HIVEJDBC\u0026rdquo;. Similar to the other data connectors, this connector needs a URL, Driver name, credentials etc to be defined as part of the connector definition. Once defined, users can use the same connector object to map multiple databases from the remote datasource to local hive metastore.\nHIVEJDBC connector requires the following values\n Name → local name to be able to reference the connector with. This name is shown in \u0026ldquo;show connectors\u0026rdquo; and will be used in connector DDLs like drop/alter and also in \u0026ldquo;create remote database ..\u0026rdquo; statements. TYPE → \u0026ldquo;HIVEJDBC\u0026rdquo; so Hive Metastore knows that Connector class to use. URL → JDBC URL for the remote HiveServer instance. DCPROPERTIES → This is a freeform list that contains other info like credentials and other optional properties. These properties will be passed onto the table definitions for the databases created using this connector  Note: Data Connectors in Hive are currently only used to read the data from remote sources. Write operations are not supported.\nHow do I use it?  Create a connector first.   CREATE CONNECTOR hiveserver_connector TYPE 'hivejdbc' URL 'jdbc:hive2://\u0026lt;maskedhost\u0026gt;:10000' WITH DCPROPERTIES (\u0026quot;hive.sql.dbcp.username\u0026quot;=\u0026quot;hive\u0026quot;, \u0026quot;hive.sql.dbcp.password\u0026quot;=\u0026quot;hive\u0026quot;); Create a database of type REMOTE in hive using the connector from Step 1. This maps a remote database named \u0026ldquo;*default*\u0026rdquo; to a hive database named \u0026ldquo;hiveserver_remote\u0026rdquo; in hive.   CREATE REMOTE DATABASE hiveserver_remote USING hiveserver_connector WITH DBPROPERTIES (\u0026quot;connector.remoteDbName\u0026quot;=\u0026quot;default\u0026quot;); 3. Use the tables in REMOTE database much like the JDBC-storagehandler based tables in hive. One big difference is that the metadata for these tables are never persisted in hive. Currently, create/alter/drop table DDLs are not supported in REMOTE databases. 0: jdbc:hive2://localhost:10000\u0026gt; USE hiveserver_remote; 0: jdbc:hive2://localhost:10000\u0026gt; **describe formatted test_emr_tbl;**\n+-------------------------------+-------------------------------------------------+----------------------------------------------------+ | col_name | data_type | comment | +-------------------------------+-------------------------------------------------+----------------------------------------------------+ | tblkey | int | from deserializer | | descr | string | from deserializer | | | NULL | NULL | | # Detailed Table Information | NULL | NULL | | Database: | emr_db | NULL | | OwnerType: | USER | NULL | | Owner: | null | NULL | | CreateTime: | UNKNOWN | NULL | | LastAccessTime: | UNKNOWN | NULL | | Retention: | 0 | NULL | | Location: | [file:/tmp/hive/warehouse/external/test_emr_tbl](http://file/tmp/hive/warehouse/external/test_emr_tbl) | NULL | | Table Type: | EXTERNAL_TABLE | NULL | | Table Parameters: | NULL | NULL | | | EXTERNAL | TRUE | | | hive.sql.database.type | HIVE | | | hive.sql.dbcp.password | | | | hive.sql.dbcp.username | hive | | | hive.sql.jdbc.driver | org.apache.hive.jdbc.HiveDriver | | | hive.sql.jdbc.url | jdbc:hive2://\u0026lt;maskedIP\u0026gt;.compute-1.amazonaws.com:10000 | | | hive.sql.schema | default | | | hive.sql.table | test_emr_tbl | | | storage_handler | org.apache.hive.storage.jdbc.JdbcStorageHandler | | | NULL | NULL | | # Storage Information | NULL | NULL | | SerDe Library: | org.apache.hive.storage.jdbc.JdbcSerDe | NULL | | InputFormat: | org.apache.hive.storage.jdbc.JdbcInputFormat | NULL | | OutputFormat: | org.apache.hive.storage.jdbc.JdbcOutputFormat | NULL | | Compressed: | No | NULL | | Num Buckets: | 0 | NULL | | Bucket Columns: | [] | NULL | | Sort Columns: | [] | NULL | | Storage Desc Params: | NULL | NULL | | | serialization.format | 1 | +-------------------------------+-------------------------------------------------+----------------------------------------------------+ 33 rows selected (6.099 seconds)\n 4. Offload the remote table to local cluster, run CTAS (example below pulls in all the data into the local table, but you can pull in select columns and rows by applying predicates) 0: jdbc:hive2://localhost:10000\u0026gt; **create table default.emr_clone as select * from test_emr_tbl;**\nINFO : Completed executing command(queryId=ngangam_20240129182608_db20e2bb-1db3-473f-9564-0d81b01228bc); Time taken: 6.492 seconds\nINFO : OK\n2 rows affected (14.802 seconds)\n0: jdbc:hive2://localhost:10000\u0026gt; select count(*) from default.emr_clone;\nINFO : Completed executing command(queryId=ngangam_20240129182647_7544c9d1-c68b-4a34-b6b0-910945a1dba5); Time taken: 2.344 seconds\nINFO : OK\n +------+ | _c0 | +------+ | 2 | +------+ 1 row selected (8.795 seconds)\n 5. To fetch data from the remote tables, run SELECT queries using column spec and predicates as you would normally with any SQL tables. 0: jdbc:hive2://localhost:10000\u0026gt; select * from test_emr_tbl where tblkey \u0026gt; 1;\nINFO : Completed executing command(queryId=ngangam_20240129191217_79b9e874-197d-4c31-8164-1ec2397bbff7); Time taken: 0.001 seconds\nINFO : OK\n +----------------------+---------------------+ | test_emr_tbl.tblkey | test_emr_tbl.descr | +----------------------+---------------------+ | 2 | test 2 | +----------------------+---------------------+ 1 row selected (8.238 seconds)\n 6. Join with local hive tables, run SELECT queries joining multiple tables (local or remote) as you would normally with any SQL tables. ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/data-connector-for-hive-and-hive-like-engines_288885794/","tags":null,"title":"Apache Hive : Data Connector for Hive and Hive-like engines"},{"categories":null,"contents":"Apache Hive : Data Connectors in Hive What is a Data connector? Data connectors (referred to as \u0026ldquo;connector\u0026rdquo; in Hive Query Language) are top level objects in Hive where users can define a set of properties required to be able to connect to a datasource from hive. So a connector has a type (closed enumerated set) that allows Hive to determine the driver class (for JDBC) and other URL params, a URL and a set of properties that could include the default credentials for the remote datasource. Once defined, users can use the same connector object to map multiple databases from the remote datasource to local hive metastore.\nWith JDBC Storage Handlers, users define a table in hive metastore for which data resides in a remote JDBC datastore. This hive table\u0026rsquo;s metadata is persisted locally in hive metastore\u0026rsquo;s backend. When Hiveserver2 runs a query against this table, data is retrieved from the remote JDBC table. While this is very powerful in itself, it has limitations.\n Each remote table in remote datasource has to be individually mapped to a local hive table. It becomes tedious if you have to map an entire database of many tables. Any new tables in the remote datasource are not automatically visible and will need to be manually mapped in hive. The metadata for the mapped table is static. It does not track changes to the remote table. If remote table has changed (added columns or dropped columns), the mapped hive table has to be dropped and re-created. This management compounds when one has constantly changing tables.  Data connectors allow users to map a database/schema in the remote datasource to a Hive database. Such databases are referred to as REMOTE databases in hive.\n CREATE REMOTE DATABASE postgres_db1 USING \u0026lt;connectorName\u0026gt; WITH DBPROPERTIES ('connector.remoteDbName'='db1');  All the tables within a mapped database (db1) are automatically visible from Hive. The metadata for these tables are not persisted in Hive metastore\u0026rsquo;s backend. They are retrieved at runtime via a live connector. All future tables will automatically be visible in Hive. The columns and their datatypes are mapped to a compatible hive datatype at runtime as well. So any metadata changes to tables in the remote datasource are immediately visible in hive. The same connector can be used to map another database to a hive database. All the connection information is shared between these REMOTE databases.  How do I use it?  Create a connector first.   CREATE CONNECTOR pg_local TYPE 'postgres' URL 'jdbc:\u0026lt;postgresql://localhost:5432\u0026gt;' WITH DCPROPERTIES (\u0026quot;hive.sql.dbcp.username\u0026quot;=\u0026quot;postgres\u0026quot;, \u0026quot;hive.sql.dbcp.password\u0026quot;=\u0026quot;postgres\u0026quot;); Create a database of type REMOTE in hive using the connector from Step1. This maps a remote database named \u0026ldquo;*hive_hms_testing*\u0026rdquo; to a hive database named \u0026ldquo;pg_hive_testing\u0026rdquo; in hive.   CREATE REMOTE DATABASE pg_hive_testing USING pg_local WITH DBPROPERTIES (\u0026quot;connector.remoteDbName\u0026quot;=\u0026quot;hive_hms_testing\u0026quot;); // USING keystore instead of cleartext passwords in DCPROPERTIES CREATE CONNECTOR pg_local_ks TYPE 'postgres' URL 'jdbc:\u0026lt;postgresql://localhost:5432/hive_hms_testing\u0026gt;' WITH DCPROPERTIES(\u0026quot;hive.sql.dbcp.username\u0026quot;=\u0026quot;postgres\u0026quot;,\u0026quot;hive.sql.dbcp.password.keystore\u0026quot;=\u0026quot;jceks://app/local/hive/secrets.jceks\u0026quot; \u0026quot;hive.sql.dbcp.password.key\u0026quot;=\u0026quot;postgres.credential\u0026quot;); CREATE REMOTE DATABASE pg_ks_local USING pg_local_ks(\u0026quot;connector.remoteDbName\u0026quot;=\u0026quot;hive_hms_testing\u0026quot;); 3. Use the tables in REMOTE database much like the JDBC-storagehandler based tables in hive. One big difference is that the metadata for these tables are never persisted in hive. Currently, create/alter/drop table DDLs are not supported in REMOTE databases. USE pg_hive_testing; SHOW TABLES; DESCRIBE [formatted] \u0026lt;tablename\u0026gt;; SELECT \u0026lt;col1\u0026gt; from \u0026lt;tablename\u0026gt; where \u0026lt;filter1\u0026gt; and \u0026lt;filter2\u0026gt;; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/data-connectors-in-hive_177049669/","tags":null,"title":"Apache Hive : Data Connectors in Hive"},{"categories":null,"contents":"Apache Hive : Datasketches Integration  Sketch functions  Naming convention List declared sketch functions   Integration with materialized views BI mode  Rewrite COUNT(DISTINCT(X)) Rewrite percentile_disc(p) withing group(order by x) Rewrite cume_dist() over (order by id) Rewrite NTILE Rewrite RANK   Examples  Simple distinct counting examples using HLL    Apache DataSketches (https://datasketches.apache.org/) is integrated into Hive via HIVE-22939.\nThis enables various kind of sketch operations thru regular sql statement.\nSketch functions Naming convention All sketch functions are registered using the following naming convention:\nds_{sketchType}_{functionName}\nFor example we have a function called: ds_hll_estimate which could be used to estimate the distinct values from an hll sketch.\nsketchType For detailed info about the sketches themself please refer to the datasketches site!\n frequency  hll cpc theta   frequent items  freq   histograms  kll    functionName    name description     sketch generates sketch data from input   estimate computes the estimate for frequency related sketches   union aggregate function to merge multiple sketches   union_f unions 2 sketches given in the arguments   n number of elements   cdf cumulative distribution   rank estimates the rank of the given element; returns a value in the range of 0~1   intersect aggregate to intersect multiple sketches   intersect_f intersect 2 sketches given in the arguments   stringify returns the the sketch in a more readable form    List declared sketch functions Given that we have ~60 functions registered I would recommend to also consider listing/getting info about a single udf.\nYou could list all functions prefixed by ds_ using:\nshow functions like \u0026lsquo;ds_%';\nAnd you can access the description of a function like:\ndesc function ds_freq_sketch;\nIntegration with materialized views Sketch aggregation(s) are exposed to Calcite by some extensions - which could enable both the usage of an MV in a smaller dimension query; or could help in incremental updates.\nBI mode Usage of sketches can give a performance boost in case we could afford to loose some accuracy. Which could come very handy in case of charts or live dashboards.\nThe BI mode is about making rewrites automatically to sketch functions if possible.\nThe BI mode can be enabled using:\nset hive.optimize.bi.enabled=true; Rewrite COUNT(DISTINCT(X)) This feature can be toggled using the hive.optimize.bi.rewrite.countdistinct.enabled conf key\nThe used distinct sketch family can be configured using: hive.optimize.bi.rewrite.countdistinct.sketch (currently only hll is available).\nThis feature could rewrite\nselect category, count(distinct id) from sketch_input group by category to use a distinct count sketch to answer the query by rewriting it to\nselect category, round(ds_hll_estimate(ds_hll_sketch(id))) from sketch_input Rewrite percentile_disc(p) withing group(order by x) This feature can be toggled using thehive.optimize.bi.rewrite.percentile_disc.enabled conf key\nThe used histogram sketch family can be configured using: hive.optimize.bi.rewrite.percentile_disc.sketch (currently only kll is available).\nThis feature could rewrite\nselect percentile_disc(0.3) within group(order by id) from sketch_input to use a histogram sketch to answer the query by rewriting to\nselect ds_kll_quantile(ds_kll_sketch(id), 0.3) from sketch_input Rewrite cume_dist() over (order by id) This feature can be toggled using the**hive.optimize.bi.rewrite.cume_dist.enabled** conf key\nThe used histogram sketch family can be configured using: hive.optimize.bi.rewrite.cume_dist.sketch (currently only kll is available).\nselect id,cume_dist() over (order by id) from sketch_input to use a histogram sketch to answer the query by rewriting to\nSELECT id, CAST(DS_KLL_RANK(t2.sketch, idVal) AS DOUBLE) FROM (SELECT id, CAST(COALESCE(CAST(id AS FLOAT), 340282346638528860000000000000000000000) AS FLOAT) AS idVal FROM sketch_input) AS t, (SELECT DS_KLL_SKETCH(CAST(`id` AS FLOAT)) AS `sketch` FROM sketch_input) AS t2 Rewrite NTILE This feature can be toggled using the hive.optimize.bi.rewrite.ntile.enabled conf key\nThe used histogram sketch family can be configured using: hive.optimize.bi.rewrite.ntile.sketch (currently only kll is available).\nThis feature can rewrite\nselect id, ntile(4) over (order by id from sketch_input order by id To use a histogram sketch to calculate the NTILE\u0026rsquo;s value:\nselect id, case when ceil(ds_kll_cdf(ds, CAST(id AS FLOAT) )[0]*4) \u0026lt; 1 then 1 else ceil(ds_kll_cdf(ds, CAST(id AS FLOAT) )[0]*4) end from sketch_input join ( select ds_kll_sketch(cast(id as float)) as ds from sketch_input ) q order by id select id, rank() over (order by id), case when ds_kll_n(ds) \u0026lt; (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1) then ds_kll_n(ds) else (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1) end Rewrite RANK This feature can be toggled using the hive.optimize.bi.rewrite.rank.enabled conf key\nThe used histogram sketch family can be configured using: hive.optimize.bi.rewrite.rank.sketch (currently only kll is available).\nselect id, rank() over (order by id) from sketch_input order by id is rewritten to\nselect id, case when ds_kll_n(ds) \u0026lt; (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1) then ds_kll_n(ds) else (ceil(ds_kll_rank(ds, CAST(id AS FLOAT) )*ds_kll_n(ds))+1) end from sketch_input join ( select ds_kll_sketch(cast(id as float)) as ds from sketch_input ) q order by id Examples Simple distinct counting examples using HLL  Prepare sample table  create table sketch_input (id int, category char(1)) STORED AS ORC TBLPROPERTIES ('transactional'='true'); insert into table sketch_input values (1,'a'),(1, 'a'), (2, 'a'), (3, 'a'), (4, 'a'), (5, 'a'), (6, 'a'), (7, 'a'), (8, 'a'), (9, 'a'), (10, 'a'), (6,'b'),(6, 'b'), (7, 'b'), (8, 'b'), (9, 'b'), (10, 'b'), (11, 'b'), (12, 'b'), (13, 'b'), (14, 'b'), (15, 'b') ;   Use HLL to compute distinct values using an intermediate table   -- build sketches per category create temporary table sketch_intermediate (category char(1), sketch binary); insert into sketch_intermediate select category, ds_hll_sketch(id) from sketch_input group by category; -- get unique count estimates per category select category, ds_hll_estimate(sketch) from sketch_intermediate; -- union sketches across categories and get overall unique count estimate select ds_hll_estimate(ds_hll_union(sketch)) from sketch_intermediate;   Use HLL to compute distinct values without intermediate table   select category, ds_hll_estimate(ds_hll_sketch(id)) from sketch_input group by category; select ds_hll_estimate(ds_hll_sketch(id)) from sketch_input;   Use HLL to compute distinct values transparently thru BI mode   set hive.optimize.bi.enabled=true; select category,count(distinct id) from sketch_input group by category; select count(distinct id) from sketch_input;   Use HLL to compute distinct values transparently thru BI mode - while utilizing a Materialized View to store the intermediate sketches.   -- create an MV to store precomputed HLL values create materialized view mv_1 as select category, ds_hll_sketch(id) from sketch_input group by category; set hive.optimize.bi.enabled=true; select category,count(distinct id) from sketch_input group by category; select count(distinct id) from sketch_input; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/datasketches-integration_177050456/","tags":null,"title":"Apache Hive : Datasketches Integration"},{"categories":null,"contents":"Apache Hive : Default Constraint (HIVE-18726) Introduction This document proposes the addition of DEFAULT clause to Hive. DEFAULT clause is a domain constraint which lets user specify a value for domain i.e. column to be used in absence of user specified value i.e. in absence of column reference. Note that this does not propose to implement DEFAULT ON NULL like ORACLE which lets user specify DEFAULT value for explicit NULLs.\nBackground Hive currently let users declare the following constraints:\n PRIMARY KEY FOREIGN KEY UNIQUE NOT NULL  DEFAULT will be a fifth addition to this list. Note that unlike existing constraints DEFAULT will only support ENABLE/ENFORCED or DISABLE keyword. VALIDATE/NOVALIDATE and RELY/NORELY will not be supported since ENABLING DEFAULT will not change existing data and the optimizer will not make use of the RELY keyword.\nProposed Changes Create Table CREATE TABLE will be updated to let user specify DEFAULT as follows:\n With column definition   CREATE TABLE (DEFAULT )   With constraint specification   CREATE TABLE (, …, CONSTRAINT DEFAULT ()  To be compliant with SQL standards, Hive will only permit default values which fall in one of the following categories:\n LITERAL DATE TIME VALUE FUNCTION, that is, CURRENT_TIME, CURRENT_DATE CURRENT_USER() NULL CAST (as PRIMITIVE TYPE)  INSERT Anytime user doesn’t specify a value explicitly for a column, its default value will be used if defined. For example:\nINSERT INTO \u0026lt;tableName\u0026gt;(co1, col3) values(\u0026lt;val1\u0026gt; , \u0026lt;val2\u0026gt;)\nAbove statement doesn’t specify a value for col2 so system will use the default value for col2 if it is defined.\nOn the other hand if user specifies an explicit value including NULL, for example:\nINSERT INTO \u0026lt;tableName\u0026gt;(col1, col2, col3) values (\u0026lt;val1\u0026gt;, \u0026lt;val2\u0026gt;, \u0026lt;val3\u0026gt;)\nthen the default value will not be used.\nAbove are also valid for all following type of DMLs:\n INSERT INTO INSERT SELECT MERGE  PARTITION COLUMNS Since in Hive a query can not be written without referencing partition columns, there could not be a situation where the value for a partition column isn’t explicit. Therefore having DEFAULT for partition columns will not make sense and we propose to not add it.\nEXTERNAL TABLE We plan to disallow DEFAULT for external table since the data isn’t managed by Hive.\nACID/MM TABLE DEFAULT constraint will be allowed and behavior will be same as non-acid tables.\nMETASTORE SCHEMA We propose to add column DEFAULT_VALUE to KEY_CONSTRAINTS table in metastore schema to store DEFAULT VALUE.\nOTHER DDL The following DDLs will be updated to accommodate DEFAULT:\n ALTER TABLE CHANGE COLUMN ALTER TABLE DROP CONSTRAINT  Restrictions  Hive will have strict type rules for defining the default value, i.e., default value will have to be the exact same type as column type. Defaults with Complex data types (Array, Struct, Map and Union) are not allowed.  Proposed Design Currently if an INSERT query is missing a value in INSERT, i.e. if user hasn’t specified a value for a column, Hive uses ‘NULL’ as default. The Hive compiler detects this missing column value at compile time and inserts a NULL. We propose to update this logic to check for default and use that instead of NULL value.\nAlong with this logic change we foresee the following changes:\n Metastore code will need to be updated to support the DEFAULT constraint.   We propose to store/serialize the default value as string after it is evaluated and constant folded. DEFAULT_VALUE will be added to KEY_CONSTRAINTS table in metastore schema.   Hive Parser will need to be updated to allow new DEFAULT keyword with default value. Error handling/Validation logic needs to be added to make sure DEFAULT value conforms to allowed categories during CREATE TABLE. Type check to make sure DEFAULT VALUE type is compatible with corresponding column type.  Further Work HIVE-19059 adds the keyword DEFAULT to enable users to add DEFAULT values in INSERT and UPDATE statements without specifying the column schema. See DEFAULT Keyword (HIVE-19059).\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/75969407/","tags":null,"title":"Apache Hive : Default Constraint (HIVE-18726)"},{"categories":null,"contents":"Apache Hive : DEFAULT Keyword (HIVE-19059) Goal We propose to add DEFAULT keyword in INSERT INTO, UPDATE and MERGE statements to let user add DEFAULT values without specifying column schema.\nBackground With the addition of DEFAULT constraint (HIVE-18726) user can define columns to have default value which will be used in case user doesn’t explicitly specify it while INSERTING data. For DEFAULT constraint to kick in user has to explicitly specify column schema leaving out the column name for which user would like the sytem to use DEFAULT value. e.g. INSERT INTO TABLE1(COL1, COL3) VALUES(1,3). This statement leaves COL2 from the schema so that Hive could insert DEFAULT value if it is defined. But if user wants to insert DEFAULT value without specifying column schema it is not possible to do so. This limitation could be overcome using DEFAULT keyword. Proposed Changes INSERT INTO INSERT INTO VALUES will be updated to let user specify DEFAULT keyword. If corresponding column has DEFAULT constraint Hive will use that otherwise NULL will be used.\nExamples:\n INSERT INTO TABLE1 VALUES(DEFAULT, DEFAULT) INSERT INTO TABLE1(COL1) VALUES(DEFAULT)  UPDATE UPDATE will be updated to let user specify DEFAULT keyword.\nExample:\n UPDATE TABLE1 SET COL1=DEFAULT, COL2=DEFAULT WHERE \u0026lt;CONDITION\u0026gt;  Proposed Design During first phase of AST analysis AST for DEFAULT will be replaced with corresponding DEFAULT value AST or NULL AST.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/75977362/","tags":null,"title":"Apache Hive : DEFAULT Keyword (HIVE-19059)"},{"categories":null,"contents":"Apache Hive : Dependent Tables Hive supports both partitioned and unpartitioned external tables. In both cases, when a new table/partition is being added, the location is also specified for the new table/partition. Let us consider a specific example:\ncreate table T (key string, value string) partitioned by (ds string, hr string);\ninsert overwrite table T partition (ds=\u0026lsquo;1\u0026rsquo;, hr=\u0026lsquo;1\u0026rsquo;) \u0026hellip;;\n..\ninsert overwrite table T partition (ds=\u0026lsquo;1\u0026rsquo;, hr=\u0026lsquo;24\u0026rsquo;) \u0026hellip;;\nT is a partitioned table by date and hour, and Tsignal is an external table which conceptually denotes the creation of the signal table.\ncreate external table Tsignal (key string, value string) partitioned by (ds string);\nWhen all the hourly partitions are created for a day (ds=\u0026lsquo;1\u0026rsquo;), the corresponding partition can be added to Tsignal\nalter table Tsignal add partition (ds=\u0026lsquo;1\u0026rsquo;) location \u0026lsquo;Location of T\u0026rsquo;/ds=1;\nThere is a implicit dependency between Tsignal@ds=1 and T@ds=1/hr=1, T@ds=1/hr=2, \u0026hellip;. T@ds=1/hr=24, but that dependency is not captured anywhere\nin the metastore. It would be useful to have an ability to explicitly create that dependency. This dependency can be used for all kinds of auditing purposes. For eg. when the following query is performed:\nselect .. from Tsignal where ds = \u0026lsquo;1\u0026rsquo;;\nthe inputs only contains Tsignal@ds=1, but is should also contain T@ds=1/hr=1, T@ds=1/hr=2,\u0026hellip;.T@ds=1/hr=24\nThis dependency should be captured by the metastore. For simplicity, let us assume we create a new notion of dependent tables (instead of overloading external tables).\ncreate dependency table Tdependent (key string, value string) partitioned by (ds string);\nThis is like a external table but also captures the dependency (we can also enhance external tables for the same).\nalter table Tdependent add partition (ds=\u0026lsquo;1\u0026rsquo;) location \u0026lsquo;/T/ds=1\u0026rsquo; dependent partitions table T partitions (ds=\u0026lsquo;1\u0026rsquo;);\nspecify the partial partition spec for the dependent partitions.\nNote that each table can point to different locations - hive needs to ensure that all the dependent partitions are under the location \u0026lsquo;T/ds=1\u0026rsquo;\n Specify the location  The metastore can store the dependencies completely or partially.\n    Materialize the dependencies both-ways\nTdependent@ds=1 depends on T@ds=1/hr=1 to T@ds=1/hr=24\nT@ds=1/hr=1 is depended upon by T@ds=1\nAdvantages: if T@ds=1/hr=1 is dropped, T@ds=1 can be notified or it can choose to dis-allow this\nAny property on Tdependent can be propagated to T\n     Is the dependency used for querying ? What happens if T@ds=1/hr=25 gets added ? The query \u0026lsquo;select .. from Tdependent where ds = 1\u0026rsquo; includes T@ds=1/hr=25, but this is not shown in the inputs.  Dont use the location for querying - then why have the location ?        Store partial dependencies\nTdependent@ds=1 depends on T@ds=1 (spec).\nAt describe time, the spec is evaluated and all the dependent partitions are computed dynamically. At add partition time, verify that the location captures all dependent partitions.\nThe partial spec is not used for querying - location is used for that. At query time, verify that the location captures all dependent partitions.\n    The dependent table does not have a location.\n The list of partitions are computed at query time - think of it like a view, where each partition has its own definition limited to \u0026lsquo;select * from T where partial/full partition spec\u0026rsquo;. Query layer needs to change. Is it possible ? Unlike a view, it does not rewritten at semantic analysis time. After partition pruning is done (on a dependent table), rewrite the  tree to contain the base table T - the columns remain the same, so it should be possible.\n  With this, it is possible that the partitions point to different tables.\nFor eg:\nalter table Tdependent add partition (ds=\u0026lsquo;1\u0026rsquo;) depends on table T1 partition (ds=\u0026lsquo;1\u0026rsquo;);\nalter table Tdependent add partition (ds=\u0026lsquo;2\u0026rsquo;) depends on table T2 partition (ds=\u0026lsquo;2\u0026rsquo;);\nSomething that can be achieved by external tables currently. The dependent partitions are computed dynamically - T1@ds=1/hr=1 does not know the fact that it is dependent upon by Tdependent@ds=1.\nT1@ds=1/hr=1 can be dropped anytime, and Tdependent@ds=1 automatically stops depending upon it from that point.\nI am leaning towards this - the user need not specify both the location and the dependent partitions.\nCan the external tables be enhanced to support this ? Will create a problem for the query layer, since the external tables are handled differently today.\n    The list of dependent partitions are materialized and stored in the metastore, and use that for querying.\nA query like \u0026lsquo;select .. from Tdependent where ds = 1\u0026rsquo; gets transformed to \u0026lsquo;select .. from (select * from T where ((ds = 1 and hr = 1) or (ds = 1 and hr = 2) \u0026hellip;. or (ds=1 and hr=24))\u0026rsquo;\nCan put a lot of load on the query layer.\n    = Final Proposal =\nInstead of enhancing external tables to solve a convoluted usecase, create a new type of tables - dependent tables. The reason for the existence of external tables is to point to some existing data.\n== Dependent Tables ==\nCreate a table which explicitly depends on another table. Consider the following scenario for the first use case mentioned:\n create table T (key string, value string) partitioned by (ds string, hr string); -- create a dependent table which specifies the dependencies explicitly -- Tdep inherits the schema of T -- Tdep is partitioned by a prefix of T (either ds or ds,hr) create dependent table Tdep partitioned by (ds string) depends on table T; In order to denote the end of a daily partition for T, the corresponding partition is added in Tdep\n -- create partition T@ds=1/hr=1 to T@ds=1/hr=24 alter table Tdep add partition (ds=1); The metastore stores the following dependencies\n Tdep depends on T, and T is dependent upon by Tdep Tdep@ds=1 depends on T@ds=1  In some usecases, it is possible that some partitions of Tdep depends on T, whereas the newer partitions of Tdep depend on T2.\n create table T (key string, value string) partitioned by (ds string, hr string); -- create a dependent table which specifies the dependencies explicitly -- Tdep inherits the schema of T -- Tdep is partitioned by a prefix of T (either ds or ds,hr) create dependent table Tdep partitioned by (ds string) depends on table T; -- create partition T@ds=1/hr=1 to T@ds=1/hr=24 alter table Tdep add partition (ds=1); -- repeat this for T@ds=1 to T@ds=10 -- T is being deprecated, and T2 is the new table (with the same schema, possibly partitioned on different keys) create table T2 (key string, value string) partitioned by (ds string, hr string, min string); alter table Tdep depends on table T2; -- create partition T2@ds=11/hr=1/min=1 to T2@ds=11/hr=24/min=60 alter table Tdep add partition (ds=1); The metastore stores the following dependencies\n Tdep depends on (T,T2), and (T,T2) are dependent upon by Tdep T and T2 have the same schema Tdep@ds=1..Tdep@ds=10 depends on T@ds=1..T@ds=10 respectively Tdep@ds=11 depends on T2@ds=11  The query on Tdep is re-written to access the underlying tables. For eg. for the query \u0026lsquo;select count(1) from Tdep where ds = 1\u0026rsquo;, once partition pruning is done (on Tdep), the operator tree (TableScan(Tdep):ds=1 -\u0026gt; Select) is rewritten to (TableScan(T):ds=1/hr=1 to ds=1/hr=24 -\u0026gt; Select). The advantage of this over external tables is that all the table properties (bucketing/sorting/list bucketing) for the underlying tables are used. This can easily extend to multiple tables. For eg. for the query \u0026lsquo;select count(1) from Tdep where ds = 1 or ds = 11\u0026rsquo;, once partition pruning is done (on Tdep), the operator tree (TableScan(Tdep):ds=1,ds=11 -\u0026gt; Select) is rewritten to (TableScan(T):ds=1/hr=1 to ds=1/hr=24 -\u0026gt; Select -\u0026gt; Union) and (TableScan(T2):ds=1/hr=1 to ds=1/hr=24 -\u0026gt; Select -\u0026gt; Union).\n If the schema of T changes, it can either be dis-allowed or propagated to Tdep. desc extended will be enhanced to show all the dependent partitions. The dependency of an existing partition can be changed: alter table Tdep partition (ds=10) depends on table T2  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/dependent-tables_30151205/","tags":null,"title":"Apache Hive : Dependent Tables"},{"categories":null,"contents":"Apache Hive : Design This page contains details about the Hive design and architecture. A brief technical report about Hive is available at hive.pdf.\n Hive Architecture Hive Data Model Metastore  Motivation Metadata Objects Metastore Architecture Metastore Interface   Hive Query Language Compiler Optimizer Hive APIs  Figure 1\nHive Architecture Figure 1 shows the major components of Hive and its interactions with Hadoop. As shown in that figure, the main components of Hive are:\n UI – The user interface for users to submit queries and other operations to the system. As of 2011 the system had a command line interface and a web based GUI was being developed. Driver – The component which receives the queries. This component implements the notion of session handles and provides execute and fetch APIs modeled on JDBC/ODBC interfaces. Compiler – The component that parses the query, does semantic analysis on the different query blocks and query expressions and eventually generates an execution plan with the help of the table and partition metadata looked up from the metastore. Metastore – The component that stores all the structure information of the various tables and partitions in the warehouse including column and column type information, the serializers and deserializers necessary to read and write data and the corresponding HDFS files where the data is stored. Execution Engine – The component which executes the execution plan created by the compiler. The plan is a DAG of stages. The execution engine manages the dependencies between these different stages of the plan and executes these stages on the appropriate system components.  Figure 1 also shows how a typical query flows through the system. The UI calls the execute interface to the Driver (step 1 in Figure 1). The Driver creates a session handle for the query and sends the query to the compiler to generate an execution plan (step 2). The compiler gets the necessary metadata from the metastore (steps 3 and 4). This metadata is used to typecheck the expressions in the query tree as well as to prune partitions based on query predicates. The plan generated by the compiler (step 5) is a DAG of stages with each stage being either a map/reduce job, a metadata operation or an operation on HDFS. For map/reduce stages, the plan contains map operator trees (operator trees that are executed on the mappers) and a reduce operator tree (for operations that need reducers). The execution engine submits these stages to appropriate components (steps 6, 6.1, 6.2 and 6.3). In each task (mapper/reducer) the deserializer associated with the table or intermediate outputs is used to read the rows from HDFS files and these are passed through the associated operator tree. Once the output is generated, it is written to a temporary HDFS file though the serializer (this happens in the mapper in case the operation does not need a reduce). The temporary files are used to provide data to subsequent map/reduce stages of the plan. For DML operations the final temporary file is moved to the table\u0026rsquo;s location. This scheme is used to ensure that dirty data is not read (file rename being an atomic operation in HDFS). For queries, the contents of the temporary file are read by the execution engine directly from HDFS as part of the fetch call from the Driver (steps 7, 8 and 9).\nHive Data Model Data in Hive is organized into:\n Tables – These are analogous to Tables in Relational Databases. Tables can be filtered, projected, joined and unioned. Additionally all the data of a table is stored in a directory in HDFS. Hive also supports the notion of external tables wherein a table can be created on prexisting files or directories in HDFS by providing the appropriate location to the table creation DDL. The rows in a table are organized into typed columns similar to Relational Databases. Partitions – Each Table can have one or more partition keys which determine how the data is stored, for example a table T with a date partition column ds had files with data for a particular date stored in the /ds=directory in HDFS. Partitions allow the system to prune data to be inspected based on query predicates, for example a query that is interested in rows from T that satisfy the predicate T.ds = \u0026lsquo;2008-09-01\u0026rsquo; would only have to look at files in /ds=2008-09-01/ directory in HDFS. Buckets – Data in each partition may in turn be divided into Buckets based on the hash of a column in the table. Each bucket is stored as a file in the partition directory. Bucketing allows the system to efficiently evaluate queries that depend on a sample of data (these are queries that use the SAMPLE clause on the table).  Apart from primitive column types (integers, floating point numbers, generic strings, dates and booleans), Hive also supports arrays and maps. Additionally, users can compose their own types programmatically from any of the primitives, collections or other user-defined types. The typing system is closely tied to the SerDe (Serailization/Deserialization) and object inspector interfaces. User can create their own types by implementing their own object inspectors, and using these object inspectors they can create their own SerDes to serialize and deserialize their data into HDFS files). These two interfaces provide the necessary hooks to extend the capabilities of Hive when it comes to understanding other data formats and richer types. Builtin object inspectors like ListObjectInspector, StructObjectInspector and MapObjectInspector provide the necessary primitives to compose richer types in an extensible manner. For maps (associative arrays) and arrays useful builtin functions like size and index operators are provided. The dotted notation is used to navigate nested types, for example a.b.c = 1 looks at field c of field b of type a and compares that with 1.\nMetastore Motivation The Metastore provides two important but often overlooked features of a data warehouse: data abstraction and data discovery. Without the data abstractions provided in Hive, a user has to provide information about data formats, extractors and loaders along with the query. In Hive, this information is given during table creation and reused every time the table is referenced. This is very similar to the traditional warehousing systems. The second functionality, data discovery, enables users to discover and explore relevant and specific data in the warehouse. Other tools can be built using this metadata to expose and possibly enhance the information about the data and its availability. Hive accomplishes both of these features by providing a metadata repository that is tightly integrated with the Hive query processing system so that data and metadata are in sync.\nMetadata Objects  Database – is a namespace for tables. It can be used as an administrative unit in the future. The database \u0026lsquo;default\u0026rsquo; is used for tables with no user-supplied database name. Table – Metadata for a table contains list of columns, owner, storage and SerDe information. It can also contain any user-supplied key and value data. Storage information includes location of the underlying data, file inout and output formats and bucketing information. SerDe metadata includes the implementation class of serializer and deserializer and any supporting information required by the implementation. All of this information can be provided during creation of the table. Partition – Each partition can have its own columns and SerDe and storage information. This facilitates schema changes without affecting older partitions.  Metastore Architecture Metastore is an object store with a database or file backed store. The database backed store is implemented using an object-relational mapping (ORM) solution called the DataNucleus. The prime motivation for storing this in a relational database is queriability of metadata. Some disadvantages of using a separate data store for metadata instead of using HDFS are synchronization and scalability issues. Additionally there is no clear way to implement an object store on top of HDFS due to lack of random updates to files. This, coupled with the advantages of queriability of a relational store, made our approach a sensible one.\nThe metastore can be configured to be used in a couple of ways: remote and embedded. In remote mode, the metastore is a Thrift service. This mode is useful for non-Java clients. In embedded mode, the Hive client directly connects to an underlying metastore using JDBC. This mode is useful because it avoids another system that needs to be maintained and monitored. Both of these modes can co-exist. (Update: Local metastore is a third possibility. See Hive Metastore Administration for details.)\nMetastore Interface Metastore provides a Thrift interface to manipulate and query Hive metadata. Thrift provides bindings in many popular languages. Third party tools can use this interface to integrate Hive metadata into other business metadata repositories.\nHive Query Language HiveQL is an SQL-like query language for Hive. It mostly mimics SQL syntax for creation of tables, loading data into tables and querying the tables. HiveQL also allows users to embed their custom map-reduce scripts. These scripts can be written in any language using a simple row-based streaming interface – read rows from standard input and write out rows to standard output. This flexibility comes at a cost of a performance hit caused by converting rows from and to strings. However, we have seen that users do not mind this given that they can implement their scripts in the language of their choice. Another feature unique to HiveQL is multi-table insert. In this construct, users can perform multiple queries on the same input data using a single HiveQL query. Hive optimizes these queries to share the scan of the input data, thus increasing the throughput of these queries several orders of magnitude. We omit more details due to lack of space. For a more complete description of the HiveQL language see the language manual.\nCompiler  Parser – Transform a query string to a parse tree representation. Semantic Analyser – Transform the parse tree to an internal query representation, which is still block based and not an operator tree. As part of this step, the column names are verified and expansions like * are performed. Type-checking and any implicit type conversions are also performed at this stage. If the table under consideration is a partitioned table, which is the common scenario, all the expressions for that table are collected so that they can be later used to prune the partitions which are not needed. If the query has specified sampling, that is also collected to be used later on. Logical Plan Generator – Convert the internal query representation to a logical plan, which consists of a tree of operators. Some of the operators are relational algebra operators like \u0026lsquo;filter\u0026rsquo;, \u0026lsquo;join\u0026rsquo; etc. But some of the operators are Hive specific and are used later on to convert this plan into a series of map-reduce jobs. One such operator is a reduceSink operator which occurs at the map-reduce boundary. This step also includes the optimizer to transform the plan to improve performance – some of those transformations include: converting a series of joins into a single multi-way join, performing a map-side partial aggregation for a group-by, performing a group-by in 2 stages to avoid the scenario when a single reducer can become a bottleneck in presence of skewed data for the grouping key. Each operator comprises a descriptor which is a serializable object. Query Plan Generator – Convert the logical plan to a series of map-reduce tasks. The operator tree is recursively traversed, to be broken up into a series of map-reduce serializable tasks which can be submitted later on to the map-reduce framework for the Hadoop distributed file system. The reduceSink operator is the map-reduce boundary, whose descriptor contains the reduction keys. The reduction keys in the reduceSink descriptor are used as the reduction keys in the map-reduce boundary. The plan consists of the required samples/partitions if the query specified so. The plan is serialized and written to a file.  Optimizer More plan transformations are performed by the optimizer. The optimizer is an evolving component. As of 2011, it was rule-based and performed the following: column pruning and predicate pushdown. However, the infrastructure was in place, and there was work under progress to include other optimizations like map-side join. (Hive 0.11 added several join optimizations.)\nThe optimizer can be enhanced to be cost-based (see Cost-based optimization in Hive and HIVE-5775). The sorted nature of output tables can also be preserved and used later on to generate better plans. The query can be performed on a small sample of data to guess the data distribution, which can be used to generate a better plan.\nA correlation optimizer was added in Hive 0.12.\nThe plan is a generic operator tree, and can be easily manipulated.\nHive APIs Hive APIs Overview describes various public-facing APIs that Hive provides.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/design_27362072/","tags":null,"title":"Apache Hive : Design"},{"categories":null,"contents":"Apache Hive : DesignDocs Hive Design Documents Proposals that appear in the \u0026ldquo;Completed\u0026rdquo; and \u0026ldquo;In Progress\u0026rdquo; sections should include a link to a JIRA ticket\nCompleted  Views (HIVE-1143) Partitioned Views (HIVE-1941) Storage Handlers (HIVE-705) HBase Integration HBase Bulk Load Locking (HIVE-1293) Indexes (HIVE-417) Bitmap Indexes (HIVE-1803) Filter Pushdown (HIVE-279) Table-level Statistics (HIVE-1361) Dynamic Partitions Binary Data Type (HIVE-2380) Decimal Precision and Scale Support HCatalog (formerly Howl) HiveServer2 (HIVE-2935) Column Statistics in Hive (HIVE-1362) List Bucketing (HIVE-3026) Group By With Rollup (HIVE-2397) Enhanced Aggregation, Cube, Grouping and Rollup (HIVE-3433) Optimizing Skewed Joins (HIVE-3086) Correlation Optimizer (HIVE-2206) Hive on Tez (HIVE-4660)  Hive-Tez Compatibility   Vectorized Query Execution (HIVE-4160) Cost Based Optimizer in Hive (HIVE-5775) Atomic Insert/Update/Delete (HIVE-5317) Transaction Manager (HIVE-5843) SQL Standard based secure authorization (HIVE-5837) Hybrid Hybrid Grace Hash Join (HIVE-9277) LLAP Daemons (HIVE-7926) Support for Hive Replication (HIVE-7973)  In Progress  Column Level Top K Statistics (HIVE-3421) Hive on Spark (HIVE-7292) Hive on Spark: Join Design (HIVE-7613) Improve ACID Performance – download docx file (HIVE-14035, HIVE-14199, HIVE-14233) Query Results Caching (HIVE-18513) Default Constraint (HIVE-18726) Different TIMESTAMP types (HIVE-21348) Support SAML 2.0 authentication (HIVE-24543)  Proposed  Spatial Queries Theta Join (HIVE-556) attachments/27362075/55476344.pdf JDBC Storage Handler MapJoin Optimization Proposal to standardize and expand Authorization in Hive Dependent Tables (HIVE-3466) AccessServer Type Qualifiers in Hive MapJoin \u0026amp; Partition Pruning (HIVE-5119) Updatable Views (HIVE-1143) Phase 2 of Replication Development (HIVE-14841) Subqueries in SELECT (HIVE-16091) DEFAULT keyword (HIVE-19059) Hive remote databases/tables  Incomplete  Authorization (Committed but not secure/deployable – see Disclaimer)  Abandoned  Hive across Multiple Data Centers (Physical Clusters) Metastore on HBase (HIVE-9452)  Other  Security Notes Hive Outer Join Behavior Metastore ER Diagram  Attachments: attachments/27362075/34177517.pdf (application/pdf)\nattachments/27362075/35193010.pdf (application/pdf)\nattachments/27362075/35193011.pdf (application/pdf)\nattachments/27362075/36438041.pdf (application/pdf)\nattachments/27362075/35193076.pdf (application/pdf)\nattachments/27362075/35193122.pdf (application/pdf)\nattachments/27362075/35193191-html (text/html)\nattachments/27362075/34177489.pdf (application/download)\nattachments/27362075/55476344.pdf (application/download)\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/designdocs_27362075/","tags":null,"title":"Apache Hive : DesignDocs"},{"categories":null,"contents":"Apache Hive : DeveloperDocs Hive Developer Documentation Information for Hive developers is available in these documents:\n Hive Developer Guide  Code organization and architecture Compiling and running Hive Unit tests Debugging Hive code Pluggable interfaces   Hive Developer FAQ  Moving files Building Hive Testing Hive   Plugin Developer Kit Writing UDTFs Hive on Spark: Getting Started  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/developerdocs_42568263/","tags":null,"title":"Apache Hive : DeveloperDocs"},{"categories":null,"contents":"Apache Hive : DeveloperGuide  Code Organization and a Brief Architecture  Introduction Hive SerDe MetaStore Query Processor   Compiling and Running Hive  Default Mode Advanced Mode Running Hive Without a Hadoop Cluster   Unit tests and debugging  Layout of the unit tests Debugging Hive Code   Pluggable interfaces  File Formats SerDe - how to add a new SerDe Map-Reduce Scripts UDFs and UDAFs - how to add new UDFs and UDAFs      Code Organization and a Brief Architecture Introduction Hive has 3 main components:\n Serializers/Deserializers (trunk/serde) - This component has the framework libraries that allow users to develop serializers and deserializers for their own data formats. This component also contains some builtin serialization/deserialization families. MetaStore (trunk/metastore) - This component implements the metadata server, which is used to hold all the information about the tables and partitions that are in the warehouse. Query Processor (trunk/ql) - This component implements the processing framework for converting SQL to a graph of map/reduce jobs and the execution time framework to run those jobs in the order of dependencies.  Apart from these major components, Hive also contains a number of other components. These are as follows:\n Command Line Interface (trunk/cli) - This component has all the java code used by the Hive command line interface. Hive Server (trunk/service) - This component implements all the APIs that can be used by other clients (such as JDBC drivers) to talk to Hive. Common (trunk/common) - This component contains common infrastructure needed by the rest of the code. Currently, this contains all the java sources for managing and passing Hive configurations(HiveConf) to all the other code components. Ant Utilities (trunk/ant) - This component contains the implementation of some ant tasks that are used by the build infrastructure. Scripts (trunk/bin) - This component contains all the scripts provided in the distribution including the scripts to run the Hive CLI (bin/hive).  The following top level directories contain helper libraries, packaged configuration files etc..:\n trunk/conf - This directory contains the packaged hive-default.xml and hive-site.xml. trunk/data - This directory contains some data sets and configurations used in the Hive tests. trunk/ivy - This directory contains the Ivy files used by the build infrastructure to manage dependencies on different Hadoop versions. trunk/lib - This directory contains the run time libraries needed by Hive. trunk/testlibs - This directory contains the junit.jar used by the JUnit target in the build infrastructure. trunk/testutils (Deprecated)  Hive SerDe What is a SerDe?\n SerDe is a short name for \u0026ldquo;Serializer and Deserializer.\u0026rdquo; Hive uses SerDe (and FileFormat) to read and write table rows. HDFS files \u0026ndash;\u0026gt; InputFileFormat \u0026ndash;\u0026gt; \u0026lt;key, value\u0026gt; \u0026ndash;\u0026gt; Deserializer \u0026ndash;\u0026gt; Row object Row object \u0026ndash;\u0026gt; Serializer \u0026ndash;\u0026gt; \u0026lt;key, value\u0026gt; \u0026ndash;\u0026gt; OutputFileFormat \u0026ndash;\u0026gt; HDFS files  Note that the \u0026ldquo;key\u0026rdquo; part is ignored when reading, and is always a constant when writing. Basically row object is stored into the \u0026ldquo;value\u0026rdquo;.\nOne principle of Hive is that Hive does not own the HDFS file format. Users should be able to directly read the HDFS files in the Hive tables using other tools or use other tools to directly write to HDFS files that can be loaded into Hive through \u0026ldquo;CREATE EXTERNAL TABLE\u0026rdquo; or can be loaded into Hive through \u0026ldquo;LOAD DATA INPATH,\u0026rdquo; which just move the file into Hive\u0026rsquo;s table directory.\nNote that org.apache.hadoop.hive.serde is the deprecated old SerDe library. Please look at org.apache.hadoop.hive.serde2 for the latest version.\nHive currently uses these FileFormat classes to read and write HDFS files:\n TextInputFormat/HiveIgnoreKeyTextOutputFormat: These 2 classes read/write data in plain text file format. SequenceFileInputFormat/SequenceFileOutputFormat: These 2 classes read/write data in Hadoop SequenceFile format.  Hive currently uses these SerDe classes to serialize and deserialize data:\n MetadataTypedColumnsetSerDe: This SerDe is used to read/write delimited records like CSV, tab-separated control-A separated records (sorry, quote is not supported yet). LazySimpleSerDe: This SerDe can be used to read the same data format as MetadataTypedColumnsetSerDe and TCTLSeparatedProtocol, however, it creates Objects in a lazy way which provides better performance. Starting in Hive 0.14.0 it also supports read/write data with a specified encode charset, for example:  ALTER TABLE person SET SERDEPROPERTIES ('serialization.encoding'='GBK'); LazySimpleSerDe can treat \u0026lsquo;T\u0026rsquo;, \u0026rsquo;t', \u0026lsquo;F\u0026rsquo;, \u0026lsquo;f\u0026rsquo;, \u0026lsquo;1\u0026rsquo;, and \u0026lsquo;0\u0026rsquo; as extended, legal boolean literals if the configuration property hive.lazysimple.extended_boolean_literal is set to true (Hive 0.14.0 and later). The default is false, which means only \u0026lsquo;TRUE\u0026rsquo; and \u0026lsquo;FALSE\u0026rsquo; are treated as legal boolean literals.\n ThriftSerDe: This SerDe is used to read/write Thrift serialized objects. The class file for the Thrift object must be loaded first. DynamicSerDe: This SerDe also read/write Thrift serialized objects, but it understands Thrift DDL so the schema of the object can be provided at runtime. Also it supports a lot of different protocols, including TBinaryProtocol, TJSONProtocol, TCTLSeparatedProtocol (which writes data in delimited records).  Also:\n For JSON files, JsonSerDe was added in Hive 0.12.0. An Amazon SerDe is available at s3://elasticmapreduce/samples/hive-ads/libs/jsonserde.jar for releases prior to 0.12.0. An Avro SerDe was added in Hive 0.9.1. Starting in Hive 0.14.0 its specification is implicit with the STORED AS AVRO clause. A SerDe for the ORC file format was added in Hive 0.11.0. A SerDe for Parquet was added via plug-in in Hive 0.10 and natively in Hive 0.13.0. A SerDe for CSV was added in Hive 0.14.  See SerDe for detailed information about input and output processing. Also see Storage Formats in the HCatalog manual, including CTAS Issue with JSON SerDe. For information about how to create a table with a custom or native SerDe, see Row Format, Storage Format, and SerDe.\nHow to Write Your Own SerDe  In most cases, users want to write a Deserializer instead of a SerDe, because users just want to read their own data format instead of writing to it. For example, the RegexDeserializer will deserialize the data using the configuration parameter \u0026lsquo;regex\u0026rsquo;, and possibly a list of column names (see serde2.MetadataTypedColumnsetSerDe). Please see serde2/Deserializer.java for details. If your SerDe supports DDL (basically, SerDe with parameterized columns and column types), you probably want to implement a Protocol based on DynamicSerDe, instead of writing a SerDe from scratch. The reason is that the framework passes DDL to SerDe through \u0026ldquo;Thrift DDL\u0026rdquo; format, and it\u0026rsquo;s non-trivial to write a \u0026ldquo;Thrift DDL\u0026rdquo; parser. For examples, see SerDe - how to add a new SerDe below.  Some important points about SerDe:\n SerDe, not the DDL, defines the table schema. Some SerDe implementations use the DDL for configuration, but the SerDe can also override that. Column types can be arbitrarily nested arrays, maps, and structures. The callback design of ObjectInspector allows lazy deserialization with CASE/IF or when using complex or nested types.  ObjectInspector Hive uses ObjectInspector to analyze the internal structure of the row object and also the structure of the individual columns.\nObjectInspector provides a uniform way to access complex objects that can be stored in multiple formats in the memory, including:\n Instance of a Java class (Thrift or native Java) A standard Java object (we use java.util.List to represent Struct and Array, and use java.util.Map to represent Map) A lazily-initialized object (for example, a Struct of string fields stored in a single Java string object with starting offset for each field)  A complex object can be represented by a pair of ObjectInspector and Java Object. The ObjectInspector not only tells us the structure of the Object, but also gives us ways to access the internal fields inside the Object.\nNOTE: Apache Hive recommends that custom ObjectInspectors created for use with custom SerDes have a no-argument constructor in addition to their normal constructors for serialization purposes. See HIVE-5380 for more details.\nRegistration of Native SerDes As of Hive 0.14a registration mechanism has been introduced for native Hive SerDes. This allows dynamic binding between a \u0026ldquo;STORED AS\u0026rdquo; keyword in place of a triplet of {SerDe, InputFormat, and OutputFormat} specification, in CreateTable statements.\nThe following mappings have been added through this registration mechanism:\n   Syntax Equivalent     STORED AS AVRO /STORED AS AVROFILE ROW FORMAT SERDE``'org.apache.hadoop.hive.serde2.avro.AvroSerDe'``STORED AS INPUTFORMAT``'org.apache.hadoop.hive.ql.io.avro.AvroContainerInputFormat'``OUTPUTFORMAT``'org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat'   STORED AS ORC /STORED AS ORCFILE ROW FORMAT SERDE````'org.apache.hadoop.hive.ql.io.orc.OrcSerde````'``STORED AS INPUTFORMAT````'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat````'``OUTPUTFORMAT````'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat````'   STORED AS PARQUET /STORED AS PARQUETFILE ROW FORMAT SERDE```'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe```'``STORED AS INPUTFORMAT```'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat```'``OUTPUTFORMAT```'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat```'   STORED AS RCFILE STORED AS INPUTFORMAT``'org.apache.hadoop.hive.ql.io.RCFileInputFormat'``OUTPUTFORMAT``'org.apache.hadoop.hive.ql.io.RCFileOutputFormat'   STORED AS SEQUENCEFILE STORED AS INPUTFORMAT``'org.apache.hadoop.mapred.SequenceFileInputFormat'``OUTPUTFORMAT``'org.apache.hadoop.mapred.SequenceFileOutputFormat'   STORED AS TEXTFILE STORED AS INPUTFORMAT``'org.apache.hadoop.mapred.TextInputFormat'``OUTPUTFORMAT``'org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat'    To add a new native SerDe with STORED AS keyword, follow these steps:\n Create a storage format descriptor class extending from AbstractStorageFormatDescriptor.java that returns a \u0026ldquo;stored as\u0026rdquo; keyword and the names of InputFormat, OutputFormat, and SerDe classes. Add the name of the storage format descriptor class to the StorageFormatDescriptor registration file.  MetaStore MetaStore contains metadata regarding tables, partitions and databases. This is used by Query Processor during plan generation.\n Metastore Server - This is the Thrift server (interface defined in metastore/if/hive_metastore.if) that services metadata requests from clients. It delegates most of the requests underlying meta data store and the Hadoop file system which contains data. Object Store - ObjectStore class handles access to the actual metadata is stored in the SQL store. The current implementation uses JPOX ORM solution which is based of JDA specification. It can be used with any database that is supported by JPOX. New meta stores (file based or xml based) can added by implementing the interface MetaStore. FileStore is a partial implementation of an older version of metastore which may be deprecated soon. Metastore Client - There are python, java, php Thrift clients in metastore/src. Java generated client is extended with HiveMetaStoreClient which is used by Query Processor (ql/metadta). This is the main interface to all other Hive components.  Query Processor The following are the main components of the Hive Query Processor:\n Parse and SemanticAnalysis (ql/parse) - This component contains the code for parsing SQL, converting it into Abstract Syntax Trees, converting the Abstract Syntax Trees into Operator Plans and finally converting the operator plans into a directed graph of tasks which are executed by Driver.java. Optimizer (ql/optimizer) - This component contains some simple rule based optimizations like pruning non referenced columns from table scans (column pruning) that the Hive Query Processor does while converting SQL to a series of map/reduce tasks. Plan Components (ql/plan) - This component contains the classes (which are called descriptors), that are used by the compiler (Parser, SemanticAnalysis and Optimizer) to pass the information to operator trees that is used by the execution code. MetaData Layer (ql/metadata) - This component is used by the query processor to interface with the MetaStore in order to retrieve information about tables, partitions and the columns of the table. This information is used by the compiler to compile SQL to a series of map/reduce tasks. Map/Reduce Execution Engine (ql/exec) - This component contains all the query operators and the framework that is used to invoke those operators from within the map/reduces tasks. Hadoop Record Readers, Input and Output Formatters for Hive (ql/io) - This component contains the record readers and the input, output formatters that Hive registers with a Hadoop Job. Sessions (ql/session) - A rudimentary session implementation for Hive. Type interfaces (ql/typeinfo) - This component provides all the type information for table columns that is retrieved from the MetaStore and the SerDes. Hive Function Framework (ql/udf) - Framework and implementation of Hive operators, Functions and Aggregate Functions. This component also contains the interfaces that a user can implement to create user defined functions. Tools (ql/tools) - Some simple tools provided by the query processing framework. Currently, this component contains the implementation of the lineage tool that can parse the query and show the source and destination tables of the query.  Compiler Parser TypeChecking Semantic Analysis Plan generation Task generation Execution Engine Plan Operators UDFs and UDAFs A helpful overview of the Hive query processor can be found in this Hive Anatomy slide deck.\nCompiling and Running Hive Ant to Maven\nAs of version 0.13 Hive uses Maven instead of Ant for its build. The following instructions are not up to date.\nSee the Hive Developer FAQ for updated instructions.\nHive can be made to compile against different versions of Hadoop.\nDefault Mode From the root of the source tree:\nant package will make Hive compile against Hadoop version 0.19.0. Note that:\n Hive uses Ivy to download the hadoop-0.19.0 distribution. However once downloaded, it\u0026rsquo;s cached and not downloaded multiple times. This will create a distribution directory in build/dist (relative to the source root) from where one can launch Hive. This distribution should only be used to execute queries against Hadoop branch 0.19. (Hive is not sensitive to minor revisions of Hadoop versions).  Advanced Mode  One can specify a custom distribution directory by using:  ant -Dtarget.dir=\u0026lt;my-install-dir\u0026gt; package  One can specify a version of Hadoop other than 0.19.0 by using (using 0.17.1 as an example):  ant -Dhadoop.version=0.17.1 package  One can also compile against a custom version of the Hadoop tree (only release 0.4 and above). This is also useful if running Ivy is problematic (in disconnected mode for example) - but a Hadoop tree is available. This can be done by specifying the root of the Hadoop source tree to be used, for example:  ant -Dhadoop.root=~/src/hadoop-19/build/hadoop-0.19.2-dev -Dhadoop.version=0.19.2-dev note that:\n Hive\u0026rsquo;s build script assumes that hadoop.root is pointing to a distribution tree for Hadoop created by running ant package in Hadoop. hadoop.version must match the version used in building Hadoop.  In this particular example - ~/src/hadoop-19 is a checkout of the Hadoop 19 branch that uses 0.19.2-dev as default version and creates a distribution directory in build/hadoop-0.19.2-dev by default.\nRun Hive from the command line with \u0026lsquo;$HIVE_HOME/bin/hive\u0026rsquo;, where $HIVE_HOME is typically build/dist under your Hive repository top-level directory.\n$ build/dist/bin/hive If Hive fails at runtime, try \u0026lsquo;ant very-clean package\u0026rsquo; to delete the Ivy cache before rebuilding.\nRunning Hive Without a Hadoop Cluster From Thejas:\nexport HIVE_OPTS='--hiveconf mapred.job.tracker=local --hiveconf fs.default.name=file:///tmp \\ --hiveconf hive.metastore.warehouse.dir=file:///tmp/warehouse \\ --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tmp/metastore_db;create=true' Then you can run \u0026lsquo;build/dist/bin/hive\u0026rsquo; and it will work against your local file system.\nUnit tests and debugging Layout of the unit tests Hive uses JUnit for unit tests. Each of the 3 main components of Hive have their unit test implementations in the corresponding src/test directory e.g. trunk/metastore/src/test has all the unit tests for metastore, trunk/serde/src/test has all the unit tests for serde and trunk/ql/src/test has all the unit tests for the query processor. The metastore and serde unit tests provide the TestCase implementations for JUnit.\nDebugging Hive Code Hive code includes both client-side code (e.g., compiler, semantic analyzer, and optimizer of HiveQL) and server-side code (e.g., operator/task/SerDe implementations). Debugging is different for client-side and server-side code, as described below.\nDebugging Client-Side Code The client-side code runs on your local machine so you can easily debug it using Eclipse the same way you debug any regular local Java code. Here are the steps to debug code within a unit test.\n Make sure that you have run ant model-jar in hive/metastore and ant gen-test in hive since the last time you ran ant clean. To run all of the unit tests for the CLI:  Open up TestCliDriver.java Click Run-\u0026gt;Debug Configurations, select TestCliDriver, and click Debug.   To run a single test within TestCliDriver.java:  Begin running the whole TestCli suite as before. Once it finishes the setup and starts executing the JUnit tests, stop the test execution. Find the desired test in the JUnit pane, Right click on that test and select Debug.    Debugging Server-Side Code The server-side code is distributed and runs on the Hadoop cluster, so debugging server-side Hive code is a little bit complicated. In addition to printing to log files using log4j, you can also attach the debugger to a different JVM under unit test (single machine mode). Below are the steps on how to debug on server-side code.\n Compile Hive code with javac.debug=on. Under Hive checkout directory:   \u0026gt; ant -Djavac.debug=on package If you have already built Hive without javac.debug=on, you can clean the build and then run the above command.\n \u0026gt; ant clean # not necessary if the first time to compile \u0026gt; ant -Djavac.debug=on package  Run ant test with additional options to tell the Java VM that is running Hive server-side code to wait for the debugger to attach. First define some convenient macros for debugging. You can put it in your .bashrc or .cshrc.   \u0026gt; export HIVE_DEBUG_PORT=8000 \u0026gt; export HIVE_DEBUG=\u0026quot;-Xdebug -Xrunjdwp:transport=dt_socket,address=${HIVE_DEBUG_PORT},server=y,suspend=y\u0026quot; In particular HIVE_DEBUG_PORT is the port number that the JVM is listening on and the debugger will attach to. Then run the unit test as follows:\n \u0026gt; export HADOOP_OPTS=$HIVE_DEBUG \u0026gt; ant test -Dtestcase=TestCliDriver -Dqfile=\u0026lt;mytest\u0026gt;.q The unit test will run until it shows:\n [junit] Listening for transport dt_socket at address: 8000  Now, you can use jdb to attach to port 8000 to debug   \u0026gt; jdb -attach 8000 or if you are running Eclipse and the Hive projects are already imported, you can debug with Eclipse. Under Eclipse Run -\u0026gt; Debug Configurations, find \u0026ldquo;Remote Java Application\u0026rdquo; at the bottom of the left panel. There should be a MapRedTask configuration already. If there is no such configuration, you can create one with the following property:\n+ Name: any task such as MapRedTask + Project: the Hive project that you imported. + Connection Type: Standard (Socket Attach) + Connection Properties: - Host: localhost - Port: 8000 Then hit the \u0026quot;Debug\u0026quot; button and Eclipse will attach to the JVM listening on port 8000 and continue running till the end. If you define breakpoints in the source code before hitting the \u0026quot;Debug\u0026quot; button, it will stop there. The rest is the same as debugging client-side Hive.  Debugging without Ant (Client and Server Side) There is another way of debugging Hive code without going through Ant.\nYou need to install Hadoop and set the environment variable HADOOP_HOME to that.\n \u0026gt; export HADOOP_HOME=\u0026lt;your hadoop home\u0026gt; Then, start Hive:\n \u0026gt; ./build/dist/bin/hive --debug It will then act similar to the debugging steps outlines in Debugging Hive code. It is faster since there is no need to compile Hive code,\nand go through Ant. It can be used to debug both client side and server side Hive.\nIf you want to debug a particular query, start Hive and perform the steps needed before that query. Then start Hive again in debug to debug that query.\n \u0026gt; ./build/dist/bin/hive \u0026gt; perform steps before the query  \u0026gt; ./build/dist/bin/hive --debug \u0026gt; run the query Note that the local file system will be used, so the space on your machine will not be released automatically (unlike debugging via Ant, where the tables created in test are automatically dropped at the end of the test). Make sure to either drop the tables explicitly, or drop the data from /User/hive/warehouse.\nPluggable interfaces File Formats Please refer to Hive User Group Meeting August 2009 Page 59-63.\nSerDe - how to add a new SerDe Please refer to Hive User Group Meeting August 2009 Page 64-70.\nMap-Reduce Scripts Please refer to Hive User Group Meeting August 2009 Page 71-73.\nUDFs and UDAFs - how to add new UDFs and UDAFs Please refer to Hive User Group Meeting August 2009 Page 74-87.\n ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/developerguide_27362074/","tags":null,"title":"Apache Hive : DeveloperGuide"},{"categories":null,"contents":"Apache Hive : DeveloperGuide UDTF Writing UDTF\u0026rsquo;s  Writing UDTF\u0026rsquo;s  GenericUDTF Interface    GenericUDTF Interface A custom UDTF can be created by extending the GenericUDTF abstract class and then implementing the initialize, process, and possibly close methods. The initialize method is called by Hive to notify the UDTF the argument types to expect. The UDTF must then return an object inspector corresponding to the row objects that the UDTF will generate. Once initialize() has been called, Hive will give rows to the UDTF using the process() method. While in process(), the UDTF can produce and forward rows to other operators by calling forward(). Lastly, Hive will call the close() method when all the rows have passed to the UDTF.\nUDTF Example:\n package org.apache.hadoop.hive.contrib.udtf.example; import java.util.ArrayList; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.ql.udf.generic.GenericUDTF; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory; /** * GenericUDTFCount2 outputs the number of rows seen, twice. It's output twice * to test outputting of rows on close with lateral view. * */ public class GenericUDTFCount2 extends GenericUDTF { Integer count = Integer.valueOf(0); Object forwardObj[] = new Object[1]; @Override public void close() throws HiveException { forwardObj[0] = count; forward(forwardObj); forward(forwardObj); } @Override public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException { ArrayList\u0026lt;String\u0026gt; fieldNames = new ArrayList\u0026lt;String\u0026gt;(); ArrayList\u0026lt;ObjectInspector\u0026gt; fieldOIs = new ArrayList\u0026lt;ObjectInspector\u0026gt;(); fieldNames.add(\u0026quot;col1\u0026quot;); fieldOIs.add(PrimitiveObjectInspectorFactory.javaIntObjectInspector); return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs); } @Override public void process(Object[] args) throws HiveException { count = Integer.valueOf(count.intValue() + 1); } } For reference, here is the abstract class:\n package org.apache.hadoop.hive.ql.udf.generic; import org.apache.hadoop.hive.ql.exec.UDFArgumentException; import org.apache.hadoop.hive.ql.metadata.HiveException; import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector; import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector; /** * A Generic User-defined Table Generating Function (UDTF) * * Generates a variable number of output rows for a single input row. Useful for * explode(array)... */ public abstract class GenericUDTF { Collector collector = null; /** * Initialize this GenericUDTF. This will be called only once per instance. * * @param args * An array of ObjectInspectors for the arguments * @return A StructObjectInspector for output. The output struct represents a * row of the table where the fields of the stuct are the columns. The * field names are unimportant as they will be overridden by user * supplied column aliases. */ public abstract StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentException; /** * Give a set of arguments for the UDTF to process. * * @param o * object array of arguments */ public abstract void process(Object[] args) throws HiveException; /** * Called to notify the UDTF that there are no more rows to process. * Clean up code or additional forward() calls can be made here. */ public abstract void close() throws HiveException; /** * Associates a collector with this UDTF. Can't be specified in the * constructor as the UDTF may be initialized before the collector has been * constructed. * * @param collector */ public final void setCollector(Collector collector) { this.collector = collector; } /** * Passes an output row to the collector. * * @param o * @throws HiveException */ protected final void forward(Object o) throws HiveException { collector.collect(o); } } ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/developerguide-udtf_27362086/","tags":null,"title":"Apache Hive : DeveloperGuide UDTF"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings Hive Contributors Meetings Active contributors to the Hive project are invited to attend the monthly Hive Contributors Meeting. Meetings are announced on the Hive Contributors meetup group.\nMeeting Minutes  April 18, 2012 December 5, 2011 September 7, 2011 July 26, 2011 June 30, 2011 April 25, 2011 January 11, 2011 (forgot to take notes) October 25, 2010 September 13, 2010 August 8, 2010 July 6, 2010 June 1, 2010  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings_27362087/","tags":null,"title":"Apache Hive : Development ContributorsMeetings"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100601 Notes provided by Namit Jain.\nThe following people were present:\n Facebook (Paul Yang; Ning Zhang; Yongqiang He; Ahmed Aly; John Sichi; Ashish Thusoo; Namit Jain) Netflix (Eva Tse; Jerome Boulon) Cloudera (Arvind Prabhakar; Vinithra Varadharajan; Carl Steinbach) Yahoo (Alan Gates)  The following were the main meeting minutes:\n  We should have these meetings more often, say every month. Cloudera will host the next meeting.\n  We should try to have a release every 4 months. We should try to push out 0.6 before end of June, For the new release, Cloudera will take a lead on the release management issues and also help with documentation. Documentation for Hive leaves a lot to be desired.\n  The test framework is pretty brittle, and it is pretty difficult for new people to do big contributions without having a very sound test-plan. Ideally, facebook should host a test cluster so that everyone can run tests there.\n  A lot of external customers are asking for ODBC/JDBC support on top of Hive. Cloudera will take the lead on that.\n  The process of making a new committer should be more transparent. In order to grow the community, it would be very desirable to add more committers outside Facebook.\n  Create new components for Drivers (ODBC/JDBC) and UDFs.\n  Yahoo will take the lead of making Hive work on top of Zebra\n  Some new tasks were identified, but they can change if new priorities come in.\n Carl will focusing on \u0026lsquo;having\u0026rsquo; support and co-related sub-queries.\n  Arvind will be focusing on the cost-based optimizer\n  The main idea was that we should meet more often and share our ideas. Time-based release will be very desirable.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings-hivecontributorsminutes100601_27362084/","tags":null,"title":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100601"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100706 Attendees: Amr Awadallah, John Sichi, Paul Yang, Olga Natkovich, Ajay Kidave, Yongqiang He, Basab Malik, Vinithra Varadharajan, bc Wong, Arvind Prabhakar, Carl Steinbach\n bc Wong gave a live demo of Cloudera\u0026rsquo;s Hue framework and the Beeswax Hive web interface.  Slides from this talk are available here: http://www.slideshare.net/cwsteinbach/cloudera-huebeeswax Hue was recently released as open source. The code is available on Github here: http://github.com/cloudera/hue   Olga Natkovich gave a whiteboard talk on HOwl.  HOwl = Hive !MetaStore + Owl = shared metadata system between Pig, Hive, and Map Reduce HOwl will likely leverage the !MetaStore schema and ORM layer. A somewhat outdated Owl design document is available here: http://wiki.apache.org/pig/owl   Carl gave an update on progress with the 0.6.0 release.  There was a discussion about the plan to move the documentation off of the wiki and into version control. Several people voiced concerns that developers/users are less likely to update the documentation if doing so requires them to submit a patch. The new proposal for documentation reached at the meeting is as follows:  The trunk version of the documentation will be maintained on the wiki. As part of the release process the documentation will be copied off of the wiki and converted to xdoc, and then checked into svn. HTML documentation generated from the xdoc will be posted to the Hive webpage when the new release is posted.   Carl is going to investigate the feasibility of writing a tool that converts documentation directly from !MoinMoin wiki markup to xdoc.   John agreed to host the next contributors meeting at Facebook.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings-hivecontributorsminutes100706_27362085/","tags":null,"title":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100706"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100808 August 8th, 2010\n Yongqiang He gave a presentation about his work on index support in Hive.  Slides are available here: http://files.meetup.com/1658206/Hive%20Index.pptx   John Sichi talked about his work on filter-pushdown optimizations. This is applicable to the HBase storage handler and the new index infrastructure. Pradeep Kamath gave an update on progress with Howl.  The Howl source code is available on GitHub here: http://github.com/yahoo/howl Starting to work on security for Howl. For the first iteration the plan is to base it on DFS permissions.   General agreement that we should aim to desupport pre-0.20.0 versions of Hadoop in Hive 0.7.0. This will allow us to remove the shim layer and will make it easier to transition to the new mapreduce APIs. But we also want to get a better idea of how many users are stuck on pre-0.20 versions of Hadoop. Remove Thrift generated code from repository.  Pro: reduce noise in diffs during reviews. Con: requires developers to install Thrift compiler.   Discussed moving the documentation from the wiki to version control.  Probably not practical to maintain the trunk version of the docs on the wiki and roll over to version control at release time, so trunk version of docs will be maintained in vcs. It was agreed that feature patches should include updates to the docs, but it is also acceptable to file a doc ticket if there is time pressure to commit.j Will maintain an errata page on the wiki for collecting updates/corrections from users. These notes will be rolled into the documentation in vcs on a monthly basis.   The next meeting will be held in September at Cloudera\u0026rsquo;s office in Palo Alto.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings-hivecontributorsminutes100808_27362082/","tags":null,"title":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100808"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100913 Meeting date: Sept 13, 2010\nLocation: Cloudera Palo Alto office\nAttendees: http://www.meetup.com/Hive-Contributors-Group/calendar/14689507/\nCarl Steinbach gave a status update on the 0.6 release. Since plans for documentation migration have been deferred to the next release, the only remaining issues are completion of the CREATE DATABASE feature (HIVE-675), metastore VARCHAR precision widening (HIVE-1364), and metastore upgrade scripts (HIVE-1427). HIVE-675 has already been committed to trunk and the backport for 0.6 is underway. Carl is still working on companion feature HIVE-1517, but unless it is done by Sept 17, we\u0026rsquo;ll drop it from the 0.6 release. Once a build containing the remaining features passes all unit tests, we\u0026rsquo;ll post a release candidate and vote on it (no additional acceptance testing is planned for this release).\nNext, HIVE-1546 (making semantic analysis pluggable) was discussed. The Howl team gave an overview of their use case for reusing Hive DDL processing within Howl\u0026rsquo;s CLI, together with a description of the roadmap for Howl/Hive project relationship. Carl raised concerns about potential dependency creep preventing future Hive refactoring, and Namit Jain proposed reworking the approach to restrict it to pre/post-analysis hooks (limiting the dependencies exposed) rather than full-blown analyzer pluggability+subclassing. It was agreed that the hooks approach was the best course for balancing all of the concerns and allowing us to achieve the desired project collaboration benefits. The Howl team also committed to getting their Howl checkins going into a public repository as soon as possible, together with setting up continuous integration to track the health of the combination of Hive+Howl trunks.\nNext, HIVE-1609 (partition filtering metastore API) was briefly discussed, and it was agreed that the Howl team would move the predicate parser from JavaCC to ANTLR and resubmit the patch.\nFinally, HIVE-1476 (metastore creating files as service user) was discussed. It was agreed that the approach in the proposed patch (performing HDFS operations on the metastore client side) was a stopgap that we don\u0026rsquo;t really want to include in Hive. Instead, the correct long-term solution being developed by Todd Lipcon is to upgrade the Thrift version used by Hive to a recent one containing his SASL support, and then add impersonation support to the metastore server. Since the Howl team\u0026rsquo;s schedule does not allow them to wait for that work to complete and get tested, they will keep the HIVE-1476 patch privately applied within their own branch of Hive; it will not be committed on Hive trunk. Once they are able to move over to the long-term solution, the stopgap can be discarded. (In the meantime, we need to work together to minimize the merge-to-branch impact as the metastore code continues to change on trunk.)\nThe October meetup will be at Facebook HQ in Palo Alto.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings-hivecontributorsminutes100913_27362083/","tags":null,"title":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes100913"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes101025 Meeting date: October 25, 2010\nLocation: Facebook Palo Alto\nAttendees: http://www.meetup.com/Hive-Contributors-Group/calendar/14875663 plus Paul, Ning, Yongqiang, Liyin, Basab\nThe TLP and bylaws votes passed, so Hive is now officially an Apache top level project! We are going ahead with moving the following resources:\n website (now at hive.apache.org) svn (new trunk location is http://svn.apache.org/repos/asf/hive/trunk); git will follow soon irc: we are making #hive the official channel on freenode.net (deprecating ##hive) review board: we will start using the new ASF facility soon wiki: we are going to move to http://cwiki.apache.org/HIVE, but we still need to work out the migration plan from MoinMoin  The 0.6 release vote passed, so we will wrap up the release and publish it!\nCarl Steinbach proposed making 0.7.0 a time-based release (rather than a feature-based release), and that we should start on it soon since a lot of features have already buffered up on trunk since we branched 0.6.0. Ning mentioned some features for Microstrategy ODBC integration that we might want to try to get in. We have a lot of in-progress features such as indexing (currently being worked on by a team of students at Harvey Mudd College as well as some other contributors), so we\u0026rsquo;ll need to mark some of those as experimental.\nYongqiang gave a presentation on Facebook\u0026rsquo;s proposal for adding authorization support to Hive (Powerpoint). This took place in the context of a general discussion on how to make Hadoop-level permissions work with Hive-level permissions. We discussed three different approaches:\n Disable Hadoop security and use Hive authorization only. This provides \u0026ldquo;advisory\u0026rdquo; authorization only; in other words, users are prevented from accidentally accessing information which they aren\u0026rsquo;t supposed to see, but nothing prevents a malicious user from circumventing these protections. Disable Hive authorization and use Hadoop security only (with Hive support for setting the permissions on new files). In this configuration, Hadoop jobs run with the privileges of the invoking user. This is the mode being developed by the Howl team. It is fundamentally incompatible with Hive-enforced features such as column-level authorization and view-level authorization. Enable Hadoop security and run Hive in server mode only (with authorization enabled), with all Hive files owned by a system user. In this configuration, Hadoop jobs run with the privileges of a system user. This matches the traditional DBMS model where the DBMS owns its files, and anything else requires import/export into user-owned directories. This mode may be incompatible with some existing Hive usages (e.g. pointing EXTERNAL tables at system-owned files).  It was generally agreed that #3 is the ideal configuration, but we\u0026rsquo;ll be dealing with configurations #1 and #2 for some time while waiting for various pieces to fall into place.\nShyam mentioned some ongoing work on real-time Hadoop that might be an interesting project for Hive collaboration.\nNext meeting will be at Cloudera.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings-hivecontributorsminutes101025_27362080/","tags":null,"title":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes101025"},{"categories":null,"contents":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes110425 Meeting date: April 25, 2011\nLocation: Facebook Palo Alto\nAttendees: http://www.meetup.com/Hive-Contributors-Group/events/17272914/\nThe 0.7 release is out, and Carl proposed an 0.7.1 release for items such as PostgreSQL metastore upgrade scripts and Maven artifact publication. Rules for a point release were discussed: no metastore changes, and no changes to API\u0026rsquo;s such as Thrift and extension interfaces. Everyone was fine with this; Carl will manage the release.\nPlans for 0.8 were also discussed; optimistic target is two months from now.\nThere were some user-oriented questions about join optimization, which is an area of much confusion; besides better documentation, it would be useful to start having User Meetups again, as well as to review existing config parameter defaults as part of each release.\nAlan gave an update on HCatalog (formerly Howl):\n incubation has been approved and is under way (svn, JIRA and mailing list are available; working on publishing website) for svn, Hive code inclusion is via external reference (not a separate copy) currently only works with secure Hadoop 0.1 branch will be cut in a few weeks 0.2 is targeted for end of June, with support for Hadoop Streaming as well as some kind of blob support  Alan is going to work on publishing the HCatalog roadmap.\nJohn opened a discussion on Hive Contributor Day (June 30, 10am - 6pm) as part of the Yahoo Hadoop Summit in Sunnyvale. The rough agenda is (1) presentations in the morning, followed by (2) hands-on UDF hacking in the afternoon. We\u0026rsquo;ll be soliciting proposals for presentations. For the afternoon, we would like to work on a \u0026ldquo;Hive SDK\u0026rdquo; which would make it easier for users to develop extensions such as UDF\u0026rsquo;s without having to set up a full Hive build environment. Facebook data science already has some useful bits built up in this area, so we\u0026rsquo;re going to work on open-sourcing that on GitHub. In addition, they also have a library of private UDF\u0026rsquo;s which should be open sourced, so we\u0026rsquo;re going to try to farm out this effort and use it as a learning experience for people who want to develop their own UDF\u0026rsquo;s.\nJohn gave a status update on the indexing support being worked on by the Harvey Mudd Clinic students; bitmap indexing has been committed, and automatic index usage is very close, but it will still be in a very \u0026ldquo;experts-only\u0026rdquo; state, so a lot of followup work remains.\nCarl brought up the need to improve the maintainability of Hive\u0026rsquo;s test suite, and John mentioned a number of other maintenance projects needed such as checkstyle enforcement and removing Thrift-generated code. The challenge with these is that the patches tend to be very large, and by the time they\u0026rsquo;re ready, they need to be rebased, ad infinitum. The proposed solution is to (1) get an assignee and committer paired up and dedicated (2) vote on a \u0026ldquo;quiet period\u0026rdquo; via the dev mailing list and (3) carry out the work during that period. We can give that a try and see if we can make progress.\nJohn mentioned some summer intern projects being considered at Facebook (list to be added to wiki), and asked for further suggestions.\nThere was discussion around using some Yahoo QA machines (or the OSUOSL cluster) for automatic patch validation; Carl got some contact info and is going to look into that more.\nWe ended with some review and discussion of HIVE-2038 (metastore listener). It was decided that generic events are going to be dealt with in a followup JIRA.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/development-contributorsmeetings-hivecontributorsminutes110425_27362081/","tags":null,"title":"Apache Hive : Development ContributorsMeetings HiveContributorsMinutes110425"},{"categories":null,"contents":"Apache Hive : Different TIMESTAMP types Overview The following overview depicts the desired timestamp semantics in comparison to the SQL standard and selected database vendors:\nTIMESTAMP and TIMESTAMP WITHOUT TIME ZONE The TIMESTAMP and TIMESTAMP WITHOUT TIME ZONE types shall behave like the LocalDateTime class of Java, i.e., each value is a recording of what can be seen on a calendar and a clock hanging on the wall, for example \u0026ldquo;1969-07-20 16:17:39\u0026rdquo;. It can be decomposed into year, month, day, hour, minute and seconds fields, but with no time zone information available, it does not correspond to any specific point in time.\nThis behaviour is consistent with the SQL standard (revisions 2003 and higher).\nTIMESTAMP WITH LOCAL TIME ZONE The TIMESTAMP WITH LOCAL TIME ZONE type shall behave like the Instant class of Java, i.e., each value identifies a single time instant, but does not contain any explicit timezone information. To achieve this semantics, the processing of timestamp literals involves an implicit normalization from the session-local time zone to a predefined one (typically but not necessarily UTC), while displaying the stored timestamps involves an implicit conversion from the predefined time zone to the session-local one. Although a predefined time zone (typically UTC) was explicitly mentioned above, it is not a part of the individual values but of the definition of the whole type instead. As such, every value that gets stored has to be normalized to this predefined time zone and therefore its original time zone information gets lost.\nFor example, if the calendar and clock hanging on the wall shows 1969-07-20 16:17:39 according to Eastern Daylight Time, that must be stored as \u0026ldquo;1969-07-20 20:17:39\u0026rdquo;, because that UTC time corresponds to the same instant. When reading that value back, we no longer know that it originated from an EDT time and we can only display it in some fixed time zone (either local or UTC or specified by the user).\nThis behaviour is consistent with some major DB engines, which is the best we can do as no type is defined by the SQL standard that would have this behaviour.\nTIMESTAMP WITH TIME ZONE The TIMESTAMP WITH TIME ZONE type shall behave like the OffsetDateTime class of Java, i.e., each individual value includes a time zone offset as well. This definition leads to timestamps that not only identify specific time instants unambiguously, but also allow retrieval of the originating time zone offset.\nIf we stored the same timestamp as above using this semantics, then the original timestamp literal could be reconstructed including some time zone information, for example \u0026ldquo;1969-07-20 16:17:39 (UTC -04:00)\u0026rdquo;. (The primary fields are typically still stored normalized to UTC to make comparison of timestamps more efficient, but this is an implementation detail and does not affect the behaviour of the type.)\nThis behaviour is consistent with the SQL standard (revisions 2003 and higher).\nComparison Let\u0026rsquo;s summarize the example of how the different semantics described above apply to a value inserted into a SQL table.\nIf the timestamp literal \u0026lsquo;1969-07-20 16:17:39\u0026rsquo; is inserted in Washington D.C. and then queried from Paris, it might be shown in the following ways based on timestamp semantics:\n   SQL type Semantics Result Explanation     TIMESTAMP [WITHOUT TIME ZONE] LocalDateTime 1969-07-20 16:17:39 Displayed like the original timestamp literal.   TIMESTAMP WITH LOCAL TIME ZONE Instant 1969-07-20 21:17:39 Differs from the original timestamp literal, but refers to the same time instant.   TIMESTAMP WITH TIME ZONE OffsetDateTime 1969-07-20 16:17:39 (UTC -04:00) Displayed like the original literal but showing the time zone offset as well.    Of course, the different semantics do not only affect the textual representations but perhaps more importantly SQL function behavior as well. These allow users to take advantage of timestamps in different ways or to explicitly create different textual representations instead of the implicit ones shown above.\nIn fact, even the implicit textual representations could be different than shown above, for example Instant could be displayed normalized to UTC or OffsetDateTime could be adjusted to the local time zone by default. The examples shown above are just common ways of creating an implicit textual representation for the different semantics, but the truly important difference lies in what details can and what details can not be reconstructed from the different semantics:\nReconstructible details:\n   SQL type Semantics Local clock reading Time instant Time zone offset     TIMESTAMP [WITHOUT TIME ZONE] LocalDateTime ✓     TIMESTAMP WITH LOCAL TIME ZONE Instant  ✓    TIMESTAMP WITH TIME ZONE OffsetDateTime ✓ ✓ ✓    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/different-timestamp-types_103091503/","tags":null,"title":"Apache Hive : Different TIMESTAMP types"},{"categories":null,"contents":"Apache Hive : Druid Integration  Introduction  Objectives   Preliminaries  Druid Storage Handlers   Usage  Discovery and management of Druid datasources from Hive  Create tables linked to existing Druid datasources Create Druid datasources from Hive Druid kafka ingestion from Hive  Start/Stop/Reset Druid Kafka ingestion   INSERT, INSERT OVERWRITE and DROP statements Queries completely executed in Druid  Select queries Timeseries queries GroupBy queries   Queries across Druid and Hive     Open Issues (JIRA)   Version information\nDruid integration is introduced in Hive 2.2.0 (HIVE-14217). Initially it was compatible with Druid 0.9.1.1, the latest stable release of Druid to that date.\nIntroduction This page documents the work done for the integration between Druid and Hive, which was started in HIVE-14217.\nObjectives Our main goal is to be able to index data from Hive into Druid, and to be able to query Druid datasources from Hive. Completing this work will bring benefits to the Druid and Hive systems alike:\n Efficient execution of OLAP queries in Hive. Druid is a system specially well tailored towards the execution of OLAP queries on event data. Hive will be able to take advantage of its efficiency for the execution of this type of queries. Introducing a SQL interface on top of Druid. Druid queries are expressed in JSON, and Druid is queried through a REST API over HTTP. Once a user has declared a Hive table that is stored in Druid, we will be able to transparently generate Druid JSON queries from the input Hive SQL queries. Being able to execute complex operations on Druid data. There are multiple operations that Druid does not support natively yet, e.g. joins. Putting Hive on top of Druid will enable the execution of more complex queries on Druid data sources. Indexing complex query results in Druid using Hive. Currently, indexing in Druid is usually done through MapReduce jobs. We will enable Hive to index the results of a given query directly into Druid, e.g., as a new table or a materialized view (HIVE-10459), and start querying and using that dataset immediately.  The initial implementation, started in HIVE-14217, focused on 1) enabling the discovery of data that is already stored in Druid from Hive, and 2) being able to query that data, trying to make use of Druid advanced querying capabilities. For instance, we put special emphasis on pushing as much computation as possible to Druid, and being able to recognize the type of queries for which Druid is specially efficient, e.g. timeseries or groupBy queries.\nFuture work after the first step is completed is being listed in HIVE-14473. If you want to collaborate on this effort, a list of remaining issues can be found at the end of this document.\nPreliminaries Before going into further detail, we introduce some background that the reader needs to be aware of in order to understand this document.\nDruid Druid is an open-source analytics data store designed for business intelligence (OLAP) queries on event data. Druid provides low latency (real-time) data ingestion, flexible data exploration, and fast data aggregation. Existing Druid deployments have scaled to trillions of events and petabytes of data. Druid is most commonly used to power user-facing analytic applications. You can find more information about Druid here. Storage Handlers You can find an overview of Hive Storage Handlers here; the integration of Druid with Hive depends upon that framework.\nUsage For the running examples, we use the wikiticker dataset included in the quickstart tutorial of Druid.\nDiscovery and management of Druid datasources from Hive First we focus on the discovery and management of Druid datasources from Hive.\nCreate tables linked to existing Druid datasources Assume that we have already stored the wikiticker dataset mentioned previously in Druid, and the address of the Druid broker is 10.5.0.10:8082.\nFirst, you need to set the Hive property hive.druid.broker.address.default in your configuration to point to the broker address:\nSET hive.druid.broker.address.default=10.5.0.10:8082; Then, to create a table that we can query from Hive, we execute the following statement in Hive:\nCREATE EXTERNAL TABLE druid_table_1 STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler' TBLPROPERTIES (\u0026quot;druid.datasource\u0026quot; = \u0026quot;wikiticker\u0026quot;); Observe that you need to specify the datasource as TBLPROPERTIES using the druid.datasource property. Further, observe that the table needs to be created as EXTERNAL, as data is stored in Druid. The table is just a logical entity that we will use to express our queries, but there is no data movement when we create the table. In fact, what happened under the hood when you execute that statement, is that Hive sends a segment metadata query to Druid in order to discover the schema (columns and their types) of the data source. Retrieval of other information that might be useful such as statistics e.g. number of rows, is in our roadmap, but it is not supported yet. Finally, note that if we change the Hive property value for the default broker address, queries on this table will automatically run against the new broker address, as the address is not stored with the table.\nIf we execute a DESCRIBE statement, we can actually see the information about the table:\nhive\u0026gt; DESCRIBE FORMATTED druid_table_1; OK # col_name data_type comment __time timestamp from deserializer added bigint from deserializer channel string from deserializer cityname string from deserializer comment string from deserializer count bigint from deserializer countryisocode string from deserializer countryname string from deserializer deleted bigint from deserializer delta bigint from deserializer isanonymous string from deserializer isminor string from deserializer isnew string from deserializer isrobot string from deserializer isunpatrolled string from deserializer metrocode string from deserializer namespace string from deserializer page string from deserializer regionisocode string from deserializer regionname string from deserializer user string from deserializer user_unique string from deserializer # Detailed Table Information Database: druid Owner: user1 CreateTime: Thu Aug 18 19:09:10 BST 2016 LastAccessTime: UNKNOWN Retention: 0 Location: hdfs:/tmp/user1/hive/warehouse/druid.db/druid_table_1 Table Type: EXTERNAL_TABLE Table Parameters: COLUMN_STATS_ACCURATE\t{\\\u0026quot;BASIC_STATS\\\u0026quot;:\\\u0026quot;true\\\u0026quot;} EXTERNAL TRUE druid.datasource wikiticker numFiles 0 numRows 0 rawDataSize 0 storage_handler org.apache.hadoop.hive.druid.DruidStorageHandler totalSize 0 transient_lastDdlTime\t1471543750 # Storage Information SerDe Library: org.apache.hadoop.hive.druid.serde.DruidSerDe InputFormat: null OutputFormat: null Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: serialization.format\t1 Time taken: 0.111 seconds, Fetched: 55 row(s) We can see there are three different groups of columns corresponding to the Druid categories: the timestamp column (__time) mandatory in Druid, the dimension columns (whose type is STRING), and the metrics columns (all the rest).\nCreate Druid datasources from Hive If we want to manage the data in the Druid datasources from Hive, there are multiple possible scenarios.\nFor instance, we might want to create an empty table backed by Druid using a CREATE TABLE statement and then append and overwrite data using INSERT and INSERT OVERWRITE Hive statements, respectively.\nCREATE EXTERNAL TABLE druid_table_1 (`__time` TIMESTAMP, `dimension1` STRING, `dimension2` STRING, `metric1` INT, `metric2` FLOAT) STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler'; Another possible scenario is that our data is stored in Hive tables and we want to preprocess it and create Druid datasources from Hive to accelerate our SQL query workload. We can do that by executing a Create Table As Select (CTAS) statement. For example:\nCREATE EXTERNAL TABLE druid_table_1 STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler' AS \u0026lt;select `timecolumn` as `__time`, `dimension1`, `dimension2`, `metric1`, `metric2`....\u0026gt;; Observe that we still create three different groups of columns corresponding to the Druid categories: the timestamp column (__time) mandatory in Druid, the dimension columns (whose type is STRING), and the metrics columns (all the rest).\nIn both statements, the column types (either specified statically for CREATE TABLE statements or inferred from the query result for CTAS statements) are used to infer the corresponding Druid column category.\nFurther, note that if we do not specify the value for the druid.datasource property, Hive automatically uses the fully qualified name of the table to create the corresponding datasource with the same name.\nVersion Info\nVersion 2.2.0: CREATE TABLE syntax when data is managed via hive.\nCREATE TABLE druid_table_1 (`__time` TIMESTAMP, `dimension1` STRING, `dimension2` STRING, `metric1` INT, `metric2` FLOAT) STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler';  NOTE - Before Hive 3.0.0, we do not use EXTERNAL tables and do not specify the value for the druid.datasource property.\nFor versions 3.0.0+, All Druid tables are EXTERNAL (HIVE-20085).\nDruid kafka ingestion from Hive Version Info\nIntegration with Druid Kafka Indexing Service is introduced in Hive 3.0.0 (HIVE-18976).\nDruid Kafka Indexing Service supports exactly-once ingestion from Kafka topic by managing the creation and lifetime of Kafka indexing tasks. We can manage Druid Kafka Ingestion using Hive CREATE TABLE statement as shown below.\nDruid Kafka Ingestion\nCREATE EXTERNAL TABLE druid_kafka_table_1(`__time` timestamp,`dimension1` string, `dimension1` string, `metric1` int, `metric2 double ....) STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler' TBLPROPERTIES ( \u0026quot;kafka.bootstrap.servers\u0026quot; = \u0026quot;localhost:9092\u0026quot;, \u0026quot;kafka.topic\u0026quot; = \u0026quot;topic1\u0026quot;, \u0026quot;druid.kafka.ingestion.useEarliestOffset\u0026quot; = \u0026quot;true\u0026quot;, \u0026quot;druid.kafka.ingestion.maxRowsInMemory\u0026quot; = \u0026quot;5\u0026quot;, \u0026quot;druid.kafka.ingestion.startDelay\u0026quot; = \u0026quot;PT1S\u0026quot;, \u0026quot;druid.kafka.ingestion.period\u0026quot; = \u0026quot;PT1S\u0026quot;, \u0026quot;druid.kafka.ingestion.consumer.retries\u0026quot; = \u0026quot;2\u0026quot; );  Observe that we specified kafka topic name and kafka bootstrap servers as part of the table properties. Other tunings for Druid Kafka Indexing Service can also be specified by prefixing them with \u0026lsquo;druid.kafka.ingestion.\u0026rsquo; e.g. to configure duration of druid ingestion tasks we can add*\u0026ldquo;druid.kafka.ingestion.taskDuration\u0026rdquo; = \u0026ldquo;PT60S\u0026rdquo;* as a table property. Start/Stop/Reset Druid Kafka ingestion We can Start/Stop/Reset druid kafka ingestion using sql statement shown below. ALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'START'); ALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'STOP'); ALTER TABLE druid_kafka_test SET TBLPROPERTIES('druid.kafka.ingestion' = 'RESET'); Note: Reseting the ingestion will reset the kafka consumer offset maintained by druid to the next offset. The consumer offsets maintained by druid will be reset to either the earliest or latest offset depending on druid.kafka.ingestion.useEarliestOffset\n table property. This can cause duplicate/missing events. We typically only need to reset kafka ingestion when messages in Kafka at the current consumer offsets are no longer available for consumption and therefore won\u0026rsquo;t be ingested into Druid.\nINSERT, INSERT OVERWRITE and DROP statements Version Info\nVersion 2.2.0 : These statements are supported by Hive managed tables (not external) backed by Druid.\nFor versions 3.0.0+, All Druid tables are EXTERNAL (HIVE-20085) and these statements are supported for any table.\nQuerying Druid from Hive\nOnce we have created our first table stored in Druid using the DruidStorageHandler, we are ready to execute our queries against Druid.\nWhen we express a query over a Druid table, Hive tries to rewrite the query to be executed efficiently by pushing as much computation as possible to Druid. This task is accomplished by the cost optimizer based in Apache Calcite, which identifies patterns in the plan and apply rules to rewrite the input query into a new equivalent query with (hopefully) more operations executed in Druid.\nIn particular, we implemented our extension to the optimizer in HIVE-14217, which builds upon the work initiated in CALCITE-1121, and extends its logic to identify more complex query patterns (timeseries queries), translate filters on the time dimension to Druid intervals, push limit into Druid select queries, etc.\nCurrently, we support the recognition of timeseries, groupBy, and select queries.\nOnce we have completed the optimization, the (sub)plan of operators that needs to be executed by Druid is translated into a valid Druid JSON query, and passed as a property to the Hive physical TableScan operator. The Druid query will be executed within the TableScan operator, which will generate the records out of the Druid query results.\nWe generate a single Hive split with the corresponding Druid query for timeseriesand groupBy, from which we generate the records. Thus, the degree of parallelism is 1 in these cases. However, for simple select queries without limit (although they might still contain filters or projections), we partition the original query into x queries and generate one split for each of them, thus incrementing the degree of parallelism for these queries, which usually return a large number of results, to x.\n Consider that depending on the query, it might not be possible to push any computation to Druid. However, our contract is that the query should always be executed. Thus, in those cases, Hive will send a select query to Druid, which basically will read all the segments from Druid, generate records, and then execute the rest of Hive operations on those records. This is also the approach that will be followed if the cost optimizer is disabled (not recommended).\nQueries completely executed in Druid We focus first on queries that can be pushed completely into Druid. In these cases, we end up with a simple plan consisting of a TableScan and a Fetch operator on top. Thus, there is no overhead related to launching containers for the execution.\nSelect queries We start with the simplest type of Druid query: select queries. Basically, a select query will be equivalent to a scan operation on the data sources, although operations such as projection, filter, or limit can still be pushed into this type of query.\nConsider the following query, a simple select query for 10 rows consisting of all the columns of the table:\nSELECT * FROM druid_table_1 LIMIT 10; The Hive plan for the query will be the following:\nhive\u0026gt; EXPLAIN \u0026gt; SELECT * FROM druid_table_1 LIMIT 10; OK Plan optimized by CBO. Stage-0 Fetch Operator limit:-1 Select Operator [SEL_1] Output:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;,\u0026quot;_col2\u0026quot;,\u0026quot;_col3\u0026quot;,\u0026quot;_col4\u0026quot;,\u0026quot;_col5\u0026quot;,\u0026quot;_col6\u0026quot;,\u0026quot;_col7\u0026quot;,\u0026quot;_col8\u0026quot;,\u0026quot;_col9\u0026quot;,\u0026quot;_col10\u0026quot;,\u0026quot;_col11\u0026quot;,\u0026quot;_col12\u0026quot;,\u0026quot;_col13\u0026quot;,\u0026quot;_col14\u0026quot;,\u0026quot;_col15\u0026quot;,\u0026quot;_col16\u0026quot;,\u0026quot;_col17\u0026quot;,\u0026quot;_col18\u0026quot;,\u0026quot;_col19\u0026quot;,\u0026quot;_col20\u0026quot;,\u0026quot;_col21\u0026quot;] TableScan [TS_0] Output:[\u0026quot;__time\u0026quot;,\u0026quot;added\u0026quot;,\u0026quot;channel\u0026quot;,\u0026quot;cityname\u0026quot;,\u0026quot;comment\u0026quot;,\u0026quot;count\u0026quot;,\u0026quot;countryisocode\u0026quot;,\u0026quot;countryname\u0026quot;,\u0026quot;deleted\u0026quot;,\u0026quot;delta\u0026quot;,\u0026quot;isanonymous\u0026quot;,\u0026quot;isminor\u0026quot;,\u0026quot;isnew\u0026quot;,\u0026quot;isrobot\u0026quot;,\u0026quot;isunpatrolled\u0026quot;,\u0026quot;metrocode\u0026quot;,\u0026quot;namespace\u0026quot;,\u0026quot;page\u0026quot;,\u0026quot;regionisocode\u0026quot;,\u0026quot;regionname\u0026quot;,\u0026quot;user\u0026quot;,\u0026quot;user_unique\u0026quot;],properties:{\u0026quot;druid.query.json\u0026quot;:\u0026quot;{\\\u0026quot;queryType\\\u0026quot;:\\\u0026quot;select\\\u0026quot;,\\\u0026quot;dataSource\\\u0026quot;:\\\u0026quot;wikiticker\\\u0026quot;,\\\u0026quot;descending\\\u0026quot;:\\\u0026quot;false\\\u0026quot;,\\\u0026quot;intervals\\\u0026quot;:[\\\u0026quot;-146136543-09-08T08:22:17.096-00:01:15/146140482-04-24T16:36:27.903+01:00\\\u0026quot;],\\\u0026quot;dimensions\\\u0026quot;:[\\\u0026quot;channel\\\u0026quot;,\\\u0026quot;cityname\\\u0026quot;,\\\u0026quot;comment\\\u0026quot;,\\\u0026quot;countryisocode\\\u0026quot;,\\\u0026quot;countryname\\\u0026quot;,\\\u0026quot;isanonymous\\\u0026quot;,\\\u0026quot;isminor\\\u0026quot;,\\\u0026quot;isnew\\\u0026quot;,\\\u0026quot;isrobot\\\u0026quot;,\\\u0026quot;isunpatrolled\\\u0026quot;,\\\u0026quot;metrocode\\\u0026quot;,\\\u0026quot;namespace\\\u0026quot;,\\\u0026quot;page\\\u0026quot;,\\\u0026quot;regionisocode\\\u0026quot;,\\\u0026quot;regionname\\\u0026quot;,\\\u0026quot;user\\\u0026quot;,\\\u0026quot;user_unique\\\u0026quot;],\\\u0026quot;metrics\\\u0026quot;:[\\\u0026quot;added\\\u0026quot;,\\\u0026quot;count\\\u0026quot;,\\\u0026quot;deleted\\\u0026quot;,\\\u0026quot;delta\\\u0026quot;],\\\u0026quot;pagingSpec\\\u0026quot;:{\\\u0026quot;threshold\\\u0026quot;:10},\\\u0026quot;context\\\u0026quot;:{\\\u0026quot;druid.query.fetch\\\u0026quot;:true}}\u0026quot;,\u0026quot;druid.query.type\u0026quot;:\u0026quot;select\u0026quot;} Time taken: 0.141 seconds, Fetched: 10 row(s) Observe that the Druid query is in the properties attached to the TableScan. For readability, we format it properly:\n{ \u0026quot;queryType\u0026quot;:\u0026quot;select\u0026quot;, \u0026quot;dataSource\u0026quot;:\u0026quot;wikiticker\u0026quot;, \u0026quot;descending\u0026quot;:\u0026quot;false\u0026quot;, \u0026quot;intervals\u0026quot;:[\u0026quot;-146136543-09-08T08:22:17.096-00:01:15/146140482-04-24T16:36:27.903+01:00\u0026quot;], \u0026quot;dimensions\u0026quot;: [\u0026quot;channel\u0026quot;,\u0026quot;cityname\u0026quot;,\u0026quot;comment\u0026quot;,\u0026quot;countryisocode\u0026quot;, \u0026quot;countryname\u0026quot;,\u0026quot;isanonymous\u0026quot;,\u0026quot;isminor\u0026quot;,\u0026quot;isnew\u0026quot;, \u0026quot;isrobot\u0026quot;,\u0026quot;isunpatrolled\u0026quot;,\u0026quot;metrocode\u0026quot;,\u0026quot;namespace\u0026quot;, \u0026quot;page\u0026quot;,\u0026quot;regionisocode\u0026quot;,\u0026quot;regionname\u0026quot;,\u0026quot;user\u0026quot;,\u0026quot;user_unique\u0026quot; ], \u0026quot;metrics\u0026quot;:[\u0026quot;added\u0026quot;,\u0026quot;count\u0026quot;,\u0026quot;deleted\u0026quot;,\u0026quot;delta\u0026quot;], \u0026quot;pagingSpec\u0026quot;:{\u0026quot;threshold\u0026quot;:10} } Observe that we get to push the limit into the Druid query (threshold). Observe as well that as we do not specify a filter on the timestamp dimension for the data source, we generate an interval that covers the range (−∞,+∞).\n In Druid, the timestamp column plays a central role. In fact, Druid allows to filter on the time dimension using the intervals property for all those queries. This is very important, as the time intervals determine the nodes that store the Druid data. Thus, specifying a precise range minimizes the number of nodes hit by the broken for a certain query. Inspired by Druid PR-2880, we implemented the intervals extraction from the filter conditions in the logical plan of a query. For instance, consider the following query:\nSELECT `__time` FROM druid_table_1 WHERE `__time` \u0026gt;= '2010-01-01 00:00:00' AND `__time` \u0026lt;= '2011-01-01 00:00:00' LIMIT 10; The Druid query generated for the SQL query above is the following (we omit the plan, as it is a simple TableScan operator).\n{ \u0026quot;queryType\u0026quot;:\u0026quot;select\u0026quot;, \u0026quot;dataSource\u0026quot;:\u0026quot;wikiticker\u0026quot;, \u0026quot;descending\u0026quot;:\u0026quot;false\u0026quot;, \u0026quot;intervals\u0026quot;:[\u0026quot;2010-01-01T00:00:00.000Z/2011-01-01T00:00:00.001Z\u0026quot;], \u0026quot;dimensions\u0026quot;:[], \u0026quot;metrics\u0026quot;:[], \u0026quot;pagingSpec\u0026quot;:{\u0026quot;threshold\u0026quot;:10} } Observe that we infer correctly the interval for the specified dates, 2010-01-01T00:00:00.000Z/2011-01-01T00:00:00.001Z, because in Druid the starting date of the interval is included, but the closing date is not. We also support recognition of multiple interval ranges, for instance in the following SQL query:\nSELECT `__time` FROM druid_table_1 WHERE (`__time` BETWEEN '2010-01-01 00:00:00' AND '2011-01-01 00:00:00') OR (`__time` BETWEEN '2012-01-01 00:00:00' AND '2013-01-01 00:00:00') LIMIT 10; Furthermore we can infer overlapping intervals too. Finally, the filters that are not specified on the time dimension will be translated into valid Druid filters and included within the query using the filter property.\nTimeseries queries Timeseries is one of the types of queries that Druid can execute very efficiently. The following SQL query translates directly into a Druid timeseries query:\n-- GRANULARITY: MONTH SELECT `floor_month`(`__time`), max(delta), sum(added) FROM druid_table_1 GROUP BY `floor_month`(`__time`); Basically, we group by a given time granularity and calculate the aggregation results for each resulting group. In particular, the floor_month function over the timestamp dimension __time represents the Druid month granularity format. Currently, we support floor_year, floor_quarter, floor_month, floor_week, floor_day, floor_hour, floor_minute, and floor_second granularities. In addition, we support two special types of granularities, all and none, which we describe below. We plan to extend our integration work to support other important Druid custom granularity constructs, such as duration and period granularities.\nThe Hive plan for the query will be the following:\nhive\u0026gt; EXPLAIN \u0026gt; SELECT `floor_month`(`__time`), max(delta), sum(added) \u0026gt; FROM druid_table_1 \u0026gt; GROUP BY `floor_month`(`__time`); OK Plan optimized by CBO. Stage-0 Fetch Operator limit:-1 Select Operator [SEL_1] Output:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;,\u0026quot;_col2\u0026quot;] TableScan [TS_0] Output:[\u0026quot;__time\u0026quot;,\u0026quot;$f1\u0026quot;,\u0026quot;$f2\u0026quot;], properties:{\u0026quot;druid.query.json\u0026quot;:\u0026quot;{\\\u0026quot;queryType\\\u0026quot;:\\\u0026quot;timeseries\\\u0026quot;,\\\u0026quot;dataSource\\\u0026quot;:\\\u0026quot;wikiticker\\\u0026quot;,\\\u0026quot;descending\\\u0026quot;:\\\u0026quot;false\\\u0026quot;,\\\u0026quot;granularity\\\u0026quot;:\\\u0026quot;MONTH\\\u0026quot;,\\\u0026quot;aggregations\\\u0026quot;:[{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;longMax\\\u0026quot;,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;$f1\\\u0026quot;,\\\u0026quot;fieldName\\\u0026quot;:\\\u0026quot;delta\\\u0026quot;},{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;longSum\\\u0026quot;,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;$f2\\\u0026quot;,\\\u0026quot;fieldName\\\u0026quot;:\\\u0026quot;added\\\u0026quot;}],\\\u0026quot;intervals\\\u0026quot;:[\\\u0026quot;-146136543-09-08T08:22:17.096-00:01:15/146140482-04-24T16:36:27.903+01:00\\\u0026quot;]}\u0026quot;,\u0026quot;druid.query.type\u0026quot;:\u0026quot;timeseries\u0026quot;} Time taken: 0.116 seconds, Fetched: 10 row(s) Observe that the Druid query is in the properties attached to the TableScan. For readability, we format it properly:\n{ \u0026quot;queryType\u0026quot;:\u0026quot;timeseries\u0026quot;, \u0026quot;dataSource\u0026quot;:\u0026quot;wikiticker\u0026quot;, \u0026quot;descending\u0026quot;:\u0026quot;false\u0026quot;, \u0026quot;granularity\u0026quot;:\u0026quot;MONTH\u0026quot;, \u0026quot;aggregations\u0026quot;:[ {\u0026quot;type\u0026quot;:\u0026quot;longMax\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;$f1\u0026quot;, \u0026quot;fieldName\u0026quot;:\u0026quot;delta\u0026quot;}, {\u0026quot;type\u0026quot;:\u0026quot;longSum\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;$f2\u0026quot;, \u0026quot;fieldName\u0026quot;:\u0026quot;added\u0026quot;} ], \u0026quot;intervals\u0026quot;:[\u0026quot;-146136543-09-08T08:22:17.096-00:01:15/146140482-04-24T16:36:27.903+01:00\u0026quot;] } Observe that the granularity for the Druid query is MONTH.\n One rather special case is all granularity, which we introduce by example below. Consider the following query:\n-- GRANULARITY: ALL SELECT max(delta), sum(added) FROM druid_table_1; As it will do an aggregation on the complete dataset, it translates into a timeseries query with granularity all. In particular, the equivalent Druid query attached to the TableScan operator is the following:\n{ \u0026quot;queryType\u0026quot;:\u0026quot;timeseries\u0026quot;, \u0026quot;dataSource\u0026quot;:\u0026quot;wikiticker\u0026quot;, \u0026quot;descending\u0026quot;:\u0026quot;false\u0026quot;, \u0026quot;granularity\u0026quot;:\u0026quot;ALL\u0026quot;, \u0026quot;aggregations\u0026quot;:[ {\u0026quot;type\u0026quot;:\u0026quot;longMax\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;$f1\u0026quot;, \u0026quot;fieldName\u0026quot;:\u0026quot;delta\u0026quot;}, {\u0026quot;type\u0026quot;:\u0026quot;longSum\u0026quot;, \u0026quot;name\u0026quot;:\u0026quot;$f2\u0026quot;, \u0026quot;fieldName\u0026quot;:\u0026quot;added\u0026quot;} ], \u0026quot;intervals\u0026quot;:[\u0026quot;-146136543-09-08T08:22:17.096-00:01:15/146140482-04-24T16:36:27.903+01:00\u0026quot;] } GroupBy queries The final type of queries we currently support is groupBy. This kind of query is more expressive than timeseries queries; however, they are less performant. Thus, we only fall back to groupBy queries when we cannot transform into timeseries queries.\nFor instance, the following SQL query will generate a Druid groupBy query:\nSELECT max(delta), sum(added) FROM druid_table_1 GROUP BY `channel`, `user`; { \u0026quot;queryType\u0026quot;:\u0026quot;groupBy\u0026quot;, \u0026quot;dataSource\u0026quot;:\u0026quot;wikiticker\u0026quot;, \u0026quot;granularity\u0026quot;:\u0026quot;ALL\u0026quot;, \u0026quot;dimensions\u0026quot;:[\u0026quot;channel\u0026quot;,\u0026quot;user\u0026quot;], \u0026quot;aggregations\u0026quot;:[ {\u0026quot;type\u0026quot;:\u0026quot;longMax\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;$f2\u0026quot;,\u0026quot;fieldName\u0026quot;:\u0026quot;delta\u0026quot;}, {\u0026quot;type\u0026quot;:\u0026quot;longSum\u0026quot;,\u0026quot;name\u0026quot;:\u0026quot;$f3\u0026quot;,\u0026quot;fieldName\u0026quot;:\u0026quot;added\u0026quot;}], \u0026quot;intervals\u0026quot;:[\u0026quot;-146136543-09-08T08:22:17.096-00:01:15/146140482-04-24T16:36:27.903+01:00\u0026quot;] } Queries across Druid and Hive Finally, we provide an example of a query that runs across Druid and Hive. In particular, let us create a second table in Hive with some data:\nCREATE TABLE hive_table_1 (col1 INT, col2 STRING); INSERT INTO hive_table_1 VALUES(1, '#en.wikipedia'); Assume we want to execute the following query:\nSELECT a.channel, b.col1 FROM ( SELECT `channel`, max(delta) as m, sum(added) FROM druid_table_1 GROUP BY `channel`, `floor_year`(`__time`) ORDER BY m DESC LIMIT 1000 ) a JOIN ( SELECT col1, col2 FROM hive_table_1 ) b ON a.channel = b.col2; The query is a simple join on columns channel and col2. The subquery a is executed completely in Druid as a groupBy query. Then the results are joined in Hive with the results of results of subquery b. The query plan and execution in Tez is shown in the following:\nhive\u0026gt; explain \u0026gt; SELECT a.channel, b.col1 \u0026gt; FROM \u0026gt; ( \u0026gt; SELECT `channel`, max(delta) as m, sum(added) \u0026gt; FROM druid_table_1 \u0026gt; GROUP BY `channel`, `floor_year`(`__time`) \u0026gt; ORDER BY m DESC \u0026gt; LIMIT 1000 \u0026gt; ) a \u0026gt; JOIN \u0026gt; ( \u0026gt; SELECT col1, col2 \u0026gt; FROM hive_table_1 \u0026gt; ) b \u0026gt; ON a.channel = b.col2; OK Plan optimized by CBO. Vertex dependency in root stage Map 2 \u0026lt;- Map 1 (BROADCAST_EDGE) Stage-0 Fetch Operator limit:-1 Stage-1 Map 2 File Output Operator [FS_11] Select Operator [SEL_10] (rows=1 width=0) Output:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;] Map Join Operator [MAPJOIN_16] (rows=1 width=0) Conds:RS_7._col0=SEL_6._col1(Inner),HybridGraceHashJoin:true,Output:[\u0026quot;_col0\u0026quot;,\u0026quot;_col2\u0026quot;] \u0026lt;-Map 1 [BROADCAST_EDGE] BROADCAST [RS_7] PartitionCols:_col0 Filter Operator [FIL_2] (rows=1 width=0) predicate:_col0 is not null Select Operator [SEL_1] (rows=1 width=0) Output:[\u0026quot;_col0\u0026quot;] TableScan [TS_0] (rows=1 width=0) druid@druid_table_1,druid_table_1,Tbl:PARTIAL,Col:NONE,Output:[\u0026quot;channel\u0026quot;],properties:{\u0026quot;druid.query.json\u0026quot;:\u0026quot;{\\\u0026quot;queryType\\\u0026quot;:\\\u0026quot;groupBy\\\u0026quot;,\\\u0026quot;dataSource\\\u0026quot;:\\\u0026quot;wikiticker\\\u0026quot;,\\\u0026quot;granularity\\\u0026quot;:\\\u0026quot;all\\\u0026quot;,\\\u0026quot;dimensions\\\u0026quot;:[{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;default\\\u0026quot;,\\\u0026quot;dimension\\\u0026quot;:\\\u0026quot;channel\\\u0026quot;},{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;extraction\\\u0026quot;,\\\u0026quot;dimension\\\u0026quot;:\\\u0026quot;__time\\\u0026quot;,\\\u0026quot;outputName\\\u0026quot;:\\\u0026quot;floor_year\\\u0026quot;,\\\u0026quot;extractionFn\\\u0026quot;:{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;timeFormat\\\u0026quot;,\\\u0026quot;format\\\u0026quot;:\\\u0026quot;yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\\\u0026quot;,\\\u0026quot;granularity\\\u0026quot;:\\\u0026quot;year\\\u0026quot;,\\\u0026quot;timeZone\\\u0026quot;:\\\u0026quot;UTC\\\u0026quot;,\\\u0026quot;locale\\\u0026quot;:\\\u0026quot;en-US\\\u0026quot;}}],\\\u0026quot;limitSpec\\\u0026quot;:{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;default\\\u0026quot;,\\\u0026quot;limit\\\u0026quot;:1000,\\\u0026quot;columns\\\u0026quot;:[{\\\u0026quot;dimension\\\u0026quot;:\\\u0026quot;$f2\\\u0026quot;,\\\u0026quot;direction\\\u0026quot;:\\\u0026quot;descending\\\u0026quot;,\\\u0026quot;dimensionOrder\\\u0026quot;:\\\u0026quot;numeric\\\u0026quot;}]},\\\u0026quot;aggregations\\\u0026quot;:[{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;doubleMax\\\u0026quot;,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;$f2\\\u0026quot;,\\\u0026quot;fieldName\\\u0026quot;:\\\u0026quot;delta\\\u0026quot;},{\\\u0026quot;type\\\u0026quot;:\\\u0026quot;doubleSum\\\u0026quot;,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;$f3\\\u0026quot;,\\\u0026quot;fieldName\\\u0026quot;:\\\u0026quot;added\\\u0026quot;}],\\\u0026quot;intervals\\\u0026quot;:[\\\u0026quot;1900-01-01T00:00:00.000/3000-01-01T00:00:00.000\\\u0026quot;]}\u0026quot;,\u0026quot;druid.query.type\u0026quot;:\u0026quot;groupBy\u0026quot;} \u0026lt;-Select Operator [SEL_6] (rows=1 width=15) Output:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;] Filter Operator [FIL_15] (rows=1 width=15) predicate:col2 is not null TableScan [TS_4] (rows=1 width=15) druid@hive_table_1,hive_table_1,Tbl:COMPLETE,Col:NONE,Output:[\u0026quot;col1\u0026quot;,\u0026quot;col2\u0026quot;] Time taken: 0.924 seconds, Fetched: 31 row(s) hive\u0026gt; SELECT a.channel, b.col1 \u0026gt; FROM \u0026gt; ( \u0026gt; SELECT `channel`, max(delta) as m, sum(added) \u0026gt; FROM druid_table_1 \u0026gt; GROUP BY `channel`, `floor_year`(`__time`) \u0026gt; ORDER BY m DESC \u0026gt; LIMIT 1000 \u0026gt; ) a \u0026gt; JOIN \u0026gt; ( \u0026gt; SELECT col1, col2 \u0026gt; FROM hive_table_1 \u0026gt; ) b \u0026gt; ON a.channel = b.col2; Query ID = user1_20160818202329_e9a8b3e8-18d3-49c7-bfe0-99d38d2402d3 Total jobs = 1 Launching Job 1 out of 1 2016-08-18 20:23:30 Running Dag: dag_1471548210492_0001_1 2016-08-18 20:23:30 Starting to run new task attempt: attempt_1471548210492_0001_1_00_000000_0 Status: Running (Executing on YARN cluster with App id application_1471548210492_0001) ---------------------------------------------------------------------------------------------- VERTICES MODE STATUS TOTAL COMPLETED RUNNING PENDING FAILED KILLED ---------------------------------------------------------------------------------------------- Map 1 .......... container SUCCEEDED 1 1 0 0 0 0 Map 2 .......... container SUCCEEDED 1 1 0 0 0 0 ---------------------------------------------------------------------------------------------- VERTICES: 02/02 [==========================\u0026gt;\u0026gt;] 100% ELAPSED TIME: 0.15 s ---------------------------------------------------------------------------------------------- 2016-08-18 20:23:31 Completed running task attempt: attempt_1471548210492_0001_1_00_000000_0 OK #en.wikipedia\t1 Time taken: 1.835 seconds, Fetched: 2 row(s)  Open Issues (JIRA)    Key Summary T Created Updated Due Assignee Reporter P Status Resolution     HIVE-14473 Druid integration II New Feature Aug 08, 2016 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-14543 Create Druid table without specifying data source Sub-task Aug 16, 2016 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-14597 Support for Druid custom granularities Sub-task Aug 22, 2016 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Minor Open Unresolved   HIVE-14722 Support creating vector row batches from Druid Sub-task Sep 08, 2016 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-15584 Early bail out when we use CTAS and Druid source already exists Sub-task Jan 11, 2017 Feb 27, 2024  Slim Bouguerra Jesús Camacho Rodríguez Minor Open Unresolved   HIVE-15640 Hive/Druid integration: null handling for metrics Bug Jan 16, 2017 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Critical Open Unresolved   HIVE-16121 Add flag to allow approximate results coming from Druid Improvement Mar 06, 2017 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-16816 Chained Group by support for druid. Sub-task Jun 02, 2017 Feb 23, 2018  Slim Bouguerra Slim Bouguerra Major Open Unresolved   HIVE-17716 Not pushing postaggregations into Druid due to CAST on constant Improvement Oct 05, 2017 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-18668 Really shade guava in ql Bug Feb 09, 2018 Oct 21, 2022  Zoltan Haindrich Zoltan Haindrich Major Patch Available Unresolved   HIVE-18731 Add Documentations about this feature. Sub-task Feb 16, 2018 Oct 17, 2018  Slim Bouguerra Slim Bouguerra Major Open Unresolved   HIVE-19044 Duplicate field names within Druid Query Generated by Calcite plan Bug Mar 25, 2018 Apr 05, 2018  Slim Bouguerra Slim Bouguerra Major Patch Available Unresolved   HIVE-19201 Hive doesn\u0026rsquo;t read Druid data correctly Bug Apr 13, 2018 Oct 05, 2018 Apr 17, 2018 Unassigned Tournadre Blocker Open Unresolved   HIVE-19300 Skip Druid/JDBC rules in optimizer when there are no Druid/JDBC sources Improvement Apr 25, 2018 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-19672 Column Names mismatch between native Druid Tables and Hive External table map Bug May 23, 2018 Oct 21, 2022  Unassigned Slim Bouguerra Major Open Unresolved   HIVE-20426 Upload Druid Test Runner logs from Build Slaves Improvement Aug 20, 2018 Aug 20, 2018  Vineet Garg Slim Bouguerra Major Open Unresolved   HIVE-20468 Add ability to skip creating druid bitmap indexes for specific string dimensions New Feature Aug 27, 2018 Aug 27, 2018  Nishant Bangarwa Nishant Bangarwa Major Open Unresolved   HIVE-20469 Do not rollup PK/FK columns when indexing to druid. Improvement Aug 27, 2018 Oct 10, 2018  Nishant Bangarwa Nishant Bangarwa Major Open Unresolved   HIVE-20687 Cancel Running Druid Query when a hive query is cancelled. Improvement Oct 03, 2018 Oct 05, 2018  Nishant Bangarwa Nishant Bangarwa Major Open Unresolved   HIVE-20997 Make Druid Cluster start on random ports. Sub-task Dec 03, 2018 Dec 04, 2018  Slim Bouguerra Slim Bouguerra Major Open Unresolved    Authenticate to retrieve your issues\nShowing 20 out of 28 issues\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/druid-integration_65866491/","tags":null,"title":"Apache Hive : Druid Integration"},{"categories":null,"contents":"Apache Hive : DynamicPartitions Dynamic Partitions  Dynamic Partitions  Documentation Terminology Syntax Design Design issues    Documentation This is the design document for dynamic partitions in Hive. Usage information is also available:\n Tutorial: Dynamic-Partition Insert Hive DML: Dynamic Partition Inserts HCatalog Dynamic Partitioning  Usage with Pig Usage from MapReduce    References:\n Original design doc HIVE-936  Terminology  Static Partition (SP) columns: in DML/DDL involving multiple partitioning columns, the columns whose values are known at COMPILE TIME (given by user). Dynamic Partition (DP) columns: columns whose values are only known at EXECUTION TIME.  Syntax DP columns are specified the same way as it is for SP columns – in the partition clause. The only difference is that DP columns do not have values, while SP columns do. In the partition clause, we need to specify all partitioning columns, even if all of them are DP columns.\nIn INSERT \u0026hellip; SELECT \u0026hellip; queries, the dynamic partition columns must be specified last among the columns in the SELECT statement and in the same order in which they appear in the PARTITION() clause.\n all DP columns – only allowed in nonstrict mode. In strict mode, we should throw an error. e.g.,   INSERT OVERWRITE TABLE T PARTITION (ds, hr) SELECT key, value, ds, hr FROM srcpart WHERE ds is not null and hr\u0026gt;10;  mixed SP \u0026amp; DP columns. e.g.,   INSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr) SELECT key, value, /*ds,*/ hr FROM srcpart WHERE ds is not null and hr\u0026gt;10;  SP is a subpartition of a DP: should throw an error because partition column order determins directory hierarchy. We cannot change the hierarchy in DML. e.g.,   -- throw an exception INSERT OVERWRITE TABLE T PARTITION (ds, hr = 11) SELECT key, value, ds/*, hr*/ FROM srcpart WHERE ds is not null and hr=11;  multi-table insert. e.g.,   FROM S INSERT OVERWRITE TABLE T PARTITION (ds='2010-03-03', hr) SELECT key, value, ds, hr FROM srcpart WHERE ds is not null and hr\u0026gt;10 INSERT OVERWRITE TABLE R PARTITION (ds='2010-03-03, hr=12) SELECT key, value, ds, hr from srcpart where ds is not null and hr = 12;  CTAS – syntax is a little bit different from CTAS on non-partitioned tables, since the schema of the target table is not totally derived from the select-clause. We need to specify the schema including partitioning columns in the create-clause. e.g.,   CREATE TABLE T (key int, value string) PARTITIONED BY (ds string, hr int) AS SELECT key, value, ds, hr+1 hr1 FROM srcpart WHERE ds is not null and hr\u0026gt;10; The above example shows the case of all DP columns in CTAS. If you want put some constant for some partitioning column, you can specify it in the select-clause. e.g,\n CREATE TABLE T (key int, value string) PARTITIONED BY (ds string, hr int) AS SELECT key, value, \u0026quot;2010-03-03\u0026quot;, hr+1 hr1 FROM srcpart WHERE ds is not null and hr\u0026gt;10; Design  In SemanticAnalyser.genFileSinkPlan(), parse the input and generate a list of SP and DP columns. We also generate a mapping from the input ExpressionNode to the output DP columns in FileSinkDesc. We also need to keep a HashFunction class in FileSinkDesc to evaluate partition directory names from the input expression value. In FileSinkOperator, setup the input -\u0026gt; DP mapping and Hash in initOp(). and determine the output path in processOp() from the mapping. ObjectInspector setup? MoveTask: since DP columns represent a subdirectory tree, it is possible to use one MoveTask at the end to move the results from the temporary directory to the final directory. post exec hook for replication: remove all existing data in DP before creating new partitions. We should make sure replication hook recognize all the modified partitions. metastore support: since we are creating multiple parititions in a DML, metastore should be able to create all these partitions. Need to investigate.  Design issues  Data type of the dynamic partitioning column:  A dynamic partitioning column could be the result of an expression. For example:\n -- part_col is partitioning column create table T as select a, concat(\u0026quot;part_\u0026quot;, part_col) from S where part_col is not null; Although currently there is not restriction on the data type of the partitioning column, allowing non-primitive columns to be partitioning column probably doesn\u0026rsquo;t make sense. The dynamic partitioning column\u0026rsquo;s type should be derived from the expression. The data type has to be able to be converted to a string in order to be saved as a directory name in HDFS.\nPartitioning column value to directory name conversion:  After converting column value to string, we still need to convert the string value to a valid directory name. Some reasons are:\n string length is unlimited in theory, but HDFS/local FS directory name length is limited. string value could contains special characters that is reserved in FS path names (such as \u0026lsquo;/\u0026rsquo; or \u0026lsquo;..'). what should we do for partition column ObjectInspector?  We need to define a UDF (say hive_qname_partition(T.part_col)) to take a primitive typed value and convert it to a qualified partition name.\nDue to 2), this dynamic partitioning scheme qualifies as a hash-based partitioning scheme, except that we define the hash function to be as close as  the input value. We should allow users to plugin their own UDF for the partition hash function. Will file a follow up JIRA if there is sufficient interests.\nIf there are multiple partitioning columns, their order is significant since that translates to the directory structure in HDFS: partitioned by (ds string, dept int) implies a directory structure of ds=2009-02-26/dept=2. In a DML or DDL involving partitioned table, So if a subset of partitioning columns are specified (static), we should throw an error if a dynamic partitioning column is lower. Example:   create table nzhang_part(a string) partitioned by (ds string, dept int); insert overwrite nzhang_part (dept=1) select a, ds, dept from T where dept=1 and ds is not null; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/dynamicpartitions_27823715/","tags":null,"title":"Apache Hive : DynamicPartitions"},{"categories":null,"contents":"Apache Hive : Enabling gRPC in Hive/Hive Metastore (Proposal) Contacts: Cameron Moberg (Google), Zhou Fang (Google), Feng Lu (Google), Thejas Nair (Cloudera), Vihang Karajgaonkar (Cloudera), Naveen Gangam (Cloudera)\nLast upated: 7/31/2020\n Objective Background Design  Overview Implementation  Pluggable gRPC Support Hive Metastore Server  Class Change Config Changes   Hive Metastore Client  Class Change Configuration Changes       Summary Future Work  Objective  To modernize Hive Metastore’s interface with a state-of-the-art serving layer based on gRPC while also keeping it backwards compatible with Thrift for minimal upgrade toil; To achieve this the proposed design is to add support for a proxy-layer between the Thrift interface and a new gRPC interface that allows for in-memory request/response translation in-between; To expand the Hive client to work with Hive Metastore server in both gRPC and Thrift mode.  Background Hive Metastore is the central repository of Apache Hive (among others like Presto and Spark) metadata. It stores metadata for tables (e.g., schema, location, and statistics) and partitions in a relational database. It provides client access to this information by using a Thrift Metastore API.\nThe Apache Thrift software framework, for scalable cross-language services development, combines a software stack with a code generation engine to build services that work efficiently and seamlessly between C++, Java, Python, and many other languages.\ngRPC is a modern open source high performance RPC framework that can run in any environment. It can efficiently connect services in and across data centers with pluggable support for load balancing, tracing, health checking and authentication. It is also applicable in the last mile of distributed computing to connect devices, mobile applications and browsers to backend services.\nProviding gRPC as an option to access Metastore brings us many benefits. Compared to Thrift, gRPC supports streaming that provides better performance for large requests. In addition, it is extensible to more advanced authentication features and is fully compatible with Google’s IAM service that supports fine grained permission checks. A path to integrate gRPC with Hive Metastore is sketched out by this proposal.\nDesign Overview The overall design of the gRPC support in Hive Metastore is illustrated in Fig.1. On the server side, based on user configuration, the Hive Metastore Server can listen on a port for Thrift or gRPC request. The lifecycle of a Thrift request has not been changed. For a gRPC request, the new HiveMetastoreGrpcServer will translate an incoming gRPC request into a Thrift request, transparently pass it to HiveMetastoreThriftServer, and translate the response back into gRPC.\nOn the client side, a similar design is used to support converting outgoing requests from Thrift to gRPC. Figure 1. An overview of the new gRPC endpoint of Hive Metastore. Clarification: The only network I/O that occurs is between the user and serving processes in HiveMetaStore (gRPC, Thrift, or both).\nThe implementation details are described in the following sections.\nImplementation Pluggable gRPC Support To have a loose coupling between Hive Metastore and the gRPC layer, we propose to have a pluggable layer which implements only a hook in the Hive Metastore repository, while implements the gRPC proxy library in a separate repository. To enable the gRPC server, a user set “metastore.custom.server.class” in the Hive configuration to the class path of the server in gRPC library. Hive Metastore will then instantiate this class and start the gRPC server described as follows. Here is an example of a similar pluggable library in Hive.\nThe gRPC layer at the client side is implemented similarly in the separate repository. Changes need to be made into Hive repository to load the gRPC Hive client if enabled by config. For example, both SessionHiveMetastoreClient.java and RetryingMetastoreClient.java can be amended to dynamically load the HiveMetastore gRPC client if the metastore.uris starts with “grpc://”.\nHive Metastore Server Class Change The following is assuming modification of the standalone-metastore package.\nTo add support for the Hive Metastore to be able to receive and process gRPC requests, additional Java classes need to be created. Due to the current coupling between application logic and startup logic in HiveMetaStore.java, a separation of logic would first be required that breaks out the Thrift implementation into a HiveMetaStoreThriftServer.java class that can then be instantiated by the HiveMetaStore.java class (the main driver program). After that, the new gRPC implementation Java class, HiveMetaStoreGrpcServer.java, can be similarly created and referenced.\nThe Thrift RPC definition files must be translated into gRPC protobuf files, while this has some direct incompatibilities (such as sets), these can be worked on a per-method implementation basis rather than worry about it in the protobuf files. An example service protobuf definition is shown below; keep in mind Table and CreateTableResponse are both also defined in protobuf files. There is no reference to Thrift files.\nproto\nservice HiveMetaStoreGrpc { rpc getTable(Table) returns (GetTableResponse); } Once the service methods are defined, the gRPC server can be created so it can be instantiated by the driver class. The HiveMetaStoreGrpcServer.java class signature would implement the gRPC server interface like below:\nclass\nprivate class HiveMetaStoreGrpcServer extends HiveMetaStoreGrpc.HiveMetaStoreImplBase An example implementation of getTable translation layer is shown below.\nsample\n// Returns gRPC Response, and takes in gRPC GetTableRequest public GetTableResponse getTable(GetTableRequest grpcTableRequest) { // convertToThriftRequest is implemented by a different library, not in Hive code Table thriftGetTableRequest = convertToThriftRequest(grpcTableRequest); // Result is a Thrift API object GetTableResult result = HiveMetaStoreThriftServer.get_table(thriftTable); return convertToThriftResponse(result); } As shown in Figure 1, green elements are newly added class while yellow is modified from the current design.\nConfig Changes With the potential of starting a new reachable endpoint the requirement of additional hive-site.xml configs are required. The current proposed configuration values are shown below.\n metastore.custom.server.class - string; the path to the class of the custom server; if configed to the gRPC server, Hive Metastore will load the class and use the server to handle gRPC requests metastore.uris (reuse existing field) - string; the socket of the listening gRPC server, used to set the gRPC port. In addition, the port can be modified by “hive \u0026ndash;service metastore -p port_number”.  Parsing metastore.custom.server.class is implemented in Hive Metastore repository, whereas parsing the gRPC specific configs can be offloaded to the gRPC library:\n metastore.grpc.service.account.keyfile - string, optional; the path to the JSON keyfile that the Metastore server will run as. metastore.grpc.authentication.class - string, optional; the gRPC class will use this class to perform authn/authz against the gRPC requests.   The detailed implementation of auth support is not in scope for this design proposal.   Additional gRPC server configs; maximal request size, max connections, port, etc.  Hive Metastore Client Class Change While a Hive Metastore that can support gRPC requests is still useful without any clients it would be helpful to also have the Hive client support gRPC communication with Hive Metastore. This is fairly similar to the previous section, but worth a section on its own for clarity.\nThe class IMetaStoreClient is a thin wrapper on top of the thrift interface and is implemented by HiveMetaStoreClient, however this is an entirely thrift based implementation. As opposed to above, as the client we want to take in a Thrift request (usually generated by code itself), convert the request to gRPC, and then send it out on the wire to the desired listening gRPC server.\nIn this case however, for backwards compatibility the easiest way to add support for gRPC would be to create a new class HiveMetaStoreGrpcClient that implements the Thrift interface IMetaStoreClient, but instead of instantiating and calling and opening a Thrift client, we create a new gRPC client and convert the input Thrift requests to the relevant gRPC, and send them down the wire with the generated gRPC client.\nExample Instantiation and usage of gRPC client:\nclient\nManagedChannel channel = ManagedChannelBuilder.forTarget(target) // Channels are secure by default (via SSL/TLS). For the example we disable TLS to avoid // needing certificates. .usePlaintext() .build(); HiveMetaStoreGrpc.HiveMetastore BlockingStub blockingStub = HiveMetaStoreGrpc.newBlockingStub(channel); blockingStub.getTable(getTableRequest); The definition of the getTable method is defined in the metastore server spec, so all the client needs to do is worry about conversion of the Thrift object to gRPC and which gRPC method to call.\nConfiguration Changes Similar to the changes to the server config, a user can populate the following fields to user a gRPC enabled client:\n metastore.custom.client.class - string; the path to the custom client class; if configed to the gRPC client, Hive Metastore will dynamically load and use that client. metastore.uris (reuse existing field) - string; the socket of the listening gRPC server, can be separated by commas to be chosen randomly between all of them for load balancing purposes. Grpc connections should be prefixed as grpc://  Summary  Creation of the protobuf definition files for the gRPC client and server [a separate ASF licensed repo]:   Add an additional HiveMetaStoreGrpcServer class that implements the logic of the gRPC service methods that translates \u0026amp; explicitly calls the predefined Thrift implementation. Add an additional HiveMetaStoreGrpcClient class that implements IMetaStoreClient that opens a gRPC connection with the gRPC metastore server and translates Thrift API requests to gRPC and sends to server   Separate logic of HiveMetaStore.java so that a gRPC server, in addition to the Thrift server, can be initialized to making a clear distinction between Thrift and gRPC implementations. Add required configuration values and implement the dynamical gRPC class loading/instantiation wiring code inside Hive Metastore Server and Client (e.g., SessionHiveMetastoreClient.java and RetryingMetastoreClient.java).  Future Work  As this is simply adding gRPC support by calling the Thrift APIs via in-memory rather than network-io streaming support is not inherently gained. This is currently just a proposal to get gRPC support a foot in the door, and from there we can iterate on the implementation to add true streaming (or other) support. Look into adding a layer below Hive.java but above IMetastoreClient.java for Thrift/gRPC  Attachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/158869886/","tags":null,"title":"Apache Hive : Enabling gRPC in Hive/Hive Metastore (Proposal)"},{"categories":null,"contents":"Apache Hive : Enhanced Aggregation, Cube, Grouping and Rollup This document describes enhanced aggregation features for the GROUP BY clause of SELECT statements.\n GROUPING SETS clause Grouping__ID function Grouping function Cubes and Rollups hive.new.job.grouping.set.cardinality Grouping__ID function (before Hive 2.3.0)  Version\nGrouping sets, CUBE and ROLLUP operators, and the GROUPING__ID function were added in Hive 0.10.0.\nSee HIVE-2397, HIVE-3433, HIVE-3471, and HIVE-3613.\nAlso see HIVE-3552 for an improvement added in Hive 0.11.0.\nVersion\nGROUPING__ID is compliant with semantics in other SQL engines starting in Hive 2.3.0 (see HIVE-16102).\nSupport for SQL grouping function was added in Hive 2.3.0 too (see HIVE-15409).\nFor general information about GROUP BY, see GroupBy in the Language Manual.\nGROUPING SETS clause The GROUPING SETS clause in GROUP BY allows us to specify more than one GROUP BY option in the same record set. All GROUPING SET clauses can be logically expressed in terms of several GROUP BY queries connected by UNION. Table-1 shows several such equivalent statements. This is helpful in forming the idea of the GROUPING SETS clause. A blank set ( ) in the GROUPING SETS clause calculates the overall aggregate.\nTable 1 - GROUPING SET queries and the equivalent GROUP BY queries\n   Aggregate Query with GROUPING SETS Equivalent Aggregate Query with GROUP BY     SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b) ) SELECT a, b, SUM(c) FROM tab1 GROUP BY a, b   SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a,b), a) SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, bUNIONSELECT a, null, SUM( c ) FROM tab1 GROUP BY a   SELECT a,b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS (a,b) SELECT a, null, SUM( c ) FROM tab1 GROUP BY aUNIONSELECT null, b, SUM( c ) FROM tab1 GROUP BY b   SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, b GROUPING SETS ( (a, b), a, b, ( ) ) SELECT a, b, SUM( c ) FROM tab1 GROUP BY a, bUNIONSELECT a, null, SUM( c ) FROM tab1 GROUP BY a, nullUNIONSELECT null, b, SUM( c ) FROM tab1 GROUP BY null, bUNIONSELECT null, null, SUM( c ) FROM tab1    Grouping__ID function When aggregates are displayed for a column its value is null. This may conflict in case the column itself has some null values. There needs to be some way to identify NULL in column, which means aggregate and NULL in column, which means value. GROUPING__ID function is the solution to that.\nThis function returns a bitvector corresponding to whether each column is present or not. For each column, a value of \u0026ldquo;1\u0026rdquo; is produced for a row in the result set if that column has been aggregated in that row, otherwise the value is \u0026ldquo;0\u0026rdquo;. This can be used to differentiate when there are nulls in the data.\nConsider the following example:\n   Column1 (key) Column2 (value)     1 NULL   1 1   2 2   3 3   3 NULL   4 5    The following query:\nSELECT key, value, GROUPING__ID, count(*) FROM T1 GROUP BY key, value WITH ROLLUP; will have the following results:\n   Column 1 (key) Column 2 (value) GROUPING__ID count(*)     NULL NULL 3 6   1 NULL 1 2   1 NULL 0 1   1 1 0 1   2 NULL 1 1   2 2 0 1   3 NULL 1 2   3 NULL 0 1   3 3 0 1   4 NULL 1 1   4 5 0 1    Note that the third column is a bitvector of columns being selected.\nFor the first row, none of the columns are being selected.\nFor the second row, only the first column is being selected, which explains the value 1.\nFor the third row, both the columns are being selected (and the second column happens to be null), which explains the value 0.\nGrouping function The grouping function indicates whether an expression in a GROUP BY clause is aggregated or not for a given row. The value 0 represents a column that is part of the grouping set, while the value 1 represents a column that is not part of the grouping set. Going back to our example above, consider the following query:\nSELECT key, value, GROUPING__ID, grouping(key, value), grouping(value, key), grouping(key), grouping(value), count(*) FROM T1 GROUP BY key, value WITH ROLLUP; This query will produce the following results.\n   Column 1 (key) Column 2 (value) GROUPING__ID grouping(key, value) grouping(value, key) grouping(key) grouping(value) count(*)     NULL NULL 3 3 3 1 1 6   1 NULL 1 1 2 0 1 2   1 NULL 0 0 0 0 0 1   1 1 0 0 0 0 0 1   2 NULL 1 1 2 0 1 1   2 2 0 0 0 0 0 1   3 NULL 1 1 2 0 1 2   3 NULL 0 0 0 0 0 1   3 3 0 0 0 0 0 1   4 NULL 1 1 2 0 1 1   4 5 0 0 0 0 0 1    Cubes and Rollups The general syntax is WITH CUBE/ROLLUP. It is used with the GROUP BY only. CUBE creates a subtotal of all possible combinations of the set of column in its argument. Once we compute a CUBE on a set of dimension, we can get answer to all possible aggregation questions on those dimensions.\nIt might be also worth mentioning here that\nGROUP BY a, b, c WITH CUBE is equivalent to\nGROUP BY a, b, c GROUPING SETS ( (a, b, c), (a, b), (b, c), (a, c), (a), (b), (c), ( )).\nROLLUP clause is used with GROUP BY to compute the aggregate at the hierarchy levels of a dimension.\nGROUP BY a, b, c with ROLLUP assumes that the hierarchy is \u0026ldquo;a\u0026rdquo; drilling down to \u0026ldquo;b\u0026rdquo; drilling down to \u0026ldquo;c\u0026rdquo;.\nGROUP BY a, b, c, WITH ROLLUP is equivalent to GROUP BY a, b, c GROUPING SETS ( (a, b, c), (a, b), (a), ( )).\nhive.new.job.grouping.set.cardinality Whether a new map-reduce job should be launched for grouping sets/rollups/cubes.\nFor a query like: select a, b, c, count(1) from T group by a, b, c with rollup;\n4 rows are created per row: (a, b, c), (a, b, null), (a, null, null), (null, null, null)\nThis can lead to explosion across map-reduce boundary if the cardinality of T is very high\nand map-side aggregation does not do a very good job.\nThis parameter decides if hive should add an additional map-reduce job. If the grouping set\ncardinality (4 in the example above), is more than this value, a new MR job is added under the\nassumption that the orginal group by will reduce the data size.\nGrouping__ID function (before Hive 2.3.0) Grouping__ID function was fixed in Hive 2.3.0, thus behavior before that release is different (this is expected). For each column, the function would return a value of \u0026ldquo;0\u0026rdquo; iif that column has been aggregated in that row, otherwise the value is \u0026ldquo;1\u0026rdquo;.\nHence the following query:\nSELECT key, value, GROUPING__ID, count(*) FROM T1 GROUP BY key, value WITH ROLLUP; will have the following results.\n   Column 1 (key) Column 2 (value) GROUPING__ID count(*)     NULL NULL 0 6   1 NULL 1 2   1 NULL 3 1   1 1 3 1   2 NULL 1 1   2 2 3 1   3 NULL 1 2   3 NULL 3 1   3 3 3 1   4 NULL 1 1   4 5 3 1    Note that the third column is a bitvector of columns being selected. For the first row, none of the columns are being selected.\nFor the second row, only the first column is being selected, which explains the count of 2.\nFor the third row, both the columns are being selected (and the second column happens to be null), which explains the count of 1.\nComments:            Is there really much value-add in the grouping sets grammar? If I think about the plan for generating a CUBE/ROLLUP (), it\u0026rsquo;s pretty much as efficient as generating the CUBE and then sub-selecting what you need from it.   Can we just provide CUBE and ROLLUP and not provide the additional syntax?    Posted by sambavi at Sep 21, 2012 15:32 | | Depends on what the use case is. By sub-selecting for the right grouping set, we would be passing more data across the map-reduce boundaries. I have started a prototype implementation, and the work for grouping set should not be substantially more than a cube or a rollup. We can stage it, and implement GROUPING_ID later, on demand.\nPosted by namit.jain at Sep 25, 2012 06:50 | | I can only implement CUBE and ROLLUP first, but keep the execution layer general. It will only require parser changes to plug in grouping sets, if need be, later.\nPosted by namit.jain at Sep 25, 2012 07:16 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/30151323/","tags":null,"title":"Apache Hive : Enhanced Aggregation, Cube, Grouping and Rollup"},{"categories":null,"contents":"Apache Hive : Exchange Partition The EXCHANGE PARTITION command will move a partition from a source table to target table and alter each table\u0026rsquo;s metadata. The Exchange Partition feature is implemented as part of HIVE-4095. Exchanging multiple partitions is supported in Hive versions 1.2.2, 1.3.0, and 2.0.0+ as part of HIVE-11745.\nWhen the command is executed, the source table\u0026rsquo;s partition folder in HDFS will be renamed to move it to the destination table\u0026rsquo;s partition folder. The Hive metastore will be updated to change the metadata of the source and destination tables accordingly.\nThe partition specification can be fully or partially specified.\nSee Language Manual DDL for additional information on the Exchange Partition feature.\nConstraints   The destination table cannot contain the partition to be exchanged.\n  The operation fails in the presence of an index.\n  Exchange partition is not allowed with transactional tables either as source or destination. Alternatively, use LOAD DATA or INSERT OVERWRITE commands to move partitions across transactional tables.\n  This command requires both the source and destination table names to have the same table schema. If the schemas are different, the following exception is thrown:\n  The tables have different schemas. Their partitions cannot be exchanged\nSyntax ALTER TABLE \u0026lt;dest_table\u0026gt; EXCHANGE PARTITION (\u0026lt;[partial] partition spec\u0026gt;) WITH TABLE \u0026lt;src_table\u0026gt; Example Usage – Basic --Create two tables, partitioned by ds CREATE TABLE T1(a string, b string) PARTITIONED BY (ds string); CREATE TABLE T2(a string, b string) PARTITIONED BY (ds string); ALTER TABLE T1 ADD PARTITION (ds='1'); --Move partition from T1 to T2 ALTER TABLE T2 EXCHANGE PARTITION (ds='1') WITH TABLE T1; Example Usage – Partial Partition Spec (Exchanging Multiple Partitions) --Create two tables with multiple partition columns. CREATE TABLE T1 (a string, b string) PARTITIONED BY (ds string, hr string); CREATE TABLE T2 (a string, b string) PARTITIONED BY (ds string, hr string); ALTER TABLE T1 ADD PARTITION (ds = '1', hr = '00'); ALTER TABLE T1 ADD PARTITION (ds = '1', hr = '01'); ALTER TABLE T1 ADD PARTITION (ds = '1', hr = '03'); --Alter the table, moving all the three partitions data where ds='1' from table T1 to table T2 (ds=1) ALTER TABLE T2 EXCHANGE PARTITION (ds='1') WITH TABLE T1; Note that the schema for T1 is being used for the newly created partition T2(ds=1). Either all the partitions of T1 will get created or the whole operation will fail. All partitions of T1 are dropped.\nExample Usage – Partition Spec With Multiple Partition Columns -- Create two tables with multiple partition columns. CREATE TABLE T1 (a int) PARTITIONED BY (d1 int, d2 int); CREATE TABLE T2 (a int) PARTITIONED BY (d1 int, d2 int); ALTER TABLE T1 ADD PARTITION (d1=1, d2=2); -- Alter the table, moving partition data d1=1, d2=2 from table T1 to table T2 ALTER TABLE T2 EXCHANGE PARTITION (d1 = 1, d2 = 2) WITH TABLE T1; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/exchange-partition_30755801/","tags":null,"title":"Apache Hive : Exchange Partition"},{"categories":null,"contents":"Apache Hive : FileFormats File Formats and Compression File Formats Hive supports several file formats:\n Text File SequenceFile RCFile Avro Files ORC Files Parquet Custom INPUTFORMAT and OUTPUTFORMAT  The hive.default.fileformat configuration parameter determines the format to use if it is not specified in a CREATE TABLE or ALTER TABLE statement. Text file is the parameter\u0026rsquo;s default value.\nFor more information, see the sections Storage Formats and Row Formats \u0026amp; SerDe on the DDL page.\nFile Compression  Compressed Data Storage LZO Compression  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/fileformats_47384180/","tags":null,"title":"Apache Hive : FileFormats"},{"categories":null,"contents":"Apache Hive : FilterPushdownDev Filter Pushdown  Filter Pushdown  Introduction Use Cases Components Involved Primary Filter Representation Other Filter Representations Filter Passing Filter Collection Filter Decomposition    Introduction This document explains how we are planning to add support in Hive\u0026rsquo;s optimizer for pushing filters down into physical access methods. This is an important optimization for minimizing the amount of data scanned and processed by an access method (e.g. for an indexed key lookup), as well as reducing the amount of data passed into Hive for further query evaluation.\nUse Cases Below are the main use cases we are targeting.\n Pushing filters down into Hive\u0026rsquo;s builtin storage formats such as RCFile Pushing filters down into storage handlers such as the HBase handler (http://issues.apache.org/jira/browse/HIVE-1226) Pushing filters down into index access plans once an indexing framework is added to Hive (http://issues.apache.org/jira/browse/HIVE-417)  Components Involved There are a number of different parts to the overall effort.\n Propagating the result of Hive\u0026rsquo;s existing predicate pushdown. Hive\u0026rsquo;s optimizer already takes care of the hard work of pushing predicates down through the query plan (controlled via configuration parameter hive.optimize.ppd=true/false). The \u0026ldquo;last mile\u0026rdquo; remaining is to send the table-level filters down into the corresponding input formats. Selection of a primary filter representation to be passed to input formats. This representation needs to be neutral (independent of the access plans which will use it) and loosely coupled with Hive (so that storage handlers can choose to minimize their dependencies on Hive internals). Helper classes for interpreting the primary representation. Many access plans will need to analyze filters in a similar fashion, e.g. decomposing conjunctions and detecting supported column comparison patterns. Hive should provide sharable utilities for such cases so that they don\u0026rsquo;t need to be duplicated in each access method\u0026rsquo;s code. Converting filters into a form specific to the access method. This part is dependent on the particular access method; e.g. for HBase, it involves converting the filter condition into corresponding calls to set up an HBase scan object.  Primary Filter Representation To achieve the loosest possible coupling, we are going to use a string as the primary representation for the filter. In particular, the string will be in the form produced when Hive unparses an ExprNodeDesc, e.g.\n((key \u0026gt;= 100) and (key \u0026lt; 200)) In general, this comes out as valid SQL, although it may not always match the original SQL exactly, e.g.\ncast(x as int) becomes\nUDFToInteger(x) Column names in this string are unqualified references to the columns of the table over which the filter operates, as they are known in the Hive metastore. These column names may be different from those known to the underlying storage; for example, the HBase storage handler maps Hive column names to HBase column names (qualified by column family). Mapping from Hive column names is the responsibility of the code interpreting the filter string.\nOther Filter Representations As mentioned above, we want to avoid duplication in code which interprets the filter string (e.g. parsing). As a first cut, we will provide access to the ExprNodeDesc tree by passing it along in serialized form as an optional companion to the filter string. In followups, we will provide parsing utilities for the string form.\nWe will also provide an IndexPredicateAnalyzer class capable of detecting simple sargable\nsubexpressions in an ExprNodeDesc tree. In followups, we will provide support for discriminating and combining more complex indexable subexpressions.\npublic class IndexPredicateAnalyzer { public IndexPredicateAnalyzer(); /** * Registers a comparison operator as one which can be satisfied * by an index search. Unless this is called, analyzePredicate * will never find any indexable conditions. * * @param udfName name of comparison operator as returned * by either {@link GenericUDFBridge#getUdfName} (for simple UDF's) * or udf.getClass().getName() (for generic UDF's). */ public void addComparisonOp(String udfName); /** * Clears the set of column names allowed in comparisons. (Initially, all * column names are allowed.) */ public void clearAllowedColumnNames(); /** * Adds a column name to the set of column names allowed. * * @param columnName name of column to be allowed */ public void allowColumnName(String columnName); /** * Analyzes a predicate. * * @param predicate predicate to be analyzed * * @param searchConditions receives conditions produced by analysis * * @return residual predicate which could not be translated to * searchConditions */ public ExprNodeDesc analyzePredicate( ExprNodeDesc predicate, final List\u0026lt;IndexSearchCondition\u0026gt; searchConditions); /** * Translates search conditions back to ExprNodeDesc form (as * a left-deep conjunction). * * @param searchConditions (typically produced by analyzePredicate) * * @return ExprNodeDesc form of search conditions */ public ExprNodeDesc translateSearchConditions( List\u0026lt;IndexSearchCondition\u0026gt; searchConditions); } public class IndexSearchCondition { /** * Constructs a search condition, which takes the form * \u0026lt;pre\u0026gt;column-ref comparison-op constant-value\u0026lt;/pre\u0026gt;. * * @param columnDesc column being compared * * @param comparisonOp comparison operator, e.g. \u0026quot;=\u0026quot; * (taken from GenericUDFBridge.getUdfName()) * * @param constantDesc constant value to search for * * @Param comparisonExpr the original comparison expression */ public IndexSearchCondition( ExprNodeColumnDesc columnDesc, String comparisonOp, ExprNodeConstantDesc constantDesc, ExprNodeDesc comparisonExpr); } Filter Passing The approach for passing the filter down to the input format will follow a pattern similar to what is already in place for pushing column projections down.\n org.apache.hadoop.hive.serde2.ColumnProjectionUtils encapsulates the pushdown communication classes such as HiveInputFormat call ColumnProjectionUtils to set the projection pushdown property (READ_COLUMN_IDS_CONF_STR) on a jobConf before instantiating a RecordReader the factory method for the RecordReader calls ColumnProjectionUtils to access this property  For filter pushdown:\n HiveInputFormat sets properties hive.io.filter.text (string form) and hive.io.filter.expr.serialized (serialized form of ExprNodeDesc) in the job conf before calling getSplits as well as before instantiating a record reader the storage handler\u0026rsquo;s input format reads these properties and processes the filter expression there is a separate optimizer interaction for negotiation of filter decomposition (described in a later section)  Note that getSplits needs to be involved since the selectivity of the filter may prune away some of the splits which would otherwise be accessed. (In theory column projection could also influence the split boundaries, but we\u0026rsquo;ll leave that for a followup.)\nFilter Collection So, where will HiveInputFormat get the filter expression to be passed down? Again, we can start with the pattern for column projections:\n during optimization, org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory's ColumnPrunerTableScanProc populates the pushdown information in TableScanOperator later, HiveInputFormat.initColumnsNeeded retrieves this information from the TableScanOperator  For filter pushdown, the equivalent is TableScanPPD in org.apache.hadoop.hive.ql.ppd.OpProcFactory. Currently, it calls createFilter, which collapsed expressions into a single expression called condn, and then sticks that on a new FilterOperator. We can call condn.getExprString() and store the result on TableScanOperator.\nHive configuration parameter hive.optimize.ppd.storage can be used to enable or disable pushing filters down to the storage handler. This will be enabled by default. However, if hive.optimize.ppd is disabled, then this implicitly prevents pushdown to storage handlers as well.\nWe are starting with non-native tables only; we\u0026rsquo;ll revisit this for pushing filters down to indexes and builtin storage formats such as RCFile.\nFilter Decomposition Consider a filter like\nx \u0026gt; 3 AND upper(y) = 'XYZ' Suppose a storage handler is capable of implementing the range scanfor x \u0026gt; 3, but does not have a facility for evaluating {{upper(y) =\n\u0026lsquo;XYZ\u0026rsquo;}}. In this case, the optimal plan would involve decomposing the filter, pushing just the first part down into the storage handler, and\nleaving only the remainder for Hive to evaluate via its own executor.\nIn order for this to be possible, the storage handler needs to be able to negotiate the decomposition with Hive. This means that Hive gives\nthe storage handler the entire filter, and the storage handler passes back a \u0026ldquo;residual\u0026rdquo;: the portion that needs to be evaluated by Hive. A null residual indicates that the storage handler was able to deal with the entire filter on its own (in which case no FilterOperator is needed).\nIn order to support this interaction, we will introduce a new (optional) interface to be implemented by storage handlers:\npublic interface HiveStoragePredicateHandler { public DecomposedPredicate decomposePredicate( JobConf jobConf, Deserializer deserializer, ExprNodeDesc predicate); public static class DecomposedPredicate { public ExprNodeDesc pushedPredicate; public ExprNodeDesc residualPredicate; } } Hive\u0026rsquo;s optimizer (during predicate pushdown) calls the decomposePredicate method, passing in the full expression and receiving back the decomposition (or null to indicate that no pushdown was possible). The pushedPredicate gets passed back to the storage handler\u0026rsquo;s input format later, and the residualPredicate is attached to the FilterOperator.\nIt is assumed that storage handlers which are sophisticated enough to implement this interface are suitable for tight coupling to the ExprNodeDesc representation.\nAgain, this interface is optional, and pushdown is still possible even without it. If the storage handler does not implement this interface, Hive will always implement the entire expression in the FilterOperator, but it will still provide the expression to the storage handler\u0026rsquo;s input format; the storage handler is free to implement as much or as little as it wants.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/filterpushdowndev_27362092/","tags":null,"title":"Apache Hive : FilterPushdownDev"},{"categories":null,"contents":"Apache Hive : GenericUDAFCaseStudy Writing GenericUDAFs: A Tutorial User-Defined Aggregation Functions (UDAFs) are an excellent way to integrate advanced data-processing into Hive. Hive allows two varieties of UDAFs: simple and generic. Simple UDAFs, as the name implies, are rather simple to write, but incur performance penalties because of the use of Java Reflection, and do not allow features such as variable-length argument lists. Generic UDAFs allow all these features, but are perhaps not quite as intuitive to write as Simple UDAFs.\nThis tutorial walks through the development of the histogram() UDAF, which computes a histogram with a fixed, user-specified number of bins, using a constant amount of memory and time linear in the input size. It demonstrates a number of features of Generic UDAFs, such as a complex return type (an array of structures), and type checking on the input. The assumption is that the reader wants to write a UDAF for eventual submission to the Hive open-source project, so steps such as modifying the function registry in Hive and writing .q tests are also included. If you just want to write a UDAF, debug and deploy locally, see this page.\nNOTE: In this tutorial, we walk through the creation of a histogram() function. Starting with the 0.6.0 release of Hive, this appears as the built-in function histogram_numeric().\n Writing GenericUDAFs: A Tutorial  Preliminaries Writing the source  Overview Writing the resolver Writing the evaluator  getNewAggregationBuffer iterate terminatePartial merge terminate     Modifying the function registry Compiling and running Creating the tests   Checklist for open source submission Tips, Tricks, Best Practices  Resolver Interface Evolution    Preliminaries Make sure you have the latest Hive trunk by running svn up in your Hive directory. More detailed instructions on downloading and setting up Hive can be found at Getting Started . Your local copy of Hive should work by running build/dist/bin/hive from the Hive root directory, and you should have some tables of data loaded into your local instance for testing whatever UDAF you have in mind. For this example, assume that a table called normal exists with a single double column called val, containing a large number of random number drawn from the standard normal distribution.\nThe files we will be editing or creating are as follows, relative to the Hive root:\n         ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFHistogram.java the main source file, to be created by you.   ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java the function registry source file, to be edited by you to register our new histogram() UDAF into Hive\u0026rsquo;s built-in function list.   ql/src/test/queries/clientpositive/udaf_histogram.q a file of sample queries for testing histogram() on sample data, to be created by you.   ql/src/test/results/clientpositive/udaf_histogram.q.out the expected output from your sample queries, to be created by ant in a later step.   ql/src/test/results/clientpositive/show_functions.q.out the expected output from the SHOW FUNCTIONS Hive query. Since we\u0026rsquo;re adding a new histogram() function, this expected output will change to reflect the new function. This file will be modified by ant in a later step.    Writing the source This section gives a high-level outline of how to implement your own generic UDAF. For a concrete example, look at any of the existing UDAF sources present in ql/src/java/org/apache/hadoop/hive/ql/udf/generic/ directory.\nOverview At a high-level, there are two parts to implementing a Generic UDAF. The first is to write a resolver class, and the second is to create an evaluator class. The resolver handles type checking and operator overloading (if you want it), and helps Hive find the correct evaluator class for a given set of argument types. The evaluator class then actually implements the UDAF logic. Generally, the top-level UDAF class extends the abstract base class org.apache.hadoop.hive.ql.udf.GenericUDAFResolver2, and the evaluator class(es) are written as static inner classes.\nWriting the resolver The resolver handles type checking and operator overloading for UDAF queries. The type checking ensures that the user isn\u0026rsquo;t passing a double expression where an integer is expected, for example, and the operator overloading allows you to have different UDAF logic for different types of arguments.\nThe resolver class must extend org.apache.hadoop.hive.ql.udf.GenericUDAFResolver2 (see #Resolver Interface Evolution for backwards compatibility information). We recommend that you extend the AbstractGenericUDAFResolver base class in order to insulate your UDAF from future interface changes in Hive.\nLook at one of the existing UDAFs for the imports you will need.\n #!Java public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver { static final Log LOG = LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName()); @Override public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException { // Type-checking goes here! return new GenericUDAFHistogramNumericEvaluator(); } public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator { // UDAF logic goes here! } } The code above shows the basic skeleton of a UDAF. The first line sets up a Log object that you can use to write warnings and errors to be fed into the Hive log. The GenericUDAFResolver class has a single overridden method: getEvaluator, which receives information about how the UDAF is being invoked. Of most interest is info.getParameters(), which provides an array of type information objects corresponding to the SQL types of the invocation parameters. For the histogram UDAF, we want two parameters: the numeric column over which to compute the histogram, and the number of histogram bins requested. The very first thing to do is to check that we have exactly two parameters (lines 3-6 below). Then, we check that the first parameter has a primitive type, and not an array or map, for example (lines 9-13). However, not only do we want it to be a primitive type column, but we also want it to be numeric, which means that we need to throw an exception if a STRING type is given (lines 14-28). BOOLEAN is excluded because the \u0026ldquo;histogram\u0026rdquo; estimation problem can be solved with a simple COUNT() query. Lines 30-41 illustrate similar type checking for the second parameter to the histogram() UDAF – the number of histogram bins. In this case, we insist that the number of histogram bins is an integer.\n #!Java public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException { TypeInfo [] parameters = info.getParameters(); if (parameters.length != 2) { throw new UDFArgumentTypeException(parameters.length - 1, \u0026quot;Please specify exactly two arguments.\u0026quot;); } // validate the first parameter, which is the expression to compute over if (parameters[0].getCategory() != ObjectInspector.Category.PRIMITIVE) { throw new UDFArgumentTypeException(0, \u0026quot;Only primitive type arguments are accepted but \u0026quot; + parameters[0].getTypeName() + \u0026quot; was passed as parameter 1.\u0026quot;); } switch (((PrimitiveTypeInfo) parameters[0]).getPrimitiveCategory()) { case BYTE: case SHORT: case INT: case LONG: case FLOAT: case DOUBLE: break; case STRING: case BOOLEAN: default: throw new UDFArgumentTypeException(0, \u0026quot;Only numeric type arguments are accepted but \u0026quot; + parameters[0].getTypeName() + \u0026quot; was passed as parameter 1.\u0026quot;); } // validate the second parameter, which is the number of histogram bins if (parameters[1].getCategory() != ObjectInspector.Category.PRIMITIVE) { throw new UDFArgumentTypeException(1, \u0026quot;Only primitive type arguments are accepted but \u0026quot; + parameters[1].getTypeName() + \u0026quot; was passed as parameter 2.\u0026quot;); } if( ((PrimitiveTypeInfo) parameters[1]).getPrimitiveCategory() != PrimitiveObjectInspector.PrimitiveCategory.INT) { throw new UDFArgumentTypeException(1, \u0026quot;Only an integer argument is accepted as parameter 2, but \u0026quot; + parameters[1].getTypeName() + \u0026quot; was passed instead.\u0026quot;); } return new GenericUDAFHistogramNumericEvaluator(); } A form of operator overloading could be implemented here. Say you had two completely different histogram construction algorithms – one designed for working with integers only, and the other for double data types. You would then create two separate Evaluator inner classes, and depending on the type of the input expression, would return the correct one as the return value of the Resolver class.\nCAVEAT: The histogram function is supposed to be used in a manner resembling the following: SELECT histogram_numeric(age, 30) FROM employees;, which means estimate the distribution of employee ages using 30 histogram bins. However, within the resolver class, there is no way of telling if the second argument is a constant or an integer column with a whole bunch of different values. Thus, a pathologically twisted user could write something like: SELECT histogram_numeric(age, age) FROM employees;, assuming that age is an integer column. Of course, this makes no sense whatsoever, but it would validate correctly in the Resolver type-checking code above.\nWe\u0026rsquo;ll deal with this problem in the Evaluator.\nWriting the evaluator All evaluators must extend from the abstract base class org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator. This class provides a few abstract methods that must be implemented by the extending class. These methods establish the processing semantics followed by the UDAF. The following is the skeleton of an Evaluator class.\n #!Java public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator { // For PARTIAL1 and COMPLETE: ObjectInspectors for original data private PrimitiveObjectInspector inputOI; private PrimitiveObjectInspector nbinsOI; // For PARTIAL2 and FINAL: ObjectInspectors for partial aggregations (list of doubles) private StandardListObjectInspector loi; @Override public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException { super.init(m, parameters); // return type goes here } @Override public Object terminatePartial(AggregationBuffer agg) throws HiveException { // return value goes here } @Override public Object terminate(AggregationBuffer agg) throws HiveException { // final return value goes here } @Override public void merge(AggregationBuffer agg, Object partial) throws HiveException { } @Override public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException { } // Aggregation buffer definition and manipulation methods static class StdAgg implements AggregationBuffer { }; @Override public AggregationBuffer getNewAggregationBuffer() throws HiveException { } @Override public void reset(AggregationBuffer agg) throws HiveException { } } What do all these functions do? The following is a brief summary of each function, in (roughly) chronological order of being called. It\u0026rsquo;s very important to remember that the computation of your aggregation must be arbitrarily divisible over the data. Think of it like writing a divide-and-conquer algorithm where the partitioning of the data is completely out of your control and handled by Hive. More formally, given any subset of the input rows, you should be able to compute a partial result, and also be able to merge any pair of partial results into another partial result. This naturally makes it difficult to port over many existing algorithms, but should guarantee researchers jobs for quite some time.\n   Function Purpose     init Called by Hive to initialize an instance of your UDAF evaluator class.   getNewAggregationBuffer Return an object that will be used to store temporary aggregation results.   iterate Process a new row of data into the aggregation buffer   terminatePartial Return the contents of the current aggregation in a persistable way. Here persistable means the return value can only be built up in terms of Java primitives, arrays, primitive wrappers (e.g. Double), Hadoop Writables, Lists, and Maps. Do NOT use your own classes (even if they implement java.io.Serializable), otherwise you may get strange errors or (probably worse) wrong results.   merge Merge a partial aggregation returned by terminatePartial into the current aggregation   terminate Return the final result of the aggregation to Hive    For writing the histogram() function, the following is the strategy that was adopted.\ngetNewAggregationBuffer The aggregation buffer for a histogram is a list of (x,y) pairs that represent the histogram\u0026rsquo;s bin centers and heights. In addition, the aggregation buffer also stores two integers with the maximum number of bins (a user-specified parameter), and the current number of bins used. The aggregation buffer is initialized to a \u0026lsquo;not ready\u0026rsquo; state with the number of bins set to 0. This is because Hive makes no distinction between a constant parameter supplied to a UDAF and a column from a table; thus, we have no way of knowing how many bins the user wants in their histogram until the first call to iterate().\niterate The first thing we do in iterate() is to check whether the histogram object in our aggregation buffer is initialized. If it is not, we parse our the second argument to iterate(), which is the number of histogram bins requested by the user. We do this exactly once and initialize the histogram object. Note that error checking is performed here – if the user supplied a negative number or zero for the number of histogram bins, a HiveException is thrown at this point and computation terminates.\nNext, we parse out the actual input data item (a number) and add it to our histogram estimation in the aggregation buffer. See the GenericUDAFHistogramNumeric.java file for details on the heuristic used to construct a histogram.\nterminatePartial The current histogram approximation is serialized as a list of DoubleWritable objects. The first two doubles in the list indicate the maximum number of histogram bins specified by the user and number of bins current used. The remaining entries are (x,y) pairs from the current histogram approximation.\nmerge At this point, we have a (possibly uninitialized) histogram estimation, and have been requested to merge it with another estimation performed on a separate subset of the rows. If N is the number of histogram bins specified by the user, the current heuristic first builds a histogram with all 2N bins from both estimations, and then iteratively merges the closest pair of bins until only N bins remain.\nterminate The final return type from the histogram() function is an array of (x,y) pairs representing histogram bin centers and heights. These can be {{explode()}}ed into a separate table, or parsed using a script and passed to Gnuplot (for example) to visualize the histogram.\nModifying the function registry Once the code for the UDAF has been written and the source file placed in ql/src/java/org/apache/hadoop/hive/ql/udf/generic, it\u0026rsquo;s time to modify the function registry and incorporate the new function into Hive\u0026rsquo;s list of functions. This simply involves editing ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java to import your UDAF class and register it\u0026rsquo;s name.\nPlease note that you will have to run the following command to update the output of the show functions Hive call:\nant test -Dtestcase=TestCliDriver -Dqfile=show_functions.q -Doverwrite=true\nCompiling and running  ant package build/dist/bin/hive Creating the tests System-level tests consist of writing some sample queries that operate on sample data, generating the expected output from the queries, and making sure that things don\u0026rsquo;t break in the future in terms of expected output. Note that the expected output is passed through diff with the actual output from Hive, so nondeterministic algorithms will have to compute some sort of statistic and then only keep the most significant digits (for example).\nThese are the simple steps needed for creating test cases for your new UDAF/UDF:\n  Create a file in ql/src/test/queries/clientpositive/udaf_XXXXX.q where XXXXX is your UDAF\u0026rsquo;s name.\n  Put some queries in the .q file – hopefully enough to cover the full range of functionality and special cases.\n  For sample data, put your own in hive/data/files and load it using LOAD DATA LOCAL INPATH..., or reuse one of the files already there (grep for LOAD in the queries directory to see table names).\n  touch ql/src/test/results/clientpositive/udaf_XXXX.q.out\n  Run the following command to generate the output into the .q.out result file.\n   ant test -Dtestcase=TestCliDriver -Dqfile=udaf_XXXXX.q -Doverwrite=true Run the following command to make sure your test runs fine.   ant test -Dtestcase=TestCliDriver -Dqfile=udaf_XXXXX.q Checklist for open source submission  Create an account on the Hive JIRA (https:\u0026ndash;issues.apache.org-jira-browse-HIVE), create an issue for your new patch under the Query Processor component. Solicit discussion, incorporate feedback. Create your UDAF, integrate it into your local Hive copy. Run ant package from the Hive root to compile Hive and your new UDAF. Create .q tests and their corresponding .q.out output. Modify the function registry if adding a new function. Run ant checkstyle and examine build/checkstyle/checkstyle-errors.html, ensure that your source files conform to the Sun Java coding convention (with the 100 character line length exception). Run ant test, ensure that tests pass. Run svn up, ensure no conflicts with the main repository. Run svn add for whatever new files you have created. Ensure that you have added .q and .q.out tests. Ensure that you have run the .q tests for all new functionality. If adding a new UDAF, ensure that show_functions.q.out has been updated. Run ant test -Dtestcase=TestCliDriver -Dqfile=show_functions.q -Doverwrite=true to do this. Run svn diff \u0026gt; HIVE-NNNN.1.patch from the Hive root directory, where NNNN is the issue number the JIRA has assigned to you. Attach your file to the JIRA issue, describe your patch in the comments section. Ask for a code review in the comments. Click Submit patch on your issue after you have completed the steps above. It is also advisable to watch your issue to monitor new comments.  Tips, Tricks, Best Practices  Hive can have unexpected behavior sometimes. It is best to first run ant clean if you\u0026rsquo;re seeing something weird, ranging from unexplained exceptions to strings being incorrectly double-quoted. When serializing the aggregation buffer in a terminatePartial() call, if your UDAF only uses a few variables to represent the buffer (such as average), consider serializing them into a list of doubles, for example, instead of complicated named structures. Strongly cast generics wherever you can. Abstract core functionality from multiple UDAFs into its own class. Examples are histogram_numeric() and percentile_approx(), which both use the same core histogram estimation functionality. If you\u0026rsquo;re stuck looking for an algorithm to adapt to the terminatePartial/merge paradigm, divide-and-conquer and parallel algorithms are predictably good places to start. Remember that the tests do a diff on the expected and actual output, and fail if there is any difference at all. An example of where this can fail horribly is a UDAF like ngrams(), where the output is a list of sorted (word,count) pairs. In some cases, different sort implementations might place words with the same count at different positions in the output. Even though the output is correct, the test will fail. In these cases, it\u0026rsquo;s better to output (for example) only the counts, or some appropriate statistic on the counts, like the sum.  Resolver Interface Evolution Old interface org.apache.hadoop.hive.ql.udf.GenericUDAFResolver was deprecated as of the 0.6.0 release. The key difference between GenericUDAFResolver and GenericUDAFResolver2 interface is the fact that the latter allows the evaluator implementation to access extra information regarding the function invocation such as the presence of DISTINCT qualifier or the invocation with the wildcard syntax such as FUNCTION(). UDAFs that implement the deprecated GenericUDAFResolver interface will not be able to tell the difference between an invocation such as FUNCTION() or FUNCTION() since the information regarding specification of the wildcard is not available. Similarly, these implementations will also not be able to tell the difference between FUNCTION(EXPR) vs FUNCTION(DISTINCT EXPR) since the information regarding the presence of the DISTINCT qualifier is also not available.\nNote that while resolvers which implement the GenericUDAFResolver2 interface are provided the extra information regarding the presence of DISTINCT qualifier of invocation with the wildcard syntax, they can choose to ignore it completely if it is of no significance to them. The underlying data filtering to compute DISTINCT values is actually done by Hive\u0026rsquo;s core query processor and not by the evaluator or resolver; the information is provided to the resolver only for validation purposes. The AbstractGenericUDAFResolver base class offers an easy way to transition previously written UDAF implementations to migrate to the new resolver interface without having to re-write the implementation since the change from implementing GenericUDAFResolver interface to extending AbstractGenericUDAFResolver class is fairly minimal. (There may be issues with implementations that are part of an inheritance hierarchy since it may not be easy to change the base class.)\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/genericudafcasestudy_27362093/","tags":null,"title":"Apache Hive : GenericUDAFCaseStudy"},{"categories":null,"contents":"Apache Hive : GettingStarted Table of Contents\n  Installation and Configuration\n - [Running HiveServer2 and Beeline](#running-hiveserver2-and-beeline)+ [Requirements](#requirements)   Installing Hive from a Stable Release Building Hive from Source  Compile Hive on master Compile Hive on branch-1 Compile Hive Prior to 0.13 on Hadoop 0.20 Compile Hive Prior to 0.13 on Hadoop 0.23   Running Hive  Running Hive CLI Running HiveServer2 and Beeline Running HCatalog Running WebHCat (Templeton)   Configuration Management Overview Runtime Configuration Hive, Map-Reduce and Local-Mode Hive Logging  HiveServer2 Logs Audit Logs Perf Logger      DDL Operations\n Creating Hive Tables Browsing through Tables Altering and Dropping Tables Metadata Store    DML Operations\n  SQL Operations\n Example Queries  SELECTS and FILTERS GROUP BY JOIN MULTITABLE INSERT STREAMING      Simple Example Use Cases\n MovieLens User Ratings Apache Weblog Data    Installation and Configuration You can install a stable release of Hive by downloading a tarball, or you can download the source code and build Hive from that.\nRunning HiveServer2 and Beeline Requirements  Java 1.7\nNote: Hive versions 1.2 onward require Java 1.7 or newer. Hive versions 0.14 to 1.1 work with Java 1.6 as well. Users are strongly advised to start moving to Java 1.8 (see HIVE-8607). Hadoop 2.x (preferred), 1.x (not supported by Hive 2.0.0 onward).\nHive versions up to 0.13 also supported Hadoop 0.20.x, 0.23.x. Hive is commonly used in production Linux and Windows environment. Mac is a commonly used development environment. The instructions in this document are applicable to Linux and Mac. Using it on Windows would require slightly different steps.  Installing Hive from a Stable Release Start by downloading the most recent stable release of Hive from one of the Apache download mirrors (see Hive Releases).\nNext you need to unpack the tarball. This will result in the creation of a subdirectory named hive-x.y.z (where x.y.z is the release number):\n $ tar -xzvf hive-x.y.z.tar.gz Set the environment variable HIVE_HOME to point to the installation directory:\n $ cd hive-x.y.z $ export HIVE_HOME={{pwd}} Finally, add $HIVE_HOME/bin to your PATH:\n $ export PATH=$HIVE_HOME/bin:$PATH Building Hive from Source The Hive GIT repository for the most recent Hive code is located here: git clone \u0026lt;https://git-wip-us.apache.org/repos/asf/hive.git\u0026gt; (the master branch).\nAll release versions are in branches named \u0026ldquo;branch-0.#\u0026rdquo; or \u0026ldquo;branch-1.#\u0026rdquo; or the upcoming \u0026ldquo;branch-2.#\u0026rdquo;, with the exception of release 0.8.1 which is in \u0026ldquo;branch-0.8-r2\u0026rdquo;. Any branches with other names are feature branches for works-in-progress. See Understanding Hive Branches for details.\nAs of 0.13, Hive is built using Apache Maven.\nCompile Hive on master To build the current Hive code from the master branch:\n $ git clone https://git-wip-us.apache.org/repos/asf/hive.git $ cd hive $ mvn clean package -Pdist [-DskipTests -Dmaven.javadoc.skip=true] $ cd packaging/target/apache-hive-{version}-SNAPSHOT-bin/apache-hive-{version}-SNAPSHOT-bin $ ls LICENSE NOTICE README.txt RELEASE_NOTES.txt bin/ (all the shell scripts) lib/ (required jar files) conf/ (configuration files) examples/ (sample input and query files) hcatalog / (hcatalog installation) scripts / (upgrade scripts for hive-metastore) Here, {version} refers to the current Hive version.\nIf building Hive source using Maven (mvn), we will refer to the directory \u0026ldquo;/packaging/target/apache-hive-{version}-SNAPSHOT-bin/apache-hive-{version}-SNAPSHOT-bin\u0026rdquo; as for the rest of the page.\nCompile Hive on branch-1 In branch-1, Hive supports both Hadoop 1.x and 2.x. You will need to specify which version of Hadoop to build against via a Maven profile. To build against Hadoop 1.x use the profile hadoop-1; for Hadoop 2.x use hadoop-2. For example to build against Hadoop 1.x, the above mvn command becomes:\n $ mvn clean package -Phadoop-1,dist Compile Hive Prior to 0.13 on Hadoop 0.20 Prior to Hive 0.13, Hive was built using Apache Ant. To build an older version of Hive on Hadoop 0.20:\n $ svn co http://svn.apache.org/repos/asf/hive/branches/branch-{version} hive $ cd hive $ ant clean package $ cd build/dist # ls LICENSE NOTICE README.txt RELEASE_NOTES.txt bin/ (all the shell scripts) lib/ (required jar files) conf/ (configuration files) examples/ (sample input and query files) hcatalog / (hcatalog installation) scripts / (upgrade scripts for hive-metastore) If using Ant, we will refer to the directory \u0026ldquo;build/dist\u0026rdquo; as \u0026lt;install-dir\u0026gt;.\nCompile Hive Prior to 0.13 on Hadoop 0.23 To build Hive in Ant against Hadoop 0.23, 2.0.0, or other version, build with the appropriate flag; some examples below:\n $ ant clean package -Dhadoop.version=0.23.3 -Dhadoop-0.23.version=0.23.3 -Dhadoop.mr.rev=23 $ ant clean package -Dhadoop.version=2.0.0-alpha -Dhadoop-0.23.version=2.0.0-alpha -Dhadoop.mr.rev=23 Running Hive Hive uses Hadoop, so:\n you must have Hadoop in your path OR export HADOOP_HOME=\u0026lt;hadoop-install-dir\u0026gt;  In addition, you must use below HDFS commands to create /tmp and /user/hive/warehouse (aka hive.metastore.warehouse.dir) and set them chmod g+w before you can create a table in Hive.\n $ $HADOOP_HOME/bin/hadoop fs -mkdir /tmp $ $HADOOP_HOME/bin/hadoop fs -mkdir /user/hive/warehouse $ $HADOOP_HOME/bin/hadoop fs -chmod g+w /tmp $ $HADOOP_HOME/bin/hadoop fs -chmod g+w /user/hive/warehouse You may find it useful, though it\u0026rsquo;s not necessary, to set HIVE_HOME:\n $ export HIVE_HOME=\u0026lt;hive-install-dir\u0026gt; Running Hive CLI To use the Hive command line interface (CLI) from the shell:\n $ $HIVE_HOME/bin/hive Running HiveServer2 and Beeline Starting from Hive 2.1, we need to run the schematool command below as an initialization step. For example, we can use \u0026ldquo;derby\u0026rdquo; as db type.  $ $HIVE_HOME/bin/schematool -dbType \u0026lt;db type\u0026gt; -initSchema HiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline. HiveCLI is now deprecated in favor of Beeline, as it lacks the multi-user, security, and other capabilities of HiveServer2. To run HiveServer2 and Beeline from shell:\n $ $HIVE_HOME/bin/hiveserver2 $ $HIVE_HOME/bin/beeline -u jdbc:hive2://$HS2_HOST:$HS2_PORT Beeline is started with the JDBC URL of the HiveServer2, which depends on the address and port where HiveServer2 was started. By default, it will be (localhost:10000), so the address will look like jdbc:hive2://localhost:10000.\nOr to start Beeline and HiveServer2 in the same process for testing purpose, for a similar user experience to HiveCLI:\n $ $HIVE_HOME/bin/beeline -u jdbc:hive2:// Running HCatalog To run the HCatalog server from the shell in Hive release 0.11.0 and later:\n $ $HIVE_HOME/hcatalog/sbin/hcat_server.sh To use the HCatalog command line interface (CLI) in Hive release 0.11.0 and later:\n $ $HIVE_HOME/hcatalog/bin/hcat For more information, see HCatalog Installation from Tarball and HCatalog CLI in the HCatalog manual.\nRunning WebHCat (Templeton) To run the WebHCat server from the shell in Hive release 0.11.0 and later:\n $ $HIVE_HOME/hcatalog/sbin/webhcat_server.sh For more information, see WebHCat Installation in the WebHCat manual.\nConfiguration Management Overview  Hive by default gets its configuration from \u0026lt;install-dir\u0026gt;/conf/hive-default.xml The location of the Hive configuration directory can be changed by setting the HIVE_CONF_DIR environment variable. Configuration variables can be changed by (re-)defining them in \u0026lt;install-dir\u0026gt;/conf/hive-site.xml Log4j configuration is stored in \u0026lt;install-dir\u0026gt;/conf/hive-log4j.properties Hive configuration is an overlay on top of Hadoop – it inherits the Hadoop configuration variables by default. Hive configuration can be manipulated by:  Editing hive-site.xml and defining any desired variables (including Hadoop variables) in it Using the set command (see next section) Invoking Hive (deprecated), Beeline or HiveServer2 using the syntax:  $ bin/hive --hiveconf x1=y1 --hiveconf x2=y2 //this sets the variables x1 and x2 to y1 and y2 respectively $ bin/hiveserver2 \u0026ndash;hiveconf x1=y1 \u0026ndash;hiveconf x2=y2 //this sets server-side variables x1 and x2 to y1 and y2 respectively $ bin/beeline \u0026ndash;hiveconf x1=y1 \u0026ndash;hiveconf x2=y2 //this sets client-side variables x1 and x2 to y1 and y2 respectively.   Setting the HIVE_OPTS environment variable to \u0026ldquo;--hiveconf x1=y1 --hiveconf x2=y2\u0026rdquo; which does the same as above.    Runtime Configuration  Hive queries are executed using map-reduce queries and, therefore, the behavior of such queries can be controlled by the Hadoop configuration variables. The HiveCLI (deprecated) and Beeline command \u0026lsquo;SET\u0026rsquo; can be used to set any Hadoop (or Hive) configuration variable. For example:   beeline\u0026gt; SET mapred.job.tracker=myhost.mycompany.com:50030; beeline\u0026gt; SET -v; The latter shows all the current settings. Without the -v option only the variables that differ from the base Hadoop configuration are displayed.\nHive, Map-Reduce and Local-Mode Hive compiler generates map-reduce jobs for most queries. These jobs are then submitted to the Map-Reduce cluster indicated by the variable:\n mapred.job.tracker While this usually points to a map-reduce cluster with multiple nodes, Hadoop also offers a nifty option to run map-reduce jobs locally on the user\u0026rsquo;s workstation. This can be very useful to run queries over small data sets – in such cases local mode execution is usually significantly faster than submitting jobs to a large cluster. Data is accessed transparently from HDFS. Conversely, local mode only runs with one reducer and can be very slow processing larger data sets.\nStarting with release 0.7, Hive fully supports local mode execution. To enable this, the user can enable the following option:\n hive\u0026gt; SET mapreduce.framework.name=local; In addition, mapred.local.dir should point to a path that\u0026rsquo;s valid on the local machine (for example /tmp/\u0026lt;username\u0026gt;/mapred/local). (Otherwise, the user will get an exception allocating local disk space.)\nStarting with release 0.7, Hive also supports a mode to run map-reduce jobs in local-mode automatically. The relevant options are hive.exec.mode.local.auto, hive.exec.mode.local.auto.inputbytes.max, and hive.exec.mode.local.auto.tasks.max:\n hive\u0026gt; SET hive.exec.mode.local.auto=false; Note that this feature is disabled by default. If enabled, Hive analyzes the size of each map-reduce job in a query and may run it locally if the following thresholds are satisfied:\n The total input size of the job is lower than: hive.exec.mode.local.auto.inputbytes.max (128MB by default) The total number of map-tasks is less than: hive.exec.mode.local.auto.tasks.max (4 by default) The total number of reduce tasks required is 1 or 0.  So for queries over small data sets, or for queries with multiple map-reduce jobs where the input to subsequent jobs is substantially smaller (because of reduction/filtering in the prior job), jobs may be run locally.\nNote that there may be differences in the runtime environment of Hadoop server nodes and the machine running the Hive client (because of different jvm versions or different software libraries). This can cause unexpected behavior/errors while running in local mode. Also note that local mode execution is done in a separate, child jvm (of the Hive client). If the user so wishes, the maximum amount of memory for this child jvm can be controlled via the option hive.mapred.local.mem. By default, it\u0026rsquo;s set to zero, in which case Hive lets Hadoop determine the default memory limits of the child jvm.\nHive Logging Hive uses log4j for logging. By default logs are not emitted to the console by the CLI. The default logging level is WARN for Hive releases prior to 0.13.0. Starting with Hive 0.13.0, the default logging level is INFO.\nThe logs are stored in the directory /tmp/\u0026lt;*user.name*\u0026gt;:\n /tmp/\u0026lt;*user.name*\u0026gt;/hive.log\nNote: In local mode, prior to Hive 0.13.0 the log file name was \u0026ldquo;.log\u0026rdquo; instead of \u0026ldquo;hive.log\u0026rdquo;. This bug was fixed in release 0.13.0 (see HIVE-5528 and HIVE-5676).  To configure a different log location, set hive.log.dir in $HIVE_HOME/conf/hive-log4j.properties. Make sure the directory has the sticky bit set (chmod 1777 \u0026lt;*dir*\u0026gt;).\n hive.log.dir=*\u0026lt;other_location\u0026gt;*  If the user wishes, the logs can be emitted to the console by adding the arguments shown below:\n bin/hive --hiveconf hive.root.logger=INFO,console //for HiveCLI (deprecated) bin/hiveserver2 --hiveconf hive.root.logger=INFO,console  Alternatively, the user can change the logging level only by using:\n bin/hive --hiveconf hive.root.logger=INFO,DRFA //for HiveCLI (deprecated) bin/hiveserver2 --hiveconf hive.root.logger=INFO,DRFA  Another option for logging is TimeBasedRollingPolicy (applicable for Hive 1.1.0 and above, HIVE-9001) by providing DAILY option as shown below:\n bin/hive --hiveconf hive.root.logger=INFO,DAILY //for HiveCLI (deprecated) bin/hiveserver2 --hiveconf hive.root.logger=INFO,DAILY  Note that setting hive.root.logger via the \u0026lsquo;set\u0026rsquo; command does not change logging properties since they are determined at initialization time.\nHive also stores query logs on a per Hive session basis in /tmp/\u0026lt;user.name\u0026gt;/, but can be configured in hive-site.xml with the [hive.querylog.location](#hive-querylog-location) property. Starting with Hive 1.1.0, EXPLAIN EXTENDED output for queries can be logged at the INFO level by setting the [hive.log.explain.output](#hive-log-explain-output) property to true.\nLogging during Hive execution on a Hadoop cluster is controlled by Hadoop configuration. Usually Hadoop will produce one log file per map and reduce task stored on the cluster machine(s) where the task was executed. The log files can be obtained by clicking through to the Task Details page from the Hadoop JobTracker Web UI.\nWhen using local mode (using mapreduce.framework.name=local), Hadoop/Hive execution logs are produced on the client machine itself. Starting with release 0.6 – Hive uses the hive-exec-log4j.properties (falling back to hive-log4j.properties only if it\u0026rsquo;s missing) to determine where these logs are delivered by default. The default configuration file produces one log file per query executed in local mode and stores it under /tmp/\u0026lt;user.name\u0026gt;. The intent of providing a separate configuration file is to enable administrators to centralize execution log capture if desired (on a NFS file server for example). Execution logs are invaluable for debugging run-time errors.\nFor information about WebHCat errors and logging, see Error Codes and Responses and Log Files in the WebHCat manual.\nError logs are very useful to debug problems. Please send them with any bugs (of which there are many!) to hive-dev@hadoop.apache.org.\nFrom Hive 2.1.0 onwards (with HIVE-13027), Hive uses Log4j2\u0026rsquo;s asynchronous logger by default. Setting hive.async.log.enabled to false will disable asynchronous logging and fallback to synchronous logging. Asynchronous logging can give significant performance improvement as logging will be handled in a separate thread that uses the LMAX disruptor queue for buffering log messages. Refer to https://logging.apache.org/log4j/2.x/manual/async.html for benefits and drawbacks.\nHiveServer2 Logs HiveServer2 operation logs are available to clients starting in Hive 0.14. See HiveServer2 Logging for configuration.\nAudit Logs Audit logs are logged from the Hive metastore server for every metastore API invocation.\nAn audit log has the function and some of the relevant function arguments logged in the metastore log file. It is logged at the INFO level of log4j, so you need to make sure that the logging at the INFO level is enabled (see HIVE-3505). The name of the log entry is \u0026ldquo;HiveMetaStore.audit\u0026rdquo;.\nAudit logs were added in Hive 0.7 for secure client connections (HIVE-1948) and in Hive 0.10 for non-secure connections (HIVE-3277; also see HIVE-2797).\nPerf Logger In order to obtain the performance metrics via the PerfLogger, you need to set DEBUG level logging for the PerfLogger class (HIVE-12675). This can be achieved by setting the following in the log4j properties file.\nlog4j.logger.org.apache.hadoop.hive.ql.log.PerfLogger=DEBUG\nIf the logger level has already been set to DEBUG at root via hive.root.logger, the above setting is not required to see the performance logs.\nDDL Operations  The Hive DDL operations are documented in Hive Data Definition Language.\nCreating Hive Tables  hive\u0026gt; CREATE TABLE pokes (foo INT, bar STRING); creates a table called pokes with two columns, the first being an integer and the other a string.\n hive\u0026gt; CREATE TABLE invites (foo INT, bar STRING) PARTITIONED BY (ds STRING); creates a table called invites with two columns and a partition column called ds. The partition column is a virtual column. It is not part of the data itself but is derived from the partition that a particular dataset is loaded into.\nBy default, tables are assumed to be of text input format and the delimiters are assumed to be ^A(ctrl-a).\nBrowsing through Tables  hive\u0026gt; SHOW TABLES; lists all the tables.\n hive\u0026gt; SHOW TABLES '.*s'; lists all the table that end with \u0026rsquo;s'. The pattern matching follows Java regular expressions. Check out this link for documentation http://java.sun.com/javase/6/docs/api/java/util/regex/Pattern.html.\nhive\u0026gt; DESCRIBE invites; shows the list of columns.\nAltering and Dropping Tables Table names can be changed and columns can be added or replaced:\n hive\u0026gt; ALTER TABLE events RENAME TO 3koobecaf; hive\u0026gt; ALTER TABLE pokes ADD COLUMNS (new_col INT); hive\u0026gt; ALTER TABLE invites ADD COLUMNS (new_col2 INT COMMENT 'a comment'); hive\u0026gt; ALTER TABLE invites REPLACE COLUMNS (foo INT, bar STRING, baz INT COMMENT 'baz replaces new_col2'); Note that REPLACE COLUMNS replaces all existing columns and only changes the table\u0026rsquo;s schema, not the data. The table must use a native SerDe. REPLACE COLUMNS can also be used to drop columns from the table\u0026rsquo;s schema:\n hive\u0026gt; ALTER TABLE invites REPLACE COLUMNS (foo INT COMMENT 'only keep the first column'); Dropping tables:\n hive\u0026gt; DROP TABLE pokes; Metadata Store Metadata is in an embedded Derby database whose disk storage location is determined by the Hive configuration variable named javax.jdo.option.ConnectionURL. By default this location is ./metastore_db (see conf/hive-default.xml).\nRight now, in the default configuration, this metadata can only be seen by one user at a time.\nMetastore can be stored in any database that is supported by JPOX. The location and the type of the RDBMS can be controlled by the two variables javax.jdo.option.ConnectionURL and javax.jdo.option.ConnectionDriverName. Refer to JDO (or JPOX) documentation for more details on supported databases. The database schema is defined in JDO metadata annotations file package.jdo at src/contrib/hive/metastore/src/model.\nIn the future, the metastore itself can be a standalone server.\nIf you want to run the metastore as a network server so it can be accessed from multiple nodes, see Hive Using Derby in Server Mode.\nDML Operations The Hive DML operations are documented in Hive Data Manipulation Language.\nLoading data from flat files into Hive:\n hive\u0026gt; LOAD DATA LOCAL INPATH './examples/files/kv1.txt' OVERWRITE INTO TABLE pokes; Loads a file that contains two columns separated by ctrl-a into pokes table. \u0026lsquo;LOCAL\u0026rsquo; signifies that the input file is on the local file system. If \u0026lsquo;LOCAL\u0026rsquo; is omitted then it looks for the file in HDFS.\nThe keyword \u0026lsquo;OVERWRITE\u0026rsquo; signifies that existing data in the table is deleted. If the \u0026lsquo;OVERWRITE\u0026rsquo; keyword is omitted, data files are appended to existing data sets.\nNOTES:\n NO verification of data against the schema is performed by the load command. If the file is in hdfs, it is moved into the Hive-controlled file system namespace.\nThe root of the Hive directory is specified by the option hive.metastore.warehouse.dir in hive-default.xml. We advise users to create this directory before trying to create tables via Hive.   hive\u0026gt; LOAD DATA LOCAL INPATH './examples/files/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15'); hive\u0026gt; LOAD DATA LOCAL INPATH './examples/files/kv3.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-08'); The two LOAD statements above load data into two different partitions of the table invites. Table invites must be created as partitioned by the key ds for this to succeed.\n hive\u0026gt; LOAD DATA INPATH '/user/myname/kv2.txt' OVERWRITE INTO TABLE invites PARTITION (ds='2008-08-15'); The above command will load data from an HDFS file/directory to the table.\nNote that loading data from HDFS will result in moving the file/directory. As a result, the operation is almost instantaneous.\nSQL Operations The Hive query operations are documented in Select.\nExample Queries Some example queries are shown below. They are available in build/dist/examples/queries.\nMore are available in the Hive sources at ql/src/test/queries/positive.\nSELECTS and FILTERS  hive\u0026gt; SELECT a.foo FROM invites a WHERE a.ds='2008-08-15'; selects column \u0026lsquo;foo\u0026rsquo; from all rows of partition ds=2008-08-15 of the invites table. The results are not stored anywhere, but are displayed on the console.\nNote that in all the examples that follow, INSERT (into a Hive table, local directory or HDFS directory) is optional.\n hive\u0026gt; INSERT OVERWRITE DIRECTORY '/tmp/hdfs_out' SELECT a.* FROM invites a WHERE a.ds='2008-08-15'; selects all rows from partition ds=2008-08-15 of the invites table into an HDFS directory. The result data is in files (depending on the number of mappers) in that directory.\nNOTE: partition columns if any are selected by the use of *. They can also be specified in the projection clauses.\nPartitioned tables must always have a partition selected in the WHERE clause of the statement.\n hive\u0026gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/local_out' SELECT a.* FROM pokes a; selects all rows from pokes table into a local directory.\n hive\u0026gt; INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a; hive\u0026gt; INSERT OVERWRITE TABLE events SELECT a.* FROM profiles a WHERE a.key \u0026lt; 100; hive\u0026gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/reg_3' SELECT a.* FROM events a; hive\u0026gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_4' select a.invites, a.pokes FROM profiles a; hive\u0026gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT COUNT(*) FROM invites a WHERE a.ds='2008-08-15'; hive\u0026gt; INSERT OVERWRITE DIRECTORY '/tmp/reg_5' SELECT a.foo, a.bar FROM invites a; hive\u0026gt; INSERT OVERWRITE LOCAL DIRECTORY '/tmp/sum' SELECT SUM(a.pc) FROM pc1 a; selects the sum of a column. The avg, min, or max can also be used. Note that for versions of Hive which don\u0026rsquo;t include HIVE-287, you\u0026rsquo;ll need to use COUNT(1) in place of COUNT(*).\nGROUP BY  hive\u0026gt; FROM invites a INSERT OVERWRITE TABLE events SELECT a.bar, count(*) WHERE a.foo \u0026gt; 0 GROUP BY a.bar; hive\u0026gt; INSERT OVERWRITE TABLE events SELECT a.bar, count(*) FROM invites a WHERE a.foo \u0026gt; 0 GROUP BY a.bar; Note that for versions of Hive which don\u0026rsquo;t include HIVE-287, you\u0026rsquo;ll need to use COUNT(1) in place of COUNT(*).\nJOIN  hive\u0026gt; FROM pokes t1 JOIN invites t2 ON (t1.bar = t2.bar) INSERT OVERWRITE TABLE events SELECT t1.bar, t1.foo, t2.foo; MULTITABLE INSERT  FROM src INSERT OVERWRITE TABLE dest1 SELECT src.* WHERE src.key \u0026lt; 100 INSERT OVERWRITE TABLE dest2 SELECT src.key, src.value WHERE src.key \u0026gt;= 100 and src.key \u0026lt; 200 INSERT OVERWRITE TABLE dest3 PARTITION(ds='2008-04-08', hr='12') SELECT src.key WHERE src.key \u0026gt;= 200 and src.key \u0026lt; 300 INSERT OVERWRITE LOCAL DIRECTORY '/tmp/dest4.out' SELECT src.value WHERE src.key \u0026gt;= 300; STREAMING  hive\u0026gt; FROM invites a INSERT OVERWRITE TABLE events SELECT TRANSFORM(a.foo, a.bar) AS (oof, rab) USING '/bin/cat' WHERE a.ds \u0026gt; '2008-08-09'; This streams the data in the map phase through the script /bin/cat (like Hadoop streaming).\nSimilarly – streaming can be used on the reduce side (please see the Hive Tutorial for examples).\nSimple Example Use Cases MovieLens User Ratings First, create a table with tab-delimited text file format:\nCREATE TABLE u_data ( userid INT, movieid INT, rating INT, unixtime STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE; Then, download the data files from MovieLens 100k on the GroupLens datasets page (which also has a README.txt file and index of unzipped files):\nwget http://files.grouplens.org/datasets/movielens/ml-100k.zip or:\ncurl --remote-name http://files.grouplens.org/datasets/movielens/ml-100k.zip Note: If the link to GroupLens datasets does not work, please report it on HIVE-5341 or send a message to the user@hive.apache.org mailing list.\nUnzip the data files:\nunzip ml-100k.zip And load u.data into the table that was just created:\nLOAD DATA LOCAL INPATH '\u0026lt;path\u0026gt;/u.data' OVERWRITE INTO TABLE u_data; Count the number of rows in table u_data:\nSELECT COUNT(*) FROM u_data; Note that for older versions of Hive which don\u0026rsquo;t include HIVE-287, you\u0026rsquo;ll need to use COUNT(1) in place of COUNT(*).\nNow we can do some complex data analysis on the table u_data:\nCreate weekday_mapper.py:\nimport sys import datetime for line in sys.stdin: line = line.strip() userid, movieid, rating, unixtime = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([userid, movieid, rating, str(weekday)]) Use the mapper script:\nCREATE TABLE u_data_new ( userid INT, movieid INT, rating INT, weekday INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'; add FILE weekday_mapper.py; INSERT OVERWRITE TABLE u_data_new SELECT TRANSFORM (userid, movieid, rating, unixtime) USING 'python weekday_mapper.py' AS (userid, movieid, rating, weekday) FROM u_data; SELECT weekday, COUNT(*) FROM u_data_new GROUP BY weekday; Note that if you\u0026rsquo;re using Hive 0.5.0 or earlier you will need to use COUNT(1) in place of COUNT(*).\nApache Weblog Data The format of Apache weblog is customizable, while most webmasters use the default.\nFor default Apache weblog, we can create a table with the following command.\nMore about RegexSerDe can be found here in HIVE-662 and HIVE-1719.\nCREATE TABLE apachelog ( host STRING, identity STRING, user STRING, time STRING, request STRING, status STRING, size STRING, referer STRING, agent STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe' WITH SERDEPROPERTIES ( \u0026quot;input.regex\u0026quot; = \u0026quot;([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\u0026quot;]*|\\\u0026quot;[^\\\u0026quot;]*\\\u0026quot;) (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\u0026quot;]*|\\\u0026quot;.*\\\u0026quot;) ([^ \\\u0026quot;]*|\\\u0026quot;.*\\\u0026quot;))?\u0026quot; ) STORED AS TEXTFILE; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/gettingstarted_27362090/","tags":null,"title":"Apache Hive : GettingStarted"},{"categories":null,"contents":"Apache Hive : GettingStarted EclipseSetup Page does not apply to trunk\nThe following page only applies to branch-0.12 and earlier. For trunk see HiveDeveloperFAQ\nAfter checking out the source code run the following command from the top-level directory:\n$ ant clean package eclipse-files Now open up Eclipse and do the following:\n File-\u0026gt;Import-\u0026gt;General-\u0026gt;Existing Projects Into Workspace-\u0026gt;Select root directory (point to )  Make sure that Eclipse Java Compiler is in 1.6 compatibility mode:\n Project-\u0026gt;Properties-\u0026gt;Java Compiler-\u0026gt;(Check)Enable project specific settings-\u0026gt;(Change to 1.6)Compiler compliance level  This should import the project and the launch configurations. You can run the Hive CLI from within Eclipse by right clicking on the HiveCLI launch configuration and selecting \u0026ldquo;Run\u0026rdquo;. You can also look for JUnit launch configurations by checking:\n Run-\u0026gt;Run Configurations-\u0026gt;JUnit.  There should be several configurations including TestCliDriver, TestJdbc, and TestHive.\nThen, you should be able to run all the unit tests by Run-\u0026gt;Run.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/gettingstarted-eclipsesetup_27362091/","tags":null,"title":"Apache Hive : GettingStarted EclipseSetup"},{"categories":null,"contents":"Apache Hive : GroupByWithRollup Group By With Rollup References:\nOriginal design doc\nHIVE-2397\n Group By With Rollup  Terminology Design  Map Aggr \u0026amp; No Skew: Map Aggr \u0026amp; Skew No Map Aggr \u0026amp; No Skew \u0026amp; No Rollup No Map Aggr \u0026amp; No Skew \u0026amp; With Rollup No Map Aggr \u0026amp; Skew \u0026amp; (No Distinct or No Rollup) No Map Aggr \u0026amp; Skew \u0026amp; Distinct \u0026amp; Rollup      Terminology  (No) Map Aggr: Shorthand for whether the configuration variable hive.map.aggr is set to true or false, meaning mapside aggregation is allowed or not respectively. (No) Skew: Shorthand for whether the configuration variable hive.groupby.skewindata is set to true or false, meaning some columns have a disproportionate number of distinct values.  Design Before the rollup option was added to the group by operator, there were 4 different plans based on the 4 possible combinations of (No) Map Aggr and (No) Skew. These were built on and expanded to 6 plans as described below:\nMap Aggr \u0026amp; No Skew: This plan remains the same, only the implementation of the map-side hash-based aggregation operator was modified to handle the extra rows needed for rollup. The plan is as follows:\nMapper:\n*Hash-based group by operator to perform partial aggregations\n*Reduce sink operator, performs some partial aggregations\nReducer:\n*MergePartial (list-based) group by operator to perform final aggregations\nMap Aggr \u0026amp; Skew Again, this plan remains the same, only the implementation of the map-side hash-based aggregation operator was modified to handle the extra rows needed for rollup. The plan is as follows:\nMapper 1:\n*Hash-based group by operator to perform partial aggregations\n*Reduce sink operator to spray by the group by and distinct keys (if there is a distinct key) or a random number otherwise\nReducer 1:\n*Partials (list-based) group by operator to perform further partial aggregations\nMapper 2:\n*Reduce sink operator, performs some partial aggregations\nReducer 2:\n*Final (list-based) group by operator to perform final aggregations\nNote that if there are no group by keys or distinct keys, Reducer 1 and Mapper 2 are removed from the plan and the reduce sink operator in Mapper 1 does not spray\nNo Map Aggr \u0026amp; No Skew \u0026amp; No Rollup This plan is the case from pre-rollup version of group by where there is no Map Aggr and No Skew, I included it for completeness as it remains an option if rollup is not used. The plan is as follows:\nMapper:\n*Reduce sink operator, performs some partial aggregations\nReducer:\n*Complete (list-based) group by operator to perform all aggregations\nNo Map Aggr \u0026amp; No Skew \u0026amp; With Rollup The plan is as follows:\nMapper 1:\n*Reduce sink operator, does not perform any partial aggregations\nReducer 1:\n*Hash-based group by operator, much like the one used in the mappers of previous cases\nMapper 2:\n*Reduce sink operator, performs some partial aggregations\nReducer 2:\n*MergePartial (list-based) group by operator to perform remaining aggregations\nNo Map Aggr \u0026amp; Skew \u0026amp; (No Distinct or No Rollup) This plan is the same as was used for the case of No Map Aggr and Skew in the pre-rollup version of group by, for this cads when rollup is not used, or none of the aggregations make use of a distinct key. The implementation of the list-based group by operator was modified to handle the extra rows required for rollup if rollup is being used. The plan is as follows:\nMapper 1:\n*Reduce sink operator to spray by the group by and distinct keys (if there is a distinct key) or a random number otherwise\nReducer 1:\n*Partial1 (list-based) group by operator to perform partial aggregations, it makes use of the new list-based group by operator implementation for rollup if necessary\nMapper 2:\n*Reduce sink operator, performs some partial aggregations\nReducer 2:\n*Final (list-based) group by operator to perform remaining aggregations\nNo Map Aggr \u0026amp; Skew \u0026amp; Distinct \u0026amp; Rollup This plan is used when there is No Map Aggr and Skew and there is an aggregation that involves a distinct key and rollup is being used. The plan is as follows:\nMapper 1:\n*Reduce sink operator to spray by the group by and distinct keys (if there is a distinct key) or a random number otherwise\nReducer 1:\n*Hash-based group by operator, much like the one used in the mappers of previous cases\nMapper 2:\n*Reduce sink operator to spray by the group by and distinct keys (if there is a distinct key) or a random number otherwise\nReducer 2:\n*Partials (list-based) group by operator to perform further partial aggregations\nMapper 3:\n*Reduce sink operator, performs some partial aggregations\nReducer 3:\n*Final (list-based) group by operator to perform final aggregations\nNote that if there are no group by keys or distinct keys, Reducer 2 and Mapper 3 are removed from the plan and the reduce sink operator in Mapper 2 does not spray. Also, note that the reason for Mapper 2 spraying is that if the skew in the data existed in a column that is not immediately nulled by the rollup (e.g. if we the group by keys are columns g1, g2, g3 in that order, we are concerned with the case where the skew exists in column g1 or g2) the skew may continue to exist after the hash aggregation, so we spray.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/groupbywithrollup_27826238/","tags":null,"title":"Apache Hive : GroupByWithRollup"},{"categories":null,"contents":"Apache Hive : Hadoop-compatible Input-Output Format for Hive Overview This is a proposal for adding API to Hive which allows reading and writing using a Hadoop compatible API. Specifically, the interfaces being implemented are:\n InputFormat: http://hadoop.apache.org/docs/mapreduce/r0.21.0/api/org/apache/hadoop/mapreduce/InputFormat.html OutputFormat: http://hadoop.apache.org/docs/mapreduce/r0.21.0/api/org/apache/hadoop/mapreduce/OutputFormat.html  The classes will be named HiveApiInputFormat and HiveApiOutputFormat.\nSee HIVE-3752 for discussion of this proposal.\nInputFormat (reading from Hive) Usage:\n Create a HiveInputDescription object. Fill it with information about the table to read from (with database, partition, columns). Initialize HiveApiInputFormat with the information. Go to town using HiveApiInputFormat with your Hadoop-compatible reading system.  More detailed information:\n The HiveInputDescription describes the database, table and columns to select. It also has a partition filter property that can be used to read from only the partitions that match the filter statement. HiveApiInputFormat supports reading from multiple tables by having a concept of profiles. Each profile stores its input description in a separate section, and the HiveApiInputFormat has a member which tells it which profile to read from. When initializing the input data in HiveApiInputFormat you can pair it with a profile. If no profile is selected then a default profile is used.  Future plans:\n Lots of performance work. Expose more direct byte[] sort of semantics. Filtering of rows returned.  OutputFormat (writing to Hive) Usage:\n Create a HiveOutputDescription object. Fill it with information about the table to write to (with database and partition). Initialize HiveApiOutputFormat with the information. Go to town using HiveApiOutputFormat with your Hadoop-compatible writing system.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hadoop-compatible-input-output-format-for-hive_30745805/","tags":null,"title":"Apache Hive : Hadoop-compatible Input-Output Format for Hive"},{"categories":null,"contents":"Apache Hive : Hbase execution plans for RawStore partition filter condition (Apologies for this doc being organized properly, I thought something is better than nothing - Thejas)\nThis is part of metastore on hbase work - HIVE-9452 Use HBase to store Hive metadata Open\nFunctionality needed\nRawStore functions that support partition filtering are the following -\n getPartitionsByExpr getPartitionsByFilter (takes filter string as argument, used from hcatalog)  We need to generate a query execution plan in terms of Hbase scan api calls for a given filter condition.\nNotes about the api to be supported getPartitionsByExpr - Current partition expression evaluation path ExprNodeGenericFuncDesc represents the partition filter expression in the plan\n It is serialized into byte[] and Metastore api is invoked with the byte[]. ObjectStore processing of expression - deserializes the byte[], prints it to convert it to Filter string Converts Filter string to ExpressionTree using parser (Filter.g) Walk ExpressionTree to create sql query (in direct sql)  getPartitionsByFilter - Evaluation of it is similar, it just skips the steps required to create the filter string. We certainly need the ability to work with filter string to support this function.\nWhy do we convert from ExprNodeGenericFuncDesc to kryo serialized byte[] and not to the filter string ?\nFilter expressions supported currently\nLeaf Operators : =, \u0026gt;, \u0026lt;, \u0026lt;=, \u0026gt;=, LIKE, !=\nLogical Operators : AND, OR\nPartition table in hbase\nPartition information is stored in with the key as a delimited string consisting of - db name, table name, partition values\nThe value contains rest of the partition information. (side note: do we need the partition values in the value part?)\nImplementation Serialization format of partition table key in hbase\nDesirable properties for key serialization format -\n It should be possible to perform filter operations on the keys without deserializing the fields (LIKE operator is not common, so its ok if we have to deserialize for that one) The real order for the partition keys and the byte order for the keys should match It should be possible to efficiently extract the relevant portion of the key for filters. ie, It should be possible to find the begin and end of bytes representing a partition value without checking every preceding byte.  BinarySortableSerDe satisfies these requirements except for number 3. Meeting requirement 3 might need some index information to be stored in end of the serialized key.\nLimitations with current storage format (no secondary keys)\nIf there are multiple partition keys for a table, and partition filter condition does not have a condition on the first partition key, we would end up scanning all partitions for the table to find the matches. For this case, we need support for secondary indexes on the table. While we could implement this using a second table, the lack of support for atomic operations across rows/tables is a problem. We would need some level of transaction support in hbase to be able to create secondary indexes reliably.\nFiltering the partitions\nThe hbase api’s used will depend on the filtering condition -\n For simple partition filtering conditions on initial partition column, that check for a particular partition or a range of partition, we can convert them into a simple Hbase Scan operation without any Filter (new Scan(byte[] startRow, byte[] stopRow)) In case of more complex queries involving additional partition columns, we need to use a scan filter with conditions on remaining columns as well. ie, new Scan(byte[] startRow, byte[] stopRow) + Scan.setFilter(..) If there are no conditions on the first partition column, then all partitions on the table would need to be scanned. In that case, start and end rows will be based only on the db+table prefix of the key.  Filters with top level “OR” conditions - Each of the conditions under OR should be evaluated to see which of the above api call pattern suits them. If any one of the conditions requires no 3 call pattern, it makes sense to represent the entire filter condition using api call pattern 3.\nExamples of conversion of query plan to hbase api calls\n merge function below does a set-union p1 represents the first partition column The scan(startRow, endRow) scans from startRow to row before endRow. ie, it represents rows where (r \u0026gt;= startRow and r \u0026lt; endRow). But it can be made to represent (r \u0026gt; startRow) by adding a zero byte to startRow, and made to represent (r \u0026lt;= endRow) by adding zero byte to endRow. ie, the plans for \u0026gt;= and \u0026gt; are similar, \u0026lt;= and = are similar. All keys corresponding to a partitions of a table have a common prefix of “db + tablename”. That is referred to as “X” in following examples.   | Filter expression | HBase calls | | p1 \u0026gt; 10 and p1 \u0026lt; 20 | Scan(X10+, X20) | | p1 = 10 (if single partition column) | Scan(X10, X10+). Optimized? : Get(X10) | | Similar case as above, if all partition columns are specified | | | p1 = 10 (multiple partition column) | Scan(X10, X+) | | p1 = 9 or p1 = 10 | merge( get(X9), get(X10)) | | p1 \u0026gt; 10 or p1 \u0026lt; 20 | merge(scan(X10, X+), scan(X ,X20)) | | (condition on columns other than first partition column) : condition1 | Scan(X, X+).setFilter(genFilter(condition1)) | | p1 \u0026gt; 10 and condition1 | scan(X10, X+).setFilter(genFilter(condition1)) | | p1 \u0026lt; 20 and condition1 | Scan(X , X20).setFilter(genFilter(condition1)) | | p1 \u0026gt; 10 and p1 \u0026gt; 20 and p1 \u0026lt; 30 and p1 \u0026lt; 40 | Scan(X20+, X30) | | p1 \u0026gt; 10 and (p1 \u0026gt; 20 or c1 = 5) =\u0026gt;(p1 \u0026gt; 10 and p1 \u0026gt; 20) or (p1 \u0026gt; 10 and c1 =5) | merge(Scan(X20+, X+), Scan(X10+,X+).setFilter(genFilter(c1 = 5))) | | (special case with OR condition, if one of the conditions results in full table scan): condition1 or condition2 | Scan(X).filter(getCombinedFilter(condition1, condition2) (ie, convert to a full table scan with filter) | | (general case with OR condition): condition1 or condition2 | merge( getResult(condition1), getResult(condition2)) | | c1 and (c2 or c3) | (c1 and c2) or (c1 and c3) | | (c1 or c2) and (c3 or c4) | (c1 and c3) or (c2 and c3) or (c1 and c4) or (c2 and c4) |\n Relevant classes :\nInput:\nExpressionTree (existing) - TreeNodes for AND/OR expressions. Leaf Node for leaf expressions with =,\u0026lt; \u0026hellip;\nOutput:\n public static abstract class FilterPlan {\n abstract FilterPlan and(FilterPlan other);\n abstract FilterPlan or(FilterPlan other);\n abstract ListgetPlans();\n }\n// represents a union of multiple ScanPlan\nMultiScanPlan extends FilterPlan\nScanPlan extends FilterPlan\n // represent Scan start\n private ScanMarker startMarker ;\n // represent Scan end\n private ScanMarker endMarker ;\n private ScanFilter filter;\npublic FilterPlan and(FilterPlan other) {\n// calls this.and(otherScanPlan) on each scan plan in other\n}\nprivate ScanPlan and(ScanPlan other) {\n // combines start marker and end marker and filters of this and other\n}\npublic FilterPlan or(FilterPlan other) {\n // just create a new FilterPlan from other, with this additional plan\n}\nPartitionFilterGenerator -\n /**\n * Visitor for ExpressionTree.\n * It first generates the ScanPlan for the leaf nodes. The higher level nodes are\n * either AND or OR operations. It then calls FilterPlan.and and FilterPlan.or with\n * the child nodes to generate the plans for higher level nodes.\n */\nInitial implementation: Convert from from ExpressionTree to Hbase filter, thereby implementing both getPartitionsByFilter and getPartitionsByExpr\nA new custom Filter class implementation needs to be created. Filter class implements Writable, and the hbase expression to be evaluated is serialized\nWe can potentially create the filter directly from ExprNodeGenericFuncDesc in case of the new fastpath config is set.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hbase-execution-plans-for-rawstore-partition-filter-condition_55151993/","tags":null,"title":"Apache Hive : Hbase execution plans for RawStore partition filter condition"},{"categories":null,"contents":"Apache Hive : HBaseBulkLoad Hive HBase Bulk Load  Hive HBase Bulk Load  Overview Decide on Target HBase Schema Estimate Resources Needed Add necessary JARs Prepare Range Partitioning Prepare Staging Location Sort Data Run HBase Script Map New Table Back Into Hive Followups Needed    This page explains how to use Hive to bulk load data into a new (empty) HBase table per HIVE-1295. (If you\u0026rsquo;re not using a build which contains this functionality yet, you\u0026rsquo;ll need to build from source and make sure this patch and HIVE-1321 are both applied.)\nOverview Ideally, bulk load from Hive into HBase would be part of HBaseIntegration, making it as simple as this:\nCREATE TABLE new_hbase_table(rowkey string, x int, y int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:x,cf:y\u0026quot;); SET hive.hbase.bulk=true; INSERT OVERWRITE TABLE new_hbase_table SELECT rowkey_expression, x, y FROM ...any_hive_query...; However, things aren\u0026rsquo;t quite as straightforward as that yet. Instead, a procedure involving a series of SQL commands is required. It should still be a lot easier and more flexible than writing your own map/reduce program, and over time we hope to enhance Hive to move closer to the ideal.\nThe procedure is based on underlying HBase recommendations, and involves the following steps:\n Decide how you want the data to look once it has been loaded into HBase. Decide on the number of reducers you\u0026rsquo;re planning to use for parallelizing the sorting and HFile creation. This depends on the size of your data as well as cluster resources available. Run Hive sampling commands which will create a file containing \u0026ldquo;splitter\u0026rdquo; keys which will be used for range-partitioning the data during sort. Prepare a staging location in HDFS where the HFiles will be generated. Run Hive commands which will execute the sort and generate the HFiles. (Optional: if HBase and Hive are running in different clusters, distcp the generated files from the Hive cluster to the HBase cluster.) Run HBase script loadtable.rb to move the files into a new HBase table. (Optional: register the HBase table as an external table in Hive so you can access it from there.)  The rest of this page explains each step in greater detail.\nDecide on Target HBase Schema Currently there are a number of constraints here:\n The target table must be new (you can\u0026rsquo;t bulk load into an existing table) The target table can only have a single column family (HBASE-1861) The target table cannot be sparse (every row will have the same set of columns); this should be easy to fix by either allowing a MAP value to be read from Hive, and/or by allowing rows to be read from Hive in pivoted form (one row per HBase cell)  Besides dealing with these constraints, probably the most important work here is deciding on how you want to assign an HBase row key to each row coming from Hive. To avoid inconsistencies between lexical and binary comparators, it is simplest to design a string row key and use it consistently all the way through. If you want to combine multiple columns into the key, use Hive\u0026rsquo;s string concat expression for this purpose. You can use CREATE VIEW to tack on your rowkey logically without having to update any existing data in Hive.\nEstimate Resources Needed TBD: provide some example numbers based on Facebook experiments; also reference Hadoop Terasort\nAdd necessary JARs You will need to add a couple jar files to your path. First, put them in DFS:\nhadoop dfs -put /usr/lib/hive/lib/hbase-VERSION.jar /user/hive/hbase-VERSION.jar hadoop dfs -put /usr/lib/hive/lib/hive-hbase-handler-VERSION.jar /user/hive/hive-hbase-handler-VERSION.jar Then add them to your hive-site.xml:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.aux.jars.path\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/user/hive/hbase-VERSION.jar,/user/hive/hive-hbase-handler-VERSION.jar\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Prepare Range Partitioning In order to perform a parallel sort on the data, we need to range-partition it. The idea is to divide the space of row keys up into nearly equal-sized ranges, one per reducer which will be used in the parallel sort. The details will vary according to your source data, and you may need to run a number of exploratory Hive queries in order to come up with a good enough set of ranges. Here\u0026rsquo;s one example:\nadd jar lib/hive-contrib-0.7.0.jar; set mapred.reduce.tasks=1; create temporary function row_sequence as 'org.apache.hadoop.hive.contrib.udf.UDFRowSequence'; select transaction_id from (select transaction_id from transactions tablesample(bucket 1 out of 10000 on transaction_id) s order by transaction_id limit 10000000) x where (row_sequence() % 910000)=0 order by transaction_id limit 11; This works by ordering all of the rows in a .01% sample of the table (using a single reducer), and then selecting every nth row (here n=910000). The value of n is chosen by dividing the total number of rows in the sample by the desired number of ranges, e.g. 12 in this case (one more than the number of partitioning keys produced by the LIMIT clause). The assumption here is that the distribution in the sample matches the overall distribution in the table; if this is not the case, the resulting partition keys will lead to skew in the parallel sort.\nOnce you have your sampling query defined, the next step is to save its results to a properly formatted file which will be used in a subsequent step. To do this, run commands like the following:\ncreate external table hb_range_keys(transaction_id_range_start string) row format serde 'org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe' stored as inputformat 'org.apache.hadoop.mapred.TextInputFormat' outputformat 'org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat' location '/tmp/hb_range_keys'; insert overwrite table hb_range_keys select transaction_id from (select transaction_id from transactions tablesample(bucket 1 out of 10000 on transaction_id) s order by transaction_id limit 10000000) x where (row_sequence() % 910000)=0 order by transaction_id limit 11; The first command creates an external table defining the format of the file to be created; be sure to set the serde and inputformat/outputformat exactly as specified.\nThe second command populates it (using the sampling query previously defined). Usage of ORDER BY guarantees that a single file will be produced in directory /tmp/hb_range_keys. The filename is unknown, but it is necessary to reference the file by name later, so run a command such as the following to copy it to a specific name:\ndfs -cp /tmp/hb_range_keys/* /tmp/hb_range_key_list; Prepare Staging Location The sort is going to produce a lot of data, so make sure you have sufficient space in your HDFS cluster, and choose the location where the files will be staged. We\u0026rsquo;ll use /tmp/hbsort in this example.\nThe directory does not actually need to exist (it will be automatically created in the next step), but if it does exist, it should be empty.\ndfs -rmr /tmp/hbsort; dfs -mkdir /tmp/hbsort; Sort Data Now comes the big step: running a sort over all of the data to be bulk loaded. Make sure that your Hive instance has the HBase jars available on its auxpath.\nset hive.execution.engine=mr; set mapred.reduce.tasks=12; set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner; set total.order.partitioner.path=/tmp/hb_range_key_list; set hfile.compression=gz; create table hbsort(transaction_id string, user_name string, amount double, ...) stored as INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.hbase.HiveHFileOutputFormat' TBLPROPERTIES ('hfile.family.path' = '/tmp/hbsort/cf'); insert overwrite table hbsort select transaction_id, user_name, amount, ... from transactions cluster by transaction_id; The CREATE TABLE creates a dummy table which controls how the output of the sort is written. Note that it uses HiveHFileOutputFormat to do this, with the table property hfile.family.path used to control the destination directory for the output. Again, be sure to set the inputformat/outputformat exactly as specified. In the example above, we select gzip (gz) compression for the result files; if you don\u0026rsquo;t set the hfile.compression parameter, no compression will be performed. (The other method available is lzo, which compresses less aggressively but does not require as much CPU power.)\nNote that the number of reduce tasks is one more than the number of partitions - this must be true or else you will get a \u0026ldquo;Wrong number of partitions in keyset\u0026rdquo; error.\nThere is a parameter hbase.hregion.max.filesize (default 256MB) which affects how HFiles are generated. If the amount of data (pre-compression) produced by a reducer exceeds this limit, more than one HFile will be generated for that reducer. This will lead to unbalanced region files. This will not cause any correctness problems, but if you want to get balanced region files, either use more reducers or set this parameter to a larger value. Note that when compression is enabled, you may see multiple files generated whose sizes are well below the limit; this is because the overflow check is done pre-compression.\nThe cf in the path specifies the name of the column family which will be created in HBase, so the directory name you choose here is important. (Note that we\u0026rsquo;re not actually using an HBase table here; HiveHFileOutputFormat writes directly to files.)\nThe CLUSTER BY clause provides the keys to be used by the partitioner; be sure that it matches the range partitioning that you came up with in the earlier step.\nThe first column in the SELECT list is interpreted as the rowkey; subsequent columns become cell values (all in a single column family, so their column names are important).\nRun HBase Script Once the sort job completes successfully, one final step is required for importing the result files into HBase. Again, we don\u0026rsquo;t know the name of the file, so we copy it over:\ndfs -copyToLocal /tmp/hbsort/cf/* /tmp/hbout If Hive and HBase are running in different clusters, use distcp to copy the files from one to the other.\nIf you are using HBase 0.90.2 or newer, you can use the completebulkload utility to load the data into HBase\nhadoop jar hbase-VERSION.jar completebulkload [-c /path/to/hbase/config/hbase-site.xml] /tmp/hbout transactions In older versions of HBase, use the bin/loadtable.rb script to import them:\nhbase org.jruby.Main loadtable.rb transactions /tmp/hbout The first argument (transactions) specifies the name of the new HBase table. For the second argument, pass the staging directory name, not the the column family child directory.\nAfter this script finishes, you may need to wait a minute or two for the new table to be picked up by the HBase meta scanner. Use the hbase shell to verify that the new table was created correctly, and do some sanity queries to locate individual cells and make sure they can be found.\nMap New Table Back Into Hive Finally, if you\u0026rsquo;d like to access the HBase table you just created via Hive:\nCREATE EXTERNAL TABLE hbase_transactions(transaction_id string, user_name string, amount double, ...) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:user_name,cf:amount,...\u0026quot;) TBLPROPERTIES(\u0026quot;hbase.table.name\u0026quot; = \u0026quot;transactions\u0026quot;); Followups Needed  Support sparse tables Support loading binary data representations once HIVE-1245 is fixed Support assignment of timestamps Provide control over file parameters such as compression Support multiple column families once HBASE-1861 is implemented Support loading into existing tables once HBASE-1923 is implemented Wrap it all up into the ideal single-INSERT-with-auto-sampling job\u0026hellip;  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hbasebulkload_27362088/","tags":null,"title":"Apache Hive : HBaseBulkLoad"},{"categories":null,"contents":"Apache Hive : HBaseIntegration Hive HBase Integration   Hive HBase Integration\n - [Avro Data Stored in HBase Columns](#avro-data-stored-in-hbase-columns)+ [Introduction](#introduction)   Storage Handlers Usage Column Mapping  Multiple Columns and Families Hive MAP to HBase Column Family Hive MAP to HBase Column Prefix  Hiding Column Prefixes   Illegal: Hive Primitive to HBase Column Family Example with Binary Columns Simple Composite Row Keys Complex Composite Row Keys and HBaseKeyFactory Avro Data Stored in HBase Columns   Put Timestamps Key Uniqueness Overwrite Potential Followups Build Tests Links Acknowledgements Open Issues (JIRA)    Version information\nAvro Data Stored in HBase Columns As of Hive 0.9.0 the HBase integration requires at least HBase 0.92, earlier versions of Hive were working with HBase 0.89/0.90\nVersion information\nHive 1.x will remain compatible with HBase 0.98.x and lower versions. Hive 2.x will be compatible with HBase 1.x and higher. (See HIVE-10990 for details.) Consumers wanting to work with HBase 1.x using Hive 1.x will need to compile Hive 1.x stream code themselves.\nIntroduction This page documents the Hive/HBase integration support originally introduced in HIVE-705. This feature allows Hive QL statements to access HBase tables for both read (SELECT) and write (INSERT). It is even possible to combine access to HBase tables with native Hive tables via joins and unions.\nA presentation is available from the HBase HUG10 Meetup\nThis feature is a work in progress, and suggestions for its improvement are very welcome.\nStorage Handlers Before proceeding, please read StorageHandlers for an overview of the generic storage handler framework on which HBase integration depends.\nUsage The storage handler is built as an independent module, hive-hbase-handler-x.y.z.jar, which must be available on the Hive client auxpath, along with HBase, Guava and ZooKeeper jars. It also requires the correct configuration property to be set in order to connect to the right HBase master. See the HBase documentation for how to set up an HBase cluster.\nHere\u0026rsquo;s an example using CLI from a source build environment, targeting a single-node HBase server. (Note that the jar locations and names have changed in Hive 0.9.0, so for earlier releases, some changes are needed.)\n$HIVE_SRC/build/dist/bin/hive --auxpath $HIVE_SRC/build/dist/lib/hive-hbase-handler-0.9.0.jar,$HIVE_SRC/build/dist/lib/hbase-0.92.0.jar,$HIVE_SRC/build/dist/lib/zookeeper-3.3.4.jar,$HIVE_SRC/build/dist/lib/guava-r09.jar --hiveconf hbase.master=hbase.yoyodyne.com:60000 Here\u0026rsquo;s an example which instead targets a distributed HBase cluster where a quorum of 3 zookeepers is used to elect the HBase master:\n$HIVE_SRC/build/dist/bin/hive --auxpath $HIVE_SRC/build/dist/lib/hive-hbase-handler-0.9.0.jar,$HIVE_SRC/build/dist/lib/hbase-0.92.0.jar,$HIVE_SRC/build/dist/lib/zookeeper-3.3.4.jar,$HIVE_SRC/build/dist/lib/guava-r09.jar --hiveconf hbase.zookeeper.quorum=zk1.yoyodyne.com,zk2.yoyodyne.com,zk3.yoyodyne.com The handler requires Hadoop 0.20 or higher, and has only been tested with dependency versions hadoop-0.20.x, hbase-0.92.0 and zookeeper-3.3.4. If you are not using hbase-0.92.0, you will need to rebuild the handler with the HBase jar matching your version, and change the --auxpath above accordingly. Failure to use matching versions will lead to misleading connection failures such as MasterNotRunningException since the HBase RPC protocol changes often.\nIn order to create a new HBase table which is to be managed by Hive, use the STORED BY clause on CREATE TABLE:\nCREATE TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf1:val\u0026quot;) TBLPROPERTIES (\u0026quot;hbase.table.name\u0026quot; = \u0026quot;xyz\u0026quot;, \u0026quot;hbase.mapred.output.outputtable\u0026quot; = \u0026quot;xyz\u0026quot;); The hbase.columns.mapping property is required and will be explained in the next section. The hbase.table.name property is optional; it controls the name of the table as known by HBase, and allows the Hive table to have a different name. In this example, the table is known as hbase_table_1 within Hive, and as xyz within HBase. If not specified, then the Hive and HBase table names will be identical. The hbase.mapred.output.outputtable property is optional; it\u0026rsquo;s needed if you plan to insert data to the table (the property is used by hbase.mapreduce.TableOutputFormat)\nAfter executing the command above, you should be able to see the new (empty) table in the HBase shell:\n$ hbase shell HBase Shell; enter 'help\u0026lt;RETURN\u0026gt;' for list of supported commands. Version: 0.20.3, r902334, Mon Jan 25 13:13:08 PST 2010 hbase(main):001:0\u0026gt; list xyz 1 row(s) in 0.0530 seconds hbase(main):002:0\u0026gt; describe \u0026quot;xyz\u0026quot; DESCRIPTION ENABLED {NAME =\u0026gt; 'xyz', FAMILIES =\u0026gt; [{NAME =\u0026gt; 'cf1', COMPRESSION =\u0026gt; 'NONE', VE true RSIONS =\u0026gt; '3', TTL =\u0026gt; '2147483647', BLOCKSIZE =\u0026gt; '65536', IN_MEMORY =\u0026gt; 'false', BLOCKCACHE =\u0026gt; 'true'}]} 1 row(s) in 0.0220 seconds hbase(main):003:0\u0026gt; scan \u0026quot;xyz\u0026quot; ROW COLUMN+CELL 0 row(s) in 0.0060 seconds Notice that even though a column name \u0026ldquo;val\u0026rdquo; is specified in the mapping, only the column family name \u0026ldquo;cf1\u0026rdquo; appears in the DESCRIBE output in the HBase shell. This is because in HBase, only column families (not columns) are known in the table-level metadata; column names within a column family are only present at the per-row level.\nHere\u0026rsquo;s how to move data from Hive into the HBase table (see GettingStarted for how to create the example table pokes in Hive first):\nINSERT OVERWRITE TABLE hbase_table_1 SELECT * FROM pokes WHERE foo=98; Use HBase shell to verify that the data actually got loaded:\nhbase(main):009:0\u0026gt; scan \u0026quot;xyz\u0026quot; ROW COLUMN+CELL 98 column=cf1:val, timestamp=1267737987733, value=val_98 1 row(s) in 0.0110 seconds And then query it back via Hive:\nhive\u0026gt; select * from hbase_table_1; Total MapReduce jobs = 1 Launching Job 1 out of 1 ... OK 98\tval_98 Time taken: 4.582 seconds Inserting large amounts of data may be slow due to WAL overhead; if you would like to disable this, make sure you have HIVE-1383 (as of Hive 0.6), and then issue this command before the INSERT:\nset hive.hbase.wal.enabled=false; Warning: disabling WAL may lead to data loss if an HBase failure occurs, so only use this if you have some other recovery strategy available.\nIf you want to give Hive access to an existing HBase table, use CREATE EXTERNAL TABLE:\nCREATE EXTERNAL TABLE hbase_table_2(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES (\u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;cf1:val\u0026quot;) TBLPROPERTIES(\u0026quot;hbase.table.name\u0026quot; = \u0026quot;some_existing_table\u0026quot;, \u0026quot;hbase.mapred.output.outputtable\u0026quot; = \u0026quot;some_existing_table\u0026quot;); Again, hbase.columns.mapping is required (and will be validated against the existing HBase table\u0026rsquo;s column families), whereas hbase.table.name is optional. The hbase.mapred.output.outputtable is optional.\nColumn Mapping There are two SERDEPROPERTIES that control the mapping of HBase columns to Hive:\n hbase.columns.mapping hbase.table.default.storage.type: Can have a value of either string (the default) or binary, this option is only available as of Hive 0.9 and the string behavior is the only one available in earlier versions  The column mapping support currently available is somewhat cumbersome and restrictive:\n for each Hive column, the table creator must specify a corresponding entry in the comma-delimited hbase.columns.mapping string (so for a Hive table with n columns, the string should have n entries); whitespace should not be used in between entries since these will be interperted as part of the column name, which is almost certainly not what you want a mapping entry must be either :key, :timestamp or of the form column-family-name:[column-name][#(binary|string) (the type specification that delimited by # was added in Hive 0.9.0, earlier versions interpreted everything as strings)  If no type specification is given the value from hbase.table.default.storage.type will be used Any prefixes of the valid values are valid too (i.e. #b instead of #binary) If you specify a column as binary the bytes in the corresponding HBase cells are expected to be of the form that HBase\u0026rsquo;s Bytes class yields.   there must be exactly one :key mapping (this can be mapped either to a string or struct column–see Simple Composite Keys and Complex Composite Keys) (note that before HIVE-1228 in Hive 0.6, :key was not supported, and the first Hive column implicitly mapped to the key; as of Hive 0.6, it is now strongly recommended that you always specify the key explictly; we will drop support for implicit key mapping in the future) if no column-name is given, then the Hive column will map to all columns in the corresponding HBase column family, and the Hive MAP datatype must be used to allow access to these (possibly sparse) columns Since HBase 1.1 (HBASE-2828) there is a way to access the HBase timestamp attribute using the special :timestamp mapping. It needs to be either bigint or timestamp. it is not necessary to reference every HBase column family, but those that are not mapped will be inaccessible via the Hive table; it\u0026rsquo;s possible to map multiple Hive tables to the same HBase table  The next few sections provide detailed examples of the kinds of column mappings currently possible.\nMultiple Columns and Families Here\u0026rsquo;s an example with three Hive columns and two HBase column families, with two of the Hive columns (value1 and value2) corresponding to one of the column families (a, with HBase column names b and c), and the other Hive column corresponding to a single column (e) in its own column family (d).\nCREATE TABLE hbase_table_1(key int, value1 string, value2 int, value3 int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,a:b,a:c,d:e\u0026quot; ); INSERT OVERWRITE TABLE hbase_table_1 SELECT foo, bar, foo+1, foo+2 FROM pokes WHERE foo=98 OR foo=100; Here\u0026rsquo;s how this looks in HBase:\nhbase(main):014:0\u0026gt; describe \u0026quot;hbase_table_1\u0026quot; DESCRIPTION ENABLED {NAME =\u0026gt; 'hbase_table_1', FAMILIES =\u0026gt; [{NAME =\u0026gt; 'a', COMPRESSION =\u0026gt; 'N true ONE', VERSIONS =\u0026gt; '3', TTL =\u0026gt; '2147483647', BLOCKSIZE =\u0026gt; '65536', IN_M EMORY =\u0026gt; 'false', BLOCKCACHE =\u0026gt; 'true'}, {NAME =\u0026gt; 'd', COMPRESSION =\u0026gt; 'NONE', VERSIONS =\u0026gt; '3', TTL =\u0026gt; '2147483647', BLOCKSIZE =\u0026gt; '65536', IN _MEMORY =\u0026gt; 'false', BLOCKCACHE =\u0026gt; 'true'}]} 1 row(s) in 0.0170 seconds hbase(main):015:0\u0026gt; scan \u0026quot;hbase_table_1\u0026quot; ROW COLUMN+CELL 100 column=a:b, timestamp=1267740457648, value=val_100 100 column=a:c, timestamp=1267740457648, value=101 100 column=d:e, timestamp=1267740457648, value=102 98 column=a:b, timestamp=1267740457648, value=val_98 98 column=a:c, timestamp=1267740457648, value=99 98 column=d:e, timestamp=1267740457648, value=100 2 row(s) in 0.0240 seconds And when queried back into Hive:\nhive\u0026gt; select * from hbase_table_1; Total MapReduce jobs = 1 Launching Job 1 out of 1 ... OK 100\tval_100\t101\t102 98\tval_98\t99\t100 Time taken: 4.054 seconds Hive MAP to HBase Column Family Here\u0026rsquo;s how a Hive MAP datatype can be used to access an entire column family. Each row can have a different set of columns, where the column names correspond to the map keys and the column values correspond to the map values.\nCREATE TABLE hbase_table_1(value map\u0026lt;string,int\u0026gt;, row_key int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;cf:,:key\u0026quot; ); INSERT OVERWRITE TABLE hbase_table_1 SELECT map(bar, foo), foo FROM pokes WHERE foo=98 OR foo=100; (This example also demonstrates using a Hive column other than the first as the HBase row key.)\nHere\u0026rsquo;s how this looks in HBase (with different column names in different rows):\nhbase(main):012:0\u0026gt; scan \u0026quot;hbase_table_1\u0026quot; ROW COLUMN+CELL 100 column=cf:val_100, timestamp=1267739509194, value=100 98 column=cf:val_98, timestamp=1267739509194, value=98 2 row(s) in 0.0080 seconds And when queried back into Hive:\nhive\u0026gt; select * from hbase_table_1; Total MapReduce jobs = 1 Launching Job 1 out of 1 ... OK {\u0026quot;val_100\u0026quot;:100}\t100 {\u0026quot;val_98\u0026quot;:98}\t98 Time taken: 3.808 seconds Note that the key of the MAP must have datatype string, since it is used for naming the HBase column, so the following table definition will fail:\nCREATE TABLE hbase_table_1(key int, value map\u0026lt;int,int\u0026gt;) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:\u0026quot; ); FAILED: Error in metadata: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException org.apache.hadoop.hive.hbase.HBaseSerDe: hbase column family 'cf:' should be mapped to map\u0026lt;string,?\u0026gt; but is mapped to map\u0026lt;int,int\u0026gt;) Hive MAP to HBase Column Prefix Also note that starting with Hive 0.12, wildcards can also be used to retrieve columns. For instance, if you want to retrieve all columns in HBase that start with the prefix \u0026ldquo;col_prefix\u0026rdquo;, a query like the following should work:\nCREATE TABLE hbase_table_1(value map\u0026lt;string,int\u0026gt;, row_key int) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;cf:col_prefix.*,:key\u0026quot; ); The same restrictions apply though. That is, the key of the map should be a string as it maps to the HBase column name and the value can be the type of value that you are retrieving. One other restriction is that all the values under the given prefix should be of the same type. That is, all of them should be of type \u0026ldquo;int\u0026rdquo; or type \u0026ldquo;string\u0026rdquo; and so on.\nHiding Column Prefixes Starting with Hive 1.3.0, it is possible to hide column prefixes in select query results. There is the SerDe boolean property hbase.columns.mapping.prefix.hide (false by default), which defines if the prefix should be hidden in keys of Hive map:\nCREATE TABLE hbase_table_1(tags map\u0026lt;string,int\u0026gt;, row_key string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;cf:tag_.*,:key\u0026quot;, \u0026quot;hbase.columns.mapping.prefix.hide\u0026quot; = \u0026quot;true\u0026quot; ); Then a value of the column \u0026ldquo;tags\u0026rdquo; (select tags from hbase_table_1) will be:\n\u0026quot;x\u0026quot; : 1 instead of:\n\u0026quot;tag_x\u0026quot; : 1 Illegal: Hive Primitive to HBase Column Family Table definitions such as the following are illegal because a\nHive column mapped to an entire column family must have MAP type:\nCREATE TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:\u0026quot; ); FAILED: Error in metadata: java.lang.RuntimeException: MetaException(message:org.apache.hadoop.hive.serde2.SerDeException org.apache.hadoop.hive.hbase.HBaseSerDe: hbase column family 'cf:' should be mapped to map\u0026lt;string,?\u0026gt; but is mapped to string) Example with Binary Columns Relying on the default value of hbase.table.default.storage.type:\nCREATE TABLE hbase_table_1 (key int, value string, foobar double) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key#b,cf:val,cf:foo#b\u0026quot; ); Specifying hbase.table.default.storage.type:\nCREATE TABLE hbase_table_1 (key int, value string, foobar double) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:val#s,cf:foo\u0026quot;, \u0026quot;hbase.table.default.storage.type\u0026quot; = \u0026quot;binary\u0026quot; ); Simple Composite Row Keys Version information\nAs of Hive 0.13.0\nHive can read and write delimited composite keys to HBase by mapping the HBase row key to a Hive struct, and using ROW FORMAT DELIMITED\u0026hellip;COLLECTION ITEMS TERMINATED BY. Example:\n-- Create a table with a composite row key consisting of two string fields, delimited by '~' CREATE EXTERNAL TABLE delimited_example(key struct\u0026lt;f1:string, f2:string\u0026gt;, value string) ROW FORMAT DELIMITED COLLECTION ITEMS TERMINATED BY '~' STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( 'hbase.columns.mapping'=':key,f:c1'); Complex Composite Row Keys and HBaseKeyFactory As of Hive 0.14.0 with HIVE-6411 (0.13.0 also supports complex composite keys, but using a different interface–see HIVE-2599 for that interface)\nFor more complex use cases, Hive allows users to specify an HBaseKeyFactory which defines the mapping of a key to fields in a Hive struct. This can be configured using the property \u0026ldquo;hbase.composite.key.factory\u0026rdquo; in the SERDEPROPERTIES option:\n-- Parse a row key with 3 fixed width fields each of width 10 -- Example taken from: https://svn.apache.org/repos/asf/hive/trunk/hbase-handler/src/test/queries/positive/hbase_custom_key2.q CREATE TABLE hbase_ck_4(key struct\u0026lt;col1:string,col2:string,col3:string\u0026gt;, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.table.name\u0026quot; = \u0026quot;hbase_custom2\u0026quot;, \u0026quot;hbase.mapred.output.outputtable\u0026quot; = \u0026quot;hbase_custom2\u0026quot;, \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:string\u0026quot;, \u0026quot;hbase.composite.key.factory\u0026quot;=\u0026quot;org.apache.hadoop.hive.hbase.SampleHBaseKeyFactory2\u0026quot;); \u0026ldquo;hbase.composite.key.factory\u0026rdquo; should be the fully qualified class name of a class implementing HBaseKeyFactory. See SampleHBaseKeyFactory2 for a fixed length example in the same package. This class must be on your classpath in order for the above example to work. TODO: place these in an accessible place; they\u0026rsquo;re currently only in test code.\nAvro Data Stored in HBase Columns As of Hive 0.14.0 with HIVE-6147\nHive 0.14.0 onward supports storing and querying Avro objects in HBase columns by making them visible as structs to Hive. This allows Hive to perform ad hoc analysis of HBase data which can be deeply structured. Prior to 0.14.0, the HBase Hive integration only supported querying primitive data types in columns.\nAn example HiveQL statement where test_col_fam is the column family and test_col is the column name:\nCREATE EXTERNAL TABLE test_hbase_avro ROW FORMAT SERDE 'org.apache.hadoop.hive.hbase.HBaseSerDe' STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,test_col_fam:test_col\u0026quot;, \u0026quot;test_col_fam.test_col.serialization.type\u0026quot; = \u0026quot;avro\u0026quot;, \u0026quot;test_col_fam.test_col.avro.schema.url\u0026quot; = \u0026quot;hdfs://testcluster/tmp/schema.avsc\u0026quot;) TBLPROPERTIES ( \u0026quot;hbase.table.name\u0026quot; = \u0026quot;hbase_avro_table\u0026quot;, \u0026quot;hbase.mapred.output.outputtable\u0026quot; = \u0026quot;hbase_avro_table\u0026quot;, \u0026quot;hbase.struct.autogenerate\u0026quot;=\u0026quot;true\u0026quot;); The important properties to note are the following three:\n\u0026quot;test_col_fam.test_col.serialization.type\u0026quot; = \u0026quot;avro\u0026quot; This property tells Hive that the given column under the given column family is an Avro column, so Hive needs to deserialize it accordingly.\n\u0026quot;test_col_fam.test_col.avro.schema.url\u0026quot; = \u0026quot;hdfs://testcluster/tmp/schema.avsc\u0026quot; Using this property you specify where the reader schema is for the column that will be used to deserialize. This can be on HDFS like mentioned here, or provided inline using something like \u0026quot;test_col_fam.test_col.avro.schema.literal\u0026quot; property. If you have a custom store where you store this schema, you can write a custom implementation of AvroSchemaRetriever and plug that in using the \u0026quot;avro.schema.retriever property\u0026quot; using a property like \u0026quot;test_col_fam.test_col.avro.schema.retriever\u0026quot;. You would need to ensure that the jar with this custom class is on the Hive classpath. For a usage discussion and links to other resources, see HIVE-6147.\n\u0026quot;hbase.struct.autogenerate\u0026quot; = \u0026quot;true\u0026quot; Specifying this property lets Hive auto-deduce the columns and types using the schema that was provided. This allows you to avoid manually creating the columns and types for Avro schemas, which can be complicated and deeply nested.\nPut Timestamps Version information\nAs of Hive 0.9.0\nIf inserting into a HBase table using Hive the HBase default timestamp is added which is usually the current timestamp. This can be overridden on a per-table basis using the SERDEPROPERTIES option hbase.put.timestamp which must be a valid timestamp or -1 to reenable the default strategy.\nKey Uniqueness One subtle difference between HBase tables and Hive tables is that HBase tables have a unique key, whereas Hive tables do not. When multiple rows with the same key are inserted into HBase, only one of them is stored (the choice is arbitrary, so do not rely on HBase to pick the right one). This is in contrast to Hive, which is happy to store multiple rows with the same key and different values.\nFor example, the pokes table contains rows with duplicate keys. If it is copied into another Hive table, the duplicates are preserved:\nCREATE TABLE pokes2(foo INT, bar STRING); INSERT OVERWRITE TABLE pokes2 SELECT * FROM pokes; -- this will return 3 SELECT COUNT(1) FROM POKES WHERE foo=498; -- this will also return 3 SELECT COUNT(1) FROM pokes2 WHERE foo=498; But in HBase, the duplicates are silently eliminated:\nCREATE TABLE pokes3(foo INT, bar STRING) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;:key,cf:bar\u0026quot; ); INSERT OVERWRITE TABLE pokes3 SELECT * FROM pokes; -- this will return 1 instead of 3 SELECT COUNT(1) FROM pokes3 WHERE foo=498; Overwrite Another difference to note between HBase tables and other Hive tables is that when INSERT OVERWRITE is used, existing rows are not deleted from the table. However, existing rows are overwritten if they have keys which match new rows.\nPotential Followups There are a number of areas where Hive/HBase integration could definitely use more love:\n more flexible column mapping (HIVE-806, HIVE-1245) default column mapping in cases where no mapping spec is given filter pushdown and indexing (see FilterPushdownDev and IndexDev) expose timestamp attribute, possibly also with support for treating it as a partition key allow per-table hbase.master configuration run profiler and minimize any per-row overhead in column mapping user defined routines for lookups and data loads via HBase client API (HIVE-758 and HIVE-791) logging is very noisy, with a lot of spurious exceptions; investigate these and either fix their cause or squelch them  Build Code for the storage handler is located under hive/trunk/hbase-handler.\nHBase and Zookeeper dependencies are fetched via ivy.\nTests Class-level unit tests are provided under hbase-handler/src/test/org/apache/hadoop/hive/hbase.\nPositive QL tests are under hbase-handler/src/test/queries. These use a HBase+Zookeeper mini-cluster for hosting the fixture tables in-process, so no free-standing HBase installation is needed in order to run them. To avoid failures due to port conflicts, don\u0026rsquo;t try to run these tests on the same machine where a real HBase master or zookeeper is running.\nThe QL tests can be executed via ant like this:\nant test -Dtestcase=TestHBaseCliDriver -Dqfile=hbase_queries.q An Eclipse launch template remains to be defined.\nLinks  For information on how to bulk load data from Hive into HBase, see HBaseBulkLoad. For another project which adds SQL-like query language support on top of HBase, see HBQL (unrelated to Hive).  Acknowledgements  Primary credit for this feature goes to Samuel Guo, who did most of the development work in the early drafts of the patch  Open Issues (JIRA)    T Key Summary Assignee Reporter P Status Resolution Created Updated Due     Improvement HIVE-27750 Use HBase\u0026rsquo;s TableMapReduceUtil.convertScanToString() API instead of using the method implementation. Dayakar M Dayakar M Major Open Unresolved Sep 28, 2023 Sep 28, 2023    Improvement HIVE-24833 Hive Uses Fetch Task For HBaseStorageHandler Scans Unassigned David Mollitor Major Open Unresolved Feb 25, 2021 Mar 08, 2021    Bug HIVE-24706 Spark SQL access hive on HBase table access exception Unassigned zhangzhanchang Major Open Unresolved Jan 30, 2021 Jan 23, 2024    Bug HIVE-24544 HBase Timestamp filter never gets converted to a timerange filter Unassigned Fabien Carrion Minor Open Unresolved Dec 16, 2020 Dec 16, 2020    Bug HIVE-24418 there is an error \u0026ldquo;java.lang.IllegalArgumentException: No columns to insert\u0026rdquo; when the result data is empty Unassigned HuiyuZhou Major Open Unresolved Nov 24, 2020 Nov 25, 2020 Nov 24, 2020   Bug HIVE-24407 Unable to read data with Hbase snapshot set Unassigned Ayush Saxena Major Open Unresolved Nov 20, 2020 Nov 20, 2020    Bug HIVE-21936 Snapshot inconsistency plan execution Unassigned Jean-Pierre Hoang Major Open Unresolved Jun 30, 2019 Jun 30, 2019    Bug HIVE-21748 HBase Operations Can Fail When Using MAPREDLOCAL Unassigned David Mollitor Major Open Unresolved May 17, 2019 Jun 03, 2019    New Feature HIVE-21277 Make HBaseSerde First-Class SerDe Unassigned David Mollitor Major Open Unresolved Feb 15, 2019 Feb 15, 2019    Improvement HIVE-21265 Hive miss-uses HBase HConnection object and that puts high load on Zookeeper Unassigned István Fajth Major Open Unresolved Feb 13, 2019 Feb 13, 2019    Improvement HIVE-21179 Move SampleHBaseKeyFactory* Into Main Code Line Unassigned David Mollitor Major Open Unresolved Jan 29, 2019 Jan 29, 2019    Bug HIVE-21068 hive table join hbase table return code 3 Unassigned Baozhu Zhao Major Open Unresolved Dec 26, 2018 May 21, 2019    Improvement HIVE-16884 Replace the deprecated HBaseInterface with Table Aihua Xu Aihua Xu Major Open Unresolved Jun 12, 2017 Jun 12, 2017    Bug HIVE-16883 HBaseStorageHandler Ignores Case for HBase Table Name Bing Li Shawn Weeks Minor Open Unresolved Jun 12, 2017 Jul 05, 2017    Bug HIVE-16818 \u0026ldquo;fixed\u0026rdquo; avro type is not handled properly with Hive HBase avro integration Unassigned Swarnim Kulkarni Major Open Unresolved Jun 02, 2017 Jun 02, 2017    Bug HIVE-15838 Escaping illegal characters allowed in HBase and disallowed in Hive DDL Unassigned Davide Gesino Minor Open Unresolved Feb 07, 2017 Feb 07, 2017    Bug HIVE-15204 Hive-Hbase integration thorws \u0026ldquo;java.lang.ClassNotFoundException: NULL::character varying\u0026rdquo; (Postgres) Unassigned Anshuman Major Open Unresolved Nov 15, 2016 Nov 16, 2016    Improvement HIVE-13584 HBaseStorageHandler should support table pre-split Svetozar Ivanov Svetozar Ivanov Major Patch Available Unresolved Apr 21, 2016 Feb 27, 2024    Improvement HIVE-13315 Option to reuse existing restored HBase snapshots Sushanth Sowmyan Liyin Tang Major Open Unresolved Mar 19, 2016 Mar 19, 2016    Bug HIVE-11327 HiveQL to HBase - Predicate Pushdown for composite key not working Unassigned Yannik Zuehlke Blocker Open Unresolved Jul 21, 2015 Jul 26, 2015     Authenticate to retrieve your issues\nShowing 20 out of 80 issues\n ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hbaseintegration_27362089/","tags":null,"title":"Apache Hive : HBaseIntegration"},{"categories":null,"contents":"Apache Hive : HBaseMetastoreDevelopmentGuide  Guide for contributors to the metastore on hbase development work. Umbrella JIRA - HIVE-9452\nThis work is discontinued and the code is removed in release 3.0.0 (HIVE-17234).\n  Building Setup for running hive against hbase metastore - Importing metadata from rdbms to hbase Design Docs  Building You will need to download the source for Tephra and build it from the develop branch. You need Tephra 0.5.1-SNAPSHOT. You can get Tephra from Cask\u0026rsquo;s github. Switch to the branch develop and doing \u0026lsquo;mvn install\u0026rsquo; will build the version you need.\nSetup for running hive against hbase metastore - Once you’ve built the code from the HBase metastore branch (hbase-metastore), here’s how to make it run against HBase:\n   Install HBase, preferably HBase 1.1.1 as that’s what is being used for testing.\n  Copy following jars into $HBASE_HOME/lib\n hive-common-.*.jar hive-metastore-.*.jar hive-serde-.*.jar    Setup HBase, http://hbase.apache.org/book.html#quickstart I run it in stand alone mode, so you have to set a couple of values in hbase-site.xml for this to work.\n  Set HADOOP_HOME if you’re not in a cluster where hadoop is already on your path.\n  Start HBase: $HBASE_HOME/bin/start-hbase.sh\n  Set it up so that HBase jars and conf file are picked up by Hive\n  export HIVE_AUX_JARS_PATH=$HBASE_HOME/lib/\n  export AUX_CLASSPATH=$HBASE_HOME/conf\n  Create the metastore tables in HBase: hive \u0026ndash;service hbaseschematool \u0026ndash;install\n  Configure Hive to use HBase as its metastore, in hive-site.xml:\n  \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.rawstore.impl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hive.metastore.hbase.HBaseStore\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.fastpath\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt;  Now start Hive as normal, all should just work.\nNotes\n Currently (as of Apr 8 2015) we have not tested the HBase metastore with the metastore service. We don\u0026rsquo;t know if it works or not. We have tested it with the command line client and HiveServer2. Not all Hive operations have been tested. Insert, select, create table, drop table, create database, add partition, drop partition have been tested. Other features may not work. Once we switched to HBase 1.1.1, Tephra no longer works. You\u0026rsquo;ll need to use the VanillaHBaseConnection (which is the default) until we get Tephra working again.   Importing metadata from rdbms to hbase Set hive-site.xml so that it has necessary jdo properties for rdbms, and setup Hive to use hbase for metadata storage as documented above\nThe following command will import the metadata from the rdbms to hbase:\nhive \u0026ndash;service hbaseimport Design Docs Overall Approach\nHbase execution plans for RawStore partition filter condition\n ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hbasemetastoredevelopmentguide_55151960/","tags":null,"title":"Apache Hive : HBaseMetastoreDevelopmentGuide"},{"categories":null,"contents":"Apache Hive : HCatalog HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid.\nThis is the HCatalog manual.  Using HCatalog Installation from Tarball HCatalog Configuration Properties Load and Store Interfaces Input and Output Interfaces Reader and Writer Interfaces Command Line Interface Storage Formats Dynamic Partitioning Notification Storage Based Authorization  The old HCatalog wiki page has many other documents including additional user documentation, further information on HBase integration, and resources for contributors.\nFor information about the REST API for HCatalog, WebHCat (formerly Templeton), see the WebHCat Manual.\n Navigation Links Next: Using HCatalog\nGeneral: WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog_33299065/","tags":null,"title":"Apache Hive : HCatalog"},{"categories":null,"contents":"Apache Hive : HCatalog Authorization Storage Based Authorization  Storage Based Authorization  Default Authorization Model of Hive Storage-System Based Authorization Model  Minimum Permissions Unused DDL for Permissions   Configuring Storage-System Based Authorization Creating New Tables or Databases Known Issues    Default Authorization Model of Hive The default authorization model of Hive supports a traditional RDBMS style of authorization based on users, groups and roles and granting them permissions to do operations on database or table. It is described in more detail in Hive Authorization and Hive deprecated authorization mode / Legacy Mode.\nThis RDBMS style of authorization is not very suitable for the typical use cases in Hadoop because of the following differences in implementation:\n Unlike a traditional RDBMS, Hive is not in complete control of all data underneath it. The data is stored in a number of files, and the file system has an independent authorization system. Also unlike a traditional RDBMS which doesn’t allow other programs to access the data directly, people tend to use other applications that read or write directly into files or directories that get used with Hive.  This creates problem scenarios like:\n You grant permissions to a user, but the user can’t access the database or file system because they don’t have file system permissions. You remove permissions for a user, but the user can still access the data directly through the file system, because they have file system permissions.  Storage-System Based Authorization Model The Hive community realizes that there might not be a one-size-fits-all authorization model, so it has support for alternative authorization models to be plugged in.\nIn the HCatalog package, we have introduced implementation of an authorization interface that uses the permissions of the underlying file system (or in general, the storage backend) as the basis of permissions on each database, table or partition.\nNote\nThis feature is also available in Hive on the metastore-side, starting with release 0.10.0 (see Storage Based Authorization in the Metastore Server in the Hive documentation). Starting in Hive 0.12.0 it also runs on the client side (HIVE-5048 and HIVE-5402).\nIn Hive, when a file system is used for storage, there is a directory corresponding to a database or a table. With this authorization model, the read/write permissions a user or group has for this directory determine the permissions a user has on the database or table. In the case of other storage systems such as HBase, the authorization of equivalent entities in the system will be done using the system’s authorization mechanism to determine the permissions in Hive.\nFor example, an alter table operation would check if the user has permissions on the table directory before allowing the operation, even if it might not change anything on the file system.\nA user would need write access to the corresponding entity on the storage system to do any type of action that can modify the state of the database or table. The user needs read access to be able to do any non-modifying action on the database or table.\nWhen the database or table is backed by a file system that has a Unix/POSIX-style permissions model (like HDFS), there are read(r) and write(w) permissions you can set for the owner user, group and ‘other’.\nDetails of HDFS permissions are given at ht``tp://hadoop.apache.org/docs/rx.x.x/hdfs_permissions_guide.html, for example:\n HDFS Permissions Guide (release 1.0.4) HDFS Permissions Guide (release 1.2.1)  Note: Support for HDFS ACL (introduced in Apache Hadoop 2.4) is not available in the released versions of Hive. Which means, that it checks only the traditional rwx style permissions to determine if a user can write to the file system. The support for ACL is available in Hive trunk HIVE-7583, which will be available in Hive 0.14. Links to documentation for different releases of Hadoop can be found here: http://hadoop.apache.org/docs/.\nNote: If hive.warehouse.subdir.inherit.perms is enabled, permissions and ACL\u0026rsquo;s for Hive-created files and directories will be set via the following permission inheritance rules.\nThe file system’s logic for determining if a user has permission on the directory or file will be used by Hive. Minimum Permissions The following table shows the minimum permissions required for Hive operations under this authorization model:\n   Operation Database Read Access Database Write Access Table Read Access Table Write Access     LOAD       X   EXPORT     X     IMPORT       X   CREATE TABLE   X       CREATE TABLE AS SELECT   X X source table     DROP TABLE   X       SELECT     X     ALTER TABLE       X   SHOW TABLES X          Caution: Hive\u0026rsquo;s current implementation of this authorization model does not prevent malicious users from doing bad things. See the Known Issues section below.\nUnused DDL for Permissions DDL statements that manage permissions for Hive\u0026rsquo;s default authorization model do not have any effect on permissions in the storage-based model.\nCaution\nAll GRANT and REVOKE statements for users, groups, and roles are ignored. See the Known Issues section below.\nConfiguring Storage-System Based Authorization The implementation of the file-system based authorization model is available through an authorization provider called StorageBasedAuthorizationProvider that is part of Hive. (Support for this was added to the Hive package in release 0.10.0 – see HIVE-3705 and Storage Based Authorization in the Metastore Server.)\nVersion\nAn earlier implementation of this called HdfsAuthorizationProvider used to exist in the HCatalog package, but has since been deprecated and removed as of Hive 0.14 trunk. If your configuration indicates use of HdfsAuthorizationProvider, please update to this configuration instead.\nThe following entries need to be added to hive-site.xml to enable authorization:\n \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;enable or disable the hive client authorization\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.manager\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;the hive client authorization manager class name. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; To disable authorization, set hive.security.authorization.enabled to false. To use the default authorization model of Hive, don’t set the hive.security.authorization.manager property.\nCreating New Tables or Databases To create new tables or databases with appropriate permissions, you can either use the Hive command line to create the table/database and then modify the permissions using a file system operation, or use the HCatalog command line (hcat) to create the database/table.\nThe HCatalog command line tool uses the same syntax as Hive, and will create the table or database with a corresponding directory being owned by the user creating it, and a group corresponding to the “-g” argument and permissions specified in the “-p” argument.\nKnown Issues  Some metadata operations (mostly read operations) do not check for authorization. See https://issues.apache.org/jira/browse/HIVE-3009. The current implementation of Hive performs the authorization checks in the client. This means that malicious users can circumvent these checks. A different authorization provider (StorageDelegationAuthorizationProvider) needs to be used for working with HBase tables as well. But that is not well tested. Partition files and directories added by a Hive query don’t inherit permissions from the table. This means that even if you grant permissions for a group to access a table, new partitions will have read permissions only for the owner, if the default umask for the cluster is configured as such. See https://issues.apache.org/jira/browse/HIVE-3094. A separate \u0026ldquo;hdfs chmod\u0026rdquo; command will be necessary to modify the permissions. Although DDL statements for managing permissions have no effect in storage-based authorization, currently they do not return error messages. See https://issues.apache.org/jira/browse/HIVE-3010.   Navigation Links Previous: Notification\nHive documents: Authorization and Storage Based Authorization in the Metastore Server\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-authorization_34014782/","tags":null,"title":"Apache Hive : HCatalog Authorization"},{"categories":null,"contents":"Apache Hive : HCatalog CLI Command Line Interface  Command Line Interface  Set Up HCatalog CLI  Owner Permissions Hive CLI   HCatalog DDL  Create/Drop/Alter Table Create/Drop/Alter View Show/Describe Create/Drop Index Create/Drop Function \u0026ldquo;dfs\u0026rdquo; Command and \u0026ldquo;set\u0026rdquo; Command Other Commands   CLI Errors  Authentication Error Log      Set Up The HCatalog command line interface (CLI) can be invoked as HIVE_HOME=hive_home hcat_home/bin/hcat where hive_home is the directory where Hive has been installed and hcat_home is the directory where HCatalog has been installed.\nIf you are using BigTop\u0026rsquo;s rpms or debs you can invoke the CLI by doing /usr/bin/hcat.\nHCatalog CLI The HCatalog CLI supports these command line options:\n   Option Usage Description     -g hcat -g mygroup ... Tells HCatalog that the table which needs to be created must have group \u0026ldquo;mygroup\u0026rdquo;.   -p hcat -p rwxr-xr-x ... Tells HCatalog that the table which needs to be created must have permissions \u0026ldquo;rwxr-xr-x\u0026rdquo;.   -f hcat -f myscript.hcatalog ... Tells HCatalog that myscript.hcatalog is a file containing DDL commands to execute.   -e hcat -e 'create table mytable(a int);' ... Tells HCatalog to treat the following string as a DDL command and execute it.   -D hcat -Dkey=value ... Passes the key-value pair to HCatalog as a Java System Property.     hcat Prints a usage message.    Note the following:\n The -g and -p options are not mandatory. Only one -e or -f option can be provided, not both. The order of options is immaterial; you can specify the options in any order.  If no option is provided, then a usage message is printed:\nUsage: hcat { -e \u0026quot;\u0026lt;query\u0026gt;\u0026quot; | -f \u0026lt;filepath\u0026gt; } [-g \u0026lt;group\u0026gt;] [-p \u0026lt;perms\u0026gt;] [-D\u0026lt;name\u0026gt;=\u0026lt;value\u0026gt;] Owner Permissions When using the HCatalog CLI, you cannot specify a permission string without read permissions for owner, such as -wxrwxr-x, because the string begins with \u0026ldquo;-\u0026rdquo;. If such a permission setting is desired, you can use the octal version instead, which in this case would be 375. Also, any other kind of permission string where the owner has read permissions (for example r-x----- or r--r--r--) will work fine.\nHive CLI Many hcat commands can be issued as hive commands, including all HCatalog DDL commands. The Hive CLI includes some commands that are not available in the HCatalog CLI. Note these differences:\n \u0026ldquo;hcat -g\u0026rdquo; and \u0026ldquo;hcat -p\u0026rdquo; for table group and permission settings are only available in the HCatalog CLI. hcat uses the -p flag for permissions but hive uses it to specify a port number. hcat uses the -D flag without a space to define key=value pairs but hive uses -d or --define with a space (also --hivevar).\nFor example, \u0026ldquo;hcat -DA=B\u0026rdquo; versus \u0026ldquo;hive -d A=B\u0026rdquo;. hcat without any flags prints a help message but hive uses the -H flag or --help.  The Hive CLI is documented here.\nHCatalog DDL HCatalog supports all Hive Data Definition Language except those operations that require running a MapReduce job. For commands that are supported, any variances are noted below.\nHCatalog does not support the following Hive DDL and other HiveQL commands:\n ALTER INDEX \u0026hellip; REBUILD CREATE TABLE \u0026hellip; AS SELECT ALTER TABLE \u0026hellip; CONCATENATE ALTER TABLE ARCHIVE/UNARCHIVE PARTITION ANALYZE TABLE \u0026hellip; COMPUTE STATISTICS IMPORT FROM \u0026hellip; EXPORT TABLE  For information about using WebHCat for DDL commands, see URL Format and WebHCat Reference: DDL Resources.\nCreate/Drop/Alter Table CREATE TABLE If you create a table with a CLUSTERED BY clause you will not be able to write to it with Pig or MapReduce. This is because they do not understand how to partition the table, so attempting to write to it would cause data corruption.\nCREATE TABLE AS SELECT Not supported. Throws an exception with the message \u0026ldquo;Operation Not Supported\u0026rdquo;.\nDROP TABLE Supported. Behavior the same as Hive.\nALTER TABLE Supported except for the REBUILD and CONCATENATE options. Behavior the same as Hive.\nCreate/Drop/Alter View Note: Pig and MapReduce cannot read from or write to views.\nCREATE VIEW Supported. Behavior same as Hive.\nDROP VIEW Supported. Behavior same as Hive.\nALTER VIEW Supported. Behavior same as Hive.\nShow/Describe SHOW TABLES Supported. Behavior same as Hive.\nSHOW PARTITIONS Not supported. Throws an exception with message \u0026ldquo;Operation Not Supported\u0026rdquo;.\nSHOW FUNCTIONS Supported. Behavior same as Hive.\nDESCRIBE Supported. Behavior same as Hive.\nCreate/Drop Index CREATE and DROP INDEX operations are supported.\nNote: Pig and MapReduce cannot write to a table that has auto rebuild on, because Pig and MapReduce do not know how to rebuild the index.\nCreate/Drop Function CREATE and DROP FUNCTION operations are supported, but created functions must still be registered in Pig and placed in CLASSPATH for MapReduce.\n\u0026ldquo;dfs\u0026rdquo; Command and \u0026ldquo;set\u0026rdquo; Command Supported. Behavior same as Hive.\nOther Commands Any command not listed above is NOT supported and throws an exception with the message \u0026ldquo;Operation Not Supported\u0026rdquo;.\nCLI Errors Authentication If a failure results in a message like \u0026ldquo;2010-11-03 16:17:28,225 WARN hive.metastore \u0026hellip; - Unable to connect metastore with URI thrift://\u0026hellip;\u0026rdquo; in /tmp//hive.log, then make sure you have run \u0026ldquo;kinit @FOO.COM\u0026rdquo; to get a Kerberos ticket and to be able to authenticate to the HCatalog server.\nError Log If other errors occur while using the HCatalog CLI, more detailed messages are written to /tmp//hive.log.\nNavigation Links Previous: Reader and Writer Interfaces\nNext: Storage Formats\nHive command line interface: Hive CLI\nHive DDL commands: Hive Data Definition Language\nWebHCat DDL resources: WebHCat Reference: DDL\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-cli_34013932/","tags":null,"title":"Apache Hive : HCatalog CLI"},{"categories":null,"contents":"Apache Hive : HCatalog Configuration Properties  Setup Storage Directives Cache Behaviour Directives Input Split Generation Behaviour Data Promotion Behaviour HCatRecordReader Error Tolerance Behaviour  Apache HCatalog\u0026rsquo;s behaviour can be modified through the use of a few configuration parameters specified in jobs submitted to it. This document details all the various knobs that users have available to them, and what they accomplish. Setup The properties described in this page are meant to be job-level properties set on HCatalog through the jobConf passed into it. This means that this page is relevant for Pig users of HCatLoader/HCatStorer, or MapReduce users of HCatInputFormat/HCatOutputFormat. For a MapReduce user of HCatalog, these must be present as key-values in the Configuration (JobConf/Job/JobContext) used to instantiate HCatOutputFormat or HCatInputFormat. For Pig users of HCatStorer, these parameters are set using the Pig \u0026ldquo;set\u0026rdquo; command before instantiating an HCatLoader/HCatStorer.\nStorage Directives    Property Default Description      hcat.pig.storer.external.location not set An override to specify where HCatStorer will write to, defined from Pig jobs, either directly by the user, or by using org.apache.hive.hcatalog.pig.HCatStorerWrapper. HCatalog will write to this specified directory, rather than writing to the table or partition directory calculated by the metadata. This will be used in lieu of the table directory if this is a table-level write (unpartitioned table write) or in lieu of the partition directory if this is a partition-level write. This parameter is used only for non-dynamic-partitioning jobs which have multiple write destinations.    hcat.dynamic.partitioning.custom.pattern not set For a dynamic partitioning job, simply specifying a custom directory is not sufficient since the job writes to multiple destinations, and thus, instead of a directory specification, it requires a pattern specification. That is where this parameter comes in. For example, given a table partitioned by the keys country and state, with a root directory location of /apps/hive/warehouse/geo/, a dynamic partition write into this table that writes partitions (country=US,state=CA) \u0026amp; (country=IN,state=KA) would create two directories: /apps/hive/warehouse/geo/country=US/state=CA/ and /apps/hive/warehouse/geo/country=IN/state=KA/. However, specifying hcat.dynamic.partitioning.custom.pattern=\u0026quot;/ext/geo/${country}-${state}\u0026quot; would create the following two partition directories: /ext/geo/US-CA and /ext/geo/IN-KA. Thus, it allows the user to specify a custom directory location pattern for all writes, and will interpolate each variable it sees when attempting to create a destination location for the partitions. See Dynamic Partitioning: External Tables for another example.   hcat.append.limit(Hive 0.15.0 and later) not set hcat.append.limit allows an HCatalog user to specify a custom append limit. By default, while appending to an existing directory HCatalog will attempt to avoid naming clashes and try to append _a_*NNN*, where NNN is a number, to the desired filename to avoid clashes. However, by default, it only tries for NNN from 0 to 999 before giving up. This can cause an issue for some tables with an extraordinarily large number of files. Ideally, this should be fixed by the user changing their usage pattern and doing some manner of compaction; however, setting this parameter can be used as a temporary fix to increase that limit. (Added in Hive 0.15.0 with HIVE-9381.)   hcat.input.ignore.invalid.path false hcat.input.ignore.invalid.path allows an HCatalog user to specify whether to ignore the path and return an empty result for it when trying to get a split for an invalid input path. The default is false, and user gets an InvalidInputException if the input path is invalid. (Added in Hive 2.1.0 with HIVE-13509.)    Cache Behaviour Directives HCatalog maintains a cache of HiveClients to talk to the metastore, managing a cache of 1 metastore client per thread, defaulting to an expiry of 120 seconds. For people that wish to modify the behaviour of this cache, a few parameters are provided:\n   Property Default Description     hcatalog.hive.client.cache.expiry.time 120 Allows users to override the expiry time specified – this is an int, and specifies the number of seconds until a cache entry expires.   hcatalog.hive.client.cache.disabled false Allows people to disable the cache altogether if they wish to. This is useful in highly multithreaded use cases.     Note: The two above properties begin with \u0026ldquo;hcatalog.\u0026rdquo; rather than \u0026ldquo;hcat.\u0026rdquo;\nInput Split Generation Behaviour    Property Default Description     hcat.desired.partition.num.splits not set This is a hint/guidance that can be provided to HCatalog to pass on to underlying InputFormats, to produce a \u0026ldquo;desired\u0026rdquo; number of splits per partition. This is useful for increasing parallelism by increasing the number of splits generated on a few large files. It is not as useful in cases where we would want to reduce the number of splits for a large number of files. It is not at all useful in cases where there are a large number of partitions that this job will read. Also note that this is merely an optimization hint, and it is not guaranteed that the underlying layer will be capable of using this optimization. Additionally, MapReduce parameters mapred.min.split.size and mapred.max.split.size can be used in conjunction with this parameter to tweak or optimize jobs.    Data Promotion Behaviour In some cases (such as some older versions of Pig), users of HCatalog may not support all the datatypes supported by Hive. There are a few configuration parameters provided to handle data promotions and conversions which allow them to read data through HCatalog. On the write side, it is expected that the user pass in HCatRecords with valid datatypes that match the schema.\n   Property Default Description     hcat.data.convert.boolean.to.integer false promotes boolean to int on read from HCatalog   hcat.data.tiny.small.int.promotion false promotes tinyint or smallint to int on read from HCatalog    HCatRecordReader Error Tolerance Behaviour While reading, it is understandable that data might contain errors, but we may not want to completely abort a task due to a couple of errors. These parameters configure how many errors HCatalog can accept before failing the task.\n   Property Default Description     hcat.input.bad.record.threshold 0.0001f A float parameter defaults to 0.0001f, which means it can deal with 1 error every 10,000 rows and still not error out. Any greater and it will.   hcat.input.bad.record.min 2 An int parameter defaults to 2, which is the minimum number of bad records encountered before applying the hcat.input.bad.record.threshold parameter. This is to prevent an initial or early bad record from causing a task abort because the ratio of errors was too high.     Navigation Links Previous: Installation from Tarball\nNext: Load and Store Interfaces\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-configuration-properties_39622369/","tags":null,"title":"Apache Hive : HCatalog Configuration Properties"},{"categories":null,"contents":"Apache Hive : HCatalog DynamicPartitions Dynamic Partitioning  Dynamic Partitioning  Overview  External Tables Hive Dynamic Partitions   Usage with Pig Usage from MapReduce    Overview When writing data in HCatalog it is possible to write all records to a single partition. In this case the partition column(s) need not be in the output data.\nThe following Pig script illustrates this:\nA = load 'raw' using HCatLoader(); ... split Z into for_us if region='us', for_eu if region='eu', for_asia if region='asia'; store for_us into 'processed' using HCatStorer(\u0026quot;ds=20110110, region=us\u0026quot;); store for_eu into 'processed' using HCatStorer(\u0026quot;ds=20110110, region=eu\u0026quot;); store for_asia into 'processed' using HCatStorer(\u0026quot;ds=20110110, region=asia\u0026quot;); In cases where you want to write data to multiple partitions simultaneously, this can be done by placing partition columns in the data and not specifying partition values when storing the data.\nA = load 'raw' using HCatLoader(); ... store Z into 'processed' using HCatStorer(); The way dynamic partitioning works is that HCatalog locates partition columns in the data passed to it and uses the data in these columns to split the rows across multiple partitions. (The data passed to HCatalog must have a schema that matches the schema of the destination table and hence should always contain partition columns.) It is important to note that partition columns can’t contain null values or the whole process will fail.\nIt is also important to note that all partitions created during a single run are part of one transaction; therefore if any part of the process fails, none of the partitions will be added to the table.\nExternal Tables Version\nThis section describes changes that occurred in HCatalog 0.5, 0.12, and 0.13 for dynamic partitions of external tables.\nStarting in HCatalog 0.5, dynamic partitioning on external tables was broken (HCATALOG-500). This issue was fixed in Hive 0.12.0 by creating dynamic partitions of external tables in locations based on metadata rather than user specifications (HIVE-5011). Starting in Hive 0.13.0, users are able to customize the locations by specifying a path pattern in the job configuration property hcat.dynamic.partitioning.custom.pattern (HIVE-6109). Static partitions for external tables can have user-specified locations in all Hive releases.\nFor example, in Hive 0.12.0 if a table named user_logs is partitioned by (year, month, day, hour, minute, country) and stored at external location \u0026ldquo;hdfs://hcat/data/user_logs\u0026rdquo;, then the locations of its dynamic partitions have the standard Hive format which includes keys as well as values, such as:\n hdfs://hcat/data/user_logs/year=2013/month=12/day=21/hour=06/minute=10/country=US  In Hive 0.13.0 and later, hcat.dynamic.partitioning.custom.pattern can be configured to a custom path pattern. For example, the pattern \u0026ldquo;${year}/${month}/${day}/${hour}/${minute}/${country}\u0026rdquo; omits keys from the path:\n hdfs://hcat/data/user_logs/2013/12/21/06/10/US  Each dynamic partition column must be present in the custom location path in the format ${column_name}, and the custom location path must consist of all dynamic partition columns. Other valid custom path strings include:\n data/${year}/${month}/${day}/${country} ${year}­‐${month}­‐${day}/country=${country} output/yr=${year}/mon=${month}/day=${day}/geo=${country}  See HCatalog Configuration Properties for another example. Also see the PDF attachment to HIVE-6019 for details of the implementation.\nHive Dynamic Partitions Information about Hive dynamic partitions is available here:\n Design Document for Dynamic Partitions  Original design doc HIVE-936   Tutorial: Dynamic-Partition Insert Hive DML: Dynamic Partition Inserts  Usage with Pig Usage from Pig is very simple! Instead of specifying all keys as one normally does for a store, users can specify the keys that are actually needed. HCatOutputFormat will trigger on dynamic partitioning usage if necessary (if a key value is not specified) and will inspect the data to write it out appropriately.\nSo this statement\u0026hellip;\nstore A into 'mytable' using HCatStorer(\u0026quot;a=1, b=1\u0026quot;); \u0026hellip;is equivalent to any of the following statements, if the data has only values where a=1 and b=1:\nstore A into 'mytable' using HCatStorer(); store A into 'mytable' using HCatStorer(\u0026quot;a=1\u0026quot;); store A into 'mytable' using HCatStorer(\u0026quot;b=1\u0026quot;); On the other hand, if there is data that spans more than one partition, then HCatOutputFormat will automatically figure out how to spray the data appropriately.\nFor example, let\u0026rsquo;s say a=1 for all values across our dataset and b takes the values 1 and 2. Then the following statement\u0026hellip;\nstore A into 'mytable' using HCatStorer(); \u0026hellip;is equivalent to either of these statements:\nstore A into 'mytable' using HCatStorer(\u0026quot;a=1\u0026quot;); split A into A1 if b='1', A2 if b='2'; store A1 into 'mytable' using HCatStorer(\u0026quot;a=1, b=1\u0026quot;); store A2 into 'mytable' using HCatStorer(\u0026quot;a=1, b=2\u0026quot;); Usage from MapReduce As with Pig, the only change in dynamic partitioning that a MapReduce programmer sees is that they don\u0026rsquo;t have to specify all the partition key/value combinations.\nA current code example for writing out a specific partition for (a=1, b=1) would go something like this:\nMap\u0026lt;String, String\u0026gt; partitionValues = new HashMap\u0026lt;String, String\u0026gt;(); partitionValues.put(\u0026quot;a\u0026quot;, \u0026quot;1\u0026quot;); partitionValues.put(\u0026quot;b\u0026quot;, \u0026quot;1\u0026quot;); HCatTableInfo info = HCatTableInfo.getOutputTableInfo(dbName, tblName, partitionValues); HCatOutputFormat.setOutput(job, info); And to write to multiple partitions, separate jobs will have to be kicked off with each of the above.\nWith dynamic partitioning, we simply specify only as many keys as we know about, or as required. It will figure out the rest of the keys by itself and spray out necessary partitions, being able to create multiple partitions with a single job.\n Navigation Links Previous: Storage Formats\nNext: Notification\nHive design document: Dynamic Partitions\nHive tutorial: Dynamic-Partition Insert\nHive DML: Dynamic Partition Inserts\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-dynamicpartitions_34014006/","tags":null,"title":"Apache Hive : HCatalog DynamicPartitions"},{"categories":null,"contents":"Apache Hive : HCatalog InputOutput Input and Output Interfaces  Input and Output Interfaces  Set Up HCatInputFormat  API   HCatOutputFormat  API   HCatRecord Running MapReduce with HCatalog  Authentication Read Example Filter Operators Scan Filter Write Filter      Set Up No HCatalog-specific setup is required for the HCatInputFormat and HCatOutputFormat interfaces.\nNote: HCatalog is not thread safe.\nHCatInputFormat The HCatInputFormat is used with MapReduce jobs to read data from HCatalog-managed tables.\nHCatInputFormat exposes a Hadoop 0.20 MapReduce API for reading data as if it had been published to a table.\nAPI The API exposed by HCatInputFormat is shown below. It includes:\n setInput setOutputSchema getTableSchema  To use HCatInputFormat to read data, first instantiate an InputJobInfo with the necessary information from the table being read and then call setInput with the InputJobInfo.\nYou can use the setOutputSchema method to include a projection schema, to specify the output fields. If a schema is not specified, all the columns in the table will be returned.\nYou can use the getTableSchema method to determine the table schema for a specified input table.\n /** * Set the input to use for the Job. This queries the metadata server with * the specified partition predicates, gets the matching partitions, puts * the information in the conf object. The inputInfo object is updated with * information needed in the client context * @param job the job object * @param inputJobInfo the input info for table to read * @throws IOException the exception in communicating with the metadata server */ public static void setInput(Job job, InputJobInfo inputJobInfo) throws IOException; /** * Set the schema for the HCatRecord data returned by HCatInputFormat. * @param job the job object * @param hcatSchema the schema to use as the consolidated schema */ public static void setOutputSchema(Job job,HCatSchema hcatSchema) throws IOException; /** * Get the HCatTable schema for the table specified in the HCatInputFormat.setInput * call on the specified job context. This information is available only after * HCatInputFormat.setInput has been called for a JobContext. * @param context the context * @return the table schema * @throws IOException if HCatInputFormat.setInput has not been called * for the current context */ public static HCatSchema getTableSchema(JobContext context) throws IOException; HCatOutputFormat HCatOutputFormat is used with MapReduce jobs to write data to HCatalog-managed tables.\nHCatOutputFormat exposes a Hadoop 0.20 MapReduce API for writing data to a table. When a MapReduce job uses HCatOutputFormat to write output, the default OutputFormat configured for the table is used and the new partition is published to the table after the job completes.\nAPI The API exposed by HCatOutputFormat is shown below. It includes:\n setOutput setSchema getTableSchema  The first call on the HCatOutputFormat must be setOutput; any other call will throw an exception saying the output format is not initialized. The schema for the data being written out is specified by the setSchema method. You must call this method, providing the schema of data you are writing. If your data has the same schema as the table schema, you can use HCatOutputFormat.getTableSchema() to get the table schema and then pass that along to setSchema().\n /** * Set the information about the output to write for the job. This queries the metadata * server to find the StorageHandler to use for the table. It throws an error if the * partition is already published. * @param job the job object * @param outputJobInfo the table output information for the job * @throws IOException the exception in communicating with the metadata server */ @SuppressWarnings(\u0026quot;unchecked\u0026quot;) public static void setOutput(Job job, OutputJobInfo outputJobInfo) throws IOException; /** * Set the schema for the data being written out to the partition. The * table schema is used by default for the partition if this is not called. * @param job the job object * @param schema the schema for the data * @throws IOException */ public static void setSchema(final Job job, final HCatSchema schema) throws IOException; /** * Get the table schema for the table specified in the HCatOutputFormat.setOutput call * on the specified job context. * @param context the context * @return the table schema * @throws IOException if HCatOutputFormat.setOutput has not been called * for the passed context */ public static HCatSchema getTableSchema(JobContext context) throws IOException; HCatRecord HCatRecord is the type supported for storing values in HCatalog tables.\nThe types in an HCatalog table schema determine the types of objects returned for different fields in HCatRecord. This table shows the mappings between Java classes for MapReduce programs and HCatalog data types:\n   HCatalog Data Type Java Class in MapReduce Values     TINYINT java.lang.Byte -128 to 127   SMALLINT java.lang.Short -(2^15) to (2^15)-1, which is -32,768 to 32,767   INT java.lang.Integer -(2^31) to (2^31)-1, which is -2,147,483,648 to 2,147,483,647   BIGINT java.lang.Long -(2^63) to (2^63)-1, which is -9,223,372,036,854,775,808 to 9,223,372,036,854,775,807   BOOLEAN java.lang.Boolean true or false   FLOAT java.lang.Float single-precision floating-point value   DOUBLE java.lang.Double double-precision floating-point value   DECIMAL java.math.BigDecimal exact floating-point value with 38-digit precision   BINARY byte[] binary data   STRING java.lang.String character string   STRUCT java.util.List structured data   ARRAY java.util.List values of one data type   MAP java.util.Map key-value pairs    For general information about Hive data types, see Hive Data Types and Type System.\nRunning MapReduce with HCatalog Your MapReduce program needs to be told where the Thrift server is. The easiest way to do this is to pass the location as an argument to your Java program. You need to pass the Hive and HCatalog jars to MapReduce as well, via the -libjars argument.\nexport HADOOP_HOME=\u0026lt;path_to_hadoop_install\u0026gt; export HCAT_HOME=\u0026lt;path_to_hcat_install\u0026gt; export HIVE_HOME=\u0026lt;path_to_hive_install\u0026gt; export LIB_JARS=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar, $HIVE_HOME/lib/hive-metastore-0.10.0.jar, $HIVE_HOME/lib/libthrift-0.7.0.jar, $HIVE_HOME/lib/hive-exec-0.10.0.jar, $HIVE_HOME/lib/libfb303-0.7.0.jar, $HIVE_HOME/lib/jdo2-api-2.3-ec.jar, $HIVE_HOME/lib/slf4j-api-1.6.1.jar export HADOOP_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar: $HIVE_HOME/lib/hive-metastore-0.10.0.jar: $HIVE_HOME/lib/libthrift-0.7.0.jar: $HIVE_HOME/lib/hive-exec-0.10.0.jar: $HIVE_HOME/lib/libfb303-0.7.0.jar: $HIVE_HOME/lib/jdo2-api-2.3-ec.jar: $HIVE_HOME/conf:$HADOOP_HOME/conf: $HIVE_HOME/lib/slf4j-api-1.6.1.jar $HADOOP_HOME/bin/hadoop --config $HADOOP_HOME/conf jar \u0026lt;path_to_jar\u0026gt; \u0026lt;main_class\u0026gt; -libjars $LIB_JARS \u0026lt;program_arguments\u0026gt; This works but Hadoop will ship libjars every time you run the MapReduce program, treating the files as different cache entries, which is not efficient and may deplete the Hadoop distributed cache.\nInstead, you can optimize to ship libjars using HDFS locations. By doing this, Hadoop will reuse the entries in the distributed cache.\nbin/hadoop fs -copyFromLocal $HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar /tmp bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/hive-metastore-0.10.0.jar /tmp bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/libthrift-0.7.0.jar /tmp bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/hive-exec-0.10.0.jar /tmp bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/libfb303-0.7.0.jar /tmp bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/jdo2-api-2.3-ec.jar /tmp bin/hadoop fs -copyFromLocal $HIVE_HOME/lib/slf4j-api-1.6.1.jar /tmp export LIB_JARS=hdfs:///tmp/hcatalog-core-0.5.0.jar, hdfs:///tmp/hive-metastore-0.10.0.jar, hdfs:///tmp/libthrift-0.7.0.jar, hdfs:///tmp/hive-exec-0.10.0.jar, hdfs:///tmp/libfb303-0.7.0.jar, hdfs:///tmp/jdo2-api-2.3-ec.jar, hdfs:///tmp/slf4j-api-1.6.1.jar # (Other statements remain the same.) Authentication If a failure results in a message like \u0026ldquo;2010-11-03 16:17:28,225 WARN hive.metastore \u0026hellip; - Unable to connect metastore with URI thrift://\u0026hellip;\u0026rdquo; in /tmp//hive.log, then make sure you have run \u0026ldquo;kinit @FOO.COM\u0026rdquo; to get a Kerberos ticket and to be able to authenticate to the HCatalog server.\nRead Example The following very simple MapReduce program reads data from one table which it assumes to have an integer in the second column (\u0026ldquo;column 1\u0026rdquo;), and counts how many instances of each distinct value it finds. That is, it does the equivalent of \u0026ldquo;select col1, count(*) from $table group by col1;\u0026rdquo;.\nFor example, if the values in the second column are {1,1,1,3,3,5} the program will produce this output of values and counts:\n1, 3\n3, 2\n5, 1\npublic class GroupByAge extends Configured implements Tool { public static class Map extends Mapper\u0026lt;WritableComparable, HCatRecord, IntWritable, IntWritable\u0026gt; { int age; @Override protected void map( WritableComparable key, HCatRecord value, org.apache.hadoop.mapreduce.Mapper\u0026lt;WritableComparable, HCatRecord, IntWritable, IntWritable\u0026gt;.Context context) throws IOException, InterruptedException { age = (Integer) value.get(1); context.write(new IntWritable(age), new IntWritable(1)); } } public static class Reduce extends Reducer\u0026lt;IntWritable, IntWritable, WritableComparable, HCatRecord\u0026gt; { @Override protected void reduce( IntWritable key, java.lang.Iterable\u0026lt;IntWritable\u0026gt; values, org.apache.hadoop.mapreduce.Reducer\u0026lt;IntWritable, IntWritable, WritableComparable, HCatRecord\u0026gt;.Context context) throws IOException, InterruptedException { int sum = 0; Iterator\u0026lt;IntWritable\u0026gt; iter = values.iterator(); while (iter.hasNext()) { sum++; iter.next(); } HCatRecord record = new DefaultHCatRecord(2); record.set(0, key.get()); record.set(1, sum); context.write(null, record); } } public int run(String[] args) throws Exception { Configuration conf = getConf(); args = new GenericOptionsParser(conf, args).getRemainingArgs(); String inputTableName = args[0]; String outputTableName = args[1]; String dbName = null; Job job = new Job(conf, \u0026quot;GroupByAge\u0026quot;); HCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, null)); // initialize HCatOutputFormat job.setInputFormatClass(HCatInputFormat.class); job.setJarByClass(GroupByAge.class); job.setMapperClass(Map.class); job.setReducerClass(Reduce.class); job.setMapOutputKeyClass(IntWritable.class); job.setMapOutputValueClass(IntWritable.class); job.setOutputKeyClass(WritableComparable.class); job.setOutputValueClass(DefaultHCatRecord.class); HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, null)); HCatSchema s = HCatOutputFormat.getTableSchema(job); System.err.println(\u0026quot;INFO: output schema explicitly set for writing:\u0026quot; + s); HCatOutputFormat.setSchema(job, s); job.setOutputFormatClass(HCatOutputFormat.class); return (job.waitForCompletion(true) ? 0 : 1); } public static void main(String[] args) throws Exception { int exitCode = ToolRunner.run(new GroupByAge(), args); System.exit(exitCode); } } Notice a number of important points about this program:\n The implementation of Map takes HCatRecord as an input and the implementation of Reduce produces it as an output. This example program assumes the schema of the input, but it could also retrieve the schema via HCatOutputFormat.getOutputSchema() and retrieve fields based on the results of that call. The input descriptor for the table to be read is created by calling InputJobInfo.create. It requires the database name, table name, and partition filter. In this example the partition filter is null, so all partitions of the table will be read. The output descriptor for the table to be written is created by calling OutputJobInfo.create. It requires the database name, the table name, and a Map of partition keys and values that describe the partition being written. In this example the table is assumed to be unpartitioned, so this Map is null.  To scan just selected partitions of a table, a filter describing the desired partitions can be passed to InputJobInfo.create. To scan a single partition, the filter string should look like: \u0026ldquo;ds=20120401\u0026rdquo; where the datestamp \u0026ldquo;ds\u0026rdquo; is the partition column name and \u0026ldquo;20120401\u0026rdquo; is the value you want to read (year, month, and day).\nFilter Operators A filter can contain the operators \u0026lsquo;and\u0026rsquo;, \u0026lsquo;or\u0026rsquo;, \u0026lsquo;like\u0026rsquo;, \u0026lsquo;()\u0026rsquo;, \u0026lsquo;=\u0026rsquo;, \u0026lsquo;\u0026lt;\u0026gt;\u0026rsquo; (not equal), \u0026lsquo;\u0026lt;\u0026rsquo;, \u0026lsquo;\u0026gt;\u0026rsquo;, \u0026lsquo;\u0026lt;=\u0026rsquo; and \u0026lsquo;\u0026gt;=\u0026rsquo;.\nFor example:\n ds \u0026gt; \u0026quot;20110924\u0026quot; ds \u0026lt; \u0026quot;20110925\u0026quot; ds \u0026lt;= \u0026quot;20110925\u0026quot; and ds \u0026gt;= \u0026quot;20110924\u0026quot;  Scan Filter Assume for example you have a web_logs table that is partitioned by the column \u0026ldquo;ds\u0026rdquo;. You could select one partition of the table by changing\nHCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, null)); to\nHCatInputFormat.setInput(job, InputJobInfo.create(dbName, inputTableName, \u0026quot;ds=\\\u0026quot;20110924\\\u0026quot;\u0026quot;)); This filter must reference only partition columns. Values from other columns will cause the job to fail.\nWrite Filter To write to a single partition you can change the above example to have a Map of key value pairs that describe all of the partition keys and values for that partition. In our example web_logs table, there is only one partition column (ds), so our Map will have only one entry. Change\nHCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, null)); to\nMap partitions = new HashMap\u0026lt;String, String\u0026gt;(1); partitions.put(\u0026quot;ds\u0026quot;, \u0026quot;20110924\u0026quot;); HCatOutputFormat.setOutput(job, OutputJobInfo.create(dbName, outputTableName, partitions)); To write multiple partitions simultaneously you can leave the Map null, but all of the partitioning columns must be present in the data you are writing.\n Navigation Links Previous: Load and Store Interfaces\nNext: Reader and Writer Interfaces\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-inputoutput_34013776/","tags":null,"title":"Apache Hive : HCatalog InputOutput"},{"categories":null,"contents":"Apache Hive : HCatalog InstallHCat Installation from Tarball  Installation from Tarball  HCatalog Installed with Hive HCatalog Command Line HCatalog Client Jars HCatalog Server    HCatalog Installed with Hive Version\nHCatalog is installed with Hive, starting with Hive release 0.11.0.\nHive installation is documented here.\nHCatalog Command Line If you install Hive from the binary tarball, the hcat command is available in the hcatalog/bin directory.\nThe hcat command line is similar to the hive command line; the main difference is that it restricts the queries that can be run to metadata-only operations such as DDL and DML queries used to read metadata (for example, \u0026ldquo;show tables\u0026rdquo;).\nThe HCatalog CLI is documented here and the Hive CLI is documented here.\nMost hcat commands can be issued as hive commands except for \u0026ldquo;hcat -g\u0026rdquo; and \u0026ldquo;hcat -p\u0026rdquo;. Note that the hcat command uses the -p flag for permissions but hive uses it to specify a port number.\nHCatalog Client Jars In the Hive tar.gz, HCatalog libraries are available under hcatalog/share/hcatalog/.\nHCatalog Server HCatalog server is the same as Hive metastore. You can just follow the Hive metastore documentation for setting it up.\n Navigation Links Previous: Using HCatalog\nNext: HCatalog Configuration Properties\nHive installation and configuration: Installing Hive, Configuring Hive, Hive Configuration Properties\nWebHCat installation and configuration: WebHCat Installation, WebHCat Configuration\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-installhcat_34013403/","tags":null,"title":"Apache Hive : HCatalog InstallHCat"},{"categories":null,"contents":"Apache Hive : HCatalog LoadStore Load and Store Interfaces  Load and Store Interfaces  Set Up Running Pig HCatLoader  Usage  Assumptions   HCatLoader Data Types  Types in Hive 0.12.0 and Earlier Types in Hive 0.13.0 and Later   Running Pig with HCatalog  The -useHCatalog Flag Jars and Configuration Files Authentication   Load Examples  Filter Operators     HCatStorer  Usage  Assumptions   Store Examples HCatStorer Data Types  Types in Hive 0.12.0 and Earlier Types in Hive 0.13.0 and Later     Data Type Mappings  Primitive Types Complex Types      Set Up The HCatLoader and HCatStorer interfaces are used with Pig scripts to read and write data in HCatalog-managed tables. No HCatalog-specific setup is required for these interfaces.\nNote: HCatalog is not thread safe.\nRunning Pig The -useHCatalog Flag\nTo bring in the appropriate jars for working with HCatalog, simply include the following flag / parameters when running Pig from the shell, Hue, or other applications:\npig -useHCatalog See section Running Pig with HCatalog below for details.\nStale Content Warning\nThe fully qualified package name changed from org.apache.hcatalog.pig to org.apache.hive.hcatalog.pig in Pig versions 0.14+. In many older web site examples you may find references to the old syntax which no longer functions.\n   Previous Pig Versions 0.14+     org.apache.hcatalog.pig.HCatLoader org.apache.hive.hcatalog.pig.HCatLoader   org.apache.hcatalog.pig.HCatStorer org.apache.hive.hcatalog.pig.HCatStorer    HCatLoader HCatLoader is used with Pig scripts to read data from HCatalog-managed tables.\nUsage HCatLoader is accessed via a Pig load statement. Using Pig 0.14+\nA = LOAD 'tablename' USING org.apache.hive.hcatalog.pig.HCatLoader(); Assumptions You must specify the table name in single quotes: LOAD \u0026lsquo;tablename\u0026rsquo;. If you are using a non-default database you must specify your input as \u0026lsquo;dbname.tablename\u0026rsquo;. If you are using Pig 0.9.2 or earlier, you must create your database and table prior to running the Pig script. Beginning with Pig 0.10 you can issue these create commands in Pig using the SQL command. Details of Pig syntax can be found in PIG-2482.\nThe Hive metastore lets you create tables without specifying a database; if you created tables this way, then the database name is \u0026lsquo;default\u0026rsquo; and is not required when specifying the table for HCatLoader.\nIf the table is partitioned, you can indicate which partitions to scan by immediately following the load statement with a partition filter statement (see Load Examples below).\nHCatLoader Data Types Restrictions apply to the types of columns HCatLoader can read from HCatalog-managed tables. HCatLoader can read only the Hive data types listed below. The tables in Data Type Mappings show how Pig will interpret each Hive data type.\nTypes in Hive 0.12.0 and Earlier Hive 0.12.0 and earlier releases support reading these Hive primitive data types with HCatLoader:\n boolean int long float double string binary  and these complex data types:\n map – key type should be string ARRAY\u0026lt;any type\u0026gt; struct\u0026lt;any type fields\u0026gt;  Types in Hive 0.13.0 and Later Hive 0.13.0 added support for reading these Hive data types with HCatLoader (HIVE-5814):\n tinyint smallint date timestamp decimal char(x) varchar(x)  See Data Type Mappings below for details of the mappings between Hive and Pig types.\nNote\nHive does not have a data type corresponding to the big integer type in Pig.\nRunning Pig with HCatalog Pig does not automatically pick up HCatalog jars. To bring in the necessary jars, you can either use a flag in the pig command or set the environment variables PIG_CLASSPATH and PIG_OPTS as described below.\nThe -useHCatalog Flag To bring in the appropriate jars for working with HCatalog, simply include the following flag:\npig -useHCatalog Jars and Configuration Files For Pig commands that omit -useHCatalog, you need to tell Pig where to find your HCatalog jars and the Hive jars used by the HCatalog client. To do this, you must define the environment variable PIG_CLASSPATH with the appropriate jars.\nHCatalog can tell you the jars it needs. In order to do this it needs to know where Hadoop and Hive are installed. Also, you need to tell Pig the URI for your metastore, in the PIG_OPTS variable.\nIn the case where you have installed Hadoop and Hive via tar, you can do this:\nexport HADOOP_HOME=\u0026lt;path_to_hadoop_install\u0026gt; export HIVE_HOME=\u0026lt;path_to_hive_install\u0026gt; export HCAT_HOME=\u0026lt;path_to_hcat_install\u0026gt; export PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:\\ $HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:\\ $HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:\\ $HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:\\ $HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:\\ $HIVE_HOME/lib/slf4j-api-*.jar export PIG_OPTS=-Dhive.metastore.uris=thrift://\u0026lt;hostname\u0026gt;:\u0026lt;port\u0026gt; Or you can pass the jars in your command line:\n\u0026lt;path_to_pig_install\u0026gt;/bin/pig -Dpig.additional.jars=\\ $HCAT_HOME/share/hcatalog/hcatalog-core*.jar:\\ $HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:\\ $HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:\\ $HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:\\ $HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/lib/slf4j-api-*.jar \u0026lt;script.pig\u0026gt; The version number found in each filepath will be substituted for *. For example, HCatalog release 0.5.0 uses these jars and conf files:\n $HCAT_HOME/share/hcatalog/hcatalog-core-0.5.0.jar $HCAT_HOME/share/hcatalog/hcatalog-pig-adapter-0.5.0.jar $HIVE_HOME/lib/hive-metastore-0.10.0.jar $HIVE_HOME/lib/libthrift-0.7.0.jar $HIVE_HOME/lib/hive-exec-0.10.0.jar $HIVE_HOME/lib/libfb303-0.7.0.jar $HIVE_HOME/lib/jdo2-api-2.3-ec.jar $HIVE_HOME/conf $HADOOP_HOME/conf $HIVE_HOME/lib/slf4j-api-1.6.1.jar  Authentication If you are using a secure cluster and a failure results in a message like \u0026ldquo;2010-11-03 16:17:28,225 WARN hive.metastore \u0026hellip; - Unable to connect metastore with URI thrift://\u0026hellip;\u0026rdquo; in /tmp//hive.log, then make sure you have run \u0026ldquo;kinit @FOO.COM\u0026rdquo; to get a Kerberos ticket and to be able to authenticate to the HCatalog server.\nLoad Examples This load statement will load all partitions of the specified table.\n/* myscript.pig */ A = LOAD 'tablename' USING org.apache.hive.hcatalog.pig.HCatLoader(); ... ... If only some partitions of the specified table are needed, include a partition filter statement immediately following the load statement in the data flow. (In the script, however, a filter statement might not immediately follow its load statement.) The filter statement can include conditions on partition as well as non-partition columns.\n/* myscript.pig */ A = LOAD 'tablename' USING org.apache.hive.hcatalog.pig.HCatLoader(); -- date is a partition column; age is not B = filter A by date == '20100819' and age \u0026lt; 30; -- both date and country are partition columns C = filter A by date == '20100819' and country == 'US'; ... ... To scan a whole table, for example:\na = load 'student_data' using org.apache.hive.hcatalog.pig.HCatLoader(); b = foreach a generate name, age; Notice that the schema is automatically provided to Pig; there\u0026rsquo;s no need to declare name and age as fields, as if you were loading from a file.\nTo scan a single partition of the table web_logs partitioned by the column datestamp, for example:\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader(); b = filter a by datestamp == '20110924'; Pig will push the datestamp filter shown here to HCatalog, so that HCatalog knows to just scan the partition where datestamp = \u0026lsquo;20110924\u0026rsquo;. You can combine this filter with others via \u0026lsquo;and\u0026rsquo;:\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader(); b = filter a by datestamp == '20110924' and user is not null; Pig will split the above filter, pushing the datestamp portion to HCatalog and retaining the \u0026lsquo;user is not null\u0026rsquo; part to apply itself. You can also give a more complex filter to retrieve a set of partitions.\nFilter Operators A filter can contain the operators \u0026lsquo;and\u0026rsquo;, \u0026lsquo;or\u0026rsquo;, \u0026lsquo;()\u0026rsquo;, \u0026lsquo;==\u0026rsquo;, \u0026lsquo;!=\u0026rsquo;, \u0026lsquo;\u0026lt;\u0026rsquo;, \u0026lsquo;\u0026gt;\u0026rsquo;, \u0026lsquo;\u0026lt;=\u0026rsquo; and \u0026lsquo;\u0026gt;=\u0026rsquo;.\nFor example:\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader(); b = filter a by datestamp \u0026gt; '20110924'; A complex filter can have various combinations of operators, such as:\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader(); b = filter a by datestamp == '20110924' or datestamp == '20110925'; These two examples have the same effect:\na = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader(); b = filter a by datestamp \u0026gt;= '20110924' and datestamp \u0026lt;= '20110925'; a = load 'web_logs' using org.apache.hive.hcatalog.pig.HCatLoader(); b = filter a by datestamp \u0026lt;= '20110925' and datestamp \u0026gt;= '20110924'; HCatStorer HCatStorer is used with Pig scripts to write data to HCatalog-managed tables.\nUsage HCatStorer is accessed via a Pig store statement.\nA = LOAD ... B = FOREACH A ... ... ... my_processed_data = ... STORE my_processed_data INTO 'tablename' USING org.apache.hive.hcatalog.pig.HCatStorer(); Assumptions You must specify the table name in single quotes: LOAD \u0026lsquo;tablename\u0026rsquo;. Both the database and table must be created prior to running your Pig script. If you are using a non-default database you must specify your input as \u0026lsquo;dbname.tablename\u0026rsquo;. If you are using Pig 0.9.2 or earlier, you must create your database and table prior to running the Pig script. Beginning with Pig 0.10 you can issue these create commands in Pig using the SQL command.\nThe Hive metastore lets you create tables without specifying a database; if you created tables this way, then the database name is \u0026lsquo;default\u0026rsquo; and you do not need to specify the database name in the store statement.\nFor the USING clause, you can have a string argument that represents key/value pairs for partition. This is a mandatory argument when you are writing to a partitioned table and the partition column is not in the output column. The values for partition keys should NOT be quoted.\nIf partition columns are present in data they need not be specified as a STORE argument. Instead HCatalog will use these values to place records in the appropriate partition(s). It is valid to specify some partition keys in the STORE statement and have other partition keys in the data.\nStore Examples You can write to a non-partitioned table simply by using HCatStorer. The contents of the table will be overwritten:\nstore z into 'web_data' using org.apache.hive.hcatalog.pig.HCatStorer(); To add one new partition to a partitioned table, specify the partition value in the store function. Pay careful attention to the quoting, as the whole string must be single quoted and separated with an equals sign:\nstore z into 'web_data' using org.apache.hive.hcatalog.pig.HCatStorer('datestamp=20110924'); To write into multiple partitions at once, make sure that the partition column is present in your data, then call HCatStorer with no argument:\nstore z into 'web_data' using org.apache.hive.hcatalog.pig.HCatStorer(); -- datestamp must be a field in the relation z HCatStorer Data Types Restrictions apply to the types of columns HCatStorer can write to HCatalog-managed tables. HCatStorer can write only the data types listed below.\nThe tables in Data Type Mappings show how HCatalog will interpret each Pig data type.\nTypes in Hive 0.12.0 and Earlier Hive 0.12.0 and earlier releases support writing these Pig primitive data types with HCatStorer:\n boolean int long float double chararray bytearray  and these complex data types:\n map bag tuple  Types in Hive 0.13.0 and Later Hive 0.13.0 added support for writing these Pig data types with HCatStorer (HIVE-5814):\n short datetime bigdecimal  and added more Hive data types that the Pig types can be written to. See Data Type Mappings below for details of the mappings between Pig and Hive types.\nNote\nHive does not have a data type corresponding to the biginteger type in Pig.\nData Type Mappings The tables below show the mappings between data types in HCatalog-managed Hive tables and Pig. For general information about Hive data types, see Hive Data Types and Type System.\nAny type mapping not listed here is not supported and will throw an exception. The user is expected to cast the value to a compatible type first (in a Pig script, for example).\nPrimitive Types    Hive Type/Value Pig Type/Value Hive → Pig Pig → Hive Available in Hive Release     BOOLEAN/boolean BOOLEAN/boolean direct/lossless mapping direct/lossless mapping    TINYINT/byte INTEGER/int direct/lossless mapping performs a range check1 0.13.0+   SMALLINT/short SMALLINT/short direct/lossless mapping performs a range check1 0.13.0+   INT/int INTEGER/int direct/lossless mapping direct/lossless mapping    BIGINT/long LONG/long direct/lossless mapping direct/lossless mapping    FLOAT/float FLOAT/float direct/lossless mapping direct/lossless mapping    DOUBLE/double DOUBLE/double direct/lossless mapping direct/lossless mapping    STRING/java.lang.String CHARARRAY/java.lang.String direct/lossless mapping direct/lossless mapping    BINARY/byte[] BYTEARRAY/org.apache.pig.data.DataByteArray direct/lossless mapping direct/lossless mapping    DATE DATETIME/org.joda.time.DateTime turn to DateTime with time part set to 0 and local timezone if time component is 0 (regardless of timezone in the DateTime value), it will be written to target; otherwise it is considered out of range1 0.13.0+   TIMESTAMP/java.sql.Timestamp DATETIME/org.joda.time.DateTime will lose ‘nanos’ and set timezone to local timezone will translate to Timestamp based on \u0026lsquo;millis\u0026rsquo; value 0.13.0+   DECIMAL/HiveDecimal (maximum 38 digits) BIGDECIMAL/java.math.BigDecimal direct/lossless mapping performs a range check1 0.13.0+   CHAR(x)/HiveChar CHARARRAY/java.lang.String direct/lossless mapping performs a range check1 0.13.0+   VARCHAR(x)/HiveVarchar CHARARRAY/java.lang.String direct/lossless mapping performs a range check1 0.13.0+   1 Range check: If the Pig value is out of range for the target Hive column, by default NULL will be written and one warning per target column/type will be logged. The user may specify “onOutOfRangeValue Throw” to HCatStorer so that an error will be raised instead. For example:store data into 'test_tbl' using org.apache.hive.hcatalog.pig.HCatStorer('','','-onOutOfRangeValue Throw');The only values for onOutOfRangeValue are Throw and Null (default).        Note\nHive does not have a data type corresponding to the BIGINTEGER type in Pig (java.math.BigInteger values).\nComplex Types     Hive Type Pig Type     map (key type should be string) map   ARRAY\u0026lt;any type\u0026gt; bag   struct\u0026lt;any type fields\u0026gt; tuple     Navigation Links Previous: HCatalog Configuration Properties\nNext: Input and Output Interfaces\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-loadstore_34013511/","tags":null,"title":"Apache Hive : HCatalog LoadStore"},{"categories":null,"contents":"Apache Hive : HCatalog Notification Notification  Notification  Notification for a New Partition Notification for a Set of Partitions Server Configuration  Enable JMS Notifications Topic Names      Since version 0.2, HCatalog provides notifications for certain events happening in the system. This way applications such as Oozie can wait for those events and schedule the work that depends on them. The current version of HCatalog supports two kinds of events:\n Notification when a new partition is added Notification when a set of partitions is added  No additional work is required to send a notification when a new partition is added: the existing addPartition call will send the notification message.\nNotification for a New Partition To receive notification that a new partition has been added, you need to follow these three steps.\n To start receiving messages, create a connection to a message bus as shown here:  ConnectionFactory connFac = new ActiveMQConnectionFactory(amqurl); Connection conn = connFac.createConnection(); conn.start();  Subscribe to a topic you are interested in. When subscribing on a message bus, you need to subscribe to a particular topic to receive the messages that are being delivered on that topic.\n The topic name corresponding to a particular table is stored in table properties and can be retrieved using the following piece of code:  HiveMetaStoreClient msc = new HiveMetaStoreClient(hiveConf); String topicName = msc.getTable(\u0026quot;mydb\u0026quot;, \u0026quot;myTbl\u0026quot;).getParameters().get(HCatConstants.HCAT_MSGBUS_TOPIC_NAME); ``\n Use the topic name to subscribe to a topic as follows:  Session session = conn.createSession(true, Session.SESSION_TRANSACTED); Destination hcatTopic = session.createTopic(topicName); MessageConsumer consumer = session.createConsumer(hcatTopic); consumer.setMessageListener(this); ``\n  To start receiving messages you need to implement the JMS interface MessageListener, which, in turn, will make you implement the method onMessage(Message msg). This method will be called whenever a new message arrives on the message bus. The message contains a partition object representing the corresponding partition, which can be retrieved as shown here:\n  @Override public void onMessage(Message msg) { // We are interested in only add_partition events on this table. // So, check message type first. if(msg.getStringProperty(HCatConstants.HCAT_EVENT).equals(HCatConstants.HCAT_ADD_PARTITION_EVENT)){ Object obj = (((ObjectMessage)msg).getObject()); } } You need to have a JMS jar in your classpath to make this work. Additionally, you need to have a JMS provider’s jar in your classpath. HCatalog is tested with ActiveMQ as a JMS provider, although any JMS provider can be used. ActiveMQ can be obtained from: http://activemq.apache.org/activemq-550-release.html.\nNotification for a Set of Partitions Sometimes you need to wait until a collection of partitions is finished before proceeding with another operation. For example, you may want to start processing after all partitions for a day are done. However, HCatalog has no notion of collections or hierarchies of partitions. To support this, HCatalog allows data writers to signal when they are finished writing a collection of partitions. Data readers may wait for this signal before beginning to read.\nThe example code below illustrates how to send a notification when a set of partitions has been added.\nTo signal, a data writer does this:\nHiveMetaStoreClient msc = new HiveMetaStoreClient(conf); // Create a map, specifying partition key names and values Map\u0026lt;String,String\u0026gt; partMap = new HashMap\u0026lt;String, String\u0026gt;(); partMap.put(\u0026quot;date\u0026quot;,\u0026quot;20110711\u0026quot;); partMap.put(\u0026quot;country\u0026quot;,\u0026quot;*\u0026quot;); // Mark the partition as \u0026quot;done\u0026quot; msc.markPartitionForEvent(\u0026quot;mydb\u0026quot;, \u0026quot;mytbl\u0026quot;, partMap, PartitionEventType.LOAD_DONE); To receive this notification, the consumer needs to do the following:\n Repeat steps one and two from above to establish the connection to the notification system and to subscribe to the topic. Receive the notification as shown in this example:  HiveMetaStoreClient msc = new HiveMetaStoreClient(conf); // Create a map, specifying partition key names and values Map\u0026lt;String,String\u0026gt; partMap = new HashMap\u0026lt;String, String\u0026gt;(); partMap.put(\u0026quot;date\u0026quot;,\u0026quot;20110711\u0026quot;); partMap.put(\u0026quot;country\u0026quot;,\u0026quot;*\u0026quot;); // Mark the partition as \u0026quot;done\u0026quot; msc.markPartitionForEvent(\u0026quot;mydb\u0026quot;, \u0026quot;mytbl\u0026quot;, partMap, PartitionEventType.LOAD_DONE); If the consumer has registered with the message bus and is currently live, it will get the callback from the message bus once the producer marks the partition as \u0026ldquo;done\u0026rdquo;. Alternatively, the consumer can ask explicitly for a particular partition from the metastore. The following code illustrates the usage from a consumer\u0026rsquo;s perspective:\n// Enquire to metastore whether a particular partition has been marked or not. boolean marked = msc.isPartitionMarkedForEvent(\u0026quot;mydb\u0026quot;, \u0026quot;mytbl\u0026quot;, partMap, PartitionEventType.LOAD_DONE); // Or register to a message bus and get asynchronous callback. ConnectionFactory connFac = new ActiveMQConnectionFactory(amqurl); Connection conn = connFac.createConnection(); conn.start(); Session session = conn.createSession(true, Session.SESSION_TRANSACTED); Destination hcatTopic = session.createTopic(topic); MessageConsumer consumer = session.createConsumer(hcatTopic); consumer.setMessageListener(this); public void onMessage(Message msg) { MapMessage mapMsg = (MapMessage)msg; Enumeration\u0026lt;String\u0026gt; keys = mapMsg.getMapNames(); // Enumerate over all keys. This will print key-value pairs specifying the // particular partition 44which was marked done. In this case, it will print: // date : 20110711 // country: * while(keys.hasMoreElements()){ String key = keys.nextElement(); System.out.println(key + \u0026quot; : \u0026quot; + mapMsg.getString(key)); } System.out.println(\u0026quot;Message: \u0026quot;+msg); Server Configuration To enable notification, you need to configure the server (see below).\nTo disable notification, you need to leave hive.metastore.event.listeners blank or remove it from hive-site.xml.\nEnable JMS Notifications You need to make (add/modify) the following changes to the hive-site.xml file of your HCatalog server to turn on notifications.\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.event.expiry.duration\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;300L\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Duration after which events expire from events table (in seconds)\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.event.clean.freq\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;360L\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Frequency at which timer task runs to purge expired events in metastore (in seconds).\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;msgbus.brokerurl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;tcp://localhost:61616\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;msgbus.username\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;msgbus.password\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; For the server to start with support for notifications, the following must be in the classpath:\n (a) activemq jar\n (b) jndi.properties file with properties suitably configured for notifications\nThen, follow these guidelines to set up your environment:\n The HCatalog server start script is $YOUR_HCAT_SERVER/share/hcatalog/scripts/hcat_server_start.sh. This script expects classpath to be set by the AUX_CLASSPATH environment variable. Therefore set AUX_CLASSPATH to satisfy (a) and (b) above. The jndi.properties file is located at $YOUR_HCAT_SERVER/etc/hcatalog/jndi.properties. You need to uncomment and set the following properties in the jndi.properties file:  java.naming.factory.initial = org.apache.activemq.jndi.ActiveMQInitialContextFactory java.naming.provider.url = tcp://localhost:61616 (This is the ActiveMQ URL in your setup.)    Topic Names If tables are created while the server is configured for notifications, a default topic name is automatically set as a table property. To use notifications with tables created previously (either in other HCatalog installations or prior to enabling notifications in the current installation) you will have to manually set a topic name. For example:\n $YOUR_HCAT_CLIENT_HOME/bin/hcat -e \u0026quot;ALTER TABLE access SET hcat.msgbus.topic.name=$TOPIC_NAME\u0026quot;\nYou then need to configure your ActiveMQ Consumer(s) to listen for messages on the topic you gave in $TOPIC_NAME. A good default policy is TOPIC_NAME = \u0026quot;$database.$table\u0026quot; (that is a literal dot).\nNavigation Links Previous: Dynamic Partitioning\nNext: Storage Based Authorization\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-notification_34014558/","tags":null,"title":"Apache Hive : HCatalog Notification"},{"categories":null,"contents":"Apache Hive : HCatalog ReaderWriter Reader and Writer Interfaces  Reader and Writer Interfaces  Overview HCatReader HCatWriter Complete Example Program    Overview HCatalog provides a data transfer API for parallel input and output without using MapReduce. This API provides a way to read data from a Hadoop cluster or write data into a Hadoop cluster, using a basic storage abstraction of tables and rows.\nThe data transfer API has three essential classes:\n HCatReader – reads data from a Hadoop cluster HCatWriter – writes data into a Hadoop cluster DataTransferFactory – generates reader and writer instances  Auxiliary classes in the data transfer API include:\n ReadEntity ReaderContext WriteEntity WriterContext  The HCatalog data transfer API is designed to facilitate integration of external systems with Hadoop.\nNote: HCatalog is not thread safe.\nHCatReader Reading is a two-step process in which the first step occurs on the master node of an external system. The second step is done in parallel on multiple slave nodes.\nReads are done on a “ReadEntity”. Before you start to read, you need to define a ReadEntity from which to read. This can be done through ReadEntity.Builder. You can specify a database name, table name, partition, and filter string. For example:\nReadEntity.Builder builder = new ReadEntity.Builder(); ReadEntity entity = builder.withDatabase(\u0026quot;mydb\u0026quot;).withTable(\u0026quot;mytbl\u0026quot;).build(); The code snippet above defines a ReadEntity object (“entity”), comprising a table named “mytbl” in a database named “mydb”, which can be used to read all the rows of this table. Note that this table must exist in HCatalog prior to the start of this operation.\nAfter defining a ReadEntity, you obtain an instance of HCatReader using the ReadEntity and cluster configuration:\nHCatReader reader = DataTransferFactory.getHCatReader(entity, config); The next step is to obtain a ReaderContext from reader as follows:\nReaderContext cntxt = reader.prepareRead(); All of the above steps occur on the master node. The master node then serializes this ReaderContext object and sends it to all the slave nodes. Slave nodes then use this reader context to read data.\nfor(InputSplit split : readCntxt.getSplits()){ HCatReader reader = DataTransferFactory.getHCatReader(split, readerCntxt.getConf()); Iterator\u0026lt;HCatRecord\u0026gt; itr = reader.read(); while(itr.hasNext()){ HCatRecord read = itr.next(); } } HCatWriter Similar to reading, writing is also a two-step process in which the first step occurs on the master node. Subsequently, the second step occurs in parallel on slave nodes.\nWrites are done on a “WriteEntity” which can be constructed in a fashion similar to reads:\nWriteEntity.Builder builder = new WriteEntity.Builder(); WriteEntity entity = builder.withDatabase(\u0026quot;mydb\u0026quot;).withTable(\u0026quot;mytbl\u0026quot;).build(); The code above creates a WriteEntity object (“entity”) which can be used to write into a table named “mytbl” in the database “mydb”.\nAfter creating a WriteEntity, the next step is to obtain a WriterContext:\nHCatWriter writer = DataTransferFactory.getHCatWriter(entity, config); WriterContext info = writer.prepareWrite(); All of the above steps occur on the master node. The master node then serializes the WriterContext object and makes it available to all the slaves.\nOn slave nodes, you need to obtain an HCatWriter using WriterContext as follows:\nHCatWriter writer = DataTransferFactory.getHCatWriter(context); Then, writer takes an iterator as the argument for the write method:\nwriter.write(hCatRecordItr); The writer then calls getNext() on this iterator in a loop and writes out all the records attached to the iterator.\nComplete Example Program A complete java program for the reader and writer examples above can be found here: https://github.com/apache/hive/blob/trunk/hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestReaderWriter.java\n Navigation Links Previous: Input and Output Interfaces\nNext: Command Line Interface\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-readerwriter_34013921/","tags":null,"title":"Apache Hive : HCatalog ReaderWriter"},{"categories":null,"contents":"Apache Hive : HCatalog StorageFormats Storage Formats  Storage Formats  SerDes and Storage Formats Usage from Hive CTAS Issue with JSON SerDe    SerDes and Storage Formats HCatalog uses Hive\u0026rsquo;s SerDe class to serialize and deserialize data. SerDes are provided for RCFile, CSV text, JSON text, and SequenceFile formats. Check the SerDe documentation for additional SerDes that might be included in new versions. For example, the Avro SerDe was added in Hive 0.9.1, the ORC file format was added in Hive 0.11.0, and Parquet was added in Hive 0.10.0 (plug-in) and Hive 0.13.0 (native).\nUsers can write SerDes for custom formats using these instructions:\n How to Write Your Own SerDe in the Developer Guide Hive User Group Meeting August 2009 pages 64-70 also see SerDe for details about input and output processing  For information about how to create a table with a custom or native SerDe, see Row Format, Storage Format, and SerDe.\nUsage from Hive Hive and HCatalog (version 0.4 and later) share the same storage abstractions, and thus, you can read from and write to HCatalog tables from within Hive, and vice versa.\nHowever, for HCatalog versions 0.4 and 0.5 Hive does not know where to find the HCatalog jar by default, so if you use any features that have been introduced by HCatalog, such as a table using the JSON SerDe, you might get a \u0026ldquo;class not found\u0026rdquo; exception. In this situation, before you run Hive, set environment variable HIVE_AUX_JARS_PATH to the directory with your HCatalog jar. (If the examples in the Installation document were followed, that should be /usr/local/hcat/share/hcatalog/.)\nAfter version 0.5, HCatalog is part of the Hive distribution and you do not have to add the HCatalog jar to HIVE_AUX_JARS_PATH.\nCTAS Issue with JSON SerDe Using the Hive CREATE TABLE \u0026hellip; AS SELECT command with a JSON SerDe results in a table that has column headers such as \u0026ldquo;_col0\u0026rdquo;, which can be read by HCatalog or Hive but cannot be easily read by external users. To avoid this issue, create the table in two steps instead of using CTAS:\n CREATE TABLE \u0026hellip; INSERT OVERWRITE TABLE \u0026hellip; SELECT \u0026hellip;  See HCATALOG-436 for details.\n Navigation Links Previous: Command Line Interface\nNext: Dynamic Partitioning\nSerDe general information: Hive SerDe\nSerDe details: SerDe\nSerDe DDL: Row Format, Storage Format, and SerDe\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-storageformats_34013997/","tags":null,"title":"Apache Hive : HCatalog StorageFormats"},{"categories":null,"contents":"Apache Hive : HCatalog Streaming Mutation API A Java API focused on mutating (insert/update/delete) records into transactional tables using Hive’s ACID feature. It is introduced in Hive 2.0.0 (HIVE-10165).\n Background Structure Data Requirements Streaming Requirements Record Layout Connection and Transaction Management Writing Data  Dynamic Partition Creation   Reading Data Example  Background In certain data processing use cases it is necessary to modify existing data when new facts arrive. An example of this is the classic ETL merge where a copy of a data set is kept in sync with a master by the frequent application of deltas. The deltas describe the mutations (inserts, updates, deletes) that have occurred to the master since the previous sync. To implement such a case using Hadoop traditionally demands that the partitions containing records targeted by the mutations be rewritten. This is a coarse approach; a partition containing millions of records might be rebuilt because of a single record change. Additionally these partitions cannot be restated atomically; at some point the old partition data must be swapped with the new partition data. When this swap occurs, usually by issuing an HDFS rm followed by a mv, the possibility exists where the data appears to be unavailable and hence any downstream jobs consuming the data might unexpectedly fail. Therefore data processing patterns that restate raw data on HDFS cannot operate robustly without some external mechanism to orchestrate concurrent access to changing data.\nThe availability of ACID tables in Hive provides a mechanism that both enables concurrent access to data stored in HDFS (so long as it\u0026rsquo;s in the ORC+ACID format) and also permits row level mutations on records within a table, without the need to rewrite the existing data. But while Hive itself supports INSERT, UPDATE and DELETE commands, and the ORC format can support large batches of mutations in a transaction, Hive\u0026rsquo;s execution engine currently submits each individual mutation operation in a separate transaction and issues table scans (M/R jobs) to execute them. It does not currently scale to the demands of processing large deltas in an atomic manner. Furthermore it would be advantageous to extend atomic batch mutation capabilities beyond Hive by making them available to other data processing frameworks. The Streaming Mutation API does just this.\nThe Streaming Mutation API, although similar to the Streaming API, has a number of differences and is built to enable very different use cases. Superficially, the Streaming API can only write new data whereas the mutation API can also modify existing data. However the two APIs are also based on very different transaction models. The Streaming API focuses on surfacing a continuous stream of new data into a Hive table and does so by batching small sets of writes into multiple short-lived transactions. Conversely the mutation API is designed to infrequently apply large sets of mutations to a data set in an atomic fashion: either all or none of the mutations will be applied. This instead mandates the use of a single long-lived transaction. This table summarises the attributes of each API:\n   Attribute Streaming API Mutation API     Ingest type Data arrives continuously. Ingests are performed periodically and the mutations are applied in a single batch.   Transaction scope Transactions are created for small batches of writes. The entire set of mutations should be applied within a single transaction.   Data availability Surfaces new data to users frequently and quickly. Change sets should be applied atomically, either the effect of the delta is visible or it is not.   Sensitive to record order No, records do not have pre-existing lastTxnIds or bucketIds. Records are likely being written into a single partition (today\u0026rsquo;s date for example). Yes, all mutated records have existing RecordIdentifiers and must be grouped by [partitionValues, bucketId] and sorted by lastTxnId. These record coordinates initially arrive in an order that is effectively random.   Impact of a write failure Transaction can be aborted and producer can choose to resubmit failed records as ordering is not important. Ingest for the respective group (partitionValues + bucketId) must be halted and failed records resubmitted to preserve sequence.   User perception of missing data Data has not arrived yet → \u0026ldquo;latency?\u0026rdquo; \u0026ldquo;This data is inconsistent, some records have been updated, but other related records have not\u0026rdquo; – consider here the classic transfer between bank accounts scenario.   API end point scope A given HiveEndPoint instance submits many transactions to a specific bucket, in a specific partition, of a specific table. A set of MutationCoordinators writes changes to unknown set of buckets, of an unknown set of partitions, of specific tables (can be more than one), within a single transaction.    Structure The API comprises two main concerns: transaction management, and the writing of mutation operations to the data set. The two concerns have a minimal coupling as it is expected that transactions will be initiated from a single job launcher type process while the writing of mutations will be scaled out across any number of worker nodes. In the context of Hadoop M/R these can be more concretely defined as the Tool and Map/Reduce task components. However, use of this architecture is not mandated and in fact both concerns could be handled within a single simple process depending on the requirements.\nNote that a suitably configured Hive instance is required to operate this system even if you do not intend to access the data from within Hive. Internally, transactions are managed by the Hive MetaStore. Mutations are performed to HDFS via ORC APIs that bypass the MetaStore. Additionally you may wish to configure your MetaStore instance to perform periodic data compactions.\nNote on packaging: The APIs are defined in the org.apache.hive.hcatalog.streaming.mutate Java package and included as the hive-hcatalog-streaming jar.\nData Requirements Generally speaking, to apply a mutation to a record one must have some unique key that identifies the record. However, primary keys are not a construct provided by Hive. Internally Hive uses RecordIdentifiers stored in a virtual ROW__ID column to uniquely identify records within an ACID table. Therefore, any process that wishes to issue mutations to a table via this API must have available the corresponding row ids for the target records. What this means in practice is that the process issuing mutations must first read in a current snapshot of the data and then join the mutations on some domain specific primary key to obtain the corresponding Hive ROW__ID. This is effectively what occurs within Hive\u0026rsquo;s table scan process when an UPDATE or DELETE statement is executed. The AcidInputFormat provides access to this data via AcidRecordReader.getRecordIdentifier().\nThe implementation of the ACID format places some constraints on the order in which records are written and it is important that this ordering is enforced. Additionally, data must be grouped appropriately to adhere to the constraints imposed by the OrcRecordUpdater. Grouping also makes it possible to parallelise the writing of mutations for the purposes of scaling. Finally, to correctly bucket new records (inserts) there is a slightly unintuitive trick that must be applied.\nAll of these data sequencing concerns are the responsibility of the client process calling the API which is assumed to have first class grouping and sorting capabilities (Hadoop Map/Reduce etc). The streaming API provides nothing more than validators that fail fast when they encounter groups and records that are out of sequence.\nIn short, API client processes should prepare data for the mutate API like so:\n MUST: Order records by ROW__ID.originalTxn, then ROW__ID.rowId. MUST: Assign a ROW__ID containing a computed bucketId to each record to be inserted. SHOULD: Group/partition by table partition value, then ROW__ID.bucketId.  The addition of bucket ids to insert records prior to grouping and sorting seems unintuitive. However, it is required to ensure both adequate partitioning of new data and bucket allocation consistent with that provided by Hive. In a typical ETL the majority of mutation events are inserts, often targeting a single partition (new data for the previous day, hour, etc.). If more that one worker is writing said events, were we to leave the bucket id empty then all inserts would go to a single worker (e.g: reducer) and the workload could be heavily skewed. The assignment of a computed bucket allows inserts to be more usefully distributed across workers. Additionally, when Hive is working with the data it may expect records to have been bucketed in a way that is consistent with its own internal scheme. A convenience type and method is provided to more easily compute and append bucket ids: BucketIdResolver and BucketIdResolverImpl.\nUpdate operations should not attempt to modify values of partition or bucketing columns. The API does not prevent this and such attempts could lead to data corruption.\nStreaming Requirements A few things are currently required to use streaming.  Currently, only ORC storage format is supported. So \u0026lsquo;stored as orc\u0026rsquo; must be specified during table creation. The Hive table must be bucketed, but not sorted. So something like \u0026lsquo;clustered by (colName) into 10 buckets\u0026rsquo; must be specified during table creation. See Bucketed Tables for a detailed example. User of the client streaming process must have the necessary permissions to write to the table or partition and create partitions in the table. Hive transactions must be configured for each table (see Hive Transactions – Table Properties) as well as in hive-site.xml (see Hive Transactions – Configuration).  Note: Hive also supports streaming mutations to unpartitioned tables.\nRecord Layout The structure, layout, and encoding of records is the exclusive concern of the client ETL mutation process and may be quite different from the target Hive ACID table. The mutation API requires concrete implementations of the MutatorFactory and Mutator classes to extract pertinent data from records and serialize data into the ACID files. Fortunately base classes are provided (AbstractMutator, RecordInspectorImpl) to simplify this effort and usually all that is required is the specification of a suitable ObjectInspector and the provision of the indexes of the ROW__ID and bucketed columns within the record structure. Note that all column indexes in these classes are with respect to your record structure, not the Hive table structure.\nYou will likely also want to use a BucketIdResolver to append bucket ids to new records for insertion. Fortunately the core implementation is provided in BucketIdResolverImpl but note that bucket column indexes must be presented in the same order as they are in the Hive table definition to ensure consistent bucketing. Note that you cannot move records between buckets and an exception will be thrown if you attempt to do so. In real terms this means that you should not attempt to modify the values in bucket columns with an UPDATE.\nConnection and Transaction Management The MutatorClient class is used to create and manage transactions in which mutations can be performed. The scope of a transaction can extend across multiple ACID tables. When a client connects, it communicates with the metastore to verify and acquire metadata for the target tables. An invocation of newTransaction then opens a transaction with the metastore, finalizes a collection of AcidTables and returns a new Transaction instance. The ACID tables are light-weight, serializable objects that are used by the mutation writing components of the API to target specific ACID file locations. Usually your MutatorClient will be running on some master node and your coordinators on worker nodes. In this event the AcidTableSerializer can be used to encode the tables in a more transportable form, for use as a Configuration property for example.\nAs you would expect, a Transaction must be initiated with a call to begin before any mutations can be applied. This invocation acquires a lock on the targeted tables using the metastore, and initiates a heartbeat to prevent transaction timeouts. It is highly recommended that you register a LockFailureListener with the client so that your process can handle any lock or transaction failures. Typically you may wish to abort the job in the event of such an error. With the transaction in place you can now start streaming mutations with one or more MutatorCoordinator instances (more on this later), and then can commit or abort the transaction when the change set has been applied, which will release the lock with the metastore client. Finally you should close the mutation client to release any held resources.\nThe MutatorClientBuilder is provided to simplify the construction of clients.\nWARNING: Hive doesn\u0026rsquo;t currently have a deadlock detector (it is being worked on as part of HIVE-9675). This API could potentially deadlock with other stream writers or with SQL users.\nWriting Data The MutatorCoordinator class is used to issue mutations to an ACID table. You will require at least one instance per table participating in the transaction. The target of a given instance is defined by the respective AcidTable used to construct the coordinator. It is recommended that a MutatorClientBuilder be used to simplify the construction process.\nMutations can be applied by invoking the respective insert, update, and delete methods on the coordinator. These methods each take as parameters the target partition of the record and the mutated record. In the case of an unpartitioned table you should simply pass an empty list as the partition value. For inserts specifically, only the bucket id will be extracted from the RecordIdentifier, the transaction id and row id will be ignored and replaced by appropriate values in the RecordUpdater.\nAdditionally, in the case of deletes, everything but the RecordIdentifier in the record will be ignored and therefore it is often easier to simply submit the original record.\nCaution: As mentioned previously, mutations must arrive in a specific order for the resultant table data to be consistent. Coordinators will verify a naturally ordered sequence of [lastTransactionId, rowId] and will throw an exception if this sequence is broken. This exception should almost certainly be escalated so that the transaction is aborted. This, along with the correct ordering of the data, is the responsibility of the client using the API.\nDynamic Partition Creation It is very likely to be desirable to have new partitions created automatically (say on a hourly basis). In such cases requiring the Hive admin to pre-create the necessary partitions may not be reasonable. The API allows coordinators to create partitions as needed (see: MutatorClientBuilder.addSinkTable(String, String, boolean)). Partition creation being an atomic action, multiple coordinators can race to create the partition, but only one would succeed, so coordinators' clients need not synchronize when creating a partition. The user of the coordinator process needs to be given write permissions on the Hive table in order to create partitions.\nCare must be taken when using this option as it requires that the coordinators maintain a connection with the metastore database. When coordinators are running in a distributed environment (as is likely the case) it is possible for them to overwhelm the metastore. In such cases it may be better to disable partition creation and collect a set of affected partitions as part of your ETL merge process. These can then be created with a single metastore connection in your client code, once the cluster side merge process is complete.\nFinally, note that when partition creation is disabled the coordinators must synthesize the partition URI as they cannot retrieve it from the metastore. This may cause problems if the layout of your partitions in HDFS does not follow the Hive standard (as implemented in org.apache.hadoop.hive.metastore.Warehouse.getPartitionPath(Path, LinkedHashMap \u0026lt;String, String\u0026gt;)).\nReading Data Although this API is concerned with writing changes to data, as previously stated we\u0026rsquo;ll almost certainly have to read the existing data first to obtain the relevant ROW__IDs. Therefore it is worth noting that reading ACID data in a robust and consistent manner requires the following:\n Obtaining a valid transaction list from the metastore (ValidTxnList). Acquiring a lock with the metastore and issuing heartbeats (LockImpl can help with this). Configuring the OrcInputFormat and then reading the data. Make sure that you also pull in the ROW__ID values. See: AcidRecordReader.getRecordIdentifier. Releasing the lock.  Example So to recap, the sequence of events required to apply mutations to a dataset using the API is:\n Create a MutatorClient to manage a transaction for the targeted ACID tables. This set of tables should include any transactional destinations or sources. Don\u0026rsquo;t forget to register a LockFailureListener so that you can handle transaction failures. Open a new Transaction with the client. Get the AcidTables from the client. Begin the transaction. Create at least one MutatorCoordinator for each table. The AcidTableSerializer can help you transport the AcidTables when your workers are in a distributed environment. Compute your mutation set (this is your ETL merge process). Optionally: collect the set of affected partitions. Append bucket ids to insertion records. A BucketIdResolver can help here. Group and sort your data appropriately. Issue mutation events to your coordinators. Close your coordinators. Abort or commit the transaction. Close your mutation client. Optionally: create any affected partitions that do not exist in the metastore.  See [ExampleUseCase](https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/ExampleUseCase.java) and [TestMutations.testUpdatesAndDeletes()](https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/TestMutations.java#L421) for some very simple usages.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-streaming-mutation-api_61337025/","tags":null,"title":"Apache Hive : HCatalog Streaming Mutation API"},{"categories":null,"contents":"Apache Hive : HCatalog Streaming Mutation API (Copy) A Java API focused on mutating (insert/update/delete) records into transactional tables using Hive’s ACID feature. It is introduced in Hive 2.0.0 (HIVE-10165).\n Background Structure Data Requirements Streaming Requirements Record Layout Connection and Transaction Management Writing Data  Dynamic Partition Creation   Reading Data Example  Background In certain data processing use cases it is necessary to modify existing data when new facts arrive. An example of this is the classic ETL merge where a copy of a data set is kept in sync with a master by the frequent application of deltas. The deltas describe the mutations (inserts, updates, deletes) that have occurred to the master since the previous sync. To implement such a case using Hadoop traditionally demands that the partitions containing records targeted by the mutations be rewritten. This is a coarse approach; a partition containing millions of records might be rebuilt because of a single record change. Additionally these partitions cannot be restated atomically; at some point the old partition data must be swapped with the new partition data. When this swap occurs, usually by issuing an HDFS rm followed by a mv, the possibility exists where the data appears to be unavailable and hence any downstream jobs consuming the data might unexpectedly fail. Therefore data processing patterns that restate raw data on HDFS cannot operate robustly without some external mechanism to orchestrate concurrent access to changing data.\nThe availability of ACID tables in Hive provides a mechanism that both enables concurrent access to data stored in HDFS (so long as it\u0026rsquo;s in the ORC+ACID format) and also permits row level mutations on records within a table, without the need to rewrite the existing data. But while Hive itself supports INSERT, UPDATE and DELETE commands, and the ORC format can support large batches of mutations in a transaction, Hive\u0026rsquo;s execution engine currently submits each individual mutation operation in a separate transaction and issues table scans (M/R jobs) to execute them. It does not currently scale to the demands of processing large deltas in an atomic manner. Furthermore it would be advantageous to extend atomic batch mutation capabilities beyond Hive by making them available to other data processing frameworks. The Streaming Mutation API does just this.\nThe Streaming Mutation API, although similar to the Streaming API, has a number of differences and is built to enable very different use cases. Superficially, the Streaming API can only write new data whereas the mutation API can also modify existing data. However the two APIs are also based on very different transaction models. The Streaming API focuses on surfacing a continuous stream of new data into a Hive table and does so by batching small sets of writes into multiple short-lived transactions. Conversely the mutation API is designed to infrequently apply large sets of mutations to a data set in an atomic fashion: either all or none of the mutations will be applied. This instead mandates the use of a single long-lived transaction. This table summarises the attributes of each API:\n   Attribute Streaming API Mutation API     Ingest type Data arrives continuously. Ingests are performed periodically and the mutations are applied in a single batch.   Transaction scope Transactions are created for small batches of writes. The entire set of mutations should be applied within a single transaction.   Data availability Surfaces new data to users frequently and quickly. Change sets should be applied atomically, either the effect of the delta is visible or it is not.   Sensitive to record order No, records do not have pre-existing lastTxnIds or bucketIds. Records are likely being written into a single partition (today\u0026rsquo;s date for example). Yes, all mutated records have existing RecordIdentifiers and must be grouped by [partitionValues, bucketId] and sorted by lastTxnId. These record coordinates initially arrive in an order that is effectively random.   Impact of a write failure Transaction can be aborted and producer can choose to resubmit failed records as ordering is not important. Ingest for the respective group (partitionValues + bucketId) must be halted and failed records resubmitted to preserve sequence.   User perception of missing data Data has not arrived yet → \u0026ldquo;latency?\u0026rdquo; \u0026ldquo;This data is inconsistent, some records have been updated, but other related records have not\u0026rdquo; – consider here the classic transfer between bank accounts scenario.   API end point scope A given HiveEndPoint instance submits many transactions to a specific bucket, in a specific partition, of a specific table. A set of MutationCoordinators writes changes to unknown set of buckets, of an unknown set of partitions, of specific tables (can be more than one), within a single transaction.    Structure The API comprises two main concerns: transaction management, and the writing of mutation operations to the data set. The two concerns have a minimal coupling as it is expected that transactions will be initiated from a single job launcher type process while the writing of mutations will be scaled out across any number of worker nodes. In the context of Hadoop M/R these can be more concretely defined as the Tool and Map/Reduce task components. However, use of this architecture is not mandated and in fact both concerns could be handled within a single simple process depending on the requirements.\nNote that a suitably configured Hive instance is required to operate this system even if you do not intend to access the data from within Hive. Internally, transactions are managed by the Hive MetaStore. Mutations are performed to HDFS via ORC APIs that bypass the MetaStore. Additionally you may wish to configure your MetaStore instance to perform periodic data compactions.\nNote on packaging: The APIs are defined in the org.apache.hive.hcatalog.streaming.mutate Java package and included as the hive-hcatalog-streaming jar.\nData Requirements Generally speaking, to apply a mutation to a record one must have some unique key that identifies the record. However, primary keys are not a construct provided by Hive. Internally Hive uses RecordIdentifiers stored in a virtual ROW__ID column to uniquely identify records within an ACID table. Therefore, any process that wishes to issue mutations to a table via this API must have available the corresponding row ids for the target records. What this means in practice is that the process issuing mutations must first read in a current snapshot of the data and then join the mutations on some domain specific primary key to obtain the corresponding Hive ROW__ID. This is effectively what occurs within Hive\u0026rsquo;s table scan process when an UPDATE or DELETE statement is executed. The AcidInputFormat provides access to this data via AcidRecordReader.getRecordIdentifier().\nThe implementation of the ACID format places some constraints on the order in which records are written and it is important that this ordering is enforced. Additionally, data must be grouped appropriately to adhere to the constraints imposed by the OrcRecordUpdater. Grouping also makes it possible to parallelise the writing of mutations for the purposes of scaling. Finally, to correctly bucket new records (inserts) there is a slightly unintuitive trick that must be applied.\nAll of these data sequencing concerns are the responsibility of the client process calling the API which is assumed to have first class grouping and sorting capabilities (Hadoop Map/Reduce etc). The streaming API provides nothing more than validators that fail fast when they encounter groups and records that are out of sequence.\nIn short, API client processes should prepare data for the mutate API like so:\n MUST: Order records by ROW__ID.originalTxn, then ROW__ID.rowId. MUST: Assign a ROW__ID containing a computed bucketId to each record to be inserted. SHOULD: Group/partition by table partition value, then ROW__ID.bucketId.  The addition of bucket ids to insert records prior to grouping and sorting seems unintuitive. However, it is required to ensure both adequate partitioning of new data and bucket allocation consistent with that provided by Hive. In a typical ETL the majority of mutation events are inserts, often targeting a single partition (new data for the previous day, hour, etc.). If more that one worker is writing said events, were we to leave the bucket id empty then all inserts would go to a single worker (e.g: reducer) and the workload could be heavily skewed. The assignment of a computed bucket allows inserts to be more usefully distributed across workers. Additionally, when Hive is working with the data it may expect records to have been bucketed in a way that is consistent with its own internal scheme. A convenience type and method is provided to more easily compute and append bucket ids: BucketIdResolver and BucketIdResolverImpl.\nUpdate operations should not attempt to modify values of partition or bucketing columns. The API does not prevent this and such attempts could lead to data corruption.\nStreaming Requirements A few things are currently required to use streaming.  Currently, only ORC storage format is supported. So \u0026lsquo;stored as orc\u0026rsquo; must be specified during table creation. The Hive table must be bucketed, but not sorted. So something like \u0026lsquo;clustered by (colName) into 10 buckets\u0026rsquo; must be specified during table creation. See Bucketed Tables for a detailed example. User of the client streaming process must have the necessary permissions to write to the table or partition and create partitions in the table. Hive transactions must be configured for each table (see Hive Transactions – Table Properties) as well as in hive-site.xml (see Hive Transactions – Configuration).  Note: Hive also supports streaming mutations to unpartitioned tables.\nRecord Layout The structure, layout, and encoding of records is the exclusive concern of the client ETL mutation process and may be quite different from the target Hive ACID table. The mutation API requires concrete implementations of the MutatorFactory and Mutator classes to extract pertinent data from records and serialize data into the ACID files. Fortunately base classes are provided (AbstractMutator, RecordInspectorImpl) to simplify this effort and usually all that is required is the specification of a suitable ObjectInspector and the provision of the indexes of the ROW__ID and bucketed columns within the record structure. Note that all column indexes in these classes are with respect to your record structure, not the Hive table structure.\nYou will likely also want to use a BucketIdResolver to append bucket ids to new records for insertion. Fortunately the core implementation is provided in BucketIdResolverImpl but note that bucket column indexes must be presented in the same order as they are in the Hive table definition to ensure consistent bucketing. Note that you cannot move records between buckets and an exception will be thrown if you attempt to do so. In real terms this means that you should not attempt to modify the values in bucket columns with an UPDATE.\nConnection and Transaction Management The MutatorClient class is used to create and manage transactions in which mutations can be performed. The scope of a transaction can extend across multiple ACID tables. When a client connects, it communicates with the metastore to verify and acquire metadata for the target tables. An invocation of newTransaction then opens a transaction with the metastore, finalizes a collection of AcidTables and returns a new Transaction instance. The ACID tables are light-weight, serializable objects that are used by the mutation writing components of the API to target specific ACID file locations. Usually your MutatorClient will be running on some master node and your coordinators on worker nodes. In this event the AcidTableSerializer can be used to encode the tables in a more transportable form, for use as a Configuration property for example.\nAs you would expect, a Transaction must be initiated with a call to begin before any mutations can be applied. This invocation acquires a lock on the targeted tables using the metastore, and initiates a heartbeat to prevent transaction timeouts. It is highly recommended that you register a LockFailureListener with the client so that your process can handle any lock or transaction failures. Typically you may wish to abort the job in the event of such an error. With the transaction in place you can now start streaming mutations with one or more MutatorCoordinator instances (more on this later), and then can commit or abort the transaction when the change set has been applied, which will release the lock with the metastore client. Finally you should close the mutation client to release any held resources.\nThe MutatorClientBuilder is provided to simplify the construction of clients.\nWARNING: Hive doesn\u0026rsquo;t currently have a deadlock detector (it is being worked on as part of HIVE-9675). This API could potentially deadlock with other stream writers or with SQL users.\nWriting Data The MutatorCoordinator class is used to issue mutations to an ACID table. You will require at least one instance per table participating in the transaction. The target of a given instance is defined by the respective AcidTable used to construct the coordinator. It is recommended that a MutatorClientBuilder be used to simplify the construction process.\nMutations can be applied by invoking the respective insert, update, and delete methods on the coordinator. These methods each take as parameters the target partition of the record and the mutated record. In the case of an unpartitioned table you should simply pass an empty list as the partition value. For inserts specifically, only the bucket id will be extracted from the RecordIdentifier, the transaction id and row id will be ignored and replaced by appropriate values in the RecordUpdater.\nAdditionally, in the case of deletes, everything but the RecordIdentifier in the record will be ignored and therefore it is often easier to simply submit the original record.\nCaution: As mentioned previously, mutations must arrive in a specific order for the resultant table data to be consistent. Coordinators will verify a naturally ordered sequence of [lastTransactionId, rowId] and will throw an exception if this sequence is broken. This exception should almost certainly be escalated so that the transaction is aborted. This, along with the correct ordering of the data, is the responsibility of the client using the API.\nDynamic Partition Creation It is very likely to be desirable to have new partitions created automatically (say on a hourly basis). In such cases requiring the Hive admin to pre-create the necessary partitions may not be reasonable. The API allows coordinators to create partitions as needed (see: MutatorClientBuilder.addSinkTable(String, String, boolean)). Partition creation being an atomic action, multiple coordinators can race to create the partition, but only one would succeed, so coordinators' clients need not synchronize when creating a partition. The user of the coordinator process needs to be given write permissions on the Hive table in order to create partitions.\nCare must be taken when using this option as it requires that the coordinators maintain a connection with the metastore database. When coordinators are running in a distributed environment (as is likely the case) it is possible for them to overwhelm the metastore. In such cases it may be better to disable partition creation and collect a set of affected partitions as part of your ETL merge process. These can then be created with a single metastore connection in your client code, once the cluster side merge process is complete.\nFinally, note that when partition creation is disabled the coordinators must synthesize the partition URI as they cannot retrieve it from the metastore. This may cause problems if the layout of your partitions in HDFS does not follow the Hive standard (as implemented in org.apache.hadoop.hive.metastore.Warehouse.getPartitionPath(Path, LinkedHashMap \u0026lt;String, String\u0026gt;)).\nReading Data Although this API is concerned with writing changes to data, as previously stated we\u0026rsquo;ll almost certainly have to read the existing data first to obtain the relevant ROW__IDs. Therefore it is worth noting that reading ACID data in a robust and consistent manner requires the following:\n Obtaining a valid transaction list from the metastore (ValidTxnList). Acquiring a lock with the metastore and issuing heartbeats (LockImpl can help with this). Configuring the OrcInputFormat and then reading the data. Make sure that you also pull in the ROW__ID values. See: AcidRecordReader.getRecordIdentifier. Releasing the lock.  Example So to recap, the sequence of events required to apply mutations to a dataset using the API is:\n Create a MutatorClient to manage a transaction for the targeted ACID tables. This set of tables should include any transactional destinations or sources. Don\u0026rsquo;t forget to register a LockFailureListener so that you can handle transaction failures. Open a new Transaction with the client. Get the AcidTables from the client. Begin the transaction. Create at least one MutatorCoordinator for each table. The AcidTableSerializer can help you transport the AcidTables when your workers are in a distributed environment. Compute your mutation set (this is your ETL merge process). Optionally: collect the set of affected partitions. Append bucket ids to insertion records. A BucketIdResolver can help here. Group and sort your data appropriately. Issue mutation events to your coordinators. Close your coordinators. Abort or commit the transaction. Close your mutation client. Optionally: create any affected partitions that do not exist in the metastore.  See [ExampleUseCase](https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/ExampleUseCase.java) and [TestMutations.testUpdatesAndDeletes()](https://github.com/apache/hive/blob/master/hcatalog/streaming/src/test/org/apache/hive/hcatalog/streaming/mutate/TestMutations.java#L421) for some very simple usages.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/283118454/","tags":null,"title":"Apache Hive : HCatalog Streaming Mutation API (Copy)"},{"categories":null,"contents":"Apache Hive : HCatalog UsingHCat Using HCatalog  Using HCatalog  Overview HCatalog Architecture  Interfaces Data Model   Data Flow Example  First: Copy Data to the Grid Second: Prepare the Data Third: Analyze the Data   HCatalog Web API    Version information\nHCatalog graduated from the Apache incubator and merged with the Hive project on March 26, 2013.\nHive version 0.11.0 is the first release that includes HCatalog.\nOverview HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid. HCatalog’s table abstraction presents users with a relational view of data in the Hadoop distributed file system (HDFS) and ensures that users need not worry about where or in what format their data is stored — RCFile format, text files, SequenceFiles, or ORC files.\nHCatalog supports reading and writing files in any format for which a SerDe (serializer-deserializer) can be written. By default, HCatalog supports RCFile, CSV, JSON, and SequenceFile, and ORC file formats. To use a custom format, you must provide the InputFormat, OutputFormat, and SerDe.\nHCatalog Architecture HCatalog is built on top of the Hive metastore and incorporates Hive\u0026rsquo;s DDL. HCatalog provides read and write interfaces for Pig and MapReduce and uses Hive\u0026rsquo;s command line interface for issuing data definition and metadata exploration commands.\nInterfaces The HCatalog interface for Pig consists of HCatLoader and HCatStorer, which implement the Pig load and store interfaces respectively. HCatLoader accepts a table to read data from; you can indicate which partitions to scan by immediately following the load statement with a partition filter statement. HCatStorer accepts a table to write to and optionally a specification of partition keys to create a new partition. You can write to a single partition by specifying the partition key(s) and value(s) in the STORE clause; and you can write to multiple partitions if the partition key(s) are columns in the data being stored. HCatLoader is implemented on top of HCatInputFormat and HCatStorer is implemented on top of HCatOutputFormat. (See Load and Store Interfaces.)\nThe HCatalog interface for MapReduce — HCatInputFormat and HCatOutputFormat — is an implementation of Hadoop InputFormat and OutputFormat. HCatInputFormat accepts a table to read data from and optionally a selection predicate to indicate which partitions to scan. HCatOutputFormat accepts a table to write to and optionally a specification of partition keys to create a new partition. You can write to a single partition by specifying the partition key(s) and value(s) in the setOutput method; and you can write to multiple partitions if the partition key(s) are columns in the data being stored. (See Input and Output Interfaces.)\nNote: There is no Hive-specific interface. Since HCatalog uses Hive\u0026rsquo;s metastore, Hive can read data in HCatalog directly.\nData is defined using HCatalog\u0026rsquo;s command line interface (CLI). The HCatalog CLI supports all Hive DDL that does not require MapReduce to execute, allowing users to create, alter, drop tables, etc. The CLI also supports the data exploration part of the Hive command line, such as SHOW TABLES, DESCRIBE TABLE, and so on. Unsupported Hive DDL includes import/export, the REBUILD and CONCATENATE options of ALTER TABLE, CREATE TABLE AS SELECT, and ANALYZE TABLE \u0026hellip; COMPUTE STATISTICS. (See Command Line Interface.)\nData Model HCatalog presents a relational view of data. Data is stored in tables and these tables can be placed in databases. Tables can also be hash partitioned on one or more keys; that is, for a given value of a key (or set of keys) there will be one partition that contains all rows with that value (or set of values). For example, if a table is partitioned on date and there are three days of data in the table, there will be three partitions in the table. New partitions can be added to a table, and partitions can be dropped from a table. Partitioned tables have no partitions at create time. Unpartitioned tables effectively have one default partition that must be created at table creation time. There is no guaranteed read consistency when a partition is dropped.\nPartitions contain records. Once a partition is created records cannot be added to it, removed from it, or updated in it. Partitions are multi-dimensional and not hierarchical. Records are divided into columns. Columns have a name and a datatype. HCatalog supports the same datatypes as Hive. See Load and Store Interfaces for more information about datatypes.\nData Flow Example This simple data flow example shows how HCatalog can help grid users share and access data.\nFirst: Copy Data to the Grid Joe in data acquisition uses distcp to get data onto the grid.\nhadoop distcp file:///file.dat hdfs://data/rawevents/20100819/data hcat \u0026quot;alter table rawevents add partition (ds='20100819') location 'hdfs://data/rawevents/20100819/data'\u0026quot; Second: Prepare the Data Sally in data processing uses Pig to cleanse and prepare the data.\nWithout HCatalog, Sally must be manually informed by Joe when data is available, or poll on HDFS.\nA = load '/data/rawevents/20100819/data' as (alpha:int, beta:chararray, ...); B = filter A by bot_finder(zeta) = 0; ... store Z into 'data/processedevents/20100819/data'; With HCatalog, HCatalog will send a JMS message that data is available. The Pig job can then be started.\nA = load 'rawevents' using org.apache.hive.hcatalog.pig.HCatLoader(); B = filter A by date = '20100819' and by bot_finder(zeta) = 0; ... store Z into 'processedevents' using org.apache.hive.hcatalog.pig.HCatStorer(\u0026quot;date=20100819\u0026quot;); Third: Analyze the Data Robert in client management uses Hive to analyze his clients' results.\nWithout HCatalog, Robert must alter the table to add the required partition.\nalter table processedevents add partition 20100819 hdfs://data/processedevents/20100819/data select advertiser_id, count(clicks) from processedevents where date = '20100819' group by advertiser_id; With HCatalog, Robert does not need to modify the table structure.\nselect advertiser_id, count(clicks) from processedevents where date = ‘20100819’ group by advertiser_id; HCatalog Web API WebHCat is a REST API for HCatalog. (REST stands for \u0026ldquo;representational state transfer\u0026rdquo;, a style of API based on HTTP verbs). The original name of WebHCat was Templeton. For more information, see the WebHCat manual.\n Navigation Links Next: HCatalog Installation\nGeneral: HCatalog Manual – WebHCat Manual – Hive Wiki Home – Hive Project Site\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hcatalog-usinghcat_34013260/","tags":null,"title":"Apache Hive : HCatalog UsingHCat"},{"categories":null,"contents":"Apache Hive : Hive across Multiple Data Centers (Physical Clusters) This project has been abandoned. We\u0026rsquo;re leaving the design doc here in case someone decides to attempt this project in the future.\n Use Cases Requirements  Use Cases Inside facebook, we are running out of power inside a data center (physical cluster), and we have a need to have a bigger cluster.\nWe can divide the cluster into multiple clusters - multiple hive instances, multiple mr and multiple dfs. This will put a burden on\nthe user - he needs to know which cluster to use. It will be very difficult to support joins across tables in different clusters, and\nwill lead to a lot of duplication of data in the long run. To get around these problems, we are planning to extend hive to span\nmultiple data centers, and make the existence of multiple clusters transparent to the end users in the long run. Note that, even\ntoday, different partitions/tables can span multiple dfs\u0026rsquo;s, and hive does not enforce any restrictions. Those dfs\u0026rsquo;s can be in different\ndata centers also. However, a single table/partition can only have a single location. We need to enhance this. We will not be able to\npartition our warehouse cluster into multiple disjoint clusters, and therefore some tables/partitions will be present in multiple clusters.\nRequirements In order to do so, we need to make some changes in hive, and this document primarily talks about those. The changes should be generic\nenough, so that they can be used by others (outside Facebook) also, if they have such a requirement. The following restrictions have\nbeen imposed to simplify the problem:\n There will be a single hive instance, possibly spanning multiple clusters (both dfs and mr) There will be a single hive metastore to keep track of the table/partition locations across different clusters. A table/partition can exist in more than one cluster. A table will have a single primary cluster, and can have multiple  secondary clusters.\n Table/Partition\u0026rsquo;s metadata will be enhanced to support multiple clusters/locations of the table. All the data for a table is available in the primary cluster, but a subset can be available in the secondary cluster.  However, an object (unpartitioned table/partition) is either fully present or not present at all in the secondary cluster.\nIt is not possible to have partial data of a partition in the secondary cluster.\n  The user can only update the table (or its partition) in the primary cluster.\n  The following mapping will be added. Cluster -\u0026gt; JobTracker\n  By default, the user will not specify any cluster for the session, and the behavior will be as follows:\n The query will be processed in a single cluster, and use the jobtracker for that cluster. If the primary cluster of any output table is different from the query processing cluster, an error is thrown.  So, a multi-table insert with tables belonging to different primary clusters will always fail.\n If the input\u0026rsquo;s table primary cluster is different from the query processing cluster, the query will only succeed  if all the partitions for that input table are also present on the query processing cluster.\n If an output is specified, the primary cluster for that output will be used. If the output specified is a new table, the output is not used in determining the query processing cluster. If no output is specified (or the output is a new table), and there are multiple inputs for the query, all the input tables  primary clusters are tried one-by-one, till a valid cluster is found.\n  Few examples will illustrate the scenario better:\n  Say T11, T12, T21, T31 are tables belonging to cluster C1, C1, C2 and C3 respectively, and it has no secondary clusters.\n The query \u0026lsquo;select .. from T11 .. ' will be processed in C1 The query \u0026lsquo;select .. from T11 join T12 .. ' will be processed in C1 The query \u0026lsquo;select .. from T21 .. ' will be processed in C2 The query \u0026lsquo;select .. from T11 join T21 .. ' will fail \u0026lsquo;Insert .. T13 select .. from T11 ..\u0026rsquo; will be processed in C1 and the T13 will be created in C1 \u0026lsquo;Insert .. T21 select .. from T11 ..\u0026rsquo; will fail    If we change the example slightly:\n  Say T11, T12, T21, T31 are tables belonging to cluster C1, C1, C2 and C3 respectively.\n  T11\u0026rsquo;s secondary cluster is C2 (and all the data for T11 is also present in C2). + The query \u0026lsquo;select .. from T11 .. ' will be processed in C1 + The query \u0026lsquo;select .. from T11 join T12 .. ' will be processed in C1 + The query \u0026lsquo;select .. from T21 .. ' will be processed in C2 + The query \u0026lsquo;select .. from T11 join T21 .. ' will be processed in C2 + The query \u0026lsquo;select .. from T11 join T31 .. ' will fail + \u0026lsquo;Insert .. T13 select .. from T11 ..\u0026rsquo; will be processed in C1 and the T13 will be created in C1 + \u0026lsquo;Insert .. T21 select .. from T11 ..\u0026rsquo; will be processed in C2, and T21 will remain in C2\nThe same idea can be extended for partitioned tables.\n The user can also decide to run in a particular cluster.  Use cluster    The system will not make an attempt to choose the cluster for the user, but only try to figure out if the query can be run  in the . If the query can run in this cluster, it will succeed. Otherwise, it will fail.\n  The user can go back to the behavior to use the default cluster.\n Use cluster    Eventually, hive will provide some utilities to copy a table/partition from the primary cluster to the secondary clusters.\n  In the first cut, the user needs to do this operation outside hive (one simple way to do so, is distcp the partition from the\nprimary to the secondary cluster, and then update the metadata directly - via the thrift api).\n This will require a change to the metastore schema. StorageDescriptor will be enhanced to add:  PrimaryCluster - ClusterStorageDescriptor\nand SecondaryClusters - SetThe ClusterStorageDescriptor contains the following:\nClusterName\nLocation\nlocation will be removed from the StorageDescriptor.\n This will require a scheme change and data migration. The thrift structure will be backward compatible  New entries will be added: ClusterName, IsPrimary etc., but existing clients using sd.location will continue to work    In order to support the above, hive metastore needs to be enhanced to have the concept of a cluster. The following parameters will\nbe added:\n hive.default.cluster.name - For migration, all tables/partitions belong to this cluster.  All new tables belong to this cluster.\n hive.use.default.cluster - The default cluster will be used depending on input table\u0026rsquo;s schema.  The existing thrift API\u0026rsquo;s will continue to work as if the user is trying to access the default cluster.\nNew APIs will be added which take the cluster as a new parameter. Almost all the existing APIs will be\nenhanced to support this. The behavior will be the same as if, the user issued the command \u0026lsquo;USE CLUSTER   A new parameter will be added to keep the filesystem and jobtrackers for a cluster\n hive.cluster.properties: This will be json - ClusterName -\u0026gt; \u0026lt;FileSystem, JobTracker\u0026gt; use cluster will fail if is not present hive.cluster.properties The other option was to support create cluster \u0026lt;\u0026gt; etc. but that would have required storing the cluster information in the  metastore including jobtracker etc. which would be difficult to change per session.\n  If the user does not intend of use multiple clusters, there should be no change in the behavior of hive commands, and all the\nexisting thrift APIs should continue to work.\nAn upgrade script will be provided to upgrade the metastore with the patch.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/27837073/","tags":null,"title":"Apache Hive : Hive across Multiple Data Centers (Physical Clusters)"},{"categories":null,"contents":"Apache Hive : Hive APIs Overview This page aims to catalogue and describe the various public facing APIs exposed by Hive in order to inform developers wishing to integrate their applications and frameworks with the Hive ecosystem. To date the following APIs have been identified in the Hive project that are either considered public, or widely used in the public domain:\n API categories  Operation based APIs Query based APIs   Available APIs  HCatClient (Java) HCatalog Storage Handlers (Java) HCatalog CLI (Command Line) Metastore (Java) WebHCat (REST) Streaming Data Ingest (Java) Streaming Mutation (Java) hive-jdbc (JDBC)    API categories The APIs can be segmented into two conceptual categories: operation based APIs and query based APIs.\nOperation based APIs Operation based APIs expose many tightly scoped methods that each implement a very specific Hive operation. Such methods usually accept and return strongly typed values appropriate to their respective operation. The implementations of the operations usually target very specific layers or subsystems within Hive and are therefore likely to be efficient in use. However, the outcome of an operation may diverge from that of the equivalent HQL as the different code paths may be invoked in each case. Operation based APIs are used for constructing processes that need to interact in a repetitive, declarative manner and provide a greater degree of compile-time checking.\nQuery based APIs Query based APIs permit the submission and execution of some subset of HQL. It is often necessary for the API client to parse and interpret any returned values as the return types are frequently quite broad in scope. The implementations of such APIs usually target the \u0026lsquo;query language\u0026rsquo; subsystem of Hive which parses the query and executes it as needed. Given that most query based APIs share a similar execution pathway, it is likely that any operation submitted via the API will have a similar outcome to equivalent HQL submitted via the Hive CLI. Query based APIs are often used for constructing processes where Hive API operations are created dynamically or where an HQL equivalent outcome is important. Drawbacks of this type of API include: lack of compile time checking, possible inefficiencies in working at a higher level of abstraction, and potential susceptibility to SQL-injection like attacks.\nAvailable APIs HCatClient (Java) Operation based Java API that presents a number of DDL type operations, however it is not as comprehensive as the Metastore API. The HCatClient was intended to be the Java based entry point to WebHCat HCatalog API although this was never realised. Currently HCatClientHMSImpl is the only concrete implementation of the API; it integrates directly with the Metastore using the Metastore API and does not utilise WebHCat whatsoever despite being packaged inside the WebHCat project. The HCatClientHMSImpl was originally provided as a reference implementation but it has over time gained traction as a public client. Anecdotally, it is now the preferred API for issuing DDL type operations from external programs; and feature contributions are encouraged. There is some minimal documentation on the HCatalog wiki in the form of a design document describing the interface but not the implementation.\nHCatalog Storage Handlers (Java) Operation based Java API. This is well documented on the wiki.\nTODO\nRequires overview.\nHCatalog CLI (Command Line) Query based API. This is well documented on the wiki.\nHive community has been working deprecating Hive Cli. Hcatalog Cli is similar to Hive Cli and will be deprecated.\nMetastore (Java) A Thrift operation based API with Java bindings, described by the IMetaStoreClient interface. The API decouples the metastore storage layer from other Hive internals. Because Hive itself uses this internally, it is required to implement a comprehensive feature set which makes it attractive to developers who might find the other APIs lacking. It was not originally intended to be a public API although it became public in version 1.0.0 (HIVE-3280) and there is a proposal that it be documented more fully (HIVE-9363). Anecdotally, its use outside of the Hive project is not currently recommended.\nTODO: API usage\nThere are numerous ways of instantiating the metastore API including: HCatUtil.getHiveMetastoreClient(), new HiveMetaStoreClient.HiveMetaStoreClient(...). It may be useful to make some recommendations on the preferred approach.\nWebHCat (REST) WebHCat is a REST operation based API for HCatalog. This is well documented on the wiki.\nThis not actively maintained and likely not be supported in future releases. For job submission, consider using Oozie or similar tools. For DDL, use JDBC.\nStreaming Data Ingest (Java) Operation based Java API focused on the writing of continuous streams of data into transactional tables using Hive’s ACID feature. New data is inserted into tables using small batches and short-lived transactions. Documented on the wiki and has package level Javadoc. Introduced in Hive version 0.13.0 (HIVE-5687).\nStreaming Mutation (Java) Operation based Java API focused on mutating (insert/update/delete) records into transactional tables using Hive’s ACID feature. Large volumes of mutations are applied atomically in a single long-lived transaction. Documented on the wiki. Scheduled for release in Hive version 2.0.0 (HIVE-10165).\nhive-jdbc (JDBC) JDBC API supported by Hive. It supports most of the functionality in JDBC spec.\nComments:            Page created after an interesting discussion.    Posted by teabot at Oct 30, 2015 17:09 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-apis-overview_61326349/","tags":null,"title":"Apache Hive : Hive APIs Overview"},{"categories":null,"contents":"Apache Hive : Hive Aws EMR Amazon Elastic MapReduce and Hive Amazon Elastic MapReduce is a web service that makes it easy to launch managed, resizable Hadoop clusters on the web-scale infrastructure of Amazon Web Services (AWS). Elastic Map Reduce makes it easy for you to launch a Hive and Hadoop cluster, provides you with flexibility to choose different cluster sizes, and allows you to tear them down automatically when processing has completed. You pay only for the resources that you use with no minimums or long-term commitments.\nAmazon Elastic MapReduce simplifies the use of Hive clusters by:\n Handling the provisioning of Hadoop clusters of up to thousands of EC2 instances Installing Hadoop across the master and slave nodes of your cluster and configuring Hadoop based on your chosen hardware Installing Hive on the master node of your cluster and configuring it for communication with the Hadoop JobTracker and NameNode Providing a simple API, a web UI, and purpose-built tools for managing, monitoring, and debugging Hadoop tasks throughout the life of the cluster Providing deep integration, and optimized performance, with AWS services such as S3 and EC2 and AWS features such as Spot Instances, Elastic IPs, and Identity and Access Management (IAM)  Please refer to the following link to view the Amazon Elastic MapReduce Getting Started Guide:\nGetting Started\nAmazon Elastic MapReduce provides you with multiple clients to run your Hive cluster. You can launch a Hive cluster using the AWS Management Console, the Amazon Elastic MapReduce Ruby Client, or the AWS Java SDK. You may also install and run multiple versions of Hive on the same cluster, allowing you to benchmark a newer Hive version alongside your previous version. You can also install a newer Hive version directly onto an existing Hive cluster.\nSupported versions:          EMR Version Hive Version   emr-7.0.0 3.1.3   emr-6.15.0 3.1.3   emr-5.36.1 2.3.9    Hive Defaults Thrift Communication port          Hive Version Thrift port   0.4 10000   0.5 10000   0.7 10001   0.7.1 10002    Log File          Hive Version Log location   0.4 /mnt/var/log/apps/hive.log   0.5 /mnt/var/log/apps/hive_05.log   0.7 /mnt/var/log/apps/hive_07.log   0.7.1 /mnt/var/log/apps/hive_07_1.log    MetaStore By default, Amazon Elastic MapReduce uses MySQL, preinstalled on the Master Node, for its Hive metastore. Alternatively, you can use the Amazon Relational Database Service (Amazon RDS) to ensure the metastore is persisted beyond the life of your cluster. This also allows you to share the metastore between multiple Hive clusters. Simply override the default location of the MySQL database to the external persistent storage location.\nHive CLI EMR configures the master node to allow SSH access. You can log onto the master node and execute Hive commands using the Hive CLI. If you have multiple versions of Hive installed on the cluster you can access each one of them via a separate command:\n         Hive Version Hive command   0.4 hive   0.5 hive-0.5   0.7 hive-0.7   0.7.1 hive-0.7.1    EMR sets up a separate Hive metastore and Hive warehouse for each installed Hive version on a given cluster. Hence, creating tables using one version does not interfere with the tables created using another version installed. Please note that if you point multiple Hive tables to same location, updates to one table become visible to other tables.\nHive Server EMR runs a Thrift Hive server on the master node of the Hive cluster. It can be accessed using any JDBC client (for example, squirrel SQL) via Hive JDBC drivers. The JDBC drivers for different Hive versions can be downloaded via the following links:\n         Hive Version Hive JDBC   0.5 http://aws.amazon.com/developertools/0196055244487017   0.7 http://aws.amazon.com/developertools/1818074809286277   0.7.1 http://aws.amazon.com/developertools/8084613472207189    Here is the process to connect to the Hive Server using a JDBC driver:\nhttp://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_Hive.html#HiveJDBCDriver\nRunning Batch Queries You can also submit queries from the command line client remotely. Please note that currently there is a limit of 256 steps on each cluster. If you have more than 256 steps to execute, it is recommended that you run the queries directly using the Hive CLI or submit queries via a JDBC driver.\nHive S3 Tables An Elastic MapReduce Hive cluster comes configured for communication with S3. You can create tables and point them to your S3 location and Hive and Hadoop will communicate with S3 automatically using your provided credentials.\nOnce you have moved data to an S3 bucket, you simply point your table to that location in S3 in order to read or process data via Hive. You can also create partitioned tables in S3. Hive on Elastic MapReduce provides support for dynamic partitioning in S3.\nHive Logs Hive application logs: All Hive application logs are redirected to /mnt/var/log/apps/ directory.\nHadoop daemon logs: Hadoop daemon logs are available in /mnt/var/log/hadoop/ folder.\nHadoop task attempt logs are available in /mnt/var/log/hadoop/userlogs/ folder on each slave node in the cluster.\nTutorials The following Hive tutorials are available for you to get started with Hive on Elastic MapReduce:\n Finding trending topics using Google Books n-grams data and Apache Hive on Elastic MapReduce  http://aws.amazon.com/articles/Elastic-MapReduce/5249664154115844   Contextual Advertising using Apache Hive and Amazon Elastic MapReduce with High Performance Computing instances  http://aws.amazon.com/articles/Elastic-MapReduce/2855   Operating a Data Warehouse with Hive, Amazon Elastic MapReduce and Amazon SimpleDB  http://aws.amazon.com/articles/Elastic-MapReduce/2854   Running Hive on Amazon ElasticMap Reduce  http://aws.amazon.com/articles/2857    In addition, Amazon provides step-by-step video tutorials:\n http://aws.amazon.com/articles/2862  Support You can ask questions related to Hive on Elastic MapReduce on Elastic MapReduce forums at:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=52\nPlease also refer to the EMR developer guide for more information:\nhttp://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/\nContributed by: Vaibhav Aggarwal\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-aws-emr_27823791/","tags":null,"title":"Apache Hive : Hive Aws EMR"},{"categories":null,"contents":"Apache Hive : Hive Configurations Hive has more than 1600 configs around the service. The hive-site.xml contains the default configurations for the service. In this config file, you can change the configs. Every config change needs to restart the service(s).\nHere you can find the most important configurations and default values.\n   Config Name Default Value Description Config file     hive.metastore.client.cache.v2.enabled true This property enabled a Caffaine Cache for Metastore client MetastoreConf    More configs are in MetastoreConf.java file\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-configurations_283118321/","tags":null,"title":"Apache Hive : Hive Configurations"},{"categories":null,"contents":"Apache Hive : Hive deprecated authorization mode / Legacy Mode  Disclaimer Prerequisites Users, Groups, and Roles  Names of Users and Roles   Creating/Dropping/Using Roles  Create/Drop Role Grant/Revoke Roles Viewing Granted Roles   Privileges  Grant/Revoke Privileges Viewing Granted Privileges   Hive Operations and Required Privileges  This document describes Hive security using the basic authorization scheme, which regulates access to Hive metadata on the client side. This was the default authorization mode used when authorization was enabled. The default was changed to SQL Standard authorization in Hive 2.0 (HIVE-12429).\nDisclaimer Hive authorization is not completely secure. The basic authorization scheme is intended primarily to prevent good users from accidentally doing bad things, but makes no promises about preventing malicious users from doing malicious things. See the Hive authorization main page for the secure options.\nPrerequisites In order to use Hive authorization, there are two parameters that should be set in hive-site.xml:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;enable or disable the hive client authorization\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.createtable.owner.grants\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;ALL\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;the privileges automatically granted to the owner whenever a table gets created. An example like \u0026quot;select,drop\u0026quot; will grant select and drop privilege to the owner of the table\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; Note that, by default, the hive.security.authorization.createtable.owner.grants are set to null, which would result in the creator of a table having no access to the table.\nUsers, Groups, and Roles At the core of Hive\u0026rsquo;s authorization system are users, groups, and roles. Roles allow administrators to give a name to a set of grants which can be easily reused. A role may be assigned to users, groups, and other roles. For example, consider a system with the following users and groups:\n :  user_all_dbs: group_db1, group_db2 user_db1: group_db1 user_db2: group_db2  If we wanted to restrict each user to a specific set of databases, we could use roles to build the authorization mechanism. The administrator would create two roles, called role_db1 and role_db2. The role_db1 role would provide privileges just for the first database, and the role_db2 role would provide privileges just for the second database. The administrator could then grant the role_db1 role to group_db1, or explicitly for the users in the group, and do the same for role_db2 with the users of the second database. In order to allow users who need to see all databases to get their appropriate privileges, a third role could be created called role_all_dbs, which would be granted role_db1 and role_db2. When user_all_dbs is granted the role_all_dbs role, the user implicitly is granted all the privileges of role_db1 and role_db2.\nHive roles must be created manually before being used, unlike users and groups. Users and groups are managed by the hive.security.authenticator.manager. When a user connects to a Metastore Server and issues a query, the Metastore will determine the username of the connecting user, and the groups associated with that ushive.security.authorization.ername. That information is then used to determine if the user should have access to the metadata being requested, by comparing the required privileges of the Hive operation to the user privileges using the following rules:\n User privileges (Has the privilege been granted to the user) Group privileges (Does the user belong to any groups that the privilege has been granted to) Role privileges (Does the user or any of the groups that the user belongs to have a role that grants the privilege)  By default, the Metastore uses the HadoopDefaultAuthenticator for determing user -\u0026gt; group mappings, which determines authorization by using the Unix usernames and groups on the machine where the Metastore is running. To make this more clear, consider a scenario where a user foo is a member of group bar on the machine running the Hive CLI, and connects to a Metastore running on a separate server that also has a user named foo, but on the Metastore Server, foo is a member of group baz. When an operation is executed, the Metastore will determine foo to be in the group baz.\nTaking this a step further, it is also possible for the groups that a user belongs to on the Metastore Server may differ from the groups that the same user belongs to, as determined by HDFS. This could be the case if Hive or HDFS are configured to use non-default user -\u0026gt; group mappers, or the Metastore and the Namenode both use the defaults, but the processes are running on different machines, and the user -\u0026gt; group mappings are not the same on each machine.\nIt is important to realize that Hive Metastore only controls authorization for metadata, and the underlying data is controlled by HDFS, so if permissions and privileges between the two systems are not in sync, users may have access to metadata, but not the physical data. If the user -\u0026gt; group mappings across the Metastore and Namenode are not in sync, as in the scenarios above, a user may have the privileges required to access a table according to the Metastore, but may not have permission to access the underlying files according to the Namenode. This could also happen due to administrator intervention, if permissions on the files were changed by hand, but Metastore grants had not been updated.\nNames of Users and Roles Role names are case sensitive. In Hive 0.13, however, there was a bug that caused it to have case insensitive behavior. That issue has been fixed in Hive 0.14.\nUser names are also case sensitive. Unlike role names, user names are not managed within Hive.\nQuoted Identifiers in Version 0.13.0+\nAs of Hive 0.13.0, user and role names may optionally be surrounded by backtick characters () when the configuration parameter hive.support.quoted.identifiers is set to column (default value). All [Unicode](http://en.wikipedia.org/wiki/List_of_Unicode_characters) characters are permitted in the quoted identifiers, with double backticks (``) representing a backtick character. However when hive.support.quoted.identifiers is set to none`, or in Hive 0.12.0 and earlier, only alphanumeric and underscore characters are permitted in user names and role names.\nFor details, see HIVE-6013 and Supporting Quoted Identifiers in Column Names.\nAs of Hive 0.14, user may be optionally surrounded by backtick characters () irrespective of the hive.support.quoted.identifiers` setting.\nCreating/Dropping/Using Roles Create/Drop Role CREATE ROLE role_name DROP ROLE role_name Grant/Revoke Roles GRANT ROLE role_name [, role_name] ... TO principal_specification [, principal_specification] ... [WITH ADMIN OPTION] REVOKE [ADMIN OPTION FOR] ROLE role_name [, role_name] ... FROM principal_specification [, principal_specification] ... principal_specification: USER user | GROUP group | ROLE role Version\nGRANT ROLE added the optional WITH ADMIN OPTION clause in Hive 0.13.0 (HIVE-5923).\nREVOKE ROLE will add the optional ADMIN OPTION FOR clause in Hive 0.14.0 (HIVE-6252).\n Viewing Granted Roles SHOW ROLE GRANT principal_specification principal_specification: USER user | GROUP group | ROLE role Version\nThe output of SHOW ROLE GRANT is in tabular format starting with Hive 0.13.0 (HIVE-6204).\nPrivileges The following privileges are supported in Hive:\n ALL - Gives users all privileges ALTER - Allows users to modify the metadata of an object UPDATE - Allows users to modify the physical data of an object CREATE - Allows users to create objects. For a database, this means users can create tables, and for a table, this means users can create partitions DROP - Allows users to drop objects INDEX - Allows users to create indexes on an object (Note: this is not currently implemented) LOCK - Allows users to lock or unlock tables when concurrency is enabled SELECT - Allows users to access data for objects SHOW_DATABASE - Allows users to view available databases  Grant/Revoke Privileges GRANT priv_type [(column_list)] [, priv_type [(column_list)]] ... [ON object_specification] TO principal_specification [, principal_specification] ... [WITH GRANT OPTION] REVOKE [GRANT OPTION FOR] priv_type [(column_list)] [, priv_type [(column_list)]] ... [ON object_specification] FROM principal_specification [, principal_specification] ... REVOKE ALL PRIVILEGES, GRANT OPTION FROM user [, user] ... priv_type: ALL | ALTER | UPDATE | CREATE | DROP | INDEX | LOCK | SELECT | SHOW_DATABASE object_specification: TABLE tbl_name | DATABASE db_name principal_specification: USER user | GROUP group | ROLE role Version\nREVOKE priv_type will add the optional GRANT OPTION FOR clause in Hive 0.14.0 (HIVE-7404).\n Viewing Granted Privileges SHOW GRANT principal_specification [ON object_specification [(column_list)]] principal_specification: USER user | GROUP group | ROLE role object_specification: TABLE tbl_name | DATABASE db_name Version\nThe output of SHOW GRANT is in tabular format starting with Hive 0.13.0 (HIVE-6204).\n Hive Operations and Required Privileges As of the release of Hive 0.7, only these operations require permissions, according to org.apache.hadoop.hive.ql.plan.HiveOperation:\n   Operation ALTER UPDATE CREATE DROP INDEX LOCK SELECT SHOW_DATABASE     LOAD  X         EXPORT       X    IMPORT X X         CREATE TABLE   X        CREATE TABLE AS SELECT   X    X    DROP TABLE    X       SELECT       X    ALTER TABLE ADD COLUMN X          ALTER TABLE REPLACE COLUMN X          ALTER TABLE RENAME X          ALTER TABLE ADD PARTITION   X        ALTER TABLE DROP PARTITION    X       ALTER TABLE ARCHIVE  X         ALTER TABLE UNARCHIVE  X         ALTER TABLE SET PROPERTIES X          ALTER TABLE SET SERDE X          ALTER TABLE SET SERDE X          ALTER TABLE SET SERDEPROPERTIES X          ALTER TABLE CLUSTER BY X          ALTER TABLE PROTECT MODE X          ALTER PARTITION PROTECT MODE X          ALTER TABLE SET FILEFORMAT X          ALTER PARTITION SET FILEFORMAT X          ALTER TABLE SET LOCATION  X         ALTER PARTITION SET LOCATION  X         ALTER TABLE CONCATENATE  X         ALTER PARTITION CONCATENATE  X         SHOW DATABASES        X   LOCK TABLE      X     UNLOCK TABLE      X      ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/45876173/","tags":null,"title":"Apache Hive : Hive deprecated authorization mode / Legacy Mode"},{"categories":null,"contents":"Apache Hive : Hive HPL/SQL Hive Hybrid Procedural SQL On Hadoop (HPL/SQL) is a tool that implements procedural SQL for Hive. It is available in Hive 2.0.0 (HIVE-11055).\nHPL/SQL is an open source tool (Apache License 2.0) that implements procedural SQL language for Apache Hive, SparkSQL, Impala as well as any other SQL-on-Hadoop implementation, any NoSQL and any RDBMS.\nHPL/SQL is a hybrid and heterogeneous language that understands syntaxes and semantics of almost any existing procedural SQL dialect, and you can use with any database, for example, running existing Oracle PL/SQL code on Apache Hive and Microsoft SQL Server, or running Transact-SQL on Oracle, Cloudera Impala or Amazon Redshift.\nHPL/SQL language is compatible to a large extent with Oracle PL/SQL, ANSI/ISO SQL/PSM (IBM DB2, MySQL, Teradata i.e), PostgreSQL PL/pgSQL (Netezza), Transact-SQL (Microsoft SQL Server and Sybase) that allows you leveraging existing SQL/DWH skills and familiar approach to implement data warehouse solutions on Hadoop. It also facilitates migration of existing business logic to Hadoop.\nHPL/SQL is an efficient way to implement ETL processes in Hadoop.\nHPL/SQL language is compatible to a large extent with Oracle PL/SQL, ANSI/ISO SQL/PSM (IBM DB2, MySQL, Teradata), PostgreSQL PL/pgSQL (PostgreSQL, Netezza) and Transact-SQL (Microsoft SQL Server and Sybase) that facilitates migration of existing business logic to Hadoop.\nSee HPL/SQL Reference for documentation.\n ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/59690156/","tags":null,"title":"Apache Hive : Hive HPL/SQL"},{"categories":null,"contents":"Apache Hive : Hive Metadata Caching Proposal Why Metastore Cache During Hive 2 benchmark, we find Hive metastore operation take a lot of time and thus slow down Hive compilation. In some extreme case, it takes much longer than the actual query run time. Especially, we find the latency of cloud db is very high and 90% of total query runtime is waiting for metastore SQL database operations. Based on this observation, the metastore operation performance will be greatly enhanced if we have a memory structure which cache the database query result.\nServer side vs client side cache We are thinking about two possible locations of cache. One is on metastore client side, the other is on metastore server side. Both client side and server side cache needs to be a singleton and shared within the JVM. Let’s take Metastore server side cache as an example and illustrated below:\nHere we show two HiveServer2 instances using a single remote metastore. The metastore server will have a cache and thus shared by both HiveServer2 instances. In practice, we usually use HiveServer2 with embedded metastore. In this picture, metastore server code lives inside HS2 JVM instance and metastore server cache is shared within the HiveServer2:\nOn the other hand, Metastore client side lives in client JVM and will go away once the client is gone.\nIf we are using HiveServer2 with embedded metastore, both client side and server side cache doesn’t make much difference as we only use one copy of the cache on HiveServer2. We need to make sure if we choose to implement both client side and server side cache, only one cache is used so that we don’t waste extra memory.\nIn general, metastore client cache has better performance in case of a cache hit since it further avoids a network roundtrip to the metastore server. Metastore server cache has a better cache hit rate as it is shared by more clients. Client and server side cache is independent and there is nothing to prevent us to implementing both. However, our experiment shows the major bottleneck is the database query not the network traffic. That favors us to focus on metastore server cache in our initial effort.\nCache Consistency If we have cache in two or more JVM instance, we need to deal with cache consistency problem. For example, suppose both metastore server A and B caches table X, and at a moment, client changes table X via metastore A. Metastore A could invalidate the cached table X and maintain a consistent cache. However, metastore B does not realize the change and continue to use the stale cached table X. We can certainly adopt a cache eviction policy to invalidate old entries, but there is an inevitable lag.\nTo address this issue, we envision several approaches to invalidate stale cache on remote JVM:\n Time based synchronization. Cache will be refreshed periodically from the database. However, we need to make sure cache is consistent during the refresh. This is part of the current implementation. Metastore has an event log (currently used for implementing replication v2). The event log captures all the changes to the metadata object. So we shall be able to monitor the event log on every cache instance and invalidate changed entries (   CachedStore: Have a whitelist/blacklist config to allow selective caching of tables/partitions and allow read while prewarming Closed\n). This might have a minor lag due to the event propagation, but that should be much shorter than the cache eviction. 3. Maintain a unique id for every object in SQL database (eg, modified timestamp, version id, or md5 signature), which is different every time we change the object in SQL database. We will check the DB if the object is changed for every cache access. However, even check the timestamp in SQL database might take some time if the database latency is high 4. In addition, we might optionally add a “flush cache” statement in Hive in case user want to enforce a cache flush. However, this should be an admin privilege statement and will complicate our security model.\nIf the requirements present, we can also work on implementing a cache consistency protocol among multiple metastore instances. Such a protocol will need to replicate changes to all the active metastore before finally committing the change and responding to a client write/update request (perhaps using something similar to a two phase commit protocol). ## Case Study: Presto\nPresto has a global metastore client cache in its coordinator (HiveServer 2 equivalent). Note Presto currently only has 1 coordinator in a cluster so it does not suffer cache consistency problem if user only changes objects via Presto. However, if user also changes objects in metastore via Hive, it suffers the same issue.\nPresto adopts a Guava based LRU cache which has the default expiration of 1h and default max entry of 10000 (tunable). The cached metastore is pluggable. The cached metastore client and non cached version are both an implementation of a common interface and either can be activated by a config.\nPresto has the following cache:\n Point lookup cache   databaseCache tableCache partitionCache userRolesCache userTablePrivileges   Range scan cache   databaseNamesCache: regex -\u0026gt; database names, facilitates database search tableNamesCache viewNamesCache partitionNamesCache: table name -\u0026gt; partition names   Other   partitionFilterCache: PS -\u0026gt; partition names, facilitates partition pruning  For every partition filter condition, Presto breaks it down into tupleDomain and remainder:\nAddExchanges.planTableScan:\n DomainTranslator.ExtractionResult decomposedPredicate = DomainTranslator.fromPredicate(\n metadata,\n session,\n deterministicPredicate,\n types);\n public static class ExtractionResult\n {\n private final TupleDomaintupleDomain;\n private final Expression remainingExpression;\n }\ntupleDomain is a mapping of column -\u0026gt; range or exact value. When converting to PS, any range will be converted into wildcard and only exact value will be considered:\nHivePartitionManager.getFilteredPartitionNames:\n for (HiveColumnHandle partitionKey : partitionKeys) {\n if (domain != null \u0026amp;\u0026amp; domain.isNullableSingleValue()) {\n filter.add(((Slice) value).toStringUtf8());\n else {\n filter.add(PARTITION_VALUE_WILDCARD);\n }\n }\nFor example, the expression “state = CA and date between ‘201612’ and ‘201701’ will be broken down to PS (state = CA) and remainder date between ‘201612’ and ‘201701’. Presto will retrieve the partitions with state = CA from the PS -\u0026gt; partition name cache and partition object cache, and evaluates “date between ‘201612’ and ‘201701’ for every partitions returned. This is a good balance compare to caching partition names for every expression.\nOur Approach Our design is a metastore server side cache and we will do metastore invalidation upon receiving metastore events. The reason for both choices are discussed in the above sections.\nFurther, in our design, metastore will read all metastore objects once at startup time (prewarm) and there is no eviction of the metastore objects ever since. The only time we change cache is when user requested a change through metastore client (eg, alter table, alter partition), and upon receiving metastore event of changes made by other metastore server. Note that during prewarm (which can take a long time if the metadata size is large), we will allow the metastore to server requests. If a table has already been cached, the requests for that table (and its partitions and statistics) can be served from the cache. If the table has not been prewarmed yet, the requests for that table will be served from the database (\n CachedStore: Store cached partitions/col stats within the table cache and make prewarm non-blocking Resolved\n).\nCurrently, the size of the metastore cache can be restricted by a combination of cache whitelist and blacklist patterns (\n CachedStore: Have a whitelist/blacklist config to allow selective caching of tables/partitions and allow read while prewarming Closed\n). Before a table is cached, it is checked against these filters to decide if it can be cached or not. Similarly, when a table is read, if it does not pass the above filters, it is read from the database and not the cache.\nQuantitative study: memory footprint and prewarm time The major concern in this approach is how much memory the metastore cache will consume and how much latency at startup time to read all metastore objects (prewarm). For that, we did some quantitative experiments.\nIn our experiments, we adopted some memory optimizations discussed below, which is separating table/partition and storage descriptor. We only cache database/table/partition objects and not count column statistics/permanent functions/constraints. In our setting, the memory footprint is dominated by partition objects which aggregates to 58M (shown in the table below). This does not discount the fact that many strings can be interned and shared by multiple objects, which could save an additional 20% according to HIVE-16079.\n           object count Avg size (byte)   table 895 1576   partition 97,863 591   storagedescriptor 412 680     If we have 6G memory reserved for metastore cache, we could afford 10,000,000 partition objects and which we think is enough for majority use cases. In case we are out of the memory boundary, we could switch to a non-cached metastore and don’t crash.\nWe also tested the prewarm time in the same setting. It takes metastore 25 seconds to load all databases/tables/partitions from MySql database. Note in slow database such as Azure, the number could be much bigger but we have not tested yet. Metastore would only open listening port after prewarm. All metastore client request will be rejected before prewarm is done.\nMemory Optimization We plan to adopt two memory optimization techniques.\nThe first one is to separate storage descriptor from table/partition. This is the same technique we used in HBaseMetastore work and is based on the observation major components of storage descriptors are shared. The only storage descriptor components which might change from partition to partition is the location and parameters. We extract both components from shared storage descriptors and stored in partition level instead.\nThe second technique is intern shared string as suggested in HIVE-16079, which is based on the observation most strings in parameters are the same.\nCached Objects The key objects to store in the cache are\n Db Table Partition Permanent functions Constraints ColumnStats  We will cache thrift objects in cache, so we can simply implement a wrapper on top of the RawStore API.\nWe don’t plan to cache roles and privileges as we are in favor of external access control system such as Ranger, and put lesser focus on SQLStdAuth.\nCache Update For local metastore request that changes an object, such as alter table/alter partition, the change request will write through to the wrapped RawStore object. In the mean time, we should be able to update the cache without further fetching data back from SQL database.\nFor remote metastore updates, we will either use a periodical synchronization (current approach), or monitor event log and fetch affected objects from SQL database (\nHIVE-18661 CachedStore: Use metastore notification log events to update cache Resolved\n). Both options are discussed already in “Cache Consistency” section.\nAggregated Statistics We already have aggregated stats module in ObjectStore (\nHIVE-10382 Aggregate stats cache for RDBMS based metastore codepath Closed\n). However, the base column statistics is not cached and needs to fetch from SQL database everytime needed. We plan to port aggregated stats module to CachedStore to use cached column statistics to do the calculation. One design choice yet to make is whether we need to cache aggregated stats, or calculate them on the fly in the CachedStore assuming all column stats are in memory. But in either case, once we turn on aggregate stats in CacheStore, we shall turn off it in ObjectStore (already have a switch) so we don’t do it twice.\ngetPartitionsByExpr This is one of the most important operations in Hive metastore we want to optimize. The ObjectStore already have the ability to evaluate an expression against a list of partitions in memory. We plan to adopt the same approach. The assumption is even though we need to evaluate expression of every partitions of the table in memory, calculation time is still much lesser than database access time even though we can push some expression conditions to sql database. This is supported by some initial testing (shown below), but need to evaluate more in the future. In case it becomes a problem, a memory index to accelerate in memory expression evaluation is possible.\nArchitecture CachedStore will implement a RawStore interface. CachedStore internally wraps a real RawStore implementation which could be anything (either ObjectStore, or HBaseStore). In HiveServer2 embedded metastore or standalone metastore setting, we will set hive.metastore.rawstore.impl to CachedStore, and hive.metastore.cached.rawstore.impl (the wrapped RawStore) to ObjectStore. If we are using HiveCli with embedded metastore, we might want to skip CachedStore since we might not want prewarm latency.\nPotential Issues There are maybe some potential issue or unimplemented featrue in the initial version due to time limitation:\n Remote metastore invalidation by monitoring event queue. Discussed above and may not make it in version 1 getPartitionsByFilter may not be implemented. This API takes a string form of expression which we need to parse and evaluate in metastore. This API is only used in HCatalog and for backward compatibility. Hive itself will use getPartitionsByExpr which takes a binary form of expression and already addressed It is better to have a cache memory estimation, so we can control the memory usage of cache. However, memory estimation could be tricky especially considering interned string. In the initial version, we might only put a limit on the number of partitions, as we saw the memory usage is dominant by partition. In current design, once cache usage exceeds threshold (number of partitions exceeds threshold, or memory usage exceeds threshold in future version), Metastore will exit. If the metastore is still running with missing cache, some operations such as getPartitionsByExpr would produce wrong result. One potential optimization is we evict cache in table level, i.e., once memory is full, evict some tables alongs with all its partitions. When we want a missing table/partition, retrieve the table along with all partitions from SQL database. This is possible but adds a lot of complexity to the current design. We might only consider it if we observe memory footprint is excessive in the future. In prewarm, we fetch all partitions of a table in one SQL operation. This might or might not be a problem. However, fetching partition one by one is either not an option as it would take excessive long time. We might need to find some way to address it (like paging) if this becomes a problem  Compare to Presto In our design, we sacrifice prewarm time and memory footprint in change of simplicity and better runtime performance. By monitoring event queue, and can solve the remote metastore consistency issue which is missing in Presto. Architecture level, CachedStore is a lightweight cache layer wrapping the real RawStore, with this design, there’s nothing prevent us to implement alternative cache strategy in addition to our current approach.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-metadata-caching-proposal_69407514/","tags":null,"title":"Apache Hive : Hive Metadata Caching Proposal"},{"categories":null,"contents":"Apache Hive : Hive MetaTool  Hive MetaTool  The metatool Command Usage Example    Hive MetaTool Version 0.10.0 and later\nIntroduced in Hive 0.10.0. See HIVE-3056 and HIVE-3443.\nThe Hive MetaTool enables administrators to do bulk updates on the location fields in database, table, and partition records in the metastore. It provides the following functionality:\n Ability to search and replace the HDFS NN (NameNode) location in metastore records that reference the NN. One use is to transition a Hive deployment to HDFS HA NN (HDFS High Availability NameNode). A command line tool to execute JDOQL against the metastore. The ability to execute JDOQL against the metastore can be a useful debugging tool for both users and Hive developers.  The metatool Command The metatool command invokes the Hive MetaTool with these options:\n   Option Description     -listFSRoot Print the location of the current file system root (NameNode). The value is prefixed with hdfs:// scheme.   -updateLocation \u0026lt;new-loc\u0026gt; \u0026lt;old-loc\u0026gt; Update records in the Hive metastore to point to a new NameNode location (file system root location). Both new-loc and old-loc should be valid URIs with valid host names and schemes. For upgrading to HDFS HA NN, new-loc should match the value of the dfs.nameservices property. The old-loc should match the value returned by the listFSRoot option.When run with the dryRun option, changes are displayed but are not persisted. When run with the serdepropKey/tablePropKey option, updateLocation looks for the serde-prop-key/table-prop-key that is specified and updates its value if found.   -serdePropKey \u0026lt;serde-prop-key\u0026gt; Specify a SerDe property key whose value field may reference the HDFS NameNode location and hence may require an update. For example to update the Haivvero schema URL, specify schema.url for this argument.This option is valid only with the updateLocation option.   -tablePropKey \u0026lt;table-prop-key\u0026gt; Specify a table property key whose value field may reference the HDFS NameNode location and hence may require an update. For example to update the Avro SerDe schema URL, specify avro.schema.url for this argument.This option is valid only with the updateLocation option.   -dryRun Perform a dry run of updateLocation changes. The updateLocation changes are displayed but not persisted.This option is valid only with the updateLocation option.   -executeJDOQL \u0026lt;query-string\u0026gt; Execute the given JDOQL query.   -help Print a list of command options and their descriptions.    If you are unsure which version of Avro SerDe is used, pass both serdePropKey and tablePropKey arguments with their respective values for the keys to updateLocation.\nHive Configuration Directory\nNote that metatool is a command for Hive administrators and it needs direct access to the metastore RDBMS. In some environments, the RDBMS login information (especially password or the password key) is only stored on the metastore server host or in a metastore server specific config directory (with more restrictive file system permissions). In such cases, make sure you run on the metastore server machine, as the hive user, and also set the HIVE_CONF_DIR environment variable appropriately (for example, export HIVE_CONF_DIR=*\u0026lt;path to metastore server config dir\u0026gt;*).\nUsage Example The following metatool command uses the updateLocation, tablePropKey, and serdePropKey options to update the NameNode location to hdfs://localhost:9000 from hdfs://namenode2:8020.\n./hive --service metatool -updateLocation hdfs://localhost:9000 hdfs://namenode2:8020 -tablePropKey avro.schema.url -serdePropKey avro.schema.url Initializing HiveMetaTool.. 15/04/22 14:18:42 INFO metastore.ObjectStore: ObjectStore, initialize called 15/04/22 14:18:42 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored 15/04/22 14:18:42 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored 15/04/22 14:18:43 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=\u0026quot;Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order\u0026quot; 15/04/22 14:18:43 INFO DataNucleus.Datastore: The class \u0026quot;org.apache.hadoop.hive.metastore.model.MFieldSchema\u0026quot; is tagged as \u0026quot;embedded-only\u0026quot; so does not have its own datastore table. 15/04/22 14:18:43 INFO DataNucleus.Datastore: The class \u0026quot;org.apache.hadoop.hive.metastore.model.MOrder\u0026quot; is tagged as \u0026quot;embedded-only\u0026quot; so does not have its own datastore table. 15/04/22 14:18:44 INFO DataNucleus.Datastore: The class \u0026quot;org.apache.hadoop.hive.metastore.model.MFieldSchema\u0026quot; is tagged as \u0026quot;embedded-only\u0026quot; so does not have its own datastore table. 15/04/22 14:18:44 INFO DataNucleus.Datastore: The class \u0026quot;org.apache.hadoop.hive.metastore.model.MOrder\u0026quot; is tagged as \u0026quot;embedded-only\u0026quot; so does not have its own datastore table. 15/04/22 14:18:44 INFO DataNucleus.Query: Reading in results for query \u0026quot;org.datanucleus.store.rdbms.query.SQLQuery@0\u0026quot; since the connection used is closing 15/04/22 14:18:44 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is MYSQL 15/04/22 14:18:44 INFO metastore.ObjectStore: Initialized ObjectStore Looking for LOCATION_URI field in DBS table to update.. Successfully updated the following locations.. Updated 0 records in DBS table Looking for LOCATION field in SDS table to update.. Successfully updated the following locations.. Updated 0 records in SDS table Looking for value of avro.schema.url key in TABLE_PARAMS table to update.. Successfully updated the following locations.. Updated 0 records in TABLE_PARAMS table Looking for value of avro.schema.url key in SD_PARAMS table to update.. Successfully updated the following locations.. Updated 0 records in SD_PARAMS table Looking for value of avro.schema.url key in SERDE_PARAMS table to update.. Successfully updated the following locations.. Updated 0 records in SERDE_PARAMS table  Using metatool to read out table information (for all metastore backends).\nHIVE_CONF_DIR=/etc/hive/conf/conf.server/ hive --service metatool -executeJDOQL 'select dbName+\u0026quot;.\u0026quot;+tableName+\u0026quot;::\u0026quot;+colName+\u0026quot;=\u0026quot;+numDVs from org.apache.hadoop.hive.metastore.model.MTableColumnStatistics';  HIVE_CONF_DIR=/etc/hive/conf/conf.server/ hive --service metatool -executeJDOQL 'select dbName+\u0026quot;.\u0026quot;+tableName+\u0026quot;(\u0026quot;+partitionName+\u0026quot;)::\u0026quot;+colName+\u0026quot;=\u0026quot;+numDVs from org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics';  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-metatool_55156221/","tags":null,"title":"Apache Hive : Hive MetaTool"},{"categories":null,"contents":"Apache Hive : Hive Metrics The metrics that Hive collects can be viewed in the HiveServer2 Web UI by using the \u0026ldquo;Metrics Dump\u0026rdquo; tab.\nThe metrics dump will display any metric available over JMX encoded in JSON: Alternatively the metrics can be written directly into HDFS, a JSON file on the local file system where the HS2 instance is running or to the console by enabling the corresponding metric reporters. By default only the JMX and the JSON file reporter are enabled.\nThese metrics include:\n jvm.pause.info-threshold (Hive 2.0.0) jvm.pause.warn-threshold (Hive 2.0.0) jvm.pause.extraSleepTime (Hive 2.0.0) open_connections (Hive 2.0.0) open_operations (Hive 2.0.0) cumulative_connection_count (Hive 2.1.0) metastore_hive_locks (Hive 2.0.0) zookeeper_hive_sharedlocks (Hive 2.0.0) zookeeper_hive_exclusivelocks (Hive 2.0.0) zookeeper_hive_semisharedlocks (Hive 2.0.0) exec_async_queue_size (Hive 2.0.0) exec_async_pool_size (Hive 2.0.0) HiveServer2 operations (Hive 2.0.0) HiveServer2 operations completed (Hive 2.0.0) SQL operations (Hive 2.1.0) SQL operations completed (Hive 2.1.0) init_total_count_dbs (Hive 2.1.0) init_total_count_tables (Hive 2.1.0) init_total_count_partitions (Hive 2.1.0) create_total_count_dbs (Hive 2.1.0) create_total_count_tables (Hive 2.1.0) create_total_count_partitions (Hive 2.1.0) delete_total_count_dbs (Hive 2.1.0) delete_total_count_tables (Hive 2.1.0) delete_total_count_partitions (Hive 2.1.0) directsql_errors (Hive 2.1.0) waiting_compile_ops (Hive 2.2.0) hive_mapred_tasks (Hive 2.2.0) hive_spark_tasks (Hive 2.2.0) hive_tez_tasks (Hive 2.2.0) acquireReadWriteLocks (Hive 0.8.0) compile (Hive 0.8.0) doAuthorization (Hive 0.8.0) Driver.execute (Hive 0.8.0) releaseLocks (Hive 0.8.0) prune-listing (Hive 0.8.0) partition-retrieving (Hive 0.8.0) PreHook (Hive 0.8.0) PostHook (Hive 0.8.0) FailureHook (Hive 0.8.0) parse (Hive 0.12.0) semanticAnalyze (Hive 0.12.0) getInputSummary (Hive 0.12.0) getSplits (Hive 0.12.0) runTasks (Hive 0.12.0) serializePlan (Hive 0.12.0) deserializePlan (Hive 0.12.0) clonePlan (Hive 0.12.0) task (Hive 0.12.0) optimizer (Hive 2.0.0) Driver.run (Hive 0.9.0) TezCompiler (Hive 2.1.0) TezSubmitToRunningDag (Hive 0.13.0) TezBuildDag (Hive 0.13.0) TezSubmitDag (Hive 0.13.0) TezRunDag (Hive 0.13.0) TezCreateVertex (Hive 0.13.0) TezRunVertex (Hive 0.13.0) TezInitializeProcessor (Hive 0.13.0) TezRunProcessor (Hive 0.13.0) TezInitializeOperators (Hive 0.13.0) LoadHashtable (Hive 0.13.0) SparkSubmitToRunning (Hive 1.1.0) SparkBuildPlan (Hive 1.1.0) SparkBuildRDDGraph (Hive 1.1.0) SparkSubmitJob (Hive 1.1.0) SparkRunJob (Hive 1.1.0) SparkCreateTran (Hive 1.1.0) SparkRunStage (Hive 1.1.0) SparkInitializeOperators (Hive 1.1.0) SparkGenerateTaskTree (Hive 1.1.0) SparkFlushHashTable (Hive 1.1.0) SparkOptimizeOperatorTree (Hive 1.1.0) SparkOptimizeTaskTree (Hive 1.1.0) hs2_open_sessions (Hive 2.2.0) hs2_active_sessions (Hive 2.2.0) hs2_abandoned_sessions (Hive 2.2.0) hs2_avg_open_session_time (Hive 2.2.0) hs2_avg_active_session_time (Hive 2.2.0) hs2_submitted_queries (Hive 2.2.0) hs2_compiling_queries (Hive 2.2.0) hs2_executing_queries (Hive 2.2.0) hs2_failed_queries (Hive 2.2.0) hs2_succeeded_queries (Hive 2.2.0) GarbageCollectorMetricSet (Hive 1.3.0) - publishing attributes of GarbageCollectorMXBeans MemoryUsageGaugeSet (Hive 1.3.0) - publishing attributes of MemoryMXBeans ThreadStatesGaugeSet (Hive 1.3.0) - publishing attributes of ThreadMXBeans ClassLoadingGaugeSet (Hive 1.3.0) - publishing attributes of ClassLoadingMXBeans BufferPoolMetricSet (Hive 1.3.0) - publishing attributes of BufferPool JMX beans compaction_num_working (Hive 4.0.0) compaction_num_initiated (Hive 4.0.0) compaction_num_failed (Hive 4.0.0) compaction_num_succeeded (Hive 4.0.0) compaction_num_did_not_initiate (Hive 4.0.0) compaction_num_ready_for_cleaning (Hive 4.0.0) compaction_oldest_enqueue_age_in_sec (Hive 4.0.0) api_compaction_initiator_cycle (Hive 4.0.0) api_compaction_cleaner_cycle_minor (Hive 4.0.0) api_compaction_worker_cycle_minor (Hive 4.0.0) num_aborted_transactions (Hive 4.0.0) oldest_aborted_txn_id (Hive 4.0.0) oldest_aborted_txn_age_in_sec (Hive 4.0.0) total_num_aborted_transactions (Hive 4.0.0) total_num_committed_transactions (Hive 4.0.0) total_num_timed_out_transactions (Hive 4.0.0) num_locks (Hive 4.0.0) oldest_lock_age_in_sec (Hive 4.0.0) compaction_num_txn_to_writeid (Hive 4.0.0) compaction_num_completed_txn_components (Hive 4.0.0) compaction_num_initiators (Hive 4.0.0) compaction_num_workers (Hive 4.0.0) compaction_num_initiator_versions (Hive 4.0.0) compaction_num_worker_versions (Hive 4.0.0) oldest_open_repl_txn_id (Hive 4.0.0) oldest_open_non_repl_txn_id (Hive 4.0.0) oldest_open_repl_txn_age_in_sec (Hive 4.0.0) oldest_open_non_repl_txn_age_in_sec (Hive 4.0.0) tables_with_x_aborted_transactions (Hive 4.0.0) num_writes_to_disabled_compaction_table (Hive 4.0.0) oldest_ready_for_cleaning_age_in_sec (Hive 4.0.0) compaction_num_active_deltas (Hive 4.0.0) compaction_num_small_deltas (Hive 4.0.0) compaction_initiator_failure_counter (Hive 4.0.0) compaction_cleaner_failure_counter (Hive 4.0.0) compaction_initiator_cycle_duration (Hive 4.0.0) compaction_cleaner_cycle_duration (Hive 4.0.0) compaction_oldest_working_age_in_sec (Hive 4.0.0) compaction_oldest_cleaning_age_in_sec (Hive 4.0.0) compaction_num_obsolete_deltas (Hive 4.0.0)  Configuration properties for metrics can be found here: Metrics.\nSee HiveServer2 Overview for more information about HiveServer2.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-metrics_65872987/","tags":null,"title":"Apache Hive : Hive Metrics"},{"categories":null,"contents":"Apache Hive : Hive on Spark  1. Introduction  1.1 Motivation 1.2 Design Principle 1.3 Comparison with Shark and Spark SQL 1.4 Other Considerations   2. High-Level Functionality  2.1 A New Execution Engine 2.2 Spark Configuration 2.3 Miscellaneous Functionality   3. Hive-Level Design  3.1 Query Planning 3.2 Job Execution 3.3 Design Considerations  Table as RDD SparkWork SparkTask Shuffle, Group, and Sort Join Number of Tasks Local MapReduce Tasks Semantic Analysis and Logical Optimizations Job Diagnostics Counters and Metrics Explain Statements Hive Variables Union Concurrency and Thread Safety Build Infrastructure Mini Spark Cluster Testing   3.4 Potentially Required Work from Spark   4. Summary  1. Introduction We propose modifying Hive to add Spark as a third execution backend(HIVE-7292), parallel to MapReduce and Tez.\nSpark is an open-source data analytics cluster computing framework that’s built outside of Hadoop\u0026rsquo;s two-stage MapReduce paradigm but on top of HDFS. Spark’s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. By being applied by a series of transformations such as groupBy and filter, or actions such as count and save that are provided by Spark, RDDs can be processed and analyzed to fulfill what MapReduce jobs can do without having intermediate stages.\nSQL queries can be easily translated into Spark transformation and actions, as demonstrated in Shark and Spark SQL. In fact, many primitive transformations and actions are SQL-oriented such as join and count.\nMore information about Spark can be found here:\n Apache Spark page: http://spark.apache.org/ Apache Spark blogpost: http://blog.cloudera.com/blog/2013/11/putting-spark-to-use-fast-in-memory-computing-for-your-big-data-applications/ Apache Spark JavaDoc: http://spark.apache.org/docs/1.0.0/api/java/index.html  1.1 Motivation Here are the main motivations for enabling Hive to run on Spark:\n Spark user benefits: This feature is very valuable to users who are already using Spark for other data processing and machine learning needs. Standardizing on one execution backend is convenient for operational management, and makes it easier to develop expertise to debug issues and make enhancements. Greater Hive adoption: Following the previous point, this brings Hive into the Spark user base as a SQL on Hadoop option, further increasing Hive’s adoption. Performance: Hive queries, especially those involving multiple reducer stages, will run faster, thus improving user experience as Tez does.  It is not a goal for the Spark execution backend to replace Tez or MapReduce. It is healthy for the Hive project for multiple backends to coexist. Users have a choice whether to use Tez, Spark or MapReduce. Each has different strengths depending on the use case. And the success of Hive does not completely depend on the success of either Tez or Spark.\n1.2 Design Principle The main design principle is to have no or limited impact on Hive’s existing code path and thus no functional or performance impact. That is, users choosing to run Hive on either MapReduce or Tez will have existing functionality and code paths as they do today. In addition, plugging in Spark at the execution layer keeps code sharing at maximum and contains the maintenance cost, so Hive community does not need to make specialized investments for Spark.\nMeanwhile, users opting for Spark as the execution engine will automatically have all the rich functional features that Hive provides. Future features (such as new data types, UDFs, logical optimization, etc) added to Hive should be automatically available to those users without any customization work to be done done in Hive’s Spark execution engine.\n1.3 Comparison with Shark and Spark SQL There are two related projects in the Spark ecosystem that provide Hive QL support on Spark: Shark and Spark SQL.\n The Shark project translates query plans generated by Hive into its own representation and executes them over Spark. Spark SQL is a feature in Spark. It uses Hive’s parser as the frontend to provide Hive QL support. Spark application developers can easily express their data processing logic in SQL, as well as the other Spark operators, in their code. Spark SQL supports a different use case than Hive.  Compared with Shark and Spark SQL, our approach by design supports all existing Hive features, including Hive QL (and any future extension), and Hive’s integration with authorization, monitoring, auditing, and other operational tools.\n1.4 Other Considerations We know that a new execution backend is a major undertaking. It inevitably adds complexity and maintenance cost, even though the design avoids touching the existing code paths. And Hive will now have unit tests running against MapReduce, Tez, and Spark. We think that the benefit outweighs the cost. From an infrastructure point of view, we can get sponsorship for more hardware to do continuous integration.\nLastly, Hive on Tez has laid some important groundwork that will be very helpful to support a new execution engine such as Spark. This project here will certainly benefit from that. On the other hand, Spark is a framework that’s very different from either MapReduce or Tez. Thus, it’s very likely to find gaps and hiccups during the integration. It’s expected that Hive community will work closely with Spark community to ensure the success of the integration.\n2. High-Level Functionality 2.1 A New Execution Engine We will introduce a new execution, Spark, in addition to existing MapReduce and Tez. To use Spark as an execution engine in Hive, set the following:\nset hive.execution.engine=spark;\nThe default value for this configuration is still “mr”. Hive continues to work on MapReduce and Tez as is on clusters that don\u0026rsquo;t have spark.\nThe new execution engine should support all Hive queries without requiring any modification of the queries. Query result should be functionally equivalent to that from either MapReduce or Tez.\n2.2 Spark Configuration When Spark is configured as Hive\u0026rsquo;s execution, a few configuration variables will be introduced such as the master URL of the Spark cluster. However, they can be completely ignored if Spark isn’t configured as the execution engine.\n2.3 Miscellaneous Functionality  Hive will display a task execution plan that’s similar to that being displayed in “explain” command for MapReduce and Tez. Hive will give appropriate feedback to the user about progress and completion status of the query when running queries on Spark. The user will be able to get statistics and diagnostic information as before (counters, logs, and debug info on the console).  3. Hive-Level Design As noted in the introduction, this project takes a different approach from that of Shark or Spark SQL in the sense that we are not going to implement SQL semantics using Spark\u0026rsquo;s primitives. On the contrary, we will implement it using MapReduce primitives. The only new thing here is that these MapReduce primitives will be executed in Spark. In fact, only a few of Spark\u0026rsquo;s primitives will be used in this design.\nThe approach of executing Hive’s MapReduce primitives on Spark that is different from what Shark or Spark SQL does has the following direct advantages:\n Spark users will automatically get the whole set of Hive’s rich features, including any new features that Hive might introduce in the future. This approach avoids or reduces the necessity of any customization work in Hive’s Spark execution engine. It will also limit the scope of the project and reduce long-term maintenance by keeping Hive-on-Spark congruent to Hive MapReduce and Tez.  The main work to implement the Spark execution engine for Hive lies in two folds: query planning, where Hive operator plan from semantic analyzer is further translated a task plan that Spark can execute, and query execution, where the generated Spark plan gets actually executed in the Spark cluster. Of course, there are other functional pieces, miscellaneous yet indispensable such as monitoring, counters, statistics, etc. Some important design details are thus also outlined below.\nIt’s worth noting that though Spark is written largely in Scala, it provides client APIs in several languages including Java. Naturally we choose Spark Java APIs for the integration, and no Scala knowledge is needed for this project.\n3.1 Query Planning Currently for a given user query Hive semantic analyzer generates an operator plan that\u0026rsquo;s composed of a graph of logical operators such as TableScanOperator, ReduceSink, FileSink, GroupByOperator, etc. MapReduceCompiler compiles a graph of MapReduceTasks and other helper tasks (such as MoveTask) from the logical, operator plan. Tez behaves similarly, yet generates a TezTask that combines otherwise multiple MapReduce tasks into a single Tez task.\nFor Spark, we will introduce SparkCompiler, parallel to MapReduceCompiler and TezCompiler. Its main responsibility is to compile from Hive logical operator plan a plan that can be execute on Spark. Thus, we will have SparkTask, depicting a job that will be executed in a Spark cluster, and SparkWork, describing the plan of a Spark task. Thus, SparkCompiler translates a Hive\u0026rsquo;s operator plan into a SparkWork instance.\nDuring the task plan generation, SparkCompiler may perform physical optimizations that\u0026rsquo;s suitable for Spark. However, for first phase of the implementation, we will focus less on this unless it\u0026rsquo;s easy and obvious. Further optimization can be done down the road in an incremental manner as we gain more and more knowledge and experience with Spark.\nHow to generate SparkWork from Hive’s operator plan is left to the implementation. However, there seems to be a lot of common logics between Tez and Spark as well as between MapReduce and Spark. If feasible, we will extract the common logic and package it into a shareable form, leaving the specific implementations to each task compiler, without destabilizing either MapReduce or Tez. 3.2 Job Execution A SparkTask instance can be executed by Hive\u0026rsquo;s task execution framework in the same way as for other tasks. Internally, the SparkTask.execute() method will make RDDs and functions out of a SparkWork instance, and submit the execution to the Spark cluster via a Spark client.\nOnce the Spark work is submitted to the Spark cluster, Spark client will continue to monitor the job execution and report progress. A Spark job can be monitored via SparkListener APIs. Currently not available in Spark Java API, We expect they will be made available soon with the help from Spark community.\nWith SparkListener APIs, we will add a SparkJobMonitor class that handles printing of status as well as reporting the final result. This class provides similar functions as HadoopJobExecHelper used for MapReduce processing, or TezJobMonitor used for Tez job processing, and will also retrieve and print the top level exception thrown at execution time, in case of job failure.\nSpark job submission is done via a SparkContext object that’s instantiated with user’s configuration. When a SparkTask is executed by Hive, such context object is created in the current user session. With the context object, RDDs corresponding to Hive tables are created and MapFunction and ReduceFunction (more details below) that are built from Hive’s SparkWork and applied to the RDDs. Job execution is triggered by applying a foreach() transformation on the RDDs with a dummy function.\nOne SparkContext per user session is right thing to do, but it seems that Spark assumes one SparkContext per application because of some thread-safety issues. We expect that Spark community will be able to address this issue timely.\n3.3 Design Considerations This section covers the main design considerations for a number of important components, either new that will be introduced or existing that deserves special treatment. For other existing components that aren’t named out, such as UDFs and custom Serdes, we expect that special considerations are either not needed or insignificant.\nTable as RDD A Hive table is nothing but a bunch of files and folders on HDFS. Spark primitives are applied to RDDs. Thus, naturally Hive tables will be treated as RDDs in the Spark execution engine. However, Hive table is more complex than a HDFS file. It can have partitions and buckets, dealing with heterogeneous input formats and schema evolution. As a result, the treatment may not be that simple, potentially having complications, which we need to be aware of.\nIt\u0026rsquo;s possible we need to extend Spark\u0026rsquo;s Hadoop RDD and implement a Hive-specific RDD. While RDD extension seems easy in Scala, this can be challenging as Spark\u0026rsquo;s Java APIs lack such capability. We will find out if RDD extension is needed and if so we will need help from Spark community on the Java APIs.\nSparkWork As discussed above, SparkTask will use SparkWork, which describes the task plan that the Spark job is going to execute upon. SparkWork will be very similar to TezWork, which is basically composed of MapWork at the leaves and ReduceWork (occassionally, UnionWork) in all other nodes.\nDefining SparkWork in terms of MapWork and ReduceWork makes the new concept easier to be understood. The “explain” command will show a pattern that Hive users are familiar with.\nSparkTask To execute the work described by a SparkWork instance, some further translation is necessary, as MapWork and ReduceWork are MapReduce-oriented concepts, and implementing them with Spark requires some traverse of the plan and generation of Spark constructs (RDDs, functions). How to traverse and translate the plan is left to the implementation, but this is very Spark specific, thus having no exposure to or impact on other components.\nAbove mentioned MapFunction will be made from MapWork, specifically, the operator chain starting from ExecMapper.map() method. ExecMapper class implements MapReduce Mapper interface, but the implementation in Hive contains some code that can be reused for Spark. Therefore, we will likely extract the common code into a separate class, MapperDriver, to be shared by both MapReduce and Spark. Note that this is just a matter of refactoring rather than redesigning.\n(Tez probably had the same situation. However, Tez has chosen to create a separate class, RecordProcessor, to do something similar.)\nSimilarly, ReduceFunction will be made of ReduceWork instance from SparkWork. To Spark, ReduceFunction has no difference from MapFunction, but the function\u0026rsquo;s implementation will be different, made of the operator chain starting from ExecReducer.reduce(). Also because some code in ExecReducer are to be reused, likely we will extract the common code into a separate class, ReducerDriver, so as to be shared by both MapReduce and Spark.\nAll functions, including MapFunction and ReduceFunction needs to be serializable as Spark needs to ship them to the cluster. This could be tricky as how to package the functions impacts the serialization of the functions, and Spark is implicit on this.\nNote that Spark\u0026rsquo;s built-in map and reduce transformation operators are functional with respect to each record. For example, Hive\u0026rsquo;s operators, however, need to be initialized before being called to process rows and be closed when done processing. MapFunction and ReduceFunction will have to perform all those in a single call() method. For the purpose of using Spark as an alternate execution backend for Hive, we will be using the mapPartitions transformation operator on RDDs, which provides an iterator on a whole partition of data. With the iterator in control, Hive can initialize the operator chain before processing the first row, and de-initialize it after all input is consumed.\nIt\u0026rsquo;s worth noting that during the prototyping Spark caches function globally in certain cases, thus keeping stale state of the function. Such culprit is hard to detect and hopefully Spark will be more specific in documenting features down the road.\nShuffle, Group, and Sort While this comes for “free” for MapReduce and Tez, we will need to provide an equivalent for Spark. Fortunately, Spark provides a few transformations that are suitable to substitute MapReduce’s shuffle capability, such as partitionBy, groupByKey, and sortByKey. Transformation partitionBy does pure shuffling (no grouping or sorting), groupByKey does shuffling and grouping, and sortByKey() does shuffling plus sorting. Therefore, for each ReduceSinkOperator in SparkWork, we will need to inject one of the transformations.\nHaving the capability of selectively choosing the exact shuffling behavior provides opportunities for optimization. For instance, Hive\u0026rsquo;s groupBy doesn\u0026rsquo;t require the key to be sorted, but MapReduce does it nevertheless. In Spark, we can choose sortByKey only if necessary key order is important (such as for SQL order by).\nWhile sortByKey provides no grouping, it’s easy to group the keys as rows with the same key will come consecutively. On the other hand, groupByKey clusters the keys in a collection, which naturally fits the MapReduce’s reducer interface.\nAs Hive is more sophisticated in using MapReduce keys to implement operations that’s not directly available such as join, above mentioned transformations may not behave exactly as Hive needs. Thus, we need to be diligent in identifying potential issues as we move forward.\nFinally, it seems that Spark community is in the process of improving/changing the shuffle related APIs. Thus, this part of design is subject to change. Please refer to https://issues.apache.org/jira/browse/SPARK-2044 for the details on Spark shuffle-related improvement.\nJoin It’s rather complicated in implementing join in MapReduce world, as manifested in Hive. Hive has reduce-side join as well as map-side join (including map-side hash lookup and map-side sorted merge). We will keep Hive’s join implementations. However, extra attention needs to be paid on the shuffle behavior (key generation, partitioning, sorting, etc), since Hive extensively uses MapReduce’s shuffling in implementing reduce-side join. It’s expected that Spark is, or will be, able to provide flexible control over the shuffling, as pointed out in the previous section(Shuffle, Group, and Sort).\nSee: Hive on Spark: Join Design Master for detailed design.\nNumber of Tasks As specified above, Spark transformations such as partitionBy will be used to connect mapper-side’s operations to reducer-side’s operations. The number of partitions can be optionally given for those transformations, which basically dictates the number of reducers.\nThe determination of the number of reducers will be the same as it’s for MapReduce and Tez.\nLocal MapReduce Tasks While we could see the benefits of running local jobs on Spark, such as avoiding sinking data to a file and then reading it from the file to memory, in the short term, those tasks will still be executed the same way as it is today. This means that Hive will always have to submit MapReduce jobs when executing locally. However, this can be further investigated and evaluated down the road.\nThe same applies for presenting the query result to the user. Presently, a fetch operator is used on the client side to fetch rows from the temporary file (produced by FileSink in the query plan). It\u0026rsquo;s possible to have the FileSink to generate an in-memory RDD instead and the fetch operator can directly read rows from the RDD. Again this can be investigated and implemented as a future work. Semantic Analysis and Logical Optimizations Neither semantic analyzer nor any logical optimizations will change. Physical optimizations and MapReduce plan generation have already been moved out to separate classes as part of Hive on Tez work.\nJob Diagnostics Basic “job succeeded/failed” as well as progress will be as discussed in “Job monitoring”. Hive’s current way of trying to fetch additional information about failed jobs may not be available immediately, but this is another area that needs more research.\nSpark provides WebUI for each SparkContext while it’s running. Note that this information is only available for the duration of the application by default. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.\nSpark’s Standalone Mode cluster manager also has its own web UI. If an application has logged events over the course of its lifetime, then the Standalone master’s web UI will automatically re-render the application’s UI after the application has finished.\nIf Spark is run on Mesos or YARN, it is still possible to reconstruct the UI of a finished application through Spark’s history server, provided that the application’s event logs exist.\nFor more information about Spark monitoring, visithttp://spark.apache.org/docs/latest/monitoring.html.\nCounters and Metrics Spark has accumulators which are variables that are only “added” to through an associative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric value types and standard mutable collections, and programmers can add support for new types. In Hive, we may use Spark accumulators to implement Hadoop counters, but this may not be done right way.\nSpark publishes runtime metrics for a running job. However, it’s very likely that the metrics are different from either MapReduce or Tez, not to mention the way to extract the metrics. The topic around this deserves a separate document, but this can be certainly improved upon incrementally.\nExplain Statements Explain statements will be similar to that of TezWork.\nHive Variables Hive variables will continue to work as it is today. The variables will be passed through to the execution engine as before. However, some execution engine related variables may not be applicable to Spark, in which case, they will be simply ignored.\nUnion While it\u0026rsquo;s mentioned above that we will use MapReduce primitives to implement SQL semantics in the Spark execution engine, union is one exception. While it\u0026rsquo;s possible to implement it with MapReduce primitives, it takes up to three MapReduce jobs to union two datasets. Using Spark\u0026rsquo;s union transformation should significantly reduce the execution time and promote interactivity.\nIn fact, Tez has already deviated from MapReduce practice with respect to union. There is an existing UnionWork where a union operator is translated to a work unit.\nConcurrency and Thread Safety Spark launches mappers and reducers differently from MapReduce in that a worker may process multiple HDFS splits in a single JVM. However, Hive’s map-side operator tree or reduce-side operator tree operates in a single thread in an exclusive JVM. Reusing the operator trees and putting them in a shared JVM with each other will more than likely cause concurrency and thread safety issues. Such problems, such as static variables, have surfaced in the initial prototyping. For instance, variable ExecMapper.done is used to determine if a mapper has finished its work. If two ExecMapper instances exist in a single JVM, then one mapper that finishes earlier will prematurely terminate the other also. We expect there will be a fair amount of work to make these operator tree thread-safe and contention-free. However, this work should not have any impact on other execution engines.\nBuild Infrastructure There will be a new “ql” dependency on Spark. Currently Spark client library comes in a single jar. The spark jar will be handled the same way Hadoop jars are handled: they will be used during compile, but not included in the final distribution. Rather we will depend on them being installed separately. The spark jar will only have to be present to run Spark jobs, they are not needed for either MapReduce or Tez execution.\nOn the other hand, to run Hive code on Spark, certain Hive libraries and their dependencies need to be distributed to Spark cluster by calling SparkContext.addJar() method. As Spark also depends on Hadoop and other libraries, which might be present in Hive’s dependents yet with different versions, there might be some challenges in identifying and resolving library conflicts. Jetty libraries posted such a challenge during the prototyping.\nMini Spark Cluster Spark jobs can be run local by giving “local” as the master URL. Most testing will be performed in this mode. In the same time, Spark offers a way to run jobs in a local cluster, a cluster made of a given number of processes in the local machine. We will further determine if this is a good way to run Hive’s Spark-related tests.\nTesting Testing, including pre-commit testing, is the same as for Tez. Currently Hive has a coverage problem as there are a few variables that requires full regression suite run, such as Tez vs MapReduce, vectorization on vs off, etc. We propose rotating those variables in pre-commit test run so that enough coverage is in place while testing time isn’t prolonged.\n3.4 Potentially Required Work from Spark During the course of prototyping and design, a few issues on Spark have been identified, as shown throughout the document. Potentially more, but the following is a summary of improvement that’s needed from Spark community for the project:\n Job monitoring API in Java. SparkContext thread safety issue. Improve shuffle functionality and API. Potentially, Java API for extending RDD.  4. Summary It can be seen from above analysis that the project of Spark on Hive is simple and clean in terms of functionality and design, while complicated and involved in implementation, which may take significant time and resources. Therefore, we are going to take a phased approach and expect that the work on optimization and improvement will be on-going in a relatively long period of time while all basic functionality will be there in the first phase.\nSecondly, we expect the integration between Hive and Spark will not be always smooth. Functional gaps may be identified and problems may arise. We anticipate that Hive community and Spark community will work closely to resolve any obstacles that might come on the way.\nNevertheless, we believe that the impact on existing code path is minimal. While Spark execution engine may take some time to stabilize, MapReduce and Tez should continue working as it is.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-on-spark_42567714/","tags":null,"title":"Apache Hive : Hive on Spark"},{"categories":null,"contents":"Apache Hive : Hive on Spark: Getting Started  Version Compatibility Spark Installation Configuring YARN Configuring Hive  Configuration property details   Configuring Spark  Tuning Details   Common Issues (Green are resolved, will be removed from this list) Recommended Configuration Design documents  Hive on Spark provides Hive with the ability to utilize Apache Spark as its execution engine.\nset hive.execution.engine=spark; Hive on Spark was added in HIVE-7292.\nVersion Compatibility Hive on Spark is only tested with a specific version of Spark, so a given version of Hive is only guaranteed to work with a specific version of Spark. Other versions of Spark may work with a given version of Hive, but that is not guaranteed. Below is a list of Hive versions and their corresponding compatible Spark versions.\n   Hive Version Spark Version     master 2.3.0   3.0.x 2.3.0   2.3.x 2.0.0   2.2.x 1.6.0   2.1.x 1.6.0   2.0.x 1.5.0   1.2.x 1.3.1   1.1.x 1.2.0    Spark Installation Follow instructions to install Spark:  YARN Mode: http://spark.apache.org/docs/latest/running-on-yarn.html Standalone Mode: https://spark.apache.org/docs/latest/spark-standalone.html\nHive on Spark supports Spark on YARN mode as default.\nFor the installation perform the following tasks:\n  Install Spark (either download pre-built Spark, or build assembly from source).\n Install/build a compatible version. Hive root pom.xml\u0026rsquo;s \u0026lt;spark.version\u0026gt; defines what version of Spark it was built/tested with. Install/build a compatible distribution. Each version of Spark has several distributions, corresponding with different versions of Hadoop. Once Spark is installed, find and keep note of the \u0026lt;spark-assembly-*.jar\u0026gt; location. Note that you must have a version of Spark which does not include the Hive jars. Meaning one which was not built with the Hive profile. If you will use Parquet tables, it\u0026rsquo;s recommended to also enable the \u0026ldquo;parquet-provided\u0026rdquo; profile. Otherwise there could be conflicts in Parquet dependency. To remove Hive jars from the installation, simply use the following command under your Spark repository:  Prior to Spark 2.0.0:\n./make-distribution.sh --name \u0026quot;hadoop2-without-hive\u0026quot; --tgz \u0026quot;-Pyarn,hadoop-provided,hadoop-2.4,parquet-provided\u0026quot; ``\nSince Spark 2.0.0:\n./dev/make-distribution.sh --name \u0026quot;hadoop2-without-hive\u0026quot; --tgz \u0026quot;-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided\u0026quot; ``\nSince Spark 2.3.0:\n./dev/make-distribution.sh --name \u0026quot;hadoop2-without-hive\u0026quot; --tgz \u0026quot;-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided,orc-provided\u0026quot; ``\n  Start Spark cluster\n Keep note of the . This can be found in Spark master WebUI.    Configuring YARN Instead of the capacity scheduler, the fair scheduler is required. This fairly distributes an equal share of resources for jobs in the YARN cluster.\nyarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler\nConfiguring Hive   To add the Spark dependency to Hive:\n Prior to Hive 2.2.0, link the spark-assembly jar to HIVE_HOME/lib. Since Hive 2.2.0, Hive on Spark runs with Spark 2.0.0 and above, which doesn\u0026rsquo;t have an assembly jar.  To run with YARN mode (either yarn-client or yarn-cluster), link the following jars to HIVE_HOME/lib.  scala-library spark-core spark-network-common   To run with LOCAL mode (for debugging only), link the following jars in addition to those above to HIVE_HOME/lib.  chill-java chill jackson-module-paranamer jackson-module-scala jersey-container-servlet-core jersey-server json4s-ast kryo-shaded minlog scala-xml spark-launcher spark-network-shuffle spark-unsafe xbean-asm5-shaded        Configure Hive execution engine to use Spark:\n  set hive.execution.engine=spark; See the Spark section of Hive Configuration Properties for other properties for configuring Hive and the Remote Spark Driver. 3. Configure Spark-application configs for Hive. See: http://spark.apache.org/docs/latest/configuration.html. This can be done either by adding a file \u0026ldquo;spark-defaults.conf\u0026rdquo; with these properties to the Hive classpath, or by setting them on Hive configuration (hive-site.xml). For instance:\nset spark.master=\u0026lt;Spark Master URL\u0026gt; set spark.eventLog.enabled=true; set spark.eventLog.dir=\u0026lt;Spark event log folder (must exist)\u0026gt; set spark.executor.memory=512m; set spark.serializer=org.apache.spark.serializer.KryoSerializer; Configuration property details * `spark.executor.memory`: Amount of memory to use per executor process. * `spark.executor.cores`: Number of cores per executor. * `spark.yarn.executor.memoryOverhead`: The amount of off heap memory (in megabytes) to be allocated per executor, when running Spark on Yarn. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. In addition to the executor's memory, the container in which the executor is launched needs some extra memory for system processes, and this is what this overhead is for. * `spark.executor.instances`: The number of executors assigned to each application. * `spark.driver.memory`: The amount of memory assigned to the Remote Spark Context (RSC). We recommend 4GB. * `spark.yarn.driver.memoryOverhead`: We recommend 400 (MB).   Allow Yarn to cache necessary spark dependency jars on nodes so that it does not need to be distributed each time when an application runs.\n Prior to Hive 2.2.0, upload spark-assembly jar to hdfs file(for example: hdfs://xxxx:8020/spark-assembly.jar) and add following in hive-site.xml  \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;spark.yarn.jar\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://xxxx:8020/spark-assembly.jar\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; ``\n Hive 2.2.0, upload all jars in $SPARK_HOME/jars to hdfs folder(for example:hdfs:///xxxx:8020/spark-jars) and add following in hive-site.xml  \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;spark.yarn.jars\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://xxxx:8020/spark-jars/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; ``\n  Configuring Spark Setting executor memory size is more complicated than simply setting it to be as large as possible. There are several things that need to be taken into consideration:\n More executor memory means it can enable mapjoin optimization for more queries. More executor memory, on the other hand, becomes unwieldy from GC perspective. Some experiments shows that HDFS client doesn’t handle concurrent writers well, so it may face race condition if executor cores are too many.  The following settings need to be tuned for the cluster, these may also apply to submission of Spark jobs outside of Hive on Spark:\n   Property Recommendation     spark.executor.cores Between 5-7, See tuning details section   spark.executor.memory yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores)    spark.yarn.executor.memoryOverhead 15-20% of spark.executor.memory   spark.executor.instances Depends on spark.executor.memory + spark.yarn.executor.memoryOverhead, see tuning details section.    Tuning Details When running Spark on YARN mode, we generally recommend setting spark.executor.cores to be 5, 6 or 7, depending on what the typical node is divisible by. For instance, if yarn.nodemanager.resource.cpu-vcores is 19, then 6 is a better choice (all executors can only have the same number of cores, here if we chose 5, then every executor only gets 3 cores; if we chose 7, then only 2 executors are used, and 5 cores will be wasted). If it’s 20, then 5 is a better choice (since this way you’ll get 4 executors, and no core is wasted).\nFor spark.executor.memory, we recommend to calculate yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores) then split that between spark.executor.memory and spark.yarn.executor.memoryOverhead. According to our experiment, we recommend setting spark.yarn.executor.memoryOverhead to be around 15-20% of the total memory.\nAfter you’ve decided on how much memory each executor receives, you need to decide how many executors will be allocated to queries. In the GA release Spark dynamic executor allocation will be supported. However for this beta only static resource allocation can be used. Based on the physical memory in each node and the configuration of spark.executor.memory and spark.yarn.executor.memoryOverhead, you will need to choose the number of instances and set spark.executor.instances.\nNow a real world example. Assuming 10 nodes with 64GB of memory per node with 12 virtual cores, e.g., yarn.nodemanager.resource.cpu-vcores=12. One node will be used as the master and as such the cluster will have 9 slave nodes. We’ll configure spark.executor.cores to 6. Given 64GB of ram yarn.nodemanager.resource.memory-mb will be 50GB. We’ll determine the amount of memory for each executor as follows: 50GB * (6/12) = 25GB. We’ll assign 20% to spark.yarn.executor.memoryOverhead, or 5120, and 80% to spark.executor.memory, or 20GB.\nOn this 9 node cluster we’ll have two executors per host. As such we can configure spark.executor.instances somewhere between 2 and 18. A value of 18 would utilize the entire cluster.\nCommon Issues (Green are resolved, will be removed from this list)    Issue Cause Resolution     Error: Could not find or load main class org.apache.spark.deploy.SparkSubmit Spark dependency not correctly set. Add Spark dependency to Hive, see Step 1 above.   org.apache.spark.SparkException: Job aborted due to stage failure: Task 5.0:0 had a not serializable result: java.io.NotSerializableException: org.apache.hadoop.io.BytesWritable Spark serializer not set to Kryo. Set spark.serializer to be org.apache.spark.serializer.KryoSerializer, see Step 3 above.   [ERROR] Terminal initialization failed; falling back to unsupportedjava.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected Hive has upgraded to Jline2 but jline 0.94 exists in the Hadoop lib. 1. Delete jline from the Hadoop lib directory (it\u0026rsquo;s only pulled in transitively from ZooKeeper). 2. export HADOOP_USER_CLASSPATH_FIRST=true 3. If this error occurs during mvn test, perform a mvn clean install on the root project and itests directory.   Spark executor gets killed all the time and Spark keeps retrying the failed stage; you may find similar information in the YARN nodemanager log.WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Container [pid=217989,containerID=container_1421717252700_0716_01_50767235] is running beyond physical memory limits. Current usage: 43.1 GB of 43 GB physical memory used; 43.9 GB of 90.3 GB virtual memory used. Killing container. For Spark on YARN, nodemanager would kill Spark executor if it used more memory than the configured size of \u0026ldquo;spark.executor.memory\u0026rdquo; + \u0026ldquo;spark.yarn.executor.memoryOverhead\u0026rdquo;. Increase \u0026ldquo;spark.yarn.executor.memoryOverhead\u0026rdquo; to make sure it covers the executor off-heap memory usage.   Run query and get an error like:FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.spark.SparkTaskIn Hive logs, it shows:java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy at org.xerial.snappy.SnappyOutputStream.(SnappyOutputStream.java:79) Happens on Mac (not officially supported).This is a general Snappy issue with Mac and is not unique to Hive on Spark, but workaround is noted here because it is needed for startup of Spark client. Run this command before starting Hive or HiveServer2:export HADOOP_OPTS=\u0026quot;-Dorg.xerial.snappy.tempdir=/tmp -Dorg.xerial.snappy.lib.name=libsnappyjava.jnilib $HADOOP_OPTS\u0026quot;   Stack trace: ExitCodeException exitCode=1: \u0026hellip;/launch_container.sh: line 27: $PWD:$PWD/spark.jar:$HADOOP_CONF_DIR\u0026hellip;/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar:/etc/hadoop/conf/secure:$PWD/app.jar:$PWD/*: bad substitution  The key mapreduce.application.classpath in /etc/hadoop/conf/mapred-site.xml contains a variable which is invalid in bash. From mapreduce.application.classpath remove :/usr/hdp/${hdp.version}/hadoop/lib/hadoop-lzo-0.6.0.${hdp.version}.jar from /etc/hadoop/conf/mapred-site.xml   Exception in thread \u0026ldquo;Driver\u0026rdquo; scala.MatchError: java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/TaskAttemptContext (of class java.lang.NoClassDefFoundError) at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:432) MR is not on the YARN classpath. If on HDP change from /hdp/apps/${hdp.version}/mapreduce/mapreduce.tar.gz#mr-framework to /hdp/apps/2.2.0.0-2041/mapreduce/mapreduce.tar.gz#mr-framework   java.lang.OutOfMemoryError: PermGen space with spark.master=local By default (SPARK-1879), Spark\u0026rsquo;s own launch scripts increase PermGen to 128 MB, so we need to increase PermGen in hive launch script. If use JDK7, append following in conf/hive-env.sh: export HADOOP_OPTS=\u0026quot;$HADOOP_OPTS -XX:MaxPermSize=128m\u0026quot; If use JDK8, append following in Conf/hive-env.sh: export HADOOP_OPTS=\u0026quot;$HADOOP_OPTS -XX:MaxMetaspaceSize=512m\u0026quot;    Recommended Configuration See HIVE-9153 for details on these settings.\nmapreduce.input.fileinputformat.split.maxsize=750000000 hive.vectorized.execution.enabled=true hive.cbo.enable=true hive.optimize.reducededuplication.min.reducer=4 hive.optimize.reducededuplication=true hive.orc.splits.include.file.footer=false hive.merge.mapfiles=true hive.merge.sparkfiles=false hive.merge.smallfiles.avgsize=16000000 hive.merge.size.per.task=256000000 hive.merge.orcfile.stripe.level=true hive.auto.convert.join=true hive.auto.convert.join.noconditionaltask=true hive.auto.convert.join.noconditionaltask.size=894435328 hive.optimize.bucketmapjoin.sortedmerge=false hive.map.aggr.hash.percentmemory=0.5 hive.map.aggr=true hive.optimize.sort.dynamic.partition=false hive.stats.autogather=true hive.stats.fetch.column.stats=true hive.vectorized.execution.reduce.enabled=false hive.vectorized.groupby.checkinterval=4096 hive.vectorized.groupby.flush.percent=0.1 hive.compute.query.using.stats=true hive.limit.pushdown.memory.usage=0.4 hive.optimize.index.filter=true hive.exec.reducers.bytes.per.reducer=67108864 hive.smbjoin.cache.rows=10000 hive.exec.orc.default.stripe.size=67108864 hive.fetch.task.conversion=more hive.fetch.task.conversion.threshold=1073741824 hive.fetch.task.aggr=false mapreduce.input.fileinputformat.list-status.num-threads=5 spark.kryo.referenceTracking=false spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch See Spark section of configuration page for additional properties.\nDesign documents  Hive on Spark: Overall Design from HIVE-7272 Hive on Spark: Join Design (HIVE-7613) Hive on Spark Configuration (HIVE-9449) attachments/44302539/53575687.pdf  Attachments: attachments/44302539/53575687.pdf (application/pdf)\nComments:            Spark has its own property to control whether to merge small files. Set hive.merge.sparkfiles=true to merge small files.    Posted by lirui at Jan 15, 2015 01:34 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/44302539/","tags":null,"title":"Apache Hive : Hive on Spark: Getting Started"},{"categories":null,"contents":"Apache Hive : Hive on Spark: Join Design Master  Purpose and Prerequisites MapReduce Summary  Figure 1. Join Processors for Hive on MapReduce   Tez Comparison Spark MapJoin Spark Join Design  Figure 2: Join Processors for Hive on Spark    Purpose and Prerequisites The purpose of this document is to summarize the findings of all the research of different joins and describe a unified design to attack the problem in Spark. It will identify the optimization processors will be involved and their responsibilities.\nIt is not the purpose to go in depth for design of the various join implementations in Spark, such as the common-join (HIVE-7384), or the optimized join variants like mapjoin (HIVE-7613), skew-join (HIVE-8406) or SMB mapjoin (HIVE-8202). It will be helpful to refer to the design documents attached on JIRA for those details before reading this document, as they will also contain some background of how they are implemented in MapReduce and comparisons. Lastly, it will also be helpful to read the overall Hive on Spark design doc before reading this document.\nMapReduce Summary This section summarizes plan-generation of different joins of Hive on MapReduce, which will serve as a model for Spark. We aim to support most of these join optimizations. Priority will be for the automatically-optimized joins, followed by those that need user input, such as hints and metadata.\nOver the years, there have been lots of join optimization introduced to Hive beyond the common-join, via Processors (partial transformation of operator-tree or work tree). The following diagram (Figure 1) shows the relationships of different Processors, each of which does a small part in transforming an operator-tree from common-join to one of the optimized join work-trees (mapjoin, bucket mapjoin, SMB mapjoin, or skewjoin).\nProcessors are represented by boxes in the following diagram: They are split into three types:\n Logical optimization processor (Green): Deals with pure operator-tree (no physical works). Work-generation processor (Blue): Deals with operator-tree and takes part in forming a physical work-tree. Physical optimization processor (Red): Deals with fully-formed physical work-tree.  Each processor box shows the triggering condition, either a Hive configuration property, or the presence of a certain operator in the tree. So, you can see how to trigger a particular join by following its path through processors and making sure all configurations are triggered and the given operator has been created by previous processors. There are further conditions to do the transform listed on the top, (ie, size of table, etc), that are not be explained by this document, and can be referred from documents of individual joins.\nFigure 1. Join Processors for Hive on MapReduce The input arrow at top before any Processor is always an operator-plan for common-join, which is shown as follows. In other words, this is the original join plan if none of the optimizer processors are activated.\nThe ‘exit’ arrows of the join paths shown in Figure 1 are the various optimized join variants. There are of course other exit paths, in which the plan remains unchanged as a common-join upon falling out of any processor’s pre-req checks, but they are not shown to simplify the diagram.\nDescription of the some important Processors follows. Again, there’s no space to describe them all, so few key ones are chosen.\n SortedBucketMapJoinProc: This logical optimization processor handles auto-conversion of common-join operator-tree to SMB join operator-tree, for further processing.  to MapJoinProcessor: This logical optimization processor handles initial conversion of common-join operator-tree to mapjoin operator-tree, for further-processing, when user has given hints to identify the small tables.  to MapJoinFactory: This follows MapJoinProcessor as part of work-generation phase. It handles the work-tree created from that, adding a local work to identify the small-tables. It also follows after SortedBucketMapJoinProc in the SMB MapJoin path as part of its work-generation phase in a similar way, also adding a local work to identify the small tables for SMB MapJoin.  MapJoinFactory MapJoin processing:\ntoMapJoinFactory SMB MapJoin processing:\nto  CommonJoinResolver: This handles auto-conversions of joins to mapjoins and goes a separate path than hint-based mapjoin conversions. This takes a working common-join work tree already generated from the common-join operator-tree, and creates an alternative work-tree. The new work-tree consists of a mapwork rooted at the big table. Pointers to the small table are retained in the new work via the LocalWork data structure.   to:  MapJoinResolver: Here, the two mapjoin paths (hint and non-hint mapjoins) unite again. One last step for both results is to make it ready for physical execution on MR cluster, described in detail in below section “Spark MapJoin”. The MapJoinResolver separates the single work into two works. First a local MapRedWork dealing with small tables, ending with HashTableSink writing the hashmap files. Then a MapRedWork dealing with big table, loading from small-table hashmap files via HashTableDummyOp.  to A brief summary of all the processor-paths of possible join plans shown in Figure 1:\n  Skewjoin (compile-time)\n  SkewJoinOptimizer: From a common-join operator tree, creates two join operator-trees connected by union operator. These will represent a join with skew key, and a join without it.\n  One or both reduce-side join might be converted to mapjoin by CommonJoinResolver, see auto-mapjoin for more details.\n  Skewjoin (runtime)\n  SkewJoinResolver: Create conditional work after the original common-join work, which is a list of mapjoin works. These will handle the skew keys.\n  MapJoinResolver: Final preparation for mapjoin works as described.\n  Auto-mapjoin\n  CommonJoinResolver: Convert common-join operator tree to mapjoin operator-tree, with big/small table(s) identified on the Mapjoin operator, as described.\n  MapJoinResolver: Final preparation for mapjoin works as described.\n  Map join query with hints\n  MapJoinProcessor: Convert common-join operator tree to mapjoin operator-tree, with big/small table(s) identified on the Mapjoin operator, as described.\n  MapJoinFactory: Adds localWork pointing to small tables in mapjoin work, as described.\n  MapJoinResolver: Final preparation for mapjoin works as described.\n  Bucket map join query with hints.\n  MapJoinProcessor: Convert common-join operator tree to mapjoin operator-tree, with big/small table(s) identified on the Mapjoin operator, as described.\n  BucketMapJoinProcessor: Add bucketing information to MapJoin op.\n  MapJoinFactory: Adds localWork pointing to small tables in mapjoin work, as described.\n  MapJoinResolver: Final preparation for mapjoin works as described.\n  SMB join query with hints\n  MapJoinProcessor: Convert common-join operator tree to mapjoin operator-tree, with big/small table(s) identified on the Mapjoin operator, as described.\n  SortedBucketMapJoinProc: Convert mapjoin operator-tree to SMBMapJoin operator-tree. Add DummyOp to small-tables.\n  MapJoinFactory: Adds localWork pointing to small tables in SMBMapjoin work, as described.\n  May be converted back to MapJoin (see #8 for details).\n  Auto-SMB join\n  SortedMergeBucketMapJoinProc: Convert mapjoin operator-tree to SMBMapJoin operator-tree. Add DummyOp to small-tables.\n  MapJoinFactory: Adds localWork pointing to small tables in SMBMapjoin work, as described.\n  May be converted to MapJoin (see #8 for details).\n  SMB join that converts to mapjoin\n  SMBJoin operator-tree constructed as mentioned in #6, #7 above.\n  SortedMergeJoinResolver: For each possible big-table candidate, create a mapjoin work. These will have LocalWork data structures to keep track of small-tables. Create ConditionalWork with all of these mapjoin works (with the original SMBJoin work as the backup task of each one), and the original SMBJoin work as the last option.\n  MapJoinResolver: For each mapjoin work created, final preparation as described.\n  Tez Comparison Hive on Tez is still evolving. They currently disable all logical-optimizer processors, and use a processor called “ConvertJoinMapJoin” located in the work-generation phase. It utilitzes stats annotated on the operator-tree to make some decisions as to what join to take. It will directly create plans for the following joins:\n MapJoin SMBJoin  These look different than MapReduce plans, and are based on the Tez physical feature “broadcast-edge”. See JIRA’s of those joins for more details.\nSpark MapJoin  For most of the joins for Hive on Spark, the overall execution will be similar to MR for the first cut. Thus, a similar work-tree as in MR will be generated, though encapsulated in SparkWork(s) instead of MapRedWork(s).\nOne difference is implementation of mapjoin, which is worth spending some time discussing. Recall the mapjoin work-tree in MapReduce:\n Run the MapredLocalWork containing small-table(s)’ operator-tree, ending it with a HashTableSink op that dumps to file. This is made into a distributed cache. Run the MapWork for the big table, which will populate small-table hashmap from the distributed cache file using HashTableDummy’s loader.  Spark mapjoin has a choice to take advantage of faster Spark functionality like broadcast-variable, or use something similar to distributed-cache. A discussion for choosing MR-style distributed cache is given in “small-table broadcasting” document in HIVE-7613, though broadcast-variable support might be added in future. Here is the plan that we want.\n  Run the small-table SparkWorks on Spark cluster, which dump to hashmap file (this is main difference with MR, as the small-table work is distributed). Run the SparkWork for the big table on Spark cluster. Mappers will lookup the small-table hashmap from the file using HashTableDummy’s loader.  For bucket map-join, each bucket of each small table goes to a separate file, and each mapper of big-table loads the specific bucket-file(s) of corresponding buckets for each small table.\nSpark Join Design Let’s redraw the processor diagram for Hive on Spark. There are several other points to note in this section:\n Logical optimizers are mostly re-used from Hive on MapReduce, directly or slightly modified. GenSparkWork is the first of the work-generator processors, which creates SparkWorks.  There are also some minor differences (improvements) over original MapReduce, beyond the one mentioned in Spark MapJoin section.\n Hive on Spark supports automatic bucket mapjoin, which is not supported in MapReduce. This is done in extra logic via SparkMapJoinOptimizer and SparkMapJoinResolver. Hive on Spark’s SMB to MapJoin conversion path is simplified, by directly converting to MapJoin if eligible.  Figure 2: Join Processors for Hive on Spark Again, we first explore some of the interesting processors:\n  SparkSortMergeJoinOptimizer: Like SortrtedBucketMapJoinProc, this logical optimization processor handles auto-conversion of common-join to SMB join operator-tree, for further processing.    to SparkSortMergeMapJoinFactory: This takes a MapWork with operator-tree already with a SMBMapJoin operator and big/small table(s) identified, and creates a LocalWork pointing at small tables.  to  SparkMapJoinProcessor: Like MapJoinProcessor, this logical optimization processor handles initial conversion of common-join to mapjoin, for further-processing, when user has given hints to identify the small tables. Final operator-tree is just a bit different than with MapReduce, with ReduceSinks of small table branch(es) still attached.  to  SparkMapJoinOptimizer: Similar to MapReduce’s MapJoinProcessor and Tez’s ConvertJoinMapJoin, this will transform a common join operator-tree to a mapjoin operator-tree, by identifying the big and small tables via stats. Like Tez’s processor, it removes the reduce-sinks from the big-table’s branch but keeps them for the small-tables.  to GenSparkWork/SparkReduceSinkMapJoinProc: During the work-generation phase, a combination of these processors will draw the appropriate work boundaries around the mapjoin operator tree. It also transforms ReduceSinks into HashTableSinks.  to SparkMapJoinResolver: Again, the various mapjoin paths (hint and automatic) converge at this final processor before execution on Spark cluster. This takes a single SparkWork with MapJoin operator and big/small table(s) identified, and splits it into dependent SparkWorks, with the small-table SparkWork(s) being parents of the big-table SparkWork. This will be sent to Spark cluster to run mapjoin as described in the section “Spark MapJoin”. The LocalWork data structure is created within the dependent (big-table) SparkWork to contain HashTableDummy’s which load small-table hashmap files.   to:  And the summary of each join plan’s processor path of Figure 2.\n  Compile-time skewjoin without mapjoin: Logical optimizer completely re-used from MapReduce.\n  SkewJoinOptimizer: This logical-optimizer processor is reused, to create two join plans out of one connected by union.\n  Follows auto-conversion to MapJoin path.\n  SMB MapJoin (with hints): again, logical optimizers are mostly similar to those in MapReduce.\n  SparkMapJoinProcessor/BucketMapJoinOptimizer/SparkSMBJoinHintOptimizer: Almost identical to MapReduce versions, these transform the operator tree to include SMBMapJoinOp with big/small table(s) identified.\n  GenSparkWork: Generate the SparkWork, which is an SMBMapJoin operator-tree rooted at big-table TS.\n  SparkSortMergeJoinFactory: Attach Localwork data structure pointing to small tables in the SMBMapJoin work as described.\n  SMB MapJoin (without hints): again, logical optimizers are mostly similar to those in MapReduce\n  SparkSortMergeJoinOptimizer: Almost identical to MapReduce version, this transforms the common-join operator tree to SMB mapjoin operator-tree, with big/small table(s) identified on SMBMapJoin operator, as described.\n  GenSparkWork: Generate the SparkWork, which is an SMBMapJoin operator-tree rooted at big-table TS.\n  SparkSortMergeJoinFactory: Attach Localwork data structure pointing to small tables in the SMBMapJoin work as described.\n  Auto-mapjoin: Mostly a rewrite, unable to reuse the MapReduce processors.\n  SparkMapJoinOptimizer: Based on stats, converts a common-join operator tree to mapjoin operator-tree, with big/small table(s) identified in MapJoinOp, as described.\n  GenSparkWork: Generate the SparkWork, which has MapJoin operator-trees rooted at various table TS’s.\n  SparkMapJoinResolver: Create two SparkWorks to achieve the mapjoin, as described.\n  Mapjoin via hints: again, logical optimizers are mostly similar to those in MapReduce\n  SparkMapJoinProcessor: Almost identical to MapReduce version, this transforms the common-join operator tree to mapjoin operator-tree, with big/small table(s) identified in MapJoinOp, as described.\n  GenSparkWork: Generate the SparkWork, which has MapJoin operator-trees rooted at various table TS’s.\n  SparkMapJoinResolver: Create two SparkWorks to achieve the mapjoin, as described.\n  SMB joins converted to mapjoin:\n  This route is avoided unlike in MapReduce. If conditions are met, join is directly sent to SparkMapJoinOptimizer and SparkMapJoinResolver, just like a normal auto-mapjoin.\n  Skew join (runtime):\n  SparkSkewJoinResolver: Takes a SparkWork with common join, and turn it in a conditional work. Then add additional SparkWork with mapjoin operator-tree as backups in the conditional work. These will handle the skew keys.\n  SparkMapJoinResolver: For each backup SparkWork with mapjoin, create two SparkWorks to achieve the mapjoin, as described.\n  Auto-bucket join\n  SparkMapJoinOptimizer: Extra logic here beyond auto-mapjoin conversion to support auto-bucket mapjoin conversion.\n  SparkMapJoinResolver: Extra logic here beyond auto-mapjoin conversion to support auto-bucket mapjoin conversion.\n  Attachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/50858744/","tags":null,"title":"Apache Hive : Hive on Spark: Join Design Master"},{"categories":null,"contents":"Apache Hive : Hive on Tez  Overview  Multiple reduce stages Pipelining In memory versus disk writes Joins Fine-tuned algorithms Limit processing   Scope Functional requirements of phase I  Example  Plan with TEZ Plan without TEZ     Design  Summary of changes Execution layer  Job submission Job monitoring Job diagnostics Counters Job execution   Query planning  MapRedWork Semantic analysis and logical optimizations Physical Optimizations and Task generation Local Job Runner Number of tasks Explain statements Hive variables   Build infrastructure Testing  Mini Tez Cluster     Installation and Configuration  Hive-Tez Compatibility    Overview Tez is a new application framework built on Hadoop Yarn that can execute complex directed acyclic graphs of general data processing tasks. In many ways it can be thought of as a more flexible and powerful successor of the map-reduce framework.\nIt generalizes map and reduce tasks by exposing interfaces for generic data processing tasks, which consist of a triplet of interfaces: input, output and processor. These tasks are the vertices in the execution graph. Edges (i.e.: data connections between tasks) are first class citizens in Tez and together with the input/output interfaces greatly increase the flexibility of how data is transferred between tasks.\nTez also greatly extends the possible ways of which individual tasks can be linked together; In fact any arbitrary DAG can be executed directly in Tez.\nIn Tez parlance a map-reduce job is basically a simple DAG consisting of a single map and reduce vertice connected by a “bipartite” edge (i.e.: the edge connects every map task to every reduce task). Map input and reduce outputs are HDFS inputs and outputs respectively. The map output class locally sorts and partitions the data by a certain key, while the reduce input class merge-sorts its data on the same key.\nTez also provides what basically is a map-reduce compat layer that let’s one run MR jobs on top of the new execution layer by implementing Map/Reduce concepts on the new execution framework.\nMore information about Tez can be found here:\n Tez website: http://tez.apache.org/ Tez design doc: https://issues.apache.org/jira/browse/TEZ-65 Tez presentation: http://www.youtube.com/watch?v=9ZLLzlsz7h8 Tez slides: http://www.slideshare.net/Hadoop_Summit/murhty-saha-june26255pmroom212  Hive uses map-reduce as its execution engine. Any query will produce a graph of MR jobs potentially interspersed with some local/client-side work. This leads to many inefficiencies in the planning and execution of queries. Here are some examples that can be improved by using the more flexible Tez primitives:\nMultiple reduce stages Whenever a query has multiple reduce sinks (that cannot be combined, i.e.: no correlation between the partition keys), Hive will break the plan apart and submit one MR job per sink. All of the MR jobs in this chain need to be scheduled one-by-one and each one has to re-read the output of the previous job from HDFS and shuffle it. In Tez several reduce sinks can be linked directly and data can be pipelined without the need of temporary HDFS files. This pattern is referred to as MRR (Map - reduce - reduce*).\nPipelining More than just MRR, Tez allows for sending the entire query plan at once thus enabling the framework to allocate resources more intelligently as well as pipelining data through the various stages. This is a huge improvement for more complicated queries as it eliminates IO/sync barriers and scheduling overhead between individual stages. An example would be a query that aggregates two tables in subqueries in the from clause and joins the resulting relations.\nIn memory versus disk writes Currently any shuffle is performed the same way regardless of the data size. Sorted partitions are written to disk, pulled by the reducers, merge-sorted and then fed into the reducers. Tez allows for small datasets to be handled entirely in memory, while no such optimization is available in map-reduce. Many warehousing queries sort or aggregate small datasets after the heavy lifting is done. These would benefit from an in memory shuffle.\nJoins Distributed join algorithms are difficult to express in map-reduce. A regular shuffle join for instance has to process different inputs in the same map task, use tags to be written to disk for distinguishing tables, use secondary sort to get the rows from each table in a predictable order, etc. Tez is a much more natural platform to implement these algorithms.\nFor example: It is possible to have one Tez task take multiple bipartite edges as input thus exposing the input relations directly to the join implementation. The case where multiple tasks feed into the same shuffle join task will be referred to as multi-parent shuffle join.\nFine-tuned algorithms All sorting in map-reduce happens using the same binary sort, regardless of the data type. Hive might for instance choose to use a more effective integer-only sort when possible. Tez makes that available.\nSince Hive uses map-reduce to compute aggregations, processing will always boil down to a sort-merge even though we’re not actually interested in the sort order. Tez will allow for more efficient hash-based algorithms to do the same.\nLimit processing Tez allows complete control over the processing, including being able to stop processing when limits are met (without simply skipping records or relying on file formats/input formats.) It’s also possible to define specific edge semantics, which could be used to provide a generic top-k edge to simplify “limit” processing.\nScope The rest of this document describes the first phase of Hive/Tez integration. The goals are:\n Bring Tez concepts and primitives into Hive, make them available to all Hive developers Take advantage of TEZ through MRR (Multiple reduce-stage jobs) Take advantage of TEZ through MPJ (multi-parent shuffle joins)  Limiting the integration to the fairly simple MRR/MPJ pattern will require minimal changes to the planner and execution framework while speeding up a wide variety of queries. At the same time it will allow us to build a solid foundation for future improvements.\nFunctional requirements of phase I  Hive continues to work as is on clusters that do not have TEZ.  MR revisions 20, 20S, 23 continue to work unchanged.   Hive can optionally submit MR jobs to TEZ without any additional improvements.  Hive can treat TEZ like just another Hadoop 23 instance.   Hive can optionally detect chains of MR jobs and optimize them to a single DAG of the form MR* and submit it to TEZ. Hive can optionally detect when a join has multiple parent tasks and combine them into a single DAG of a tree shape. Hive will display the MRR optimization in explain plans. Hive will give appropriate feedback to the user about progress and completion status of the query when running MRR queries. The user will be able to get statistics and diagnostic information as before (counters, logs, debug info on the console). Hive has unit tests to cover all new functionality.  The following things are out of scope for the first phase:\n Local tasks will still run as MR only. Only Map and Reduce Tez tasks with SimpleEdges will be used (i.e.: no new tasks, new input/output/processors, no new edge types). No multi-output task optimizations will be introduced.  One new configuration variable will be introduced:\n hive.optimize.tez hive.execution.engine (changed in HIVE-6103)  True tez: Submit native TEZ dags, optimized for MRR/MPJ False mr (default): Submit single map, single reduce plans   Update: Several configuration variables were introduced in Hive 0.13.0. See the Tez section in Configuration Properties.  Note: It is possible to execute an MR plan against TEZ. In order to do so, one simply has to change the following variable (assuming Tez is installed on the cluster):\n mapreduce.framework.name = yarn-tez  Example Here’s a TPC-DS query and plans with and without Tez optimizations enabled:\nThe query (rewritten for Hive):\nselect i_item_desc ,i_category ,i_class ,i_current_price ,i_item_id ,itemrevenue ,itemrevenue*100/sum(itemrevenue) over (partition by i_class) as revenueratio from (select i_item_desc ,i_category ,i_class ,i_current_price ,i_item_id ,sum(ws_ext_sales_price) as itemrevenue from web_sales join item on (web_sales.ws_item_sk = item.i_item_sk) join date_dim on (web_sales.ws_sold_date_sk = date_dim.d_date_sk) where i_category in ('1', '2', '3') and year(d_date) = 2001 and month(d_date) = 10 group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price) tmp order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratio; Plan with TEZ Stage 0:\nLocal Work: Generate hash table for date dim\nStage 1:\nMap: SMB join item + web_sales, mapjoin date_dim + web_sales, map-side group by/aggregate\nReduce 1: Reduce side group by/aggregate, shuffle for windowing\nReduce 2: Compute windowing function, shuffle for order by\nReduce 3: Order by, write to HDFS\nPlan without TEZ Local Work: Generate hash table for date dim\nStage 1:\nMap: SMB join item + web_sales, mapjoin date_dim + web_sales, map-side group by/aggregate\nReduce: Reduce side group by/aggregate, write to HDFS\nStage 2:\nMap: Read tmp file, shuffle for windowing\nReduce: Compute windowing function, write to HDFS\nStage 3:\nMap: Read tmp file, shuffle for order by\nReduce: Order by, write to HDFS\nDesign Summary of changes Changes that impact current Hive code paths:\n Split MR compilation from SemanticAnalyzer (simple) Break MapRedWork into MapWork and ReduceWork (straight forward but a number of changes) Move execution specific classes from exec to exec.mr package (simple) Move some functions from ExecDriver to exec.Utilities to share between Tez and MR Build system: Add Tez dependencies and Tez testing  I believe that all of these are valuable by themselves and make the code cleaner and easier to maintain. Especially the second item will touch quite a few places in the code though. None of them change functionality.\nNew code paths (only active when running Tez):\n Execution: TezWork, TezTask, TezJobMonitor (small) Planning: Compiler to generate TezTasks, Perform physical optimizations on Tez (large)  The following outlines the changes across the various Hive components:\nExecution layer We’ve initially investigated to add Tez as a simple shim option to the code base. This didn’t work out mostly because Tez’ API is very different from the MR api. It does not make much sense to move the entire “execute” infrastructure to the shim layer. That would require large code changes with little benefit. Instead there will be separate “Task” implementations for MR and TEZ and Hive will decide at runtime which implementation to use.\nWe’re planning to have two packages:\n org.apache.hadoop.hive.ql.exec.mr org.apache.hadoop.hive.ql.exec.tez  Both will contain implementations of the Task interface, which is used to encapsulate units of work to be scheduled and executed by the Driver class.\nBoth of these packages will have classes for job monitoring and job diagnostics, although they are package private and do not follow a common interface.\nJob submission Currently ExecDriver and MapRedTask (both are of type “Task”) will submit map-reduce work via JobClient (either via local-job runner or against the cluster). All MR specific job submission concepts are hidden behind these classes.\nWe will add a TezTask as the entry point for Tez execution. TezTask will hide building of the job DAG and orchestrate monitoring and diagnostics of DAG execution.\nHive’s driver will still deal with a graph of Tasks to handle execution. No changes are required to handle this. The only difference is that now the planner might transparently add TezTasks to the mix at runtime.\nJob monitoring We will add a TezJobMonitor class that handles printing of status as well as reporting the final result. This class provides similar functions than HadoopJobExecHelper used for MR processing. TezJobMonitor will also retrieve and print the top level exception thrown at execution time.\nJob diagnostics Basic ‘job succeeded/failed’ as well as progress will be as discussed in “Job monitoring”. Hive’s current way of trying to fetch additional information about failed jobs will not be available in phase I.\nCurrently Tez offers limited debugging support once a job is complete. The only way to get access to detailed logs, counters, etc is to look at the log of the AM, find the appropriate url for specific task logs and access them through copy and paste. This will change over time and historical logging information will be accessible, but for the first phase debugging support will be limited.\nCounters API for retrieving counters will be different in Tez and we will thus add a shim api for that. Incrementing counters at execution time will work unchanged.\nJob execution The basic execution flow will remain unchanged in phase I. ExecMapper/ExecReducer will be used through Tez’ MR compat layer. The operator plan + settings will be communicated via ‘scratch dir’ as before. ExecMapper/Reducer will load and configure themselves accordingly.\nQuery planning MapRedWork MapRedWork is where we currently record all information about the MR job during compile/optimize time.\nThis class will be refactored to capture Map information and reduce information separately (in MapWork and ReduceWork). This requires quite a few - albeit straight-forward - changes to both planning and execution.\nThe refactor has benefits for pure MR execution as well. It removes the need for Mappers/Reducers to load and de-serialize information they don’t use and it makes it easier to read and maintain the code, because it clearly delineates what information is used at what stage.\nMapWork and ReduceWork will be shared by both MapRedWork and TezWork. MapRedWork is basically a composition of 1M + 0-1R, while TezWork is a tree of Map/ReduceWork with MapWork classes as leaves only.\nAs discussed above, TezTask will use TezWork, while MapRedTask and ExecDriver will use MapReduceWork.\nSemantic analysis and logical optimizations Neither semantic analyzer nor any logical optimizations will change. Physical optimizations and MR plan generation are currently also done in the SemanticAnalyzer and will be moved out to separate classes.\nPhysical Optimizations and Task generation The MapReduceCompiler does two things at the same time right now. After breaking down the operator plan into map-reduce tasks, it optimizes the number of tasks and also performs physical optimizations (picks join implementations, total order sort, etc).\nIn order to limit the impact of Tez, we will provide a separate implementation: TezCompiler. The Tez compiler will attempt to perform most physical optimizations at the plan level, leaving the breakdown of the plan into Tez jobs for a second round of optimizations.\nLater we may decide to use that physical optimizer (at the plan level) for both MR and Tez, while leaving specific optimizations in the two layers.\nLocal Job Runner In the short term Tez will not support a “LocalTezDagRunner”. That means that Hive will always have to submit MR jobs when executing locally. In order to avoid replanning the query after execution has started in Tez mode some optimizations for converting stages to local jobs will not be available.\nNumber of tasks Some MR jobs have a predetermined number of reducers. This happens for order by (numReducers = 1) and scenarios where bucketing is used (numReducers = numBuckets). The user can also set the number of reducers manually. The same numbers will be used for each reduce tasks. Initially there will be no way for the user to set different numbers of reducers for each of the separate reduce stages. There is already a ticket (HIVE-3946) to address this shortcoming which can be used for both Tez and MR.\nIn most cases Hive will determine the number of reducers by looking at the input size of a particular MR job. Hive will then guess the correct number of reducers. The same guess will be used for subsequent reduce phases in a Tez plan. Ultimately, this number will have to be determined using statistics which is out of scope, but applies equally to MR and Tez.\nExplain statements Explain statements are driven (in part) off of fields in the MapReduceWork. Part of extending/refactoring MapReduceWork will be to add sufficient information to print the correct operator trees in explain for Tez.\nHive variables The “set” mechanism for Hive variables will not change. The variables will be passed through to the execution engine as before. However, Hive will not shim or map any mapreduce variables. If a variable is not supported in Hive it will be silently ignored.\nBuild infrastructure There will be a new “ql” dependency on Tez. The jars will be handled the same way Hadoop jars are handled, i.e.: They will be used during compile, but not included in the final distribution. Rather we will depend on them being installed separately. The jars will only have to be present to run Tez jobs, they are not needed for regular MR execution.\nTesting Mini Tez Cluster Mini Tez Cluster will initially be the only way to run Tez during unit tests. LocalRunner is not yet available. If mr.rev is set to tez all MiniMr tests will run against Tez.\nInstallation and Configuration For information about how to set up Tez on a Hadoop 2 cluster, see https://github.com/apache/incubator-tez/blob/branch-0.2.0/INSTALL.txt. For information about how to configure Hive 0.13.0+ for Tez, see the release notes for HIVE-6098, Merge Tez branch into trunk. Also see Configuration Properties: Tez for descriptions of all the Tez parameters.\nHive-Tez Compatibility For a list of Hive and Tez releases that are compatible with each other, see Hive-Tez Compatibility.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-on-tez_33296197/","tags":null,"title":"Apache Hive : Hive on Tez"},{"categories":null,"contents":"Apache Hive : Hive Operators Operators Precedences    Example Operators Description     A[B] , A.identifier bracket_op([]), dot(.) element selector, dot   -A unary(+), unary(-), unary(~) unary prefix operators   A IS [NOT] (NULL TRUE FALSE)   A ^ B bitwise xor(^) bitwise xor   A * B star(*), divide(/), mod(%), div(DIV) multiplicative operators   A + B plus(+), minus(-) additive operators   A  B   A \u0026amp; B bitwise and(\u0026amp;) bitwise and   A B bitwise or(    Relational Operators The following operators compare the passed operands and generate a TRUE or FALSE value depending on whether the comparison between the operands holds.\n   Operator Operand types Description     A = B All primitive types TRUE if expression A is equal to expression B otherwise FALSE.   A == B All primitive types Synonym for the = operator.   A \u0026lt;=\u0026gt; B All primitive types Returns same result with EQUAL(=) operator for non-null operands, but returns TRUE if both are NULL, FALSE if one of them is NULL.    A \u0026lt;\u0026gt; B All primitive types NULL if A or B is NULL, TRUE if expression A is NOT equal to expression B, otherwise FALSE.   A != B All primitive types Synonym for the \u0026lt;\u0026gt; operator.   A \u0026lt; B All primitive types NULL if A or B is NULL, TRUE if expression A is less than expression B, otherwise FALSE.   A \u0026lt;= B All primitive types NULL if A or B is NULL, TRUE if expression A is less than or equal to expression B, otherwise FALSE.   A \u0026gt; B All primitive types NULL if A or B is NULL, TRUE if expression A is greater than expression B, otherwise FALSE.   A \u0026gt;= B All primitive types NULL if A or B is NULL, TRUE if expression A is greater than or equal to expression B, otherwise FALSE.   A [NOT] BETWEEN B AND C All primitive types NULL if A, B, or C is NULL, TRUE if A is greater than or equal to B AND A less than or equal to C, otherwise FALSE. This can be inverted by using the NOT keyword.    A IS NULL All types TRUE if expression A evaluates to NULL, otherwise FALSE.   A IS NOT NULL All types FALSE if expression A evaluates to NULL, otherwise TRUE.   A IS [NOT] (TRUE FALSE) Boolean types   A [NOT] LIKE B strings NULL if A or B is NULL, TRUE if string A matches the SQL simple regular expression B, otherwise FALSE. The comparison is done character by character. The _ character in B matches any character in A (similar to . in POSIX regular expressions) while the % character in B matches an arbitrary number of characters in A (similar to .* in posix regular expressions). For example, \u0026lsquo;foobar\u0026rsquo; like \u0026lsquo;foo\u0026rsquo; evaluates to FALSE whereas \u0026lsquo;foobar\u0026rsquo; like \u0026lsquo;foo_ _ _\u0026rsquo; evaluates to TRUE and so does \u0026lsquo;foobar\u0026rsquo; like \u0026lsquo;foo%\u0026rsquo;.   A RLIKE B strings NULL if A or B is NULL, TRUE if any (possibly empty) substring of A matches the Java regular expression B, otherwise FALSE. For example, \u0026lsquo;foobar\u0026rsquo; RLIKE \u0026lsquo;foo\u0026rsquo; evaluates to TRUE and so does \u0026lsquo;foobar\u0026rsquo; RLIKE \u0026lsquo;^f.*r$\u0026rsquo;.   A REGEXP B strings Same as RLIKE.    Arithmetic Operators The following operators support various common arithmetic operations on the operands. All return number types; if any of the operands are NULL, then the result is also NULL.\n   Operator Operand types Description     A + B All number types Gives the result of adding A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands. For example, since every integer is a float, therefore float is a containing type of integer so the + operator on a float and an int will result in a float.   A - B All number types Gives the result of subtracting B from A. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A * B All number types Gives the result of multiplying A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands. Note that if the multiplication causes overflow, you will have to cast one of the operators to a type higher in the type hierarchy.   A / B All number types Gives the result of dividing A by B. The result is a double type in most cases. When A and B are both integers, the result is a double type except when the hive.compat configuration parameter is set to \u0026ldquo;0.13\u0026rdquo; or \u0026ldquo;latest\u0026rdquo; in which case the result is a decimal type.   A DIV B Integer types Gives the integer part resulting from dividing A by B. E.g 17 div 3 results in 5.   A % B All number types Gives the remainder resulting from dividing A by B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A \u0026amp; B All number types Gives the result of bitwise AND of A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A B All number types   A ^ B All number types Gives the result of bitwise XOR of A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   ~A All number types Gives the result of bitwise NOT of A. The type of the result is the same as the type of A.    Logical Operators The following operators provide support for creating logical expressions. All of them return a boolean TRUE, FALSE, or NULL depending upon the boolean values of the operands. NULL behaves as an \u0026ldquo;unknown\u0026rdquo; flag, so if the result depends on the state of an unknown, the result itself is unknown.\n   Operator Operand types Description     A AND B boolean TRUE if both A and B are TRUE, otherwise FALSE. NULL if A or B is NULL.   A OR B boolean TRUE if either A or B or both are TRUE, FALSE OR NULL is NULL, otherwise FALSE.   NOT A boolean TRUE if A is FALSE or NULL if A is NULL. Otherwise FALSE.   ! A boolean Same as NOT A.   A IN (val1, val2, \u0026hellip;) boolean TRUE if A is equal to any of the values.   A NOT IN (val1, val2, \u0026hellip;) boolean TRUE if A is not equal to any of the values.    [NOT] EXISTS (subquery) boolean TRUE if the subquery returns at least one row.    String Operator    Operator Operand types Description     A  B    Complex Type Constructors The following functions construct instances of complex types.\n   Constructor Function Operands Description     map (key1, value1, key2, value2, \u0026hellip;) Creates a map with the given key/value pairs.   struct (val1, val2, val3, \u0026hellip;) Creates a struct with the given field values. Struct field names will be col1, col2, etc   named_struct (name1, val1, name2, val2, \u0026hellip;) Creates a struct with the given field names and values.   array (val1, val2, \u0026hellip;) Creates an array with the given elements.   create_union (tag, val1, val2, \u0026hellip;) Creates a union type with the value that is being pointed to by the tag parameter.    Operators on Complex Types The following operators provide mechanisms to access elements in Complex Types.\n   Operator Operand types Description     A[n] A is an Array and n is an int Returns the nth element in the array A. The first element has index 0. For example, if A is an array comprising of [\u0026lsquo;foo\u0026rsquo;, \u0026lsquo;bar\u0026rsquo;] then A[0] returns \u0026lsquo;foo\u0026rsquo; and A[1] returns \u0026lsquo;bar\u0026rsquo;.   M[key] M is a Map\u0026lt;K, V\u0026gt; and key has type K Returns the value corresponding to the key in the map. For example, if M is a map comprising of {\u0026lsquo;f\u0026rsquo; -\u0026gt; \u0026lsquo;foo\u0026rsquo;, \u0026lsquo;b\u0026rsquo; -\u0026gt; \u0026lsquo;bar\u0026rsquo;, \u0026lsquo;all\u0026rsquo; -\u0026gt; \u0026lsquo;foobar\u0026rsquo;} then M[\u0026lsquo;all\u0026rsquo;] returns \u0026lsquo;foobar\u0026rsquo;.   S.x S is a struct Returns the x field of S. For example for the struct foobar {int foo, int bar}, foobar.foo returns the integer stored in the foo field of the struct.    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-operators_283118406/","tags":null,"title":"Apache Hive : Hive Operators"},{"categories":null,"contents":"Apache Hive : Hive remote databases/tables Abstract At the 2018 DataWorks conference in Berlin, Hotels.com presented Waggle Dance, a tool for federating multiple Hive clusters and providing the illusion of a unified data catalog from disparate instances. We’ve been running Waggle Dance in production for well over a year and it has formed a critical part of our data platform architecture and infrastructure.\nWe believe that this type of functionality will be of increasing importance as Hadoop and Hive workloads migrate to the cloud. While Waggle Dance is one solution, significant benefits could be realized if these kinds of abilities were an integral part of the Hive platform.\nThis proposal outlines why such a feature is needed in Hive, the benefits gained by offering it as a built-in feature, and representation of a possible implementation. Our proposed implementation draws inspiration from the remote table features present in some traditional RDBMSes, which may already be familiar to you.\nThe cloud, a rich source of new architectures Waggle Dance was created to overcome the problems created by architectural patterns that arose in cloud deployments of Hive. Unlike the monolithic deployment model typical of on premises deployments, in the cloud users are able to trivially create multiple cluster instances, either by design or unintentionally through organic growth. This ability to create new clusters in the cloud provides new levels of flexibility and agility to organizations. They are no longer bound to inelastic resource pools, specific platform versions, and lengthy upgrade cycles. Different parts of an organization are free to technologically innovate at their own pace.\nHowever, Hive is designed around the monolithic cluster architecture and provides no means to enable cross cluster access of datasets. One Hive cluster cannot concurrently access local datasets and those in another cluster. This results in a set of data silos in the cloud, inhibiting data exploration, discovery and sharing and ultimately limiting an organization\u0026rsquo;s ability to realize the full potential of their datasets.\nHow Waggle Dance works Waggle Dance provides a federated view of these disparately located datasets, allowing users in one cluster to explore and access datasets in multiple other clusters. Waggle Dance operates as a request routing metastore proxy service. It implements the Hive metastore Thrift API, so that as far as a workload (ETL, Query, Analytics tool) is concerned, it is communicating directly with a Hive metastore service instance. Waggle Dance connects to multiple metastore services, located in otherwise unconnected clusters. It routes and transforms metadata requests from the workload, to the appropriate metastore using database identifiers encoded in the payloads. Finally it returns the responses from the different metastores to the workload. These responses typically contain data file paths that the workload then uses to consume data. Typically these are located in a region wide object store such as S3, and hence the underlying dataset data can be read fairly seamlessly between clusters.\nProblems with Waggle Dance We’ve been successfully using Waggle Dance in production to federate many Hive instances, often bridging across different AWS accounts in the same region. It has enabled the sharing of large datasets across different organizational units and has restored the ability to easily explore, discover and share datasets as we once did on our on-premises organisation-wide Hive cluster. Our user groups include Data scientists, analysts, and engineers who are interacting with the platform using a diverse range of tools including Hive, Spark, Cascading, HS2+JDBC (+ many clients), Qubole, and Databricks.\nWhile we are very happy with the functionality that Waggle Dance provides, there are some drawbacks:\nAdditional infrastructure requirements To deliver Waggle Dance as a highly available service we must operate multiple additional load balanced virtual servers in a cross availability zone arrangement. These servers have a similar memory footprint to our HMS instances as they are required to serialize and deserialize Thrift objects passing between workloads and federated metastore instances. This Waggle Dance infrastructure creates an additional operational burden on our data platform.\nThrift API limits integration opportunities The HMS Thrift API has proved to be a very convenient integration and interception point for Waggle Dance. However, we are seeing a trends and possible needs for alternative means of integrating data catalogue services with Hive.\n Amazon’s Glue catalog service uses a custom HiveMetaStoreClientFactory implementation to deliver metadata to Hive and other workloads, side-stepping any need for Thrift API compatibility. Currently this precludes us from federating Glue catalogs into our unified catalog. Additionally, the recently announced iceberg table format from Netflix provides many metastore-like attributes in a highly scalable design, yet requires minimal metadata serving infrastructure other than S3. It’s not hard to imagine that this data format could also be integrated with Hive, partly with a HiveMetaStoreClientFactory implementation. What is clear, is that a Thrift API is redundant in this case and so could not be used as a federation endpoint for Waggle Dance. While the HMS Thrift API has provided excellent backward compatibility across releases, it has not been perfect in all cases. With some new Hive releases, we’ve had to implement work arounds in Waggle Dance so that we can support federation between different versions of Hive. We anticipate that Waggle Dance will be shown to be quite brittle in the event of any significant breaking changes in the HMS Thrift API, and will thus need to adopt a different approach or redesign.  Federation management is externalised Waggle Dance is a service separate from HMS, any configuration relating to the metastores and databases we wish to expose in a federated view must be managed separately via YAML config files that are loaded by Waggle Dance on start up. This creates an administration overhead, and prevents Hive users from creating and managing federated access to the datasets that they require.\nName overloads As our multiple cloud-based Hive clusters are independent, they have no shared global schema namespace. Consequently it is possible for database and/or table name overloads to occur when federating multiple metastores; some may well include database or table names that are already in use in one or more of the other metastores in the federation. Waggle Dance handles this by either applying a prefix to remote database names, or by allowing administrators to carefully curate a list of databases that should be exposed locally, so that overloads can at least be excluded. However, both approaches have drawbacks. Prefixing creates localized database names, and consequently encourages the creation of non-portable Hive SQL scripts. The curation of imported databases is an operational burden, and is not useful when one wants to access two databases with the same name in a unified view.\nFederation with remote databases and tables While Waggle Dance is working well for us, its design was highly influenced by our need to quickly prove a concept and unblock groups of users. Therefore we intentionally avoided any design that might require changes to HMS and by implication, extended release cycles. However, in the data warehousing world there are already good examples of features that solve these kind of federation based problems. These features are often tightly integrated into the data warehousing platform, giving users a seamless experience. For example, the Oracle platform includes features such as Database links, and Remote tables that allow catalog entities to be projected and accessed from a remote database server into a local instance.\nAn implementation of \u0026lsquo;remotes\u0026rsquo; in Hive We therefore propose that the concept of remotes be added to Hive. Practically this would encapsulate and deliver the proven functionality and utility of Waggle Dance while simultaneously overcoming the deficiencies in the Waggle Dance design. Before exploring the full scope of this idea, let\u0026rsquo;s consider the anatomy of the most typical use case from a user\u0026rsquo;s perspective; creating a link to a table in a remote cluster to enable local access:\n CREATE REMOTE TABLE local_db.local_tbl CONNECTED TO remote_db.remote_tbl VIA 'org.apache.hadoop.hive.metastore.ThriftHiveMetastoreClientFactory' WITH TBLPROPERTIES ( 'hive.metastore.uris' = 'thrift://remote-hms:9083' ); Once created the user can expect to access the table remote_db.remote_tbl, located in a remote Hive cluster, as if it were a cluster local entity, using the synonym local_db.local_tbl.\n Firstly notice that this can be an entirely user-driven process, managed from within a Hive client. If desired, the creation of remotes could possibly be authorized by appropriate GRANTs. The user is free to choose the local synonym and target database for the table. This provides a mechanism to re-map names to avoid issues of overloads, or to make names \u0026lsquo;contextually relevant\u0026rsquo; to the domain of the local cluster. However, an option would be provided to simply adopt the remote names when no re-mapping is desired, yielding a simpler CREATE REMOTE TABLE (CRT) statement. The remote table that will be exposed locally is declared via a CONNECTED TO stanza. The full-qualified table name should exist in the target remote Hive cluster. The statement includes a VIA stanza that describes the client implementation that will be used to connect to the remote metadata catalog. This is the fully qualified name of a class that implements org.apache.hadoop.hive.ql.metadata.HiveMetaStoreClientFactory that, along with its dependencies, must be on the classpath. This provides far greater scope for integrations with other metadata catalogs such as AWS Glue and Netflix iceberg, and also provides a migration path for HMS endpoints based around protocols other than Thrift. It also allows the inclusion of additional transport features with appropriate implementations. For example, Waggle Dance provides the option of multi-hop SSH tunneling for HMS Thrift transports, a useful feature when federating data catalogs across accounts and networks. The aforementioned client factories can be configured using the table properties declared in this remote table definition. In this instance they are used to declare the hive.metastore.uris of the HMS in which the remote table resides. However, one can imagine that different client implementations might support a variety of specific properties. In our SSH tunneling example, we might for example wish to declare the tunnel configuration.  Once the remote table has been created the user is free to interact with it as they would any other table (we assume read only at this time). As we demonstrated with Waggle Dance, users are able to efficiently and transparently select, join, and describe tables from multiple distributed clusters in a single local unified view.\nThe HiveMetaStoreClientFactory abstraction The proposed implementation leans heavily on the org.apache.hadoop.hive.ql.metadata.HiveMetaStoreClientFactory and HiveMetaStoreClient abstractions for delivering different metadata catalog client implementations. At this time this appears to be a construct introduced by Amazon into their EMR platform for the purposes of integrating with their AWS Glue data catalog. This is achieved by specifying the relevant class name under the hive-site.xml key hive.metastore.client.factory.class. It is unclear whether this abstraction, and associated changes will be contributed back to the Apache Hive project. However, irrespective of Amazon\u0026rsquo;s intentions, this or something similar will need to be introduced to allow the integration of different catalog connectors.\nOther use cases Our first example dealt with the simple federating of a single table from one remote metastore. The following examples aim to describe the greater scope of the implementation.\nRemote databases Waggle Dance actually federates databases, and hence sets of tables. We could achieve a similar feat with a CREATE REMOTE DATABASE (CRD) statement. This would expose all tables in the remote database to the local Hive cluster\n CREATE REMOTE DATABASE local_db_name CONNECTED TO remote_db_name VIA 'org.apache.hadoop.hive.metastore.ThriftHiveMetastoreClientFactory' WITH DBPROPERTIES ( 'hive.metastore.uris' = 'thrift://remote-hms:9083' ); Statement defaults The CRT and CRD statements can be simplified if we assume some sensible defaults. Here we assume that if a VIA stanza is not supplied, we\u0026rsquo;ll default to the HMS Thrift implementation. If the CONNECT TO stanza is omitted, the remote database name is assumed to be equal to user supplied local name:\n CREATE REMOTE DATABASE db_name WITH DBPROPERTIES ( 'hive.metastore.uris' = 'thrift://remote-hms:9083' ); Now, for a remote table we can also derive the local database name from the user\u0026rsquo;s currently selected database, and expect that the remote table name is equal to the user supplied local name:\nCREATE REMOTE TABLE tbl_name WITH TBLPROPERTIES ( 'hive.metastore.uris' = 'thrift://remote-hms:9083' ); SSH Tunneling and bastion hosts With a suitable connector, remotes could be configured to use a SSH tunnel to access a remote Hive metastore in cases where certain network restrictions prevent a direct connection from the local cluster to the machine running the Thrift Hive metastore service. A SSH tunnel consists of one or more hops or jump-boxes. The connection between each pair of nodes requires a user and a private key to establish the SSH connection.\n CREATE REMOTE TABLE tbl_name VIA 'org.apache.hadoop.hive.metastore.SSHThriftHiveMetastoreClientFactory' WITH TBLPROPERTIES ( 'hive.metastore.uris' = 'thrift://metastore.domain:9083' 'ssh.tunnel.route' = 'bastionuser@bastion-host.domain -\u0026gt; user@cluster-node.domain' 'ssh.tunnel.private.keys' = '/home/user/.ssh/bastionuser-key-pair.pem,/home/user/.ssh/user-key-pair.pem' 'ssh.tunnel.known.hosts' = '/home/user/.ssh/known_hosts' ); Non-Thrift catalog integrations Using different HiveMetastoreClientFactory we can import database and table entities for other catalog implementations, or HMS endpoints that use alternative protocols such as REST or GRPC. Consider these illustrative examples:\nAWS Glue  CREATE REMOTE TABLE tbl_name VIA 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory' WITH TBLPROPERTIES ( -- Glue endpoint configuration ); Netflix iceberg  CREATE REMOTE TABLE tbl_name VIA 'xxx.yyy.iceberg.hive.zzz.IcebergTableHiveClientFactory' WITH TBLPROPERTIES ( 'iceberg.table.path' = 'an-atomic-store:/tables/tbl_name' ) Behaviour of DESCRIBE and SHOW operations On executing DESCRIBE operations on remote tables and databases, we envisage that the user be returned the description from the remote catalog to which the remote configuration is appended.\nSummary The cloud provides a very different environment for the provisioning of Hive clusters. Clusters may be created in minutes by ordinary users, compared with the months that might be required of an operations team with a physical cluster. Therefore it is not uncommon, and should be expected, that a single organization might become dependent on multiple clusters. With Hive\u0026rsquo;s current monolithic architecture, these form data silos which in turn create a barrier for data discovery, exploration, and sharing - decreasing the potential value that organizations can derive from their datasets. With Waggle Dance, we have shown that cross-Hive-cluster federation is both a possible and effective solution to this problem. However, significant benefits could be gained if such a feature were a core offering of the Hive platform.\nThe proposed solution of Hive \u0026lsquo;remotes\u0026rsquo; offers feature parity with Waggle Dance while overcoming its deficiencies. It also offers opportunities to integrate with more recent HMS alternatives in a complimentary manner.\nFAQ Why not replicate tables between clusters instead? We could instead replicate tables between our siloed clusters in the cloud. However there are some significant drawbacks. Replication creates large operational overheads. Processes must be deployed, configured, and managed. Greater storage costs are also incurred for each new replica. Finally, one needs to ensure all replicas are in sync, and detect and communicate information concerning any divergent tables.\nHow is authentication applied? Waggle Dance has rudimentary support for the propagation of identity to remote federated metastores. It does this by simply passing along the current UGI principal in the Thrift requests. Clearly, this principal needs to be known in the target HMS. The proposed remoting feature could provide similar functionality. At this time Waggle Dance does not support Kerberos. However, we see no technical reason why this could not be implemented both in Waggle Dance and the remote tables feature.\nHow is authorization applied? Currently, the only comprehensive authorization scheme that Hive offers is applied in the HiveServer2 component, and not the HMS. Additionally, HMS resident authorization schemes (file based) do not function on the file stores offered by the cloud providers targeted by the remotes feature. Therefore, authorization appears to be a client issue and is not in the scope of this proposal.\nWhat access patterns are supported? Waggle Dance is primarily used for read only access of tables in remote Hive clusters, and full control of tables in the local cluster. This proposal assumes a similar set of constraints. However, one can imagine that it might be useful to write to AWS Glue, or Netflix iceberg tables for the purposes of incremental migration to those platforms.\nWill it work with ACID tables? Remote tables should work in the context of read only access. To read ACID, one needs only the ValidTxnList from the remote metastore and access to the set of base and delta files. Writing of remote ACID tables does not seem practical as there is no global transaction manager in this architecture. Note that at this time ACID does not function reliably on S3, although this capability has been promised.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/80452092/","tags":null,"title":"Apache Hive : Hive remote databases/tables"},{"categories":null,"contents":"Apache Hive : Hive Schema Tool  Metastore Schema Verification The Hive Schema Tool  The schematool Command Usage Examples    Metastore Schema Verification Version\nIntroduced in Hive 0.12.0. See HIVE-3764.\nHive now records the schema version in the metastore database and verifies that the metastore schema version is compatible with Hive binaries that are going to accesss the metastore. Note that the Hive properties to implicitly create or alter the existing schema are disabled by default. Hive will not attempt to change the metastore schema implicitly. When you execute a Hive query against an old schema, it will fail to access the metastore:\n$ build/dist/bin/hive -e \u0026quot;show tables\u0026quot; FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient The log will contain an error about version information not found:\n... Caused by: MetaException(message:Version information not found in metastore. ) ... By default the configuration property hive.metastore.schema.verification is false and metastore to implicitly write the schema version if it\u0026rsquo;s not matching. To enable the strict schema verification, you need to set this property to true in hive-site.xml.\nSee Hive Metastore Administration for general information about the metastore.\nThe Hive Schema Tool Version\nIntroduced in Hive 0.12.0. See HIVE-5301. (Also see HIVE-5449 for a bug fix.)\nThe Hive distribution now includes an offline tool for Hive metastore schema manipulation. This tool can be used to initialize the metastore schema for the current Hive version. It can also handle upgrading the schema from an older version to current. It tries to find the current schema from the metastore if it is available. This will be applicable to future upgrades like 0.12.0 to 0.13.0. In case of upgrades from older releases like 0.7.0 or 0.10.0, you can specify the schema version of the existing metastore as a command line option to the tool.\nThe schematool figures out the SQL scripts required to initialize or upgrade the schema and then executes those scripts against the backend database. The metastore DB connection information like JDBC URL, JDBC driver and DB credentials are extracted from the Hive configuration. You can provide alternate DB credentials if needed.\nThe schematool Command The schematool command invokes the Hive schema tool with these options:\n$ schematool -help usage: schemaTool -dbType \u0026lt;databaseType\u0026gt; Metastore database type -driver \u0026lt;driver\u0026gt; Driver name for connection -dryRun List SQL scripts (no execute) -help Print this message -info Show config and schema details -initSchema Schema initialization -initSchemaTo \u0026lt;initTo\u0026gt; Schema initialization to a version -metaDbType \u0026lt;metaDatabaseType\u0026gt; Used only if upgrading the system catalog for hive -passWord \u0026lt;password\u0026gt; Override config file password -upgradeSchema Schema upgrade -upgradeSchemaFrom \u0026lt;upgradeFrom\u0026gt; Schema upgrade from a version -url \u0026lt;url\u0026gt; Connection url to the database -userName \u0026lt;user\u0026gt; Override config file user name -verbose Only print SQL statements (Additional catalog related options added in Hive 3.0.0 (HIVE-19135] release are below. -createCatalog \u0026lt;catalog\u0026gt; Create catalog with given name -catalogLocation \u0026lt;location\u0026gt; Location of new catalog, required when adding a catalog -catalogDescription \u0026lt;description\u0026gt; Description of new catalog -ifNotExists If passed then it is not an error to create an existing catalog -moveDatabase \u0026lt;database\u0026gt; Move a database between catalogs. All tables under it would still be under it as part of new catalog. Argument is the database name. Requires --fromCatalog and --toCatalog parameters as well -moveTable \u0026lt;table\u0026gt; Move a table to a different database. Argument is the table name. Requires --fromCatalog, --toCatalog, --fromDatabase, and --toDatabase -toCatalog \u0026lt;catalog\u0026gt; Catalog a moving database or table is going to. This is required if you are moving a database or table. -fromCatalog \u0026lt;catalog\u0026gt; Catalog a moving database or table is coming from. This is required if you are moving a database or table. -toDatabase \u0026lt;database\u0026gt; Database a moving table is going to. This is required if you are moving a table. -fromDatabase \u0026lt;database\u0026gt; Database a moving table is coming from. This is required if you are moving a table. The dbType is required and can be one of:\n derby|mysql|postgres|oracle|mssql Version\nThe dbType \u0026ldquo;mssql\u0026rdquo; was added in Hive 0.13.1 with HIVE-6862.\nUsage Examples  Initialize to current schema for a new Hive setup:  $ schematool -dbType derby -initSchema Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=true Metastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriver Metastore connection User: APP Starting metastore schema initialization to 0.13.0 Initialization script hive-schema-0.13.0.derby.sql Initialization script completed schemaTool completed  Get schema information:  $ schematool -dbType derby -info Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=true Metastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriver Metastore connection User: APP Hive distribution version: 0.13.0 Metastore schema version: 0.13.0 schemaTool completed  Attempt to get schema information with older metastore:  $ schematool -dbType derby -info Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=true Metastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriver Metastore connection User: APP Hive distribution version: 0.13.0 org.apache.hadoop.hive.metastore.HiveMetaException: Failed to get schema version. *** schemaTool failed *** Since the older metastore doesn\u0026rsquo;t store the version information, the tool reports an error retrieving it.\n Upgrade schema from an 0.10.0 release by specifying the \u0026lsquo;from\u0026rsquo; version:  $ schematool -dbType derby -upgradeSchemaFrom 0.10.0 Metastore connection URL: jdbc:derby:;databaseName=metastore_db;create=true Metastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriver Metastore connection User: APP Starting upgrade metastore schema from version 0.10.0 to 0.13.0 Upgrade script upgrade-0.10.0-to-0.11.0.derby.sql Completed upgrade-0.10.0-to-0.11.0.derby.sql Upgrade script upgrade-0.11.0-to-0.12.0.derby.sql Completed upgrade-0.11.0-to-0.12.0.derby.sql Upgrade script upgrade-0.12.0-to-0.13.0.derby.sql Completed upgrade-0.12.0-to-0.13.0.derby.sql schemaTool completed  Upgrade dry run can be used to list the required scripts for the given upgrade.  $ build/dist/bin/schematool -dbType derby -upgradeSchemaFrom 0.7.0 -dryRun Metastore Connection Driver : org.apache.derby.jdbc.EmbeddedDriver Metastore connection User: APP Starting upgrade metastore schema from version 0.7.0 to 0.13.0 Upgrade script upgrade-0.7.0-to-0.8.0.derby.sql Upgrade script upgrade-0.8.0-to-0.9.0.derby.sql Upgrade script upgrade-0.9.0-to-0.10.0.derby.sql Upgrade script upgrade-0.10.0-to-0.11.0.derby.sql Upgrade script upgrade-0.11.0-to-0.12.0.derby.sql Upgrade script upgrade-0.12.0-to-0.13.0.derby.sql schemaTool completed This is useful if you just want to find out all the required scripts for the schema upgrade.\n Moving a database and tables under it from default Hive catalog to a custom spark catalog  build/dist/bin/schematool -moveDatabase db1 -fromCatalog hive -toCatalog spark  Moving a table from Hive catalog to Spark Catalog  # Create the desired target database in spark catalog if it doesn't already exist. beeline ... -e \u0026quot;create database if not exists newdb\u0026quot;; schematool -moveDatabase newdb -fromCatalog hive -toCatalog spark # Now move the table to target db under the spark catalog. schematool -moveTable table1 -fromCatalog hive -toCatalog spark -fromDatabase db1 -toDatabase newdb ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-schema-tool_34835119/","tags":null,"title":"Apache Hive : Hive Schema Tool"},{"categories":null,"contents":"Apache Hive : Hive Transactions ACID and Transactions in Hive  ACID and Transactions in Hive  What is ACID and why should you use it? Limitations Streaming APIs Grammar Changes Basic Design  Base and Delta Directories Compactor  Delta File Compaction Initiator Worker Cleaner AcidHouseKeeperService SHOW COMPACTIONS   Transaction/Lock Manager   Configuration  New Configuration Parameters for Transactions Configuration Values to Set for INSERT, UPDATE, DELETE Configuration Values to Set for Compaction Compaction pooling   Table Properties   Talks and Presentations  Hive 3 Warning\nAny transactional tables created by a Hive version prior to Hive 3 require Major Compaction to be run on every partition before upgrading to 3.0. More precisely, any partition which has had any update/delete/merge statements executed on it since the last Major Compaction, has to undergo another Major Compaction. No more update/delete/merge may happen on this partition until after Hive is upgraded to Hive 3.\nWhat is ACID and why should you use it? ACID stands for four traits of database transactions: Atomicity (an operation either succeeds completely or fails, it does not leave partial data), Consistency (once an application performs an operation the results of that operation are visible to it in every subsequent operation), Isolation (an incomplete operation by one user does not cause unexpected side effects for other users), and Durability (once an operation is complete it will be preserved even in the face of machine or system failure). These traits have long been expected of database systems as part of their transaction functionality. Up until Hive 0.13, atomicity, consistency, and durability were provided at the partition level. Isolation could be provided by turning on one of the available locking mechanisms (ZooKeeper or in memory). With the addition of transactions in Hive 0.13 it is now possible to provide full ACID semantics at the row level, so that one application can add rows while another reads from the same partition without interfering with each other.\nTransactions with ACID semantics have been added to Hive to address the following use cases:\n Streaming ingest of data. Many users have tools such as Apache Flume, Apache Storm, or Apache Kafka that they use to stream data into their Hadoop cluster. While these tools can write data at rates of hundreds or more rows per second, Hive can only add partitions every fifteen minutes to an hour. Adding partitions more often leads quickly to an overwhelming number of partitions in the table. These tools could stream data into existing partitions, but this would cause readers to get dirty reads (that is, they would see data written after they had started their queries) and leave many small files in their directories that would put pressure on the NameNode. With this new functionality this use case will be supported while allowing readers to get a consistent view of the data and avoiding too many files. Slow changing dimensions. In a typical star schema data warehouse, dimensions tables change slowly over time. For example, a retailer will open new stores, which need to be added to the stores table, or an existing store may change its square footage or some other tracked characteristic. These changes lead to inserts of individual records or updates of records (depending on the strategy chosen). Starting with 0.14, Hive is able to support this. Data restatement. Sometimes collected data is found to be incorrect and needs correction. Or the first instance of the data may be an approximation (90% of servers reporting) with the full data provided later. Or business rules may require that certain transactions be restated due to subsequent transactions (e.g., after making a purchase a customer may purchase a membership and thus be entitled to discount prices, including on the previous purchase). Or a user may be contractually required to remove their customer’s data upon termination of their relationship. Starting with Hive 0.14 these use cases can be supported via INSERT, UPDATE, and DELETE. Bulk updates using SQL MERGE statement.  Limitations  BEGIN, COMMIT, and ROLLBACK are not yet supported. All language operations are auto-commit. The plan is to support these in a future release. Only ORC file format is supported in this first release. The feature has been built such that transactions can be used by any storage format that can determine how updates or deletes apply to base records (basically, that has an explicit or implicit row id), but so far the integration work has only been done for ORC. By default transactions are configured to be off. See the Configuration section below for a discussion of which values need to be set to configure it. Tables must be bucketed to make use of these features. Tables in the same system not using transactions and ACID do not need to be bucketed. External tables cannot be made ACID tables since the changes on external tables are beyond the control of the compactor (HIVE-13175). Reading/writing to an ACID table from a non-ACID session is not allowed. In other words, the Hive transaction manager must be set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager in order to work with ACID tables. At this time only snapshot level isolation is supported. When a given query starts it will be provided with a consistent snapshot of the data. There is no support for dirty read, read committed, repeatable read, or serializable. With the introduction of BEGIN the intention is to support snapshot isolation for the duration of transaction rather than just a single query. Other isolation levels may be added depending on user requests. The existing ZooKeeper and in-memory lock managers are not compatible with transactions. There is no intention to address this issue. See Basic Design below for a discussion of how locks are stored for transactions. Schema changes using ALTER TABLE is NOT supported for ACID tables. HIVE-11421 is tracking it. Fixed in 1.3.0/2.0.0. Using Oracle as the Metastore DB and \u0026ldquo;datanucleus.connectionPoolingType=BONECP\u0026rdquo; may generate intermittent \u0026ldquo;No such lock..\u0026rdquo; and \u0026ldquo;No such transaction\u0026hellip;\u0026rdquo; errors. Setting \u0026ldquo;datanucleus.connectionPoolingType=DBCP\u0026rdquo; is recommended in this case. LOAD DATA\u0026hellip; statement is not supported with transactional tables. (This was not properly enforced until HIVE-16732)  Streaming APIs Hive offers APIs for streaming data ingest and streaming mutation:\n Hive HCatalog Streaming API Hive Streaming API (Since Hive 3) HCatalog Streaming Mutation API (available in Hive 2.0.0 and later)  A comparison of these two APIs is available in the Background section of the Streaming Mutation document.\nGrammar Changes INSERT\u0026hellip;VALUES, UPDATE, and DELETE have been added to the SQL grammar, starting in Hive 0.14. See LanguageManual DML for details.\nSeveral new commands have been added to Hive\u0026rsquo;s DDL in support of ACID and transactions, plus some existing DDL has been modified. A new command SHOW TRANSACTIONS has been added, see Show Transactions for details.\nA new command SHOW COMPACTIONS has been added, see Show Compactions for details.\nThe SHOW LOCKS command has been altered to provide information about the new locks associated with transactions. If you are using the ZooKeeper or in-memory lock managers you will notice no difference in the output of this command. See Show Locks for details.\nA new option has been added to ALTER TABLE to request a compaction of a table or partition. In general users do not need to request compactions, as the system will detect the need for them and initiate the compaction. However, if compaction is turned off for a table or a user wants to compact the table at a time the system would not choose to, ALTER TABLE can be used to initiate the compaction. See Alter Table/Partition Compact for details. This will enqueue a request for compaction and return. To watch the progress of the compaction the user can use SHOW COMPACTIONS.\nA new command ABORT TRANSACTIONS has been added, see Abort Transactions for details.\nBasic Design HDFS does not support in-place changes to files. It also does not offer read consistency in the face of writers appending to files being read by a user. In order to provide these features on top of HDFS we have followed the standard approach used in other data warehousing tools. Data for the table or partition is stored in a set of base files. New records, updates, and deletes are stored in delta files. A new set of delta files is created for each transaction (or in the case of streaming agents such as Flume or Storm, each batch of transactions) that alters a table or partition. At read time the reader merges the base and delta files, applying any updates and deletes as it reads.\nBase and Delta Directories Previously all files for a partition (or a table if the table is not partitioned) lived in a single directory. With these changes, any partitions (or tables) written with an ACID aware writer will have a directory for the base files and a directory for each set of delta files. Here is what this may look like for an unpartitioned table \u0026ldquo;t\u0026rdquo;:\nFilesystem Layout for Table \u0026ldquo;t\u0026rdquo;\nhive\u0026gt; dfs -ls -R /user/hive/warehouse/t; drwxr-xr-x - ekoifman staff 0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022 -rw-r--r-- 1 ekoifman staff 602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000 drwxr-xr-x - ekoifman staff 0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000 -rw-r--r-- 1 ekoifman staff 611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000 drwxr-xr-x - ekoifman staff 0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000 -rw-r--r-- 1 ekoifman staff 610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000 Compactor Compactor is a set of background processes running inside the Metastore to support ACID system. It consists of Initiator, Worker, Cleaner, AcidHouseKeeperService and a few others.\nDelta File Compaction As operations modify the table more and more delta files are created and need to be compacted to maintain adequate performance. There are three types of compactions, minor, major and rebalance.\n Minor compaction takes a set of existing delta files and rewrites them to a single delta file per bucket. Major compaction takes one or more delta files and the base file for the bucket and rewrites them into a new base file per bucket. Major compaction is more expensive but is more effective. More information about rebalance compaction can be found here: Rebalance compaction  All compactions are done in the background. Minor and major compactions do not prevent concurrent reads and writes of the data. Rebalance compaction uses exclusive write lock, therefore it prevents concurrent writes. After a compaction the system waits until all readers of the old files have finished and then removes the old files.\nInitiator This module is responsible for discovering which tables or partitions are due for compaction. This should be enabled in a Metastore using hive.compactor.initiator.on. There are several properties of the form *.threshold in \u0026ldquo;New Configuration Parameters for Transactions\u0026rdquo; table below that control when a compaction task is created and which type of compaction is performed. Each compaction task handles 1 partition (or whole table if the table is unpartitioned). If the number of consecutive compaction failures for a given partition exceeds hive.compactor.initiator.failed.compacts.threshold, automatic compaction scheduling will stop for this partition. See Configuration Parameters table for more info.\nWorker Each Worker handles a single compaction task. A compaction is a MapReduce job with name in the following form: -compactor-... Each worker submits the job to the cluster (via hive.compactor.job.queue if defined) and waits for the job to finish. hive.compactor.worker.threads determines the number of Workers in each Metastore. The total number of Workers in the Hive Warehouse determines the maximum number of concurrent compactions.\nCleaner This process is a process that deletes delta files after compaction and after it determines that they are no longer needed.\nAcidHouseKeeperService This process looks for transactions that have not heartbeated in hive.txn.timeout time and aborts them. The system assumes that a client that initiated a transaction stopped heartbeating crashed and the resources it locked should be released.\nSHOW COMPACTIONS This commands displays information about currently running compaction and recent history (configurable retention period) of compactions. This history display is available since HIVE-12353.\nAlso see LanguageManual DDL#ShowCompactions for more information on the output of this command and NewConfigurationParametersforTransactions/Compaction History for configuration properties affecting the output of this command. The system retains the last N entries of each type: failed, succeeded, attempted (where N is configurable for each type).\nTransaction/Lock Manager A new logical entity called \u0026ldquo;transaction manager\u0026rdquo; was added which incorporated previous notion of \u0026ldquo;database/table/partition lock manager\u0026rdquo; (hive.lock.manager with default of org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager). The transaction manager is now additionally responsible for managing of transactions locks. The default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases. A newly added DbTxnManager manages all locks/transactions in Hive metastore with DbLockManager (transactions and locks are durable in the face of server failure). This means that previous behavior of locking in ZooKeeper is not present anymore when transactions are enabled. To avoid clients dying and leaving transaction or locks dangling, a heartbeat is sent from lock holders and transaction initiators to the metastore on a regular basis. If a heartbeat is not received in the configured amount of time, the lock or transaction will be aborted.\nAs of Hive 1.3.0, the length of time that the DbLockManger will continue to try to acquire locks can be controlled via [hive.lock.numretires](http://Configuration Properties#hive.lock.numretires) and [hive.lock.sleep.between.retries](http://Configuration Properties#hive.lock.sleep.between.retries). When the DbLockManager cannot acquire a lock (due to existence of a competing lock), it will back off and try again after a certain time period. In order to support short running queries and not overwhelm the metastore at the same time, the DbLockManager will double the wait time after each retry. The initial back off time is 100ms and is capped by hive.lock.sleep.between.retries. hive.lock.numretries is the total number of times it will retry a given lock request. Thus the total time that the call to acquire locks will block (given values of 100 retries and 60s sleep time) is (100ms + 200ms + 400ms + \u0026hellip; + 51200ms + 60s + 60s + \u0026hellip; + 60s) = 91m:42s:300ms.\nMore details on locks used by this Lock Manager.\nNote that the lock manager used by DbTxnManager will acquire locks on all tables, even those without \u0026ldquo;transactional=true\u0026rdquo; property. By default, Insert operation into a non-transactional table will acquire an exclusive lock and thus block other inserts and reads. While technically correct, this is a departure from how Hive traditionally worked (i.e. w/o a lock manger). For backwards compatibility, [hive.txn.strict.locking.mode](http://Configuration Properties#hive.txn.strict.locking.mode) (see table below) is provided which will make this lock manager acquire shared locks on insert operations on non-transactional tables. This restores previous semantics while still providing the benefit of a lock manager such as preventing table drop while it is being read. Note that for transactional tables, insert always acquires share locks since these tables implement MVCC architecture at the storage layer and are able to provide strong read consistency (Snapshot Isolation) even in presence of concurrent modification operations.\nConfiguration Minimally, these configuration parameters must be set appropriately to turn on transaction support in Hive:\nClient Side\n hive.support.concurrency – true hive.enforce.bucketing – true (Not required as of Hive 2.0) hive.exec.dynamic.partition.mode – nonstrict hive.txn.manager – org.apache.hadoop.hive.ql.lockmgr.DbTxnManager  Server Side (Metastore)\n hive.compactor.initiator.on – true (See table below for more details) hive.compactor.cleaner.on – true (See table below for more details) hive.compactor.worker.threads – a positive number on at least one instance of the Thrift metastore service  The following sections list all of the configuration parameters that affect Hive transactions and compaction. Also see Limitations above and Table Properties below.\nNew Configuration Parameters for Transactions A number of new configuration parameters have been added to the system to support transactions.\n   Configuration key Values Location Notes     hive.txn.manager  Default: org.apache.hadoop.hive.ql.lockmgr.DummyTxnManagerValue required for transactions: org.apache.hadoop.hive.ql.lockmgr.DbTxnManager Client/HiveServer2 DummyTxnManager replicates pre Hive-0.13 behavior and provides no transactions.   hive.txn.strict.locking.mode Default: true Client/ HiveServer2 In strict mode non-ACID resources use standard R/W lock semantics, e.g. INSERT will acquire exclusive lock. In non-strict mode, for non-ACID resources, INSERT will only acquire shared lock, which allows two concurrent writes to the same partition but still lets lock manager prevent DROP TABLE etc. when the table is being written to (as of Hive 2.2.0).   hive.txn.timeout  Default: 300 Client/HiveServer2/Metastore  Time after which transactions are declared aborted if the client has not sent a heartbeat, in seconds. It\u0026rsquo;s critical that this property has the same value for all components/services.5   hive.txn.heartbeat.threadpool.size Default: 5 Client/HiveServer2 The number of threads to use for heartbeating (as of Hive 1.3.0 and 2.0.0).   hive.timedout.txn.reaper.start Default: 100s Metastore Time delay of first reaper (the process which aborts timed-out transactions) run after the metastore starts (as of Hive 1.3.0). Controls AcidHouseKeeperServcie above.   hive.timedout.txn.reaper.interval Default: 180s Metastore Time interval describing how often the reaper (the process which aborts timed-out transactions) runs (as of Hive 1.3.0). Controls AcidHouseKeeperServcie above.   hive.txn.max.open.batch Default: 1000 Client Maximum number of transactions that can be fetched in one call to open_txns().1   hive.max.open.txns Default: 100000 HiveServer2/ Metastore Maximum number of open transactions. If current open transactions reach this limit, future open transaction requests will be rejected, until the number goes below the limit. (As of Hive 1.3.0 and 2.1.0.)   hive.count.open.txns.interval Default: 1s HiveServer2/ Metastore Time in seconds between checks to count open transactions (as of Hive 1.3.0 and 2.1.0).   hive.txn.retryable.sqlex.regex Default: \u0026quot;\u0026quot; (empty string) HiveServer2/ Metastore Comma separated list of regular expression patterns for SQL state, error code, and error message of retryable SQLExceptions, that\u0026rsquo;s suitable for the Hive metastore database (as of Hive 1.3.0 and 2.1.0).For an example, see Configuration Properties.   hive.compactor.initiator.on Default: falseValue required for transactions: true (for exactly one instance of the Thrift metastore service) Metastore Whether to run the initiator thread on this metastore instance. Prior to Hive 1.3.0 it\u0026rsquo;s critical that this is enabled on exactly one standalone metastore service instance (not enforced yet).As of Hive 1.3.0 this property may be enabled on any number of standalone metastore instances.   hive.compactor.cleaner.on Default: falseValue required for transactions: true (for exactly one instance of the Thrift metastore service) Metastore Whether to run the cleaner thread on this metastore instance. Before Hive 4.0.0 Cleaner thread can be started/stopped with config hive.compactor.initiator.on. This config helps to enable/disable initiator/cleaner threads independently   hive.compactor.worker.threads Default: 0Value required for transactions: \u0026gt; 0 on at least one instance of the Thrift metastore service Metastore How many compactor worker threads to run on this metastore instance.2   hive.compactor.worker.timeout Default: 86400 Metastore Time in seconds after which a compaction job will be declared failed and the compaction re-queued.   hive.compactor.cleaner.run.interval Default: 5000 Metastore Time in milliseconds between runs of the cleaner thread. (Hive 0.14.0 and later.)   hive.compactor.check.interval Default: 300 Metastore Time in seconds between checks to see if any tables or partitions need to be compacted.3   hive.compactor.delta.num.threshold Default: 10 Metastore Number of delta directories in a table or partition that will trigger a minor compaction.   hive.compactor.delta.pct.threshold Default: 0.1 Metastore Percentage (fractional) size of the delta files relative to the base that will trigger a major compaction. 1 = 100%, so the default 0.1 = 10%.   hive.compactor.abortedtxn.threshold Default: 1000 Metastore Number of aborted transactions involving a given table or partition that will trigger a major compaction.   hive.compactor.aborted.txn.time.threshold Default: 12h Metastore Age of table/partition\u0026rsquo;s oldest aborted transaction when compaction will be triggered. Default time unit is: hours. Set to a negative number to disable.   hive.compactor.max.num.delta Default: 500 Metastore Maximum number of delta files that the compactor will attempt to handle in a single job (as of Hive 1.3.0).4   hive.compactor.job.queue Default: \u0026quot;\u0026quot; (empty string)  Metastore  Used to specify name of Hadoop queue to which Compaction jobs will be submitted. Set to empty string to let Hadoop choose the queue (as of Hive 1.3.0).   Compaction History      hive.compactor.history.retention.succeeded Default: 3 Metastore Number of successful compaction entries to retain in history (per partition).   hive.compactor.history.retention.failed Default: 3 Metastore Number of failed compaction entries to retain in history (per partition).   hive.compactor.history.retention.attempted Default: 2 Metastore Number of attempted compaction entries to retain in history (per partition).   hive.compactor.initiator.failed.compacts.threshold Default: 2 Metastore Number of of consecutive failed compactions for a given partition after which the Initiator will stop attempting to schedule compactions automatically. It is still possible to use ALTER TABLE to initiate compaction. Once a manually initiated compaction succeeds auto initiated compactions will resume. Note that this must be less than hive.compactor.history.retention.failed.   hive.compactor.history.reaper.interval Default: 2m Metastore Controls how often the process to purge historical record of compactions runs.      hive.txn.max.open.batch controls how many transactions streaming agents such as Flume or Storm open simultaneously. The streaming agent then writes that number of entries into a single file (per Flume agent or Storm bolt). Thus increasing this value decreases the number of delta files created by streaming agents. But it also increases the number of open transactions that Hive has to track at any given time, which may negatively affect read performance.\n  Worker threads spawn MapReduce jobs to do compactions. They do not do the compactions themselves. Increasing the number of worker threads will decrease the time it takes tables or partitions to be compacted once they are determined to need compaction. It will also increase the background load on the Hadoop cluster as more MapReduce jobs will be running in the background. Each compaction can handle one partition at a time (or whole table if it\u0026rsquo;s unpartitioned).   Decreasing this value will reduce the time it takes for compaction to be started for a table or partition that requires compaction. However, checking if compaction is needed requires several calls to the NameNode for each table or partition that has had a transaction done on it since the last major compaction. So decreasing this value will increase the load on the NameNode.\n  If the compactor detects a very high number of delta files, it will first run several partial minor compactions (currently sequentially) and then perform the compaction actually requested.\n  If the value is not the same active transactions may be determined to be \u0026ldquo;timed out\u0026rdquo; and consequently Aborted. This will result in errors like \u0026ldquo;No such transaction\u0026hellip;\u0026rdquo;, \u0026ldquo;No such lock \u0026hellip;\u0026rdquo;\n  Configuration Values to Set for INSERT, UPDATE, DELETE In addition to the new parameters listed above, some existing parameters need to be set to support *INSERT \u0026hellip; VALUES, UPDATE,*and DELETE.\n   Configuration key Must be set to     hive.support.concurrency true (default is false)   hive.enforce.bucketing true (default is false) (Not required as of Hive 2.0)   hive.exec.dynamic.partition.mode nonstrict (default is strict)    Configuration Values to Set for Compaction If the data in your system is not owned by the Hive user (i.e., the user that the Hive metastore runs as), then Hive will need permission to run as the user who owns the data in order to perform compactions. If you have already set up HiveServer2 to impersonate users, then the only additional work to do is assure that Hive has the right to impersonate users from the host running the Hive metastore. This is done by adding the hostname to hadoop.proxyuser.hive.hosts in Hadoop\u0026rsquo;s core-site.xml file. If you have not already done this, then you will need to configure Hive to act as a proxy user. This requires you to set up keytabs for the user running the Hive metastore and add hadoop.proxyuser.hive.hosts and hadoop.proxyuser.hive.groups to Hadoop\u0026rsquo;s core-site.xml file. See the Hadoop documentation on secure mode for your version of Hadoop (e.g., for Hadoop 2.5.1 it is at Hadoop in Secure Mode).\nCompaction pooling More in formation on compaction pooling can be found here: Compaction pooling\nTable Properties If a table is to be used in ACID writes (insert, update, delete) then the table property \u0026ldquo;transactional=true\u0026rdquo; must be set on that table, starting with Hive 0.14.0. Note, once a table has been defined as an ACID table via TBLPROPERTIES (\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;true\u0026rdquo;), it cannot be converted back to a non-ACID table, i.e., changing TBLPROPERTIES (\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;false\u0026rdquo;) is not allowed. Also, hive.txn.manager must be set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager either in hive-site.xml or in the beginning of the session before any query is run. Without those, inserts will be done in the old style; updates and deletes will be prohibited prior to HIVE-11716. Since HIVE-11716 operations on ACID tables without DbTxnManager are not allowed. However, this does not apply to Hive 0.13.0.\nIf a table owner does not wish the system to automatically determine when to compact, then the table property \u0026ldquo;NO_AUTO_COMPACTION\u0026rdquo; can be set. This will prevent all automatic compactions. Manual compactions can still be done with Alter Table/Partition Compact statements.\nTable properties are set with the TBLPROPERTIES clause when a table is created or altered, as described in the Create Table and Alter Table Properties sections of Hive Data Definition Language. The \u0026ldquo;transactional\u0026rdquo; and \u0026ldquo;NO_AUTO_COMPACTION\u0026rdquo; table properties are case-sensitive in Hive releases 0.x and 1.0, but they are case-insensitive starting with release 1.1.0 (HIVE-8308).\nMore compaction related options can be set via TBLPROPERTIES as of Hive 1.3.0 and 2.1.0. They can be set at both table-level via CREATE TABLE, and on request-level via ALTER TABLE/PARTITION COMPACT. These are used to override the Warehouse/table wide settings. For example, to override an MR property to affect a compaction job, one can add \u0026ldquo;compactor.=\u0026rdquo; in either CREATE TABLE statement or when launching a compaction explicitly via ALTER TABLE. The \u0026ldquo;=\u0026rdquo; will be set on JobConf of the compaction MR job. Similarly, \u0026ldquo;tblprops.=\u0026rdquo; can be used to set/override any table property which is interpreted by the code running on the cluster. Finally, \u0026ldquo;compactorthreshold.=\u0026rdquo; can be used to override properties from the \u0026ldquo;New Configuration Parameters for Transactions\u0026rdquo; table above that end with \u0026ldquo;.threshold\u0026rdquo; and control when compactions are triggered by the system. Examples:\nExample: Set compaction options in TBLPROPERTIES at table level\nCREATE TABLE table_name ( id int, name string ) CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES (\u0026quot;transactional\u0026quot;=\u0026quot;true\u0026quot;, \u0026quot;compactor.mapreduce.map.memory.mb\u0026quot;=\u0026quot;2048\u0026quot;, -- specify compaction map job properties \u0026quot;compactorthreshold.hive.compactor.delta.num.threshold\u0026quot;=\u0026quot;4\u0026quot;, -- trigger minor compaction if there are more than 4 delta directories \u0026quot;compactorthreshold.hive.compactor.delta.pct.threshold\u0026quot;=\u0026quot;0.5\u0026quot; -- trigger major compaction if the ratio of size of delta files to -- size of base files is greater than 50% ); Example: Set compaction options in TBLPROPERTIES at request level\nALTER TABLE table_name COMPACT 'minor' WITH OVERWRITE TBLPROPERTIES (\u0026quot;compactor.mapreduce.map.memory.mb\u0026quot;=\u0026quot;3072\u0026quot;); -- specify compaction map job properties ALTER TABLE table_name COMPACT 'major' WITH OVERWRITE TBLPROPERTIES (\u0026quot;tblprops.orc.compress.size\u0026quot;=\u0026quot;8192\u0026quot;); -- change any other Hive table properties Talks and Presentations Transactional Operations In Hive by Eugene Koifman at Dataworks Summit 2017, San Jose, CA, USA\n Slides Video  DataWorks Summit 2018, San Jose, CA, USA - Covers Hive 3 and ACID V2 features\n Slides Video  Save\nSave\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-transactions_40509723/","tags":null,"title":"Apache Hive : Hive Transactions"},{"categories":null,"contents":"Apache Hive : Hive Transactions (Hive ACID) ACID and Transactions in Hive  ACID and Transactions in Hive  What is ACID and why should you use it? Limitations Streaming APIs Grammar Changes Basic Design  Base and Delta Directories Compactor  Delta File Compaction Initiator Worker Cleaner AcidHouseKeeperService SHOW COMPACTIONS   Transaction/Lock Manager   Configuration  New Configuration Parameters for Transactions Configuration Values to Set for Hive ACID (INSERT, UPDATE, DELETE) Configuration Values to Set for Compaction Compaction pooling   Table Properties   Talks and Presentations  What is ACID and why should you use it? ACID stands for four traits of database transactions: Atomicity (an operation either succeeds completely or fails, it does not leave partial data), Consistency (once an application performs an operation the results of that operation are visible to it in every subsequent operation), Isolation (an incomplete operation by one user does not cause unexpected side effects for other users), and Durability (once an operation is complete it will be preserved even in the face of machine or system failure). These traits have long been expected of database systems as part of their transaction functionality. Transactions with ACID semantics have been added to Hive to address the following use cases:\n Streaming ingest of data. Many users have tools such as Apache Flume, Apache Storm, or Apache Kafka that they use to stream data into their Hadoop cluster. While these tools can write data at rates of hundreds or more rows per second, Hive can only add partitions every fifteen minutes to an hour. Adding partitions more often leads quickly to an overwhelming number of partitions in the table. These tools could stream data into existing partitions, but this would cause readers to get dirty reads (that is, they would see data written after they had started their queries) and leave many small files in their directories that would put pressure on the NameNode. With this new functionality this use case will be supported while allowing readers to get a consistent view of the data and avoiding too many files. Slow changing dimensions. In a typical star schema data warehouse, dimensions tables change slowly over time. For example, a retailer will open new stores, which need to be added to the stores table, or an existing store may change its square footage or some other tracked characteristic. These changes lead to inserts of individual records or updates of records (depending on the strategy chosen). Data restatement. Sometimes collected data is found to be incorrect and needs correction. Or the first instance of the data may be an approximation (90% of servers reporting) with the full data provided later. Or business rules may require that certain transactions be restated due to subsequent transactions (e.g., after making a purchase a customer may purchase a membership and thus be entitled to discount prices, including on the previous purchase). Or a user may be contractually required to remove their customer’s data upon termination of their relationship. Bulk updates using SQL MERGE statement.  Limitations  BEGIN, COMMIT, and ROLLBACK are not yet supported. All language operations are auto-commit. Only ORC file format is supported. The feature has been built such that transactions can be used by any storage format that can determine how updates or deletes apply to base records (basically, that has an explicit or implicit row id), but so far the integration work has only been done for ORC. By default transactions are configured to be off. See the Configuration section below for a discussion of which values need to be set to configure it. Tables must be bucketed to make use of these features. Tables in the same system not using transactions and ACID do not need to be bucketed. Reading/writing to an ACID table from a non-ACID session is not allowed. In other words, the Hive transaction manager must be set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager in order to work with ACID tables. At this time only snapshot level isolation is supported. When a given query starts it will be provided with a consistent snapshot of the data. There is no support for dirty read, read committed, repeatable read, or serializable. With the introduction of BEGIN the intention is to support snapshot isolation for the duration of transaction rather than just a single query. Other isolation levels may be added depending on user requests. The existing ZooKeeper and in-memory lock managers are not compatible with transactions. There is no intention to address this issue. See Basic Design below for a discussion of how locks are stored for transactions. Using Oracle as the Metastore DB and \u0026ldquo;datanucleus.connectionPoolingType=BONECP\u0026rdquo; may generate intermittent \u0026ldquo;No such lock..\u0026rdquo; and \u0026ldquo;No such transaction\u0026hellip;\u0026rdquo; errors. Setting \u0026ldquo;datanucleus.connectionPoolingType=DBCP\u0026rdquo; is recommended in this case. LOAD DATA\u0026hellip; statement is not supported with transactional tables. (This was not properly enforced until HIVE-16732)  Streaming APIs Hive offers APIs for streaming data ingest and streaming mutation:\n Hive HCatalog Streaming API Hive Streaming API (Since Hive 3) HCatalog Streaming Mutation API (Copy) (available in Hive 2.0.0 and later)  A comparison of these two APIs is available in the Background section of the Streaming Mutation document.\nGrammar Changes INSERT\u0026hellip;VALUES, UPDATE, and DELETE have been added to the SQL grammar, starting in Hive 0.14. See LanguageManual DML for details.\nSeveral new commands have been added to Hive\u0026rsquo;s DDL in support of ACID and transactions, plus some existing DDL has been modified. A new command SHOW TRANSACTIONS has been added, see Show Transactions for details.\nA new command SHOW COMPACTIONS has been added, see Show Compactions for details.\nThe SHOW LOCKS command has been altered to provide information about the new locks associated with transactions. If you are using the ZooKeeper or in-memory lock managers you will notice no difference in the output of this command. See Show Locks for details.\nA new option has been added to ALTER TABLE to request a compaction of a table or partition. In general users do not need to request compactions, as the system will detect the need for them and initiate the compaction. However, if compaction is turned off for a table or a user wants to compact the table at a time the system would not choose to, ALTER TABLE can be used to initiate the compaction. See Alter Table/Partition Compact for details. This will enqueue a request for compaction and return. To watch the progress of the compaction the user can use SHOW COMPACTIONS.\nA new command ABORT TRANSACTIONS has been added, see Abort Transactions for details.\nBasic Design HDFS does not support in-place changes to files. It also does not offer read consistency in the face of writers appending to files being read by a user. In order to provide these features on top of HDFS we have followed the standard approach used in other data warehousing tools. Data for the table or partition is stored in a set of base files. New records, updates, and deletes are stored in delta files. A new set of delta files is created for each transaction (or in the case of streaming agents such as Flume or Storm, each batch of transactions) that alters a table or partition. At read time the reader merges the base and delta files, applying any updates and deletes as it reads.\nBase and Delta Directories Previously all files for a partition (or a table if the table is not partitioned) lived in a single directory. With these changes, any partitions (or tables) written with an ACID aware writer will have a directory for the base files and a directory for each set of delta files. Here is what this may look like for an unpartitioned table \u0026ldquo;t\u0026rdquo;:\nFilesystem Layout for Table \u0026ldquo;t\u0026rdquo;\nhive\u0026gt; dfs -ls -R /user/hive/warehouse/t; drwxr-xr-x - ekoifman staff 0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022 -rw-r--r-- 1 ekoifman staff 602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000 drwxr-xr-x - ekoifman staff 0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000 -rw-r--r-- 1 ekoifman staff 611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000 drwxr-xr-x - ekoifman staff 0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000 -rw-r--r-- 1 ekoifman staff 610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000 Compactor Compactor is a set of background processes running inside the Metastore to support ACID system. It consists of Initiator, Worker, Cleaner, AcidHouseKeeperService and a few others.\nDelta File Compaction As operations modify the table more and more delta files are created and need to be compacted to maintain adequate performance. There are three types of compactions, minor, major and rebalance.\n Minor compaction takes a set of existing delta files and rewrites them to a single delta file per bucket. Major compaction takes one or more delta files and the base file for the bucket and rewrites them into a new base file per bucket. Major compaction is more expensive but is more effective. More information about rebalance compaction can be found here: Rebalance compaction  All compactions are done in the background. Minor and major compactions do not prevent concurrent reads and writes of the data. Rebalance compaction uses exclusive write lock, therefore it prevents concurrent writes. After a compaction the system waits until all readers of the old files have finished and then removes the old files.\nInitiator This module is responsible for discovering which tables or partitions are due for compaction. This should be enabled in a Metastore using hive.compactor.initiator.on. There are several properties of the form *.threshold in \u0026ldquo;New Configuration Parameters for Transactions\u0026rdquo; table below that control when a compaction task is created and which type of compaction is performed. Each compaction task handles 1 partition (or whole table if the table is unpartitioned). If the number of consecutive compaction failures for a given partition exceeds hive.compactor.initiator.failed.compacts.threshold, automatic compaction scheduling will stop for this partition. See Configuration Parameters table for more info.\nWorker Each Worker handles a single compaction task. A compaction is a MapReduce job with name in the following form: -compactor-... Each worker submits the job to the cluster (via hive.compactor.job.queue if defined) and waits for the job to finish. hive.compactor.worker.threads determines the number of Workers in each Metastore. The total number of Workers in the Hive Warehouse determines the maximum number of concurrent compactions.\nCleaner This process is a process that deletes delta files after compaction and after it determines that they are no longer needed.\nAcidHouseKeeperService This process looks for transactions that have not heartbeated in hive.txn.timeout time and aborts them. The system assumes that a client that initiated a transaction stopped heartbeating crashed and the resources it locked should be released.\nSHOW COMPACTIONS This commands displays information about currently running compaction and recent history (configurable retention period) of compactions. This history display is available since HIVE-12353.\nAlso see LanguageManual DDL#ShowCompactions for more information on the output of this command and NewConfigurationParametersforTransactions/Compaction History for configuration properties affecting the output of this command. The system retains the last N entries of each type: failed, succeeded, attempted (where N is configurable for each type).\nTransaction/Lock Manager A new logical entity called \u0026ldquo;transaction manager\u0026rdquo; was added which incorporated previous notion of \u0026ldquo;database/table/partition lock manager\u0026rdquo; (hive.lock.manager with default of org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager). The transaction manager is now additionally responsible for managing of transactions locks. The default DummyTxnManager emulates behavior of old Hive versions: has no transactions and uses hive.lock.manager property to create lock manager for tables, partitions and databases. A newly added DbTxnManager manages all locks/transactions in Hive metastore with DbLockManager (transactions and locks are durable in the face of server failure). This means that previous behaviour of locking in ZooKeeper is not present anymore when transactions are enabled. To avoid clients dying and leaving transaction or locks dangling, a heartbeat is sent from lock holders and transaction initiators to the metastore on a regular basis. If a heartbeat is not received in the configured amount of time, the lock or transaction will be aborted.\nThe length of time that the DbLockManger will continue to try to acquire locks can be controlled via [hive.lock.numretires](http://Configuration Properties#hive.lock.numretires) and [hive.lock.sleep.between.retries](http://Configuration Properties#hive.lock.sleep.between.retries). When the DbLockManager cannot acquire a lock (due to existence of a competing lock), it will back off and try again after a certain time period. In order to support short running queries and not overwhelm the metastore at the same time, the DbLockManager will double the wait time after each retry. The initial back off time is 100ms and is capped by hive.lock.sleep.between.retries. hive.lock.numretries is the total number of times it will retry a given lock request. Thus the total time that the call to acquire locks will block (given values of 100 retries and 60s sleep time) is (100ms + 200ms + 400ms + \u0026hellip; + 51200ms + 60s + 60s + \u0026hellip; + 60s) = 91m:42s:300ms.\nMore details on locks used by this Lock Manager.\nNote that the lock manager used by DbTxnManager will acquire locks on all tables, even those without \u0026ldquo;transactional=true\u0026rdquo; property. By default, Insert operation into a non-transactional table will acquire an exclusive lock and thus block other inserts and reads. While technically correct, this is a departure from how Hive traditionally worked (i.e. w/o a lock manger). For backwards compatibility, [hive.txn.strict.locking.mode](http://Configuration Properties#hive.txn.strict.locking.mode) (see table below) is provided which will make this lock manager acquire shared locks on insert operations on non-transactional tables. This restores previous semantics while still providing the benefit of a lock manager such as preventing table drop while it is being read. Note that for transactional tables, insert always acquires share locks since these tables implement MVCC architecture at the storage layer and are able to provide strong read consistency (Snapshot Isolation) even in presence of concurrent modification operations.\nConfiguration Minimally, these configuration parameters must be set appropriately to turn on transaction support in Hive:\nClient Side\n hive.support.concurrency – true hive.exec.dynamic.partition.mode – nonstrict hive.txn.manager – org.apache.hadoop.hive.ql.lockmgr.DbTxnManager  Server Side (Metastore)\n hive.compactor.initiator.on – true (See table below for more details) hive.compactor.cleaner.on – true (See table below for more details) hive.compactor.worker.threads – a positive number on at least one instance of the Thrift metastore service  The following sections list all of the configuration parameters that affect Hive transactions and compaction. Also see Limitations above and Table Properties below.\nNew Configuration Parameters for Transactions A number of new configuration parameters have been added to the system to support transactions.\n   Configuration key Values Location Notes     hive.txn.manager  Default: org.apache.hadoop.hive.ql.lockmgr.DummyTxnManagerValue required for transactions: org.apache.hadoop.hive.ql.lockmgr.DbTxnManager Client/HiveServer2 DummyTxnManager replicates pre Hive-0.13 behavior and provides no transactions.   hive.txn.strict.locking.mode Default: true Client/ HiveServer2 In strict mode non-ACID resources use standard R/W lock semantics, e.g. INSERT will acquire exclusive lock. In non-strict mode, for non-ACID resources, INSERT will only acquire shared lock, which allows two concurrent writes to the same partition but still lets lock manager prevent DROP TABLE etc. when the table is being written to (as of Hive 2.2.0).   hive.txn.timeout deprecated. Use metastore.txn.timeout instead Default: 300 Client/HiveServer2/Metastore  Time after which transactions are declared aborted if the client has not sent a heartbeat, in seconds. It\u0026rsquo;s critical that this property has the same value for all components/services.5   hive.txn.heartbeat.threadpool.size deprecated - but still in use Default: 5 Client/HiveServer2 The number of threads to use for heartbeating (as of Hive 1.3.0 and 2.0.0).   hive.timedout.txn.reaper.start deprecated Default: 100s Metastore Time delay of first reaper (the process which aborts timed-out transactions) run after the metastore starts (as of Hive 1.3.0). Controls AcidHouseKeeperServcie above.   hive.timedout.txn.reaper.interval deprecated Default: 180s Metastore Time interval describing how often the reaper (the process which aborts timed-out transactions) runs (as of Hive 1.3.0). Controls AcidHouseKeeperServcie above.   hive.txn.max.open.batch deprecated. Use metastore.txn.max.open.batch instead Default: 1000 Client Maximum number of transactions that can be fetched in one call to open_txns().1   hive.max.open.txns deprecated. Use metastore.max.open.txns instead. Default: 100000 HiveServer2/ Metastore Maximum number of open transactions. If current open transactions reach this limit, future open transaction requests will be rejected, until the number goes below the limit. (As of Hive 1.3.0 and 2.1.0.)   hive.count.open.txns.interval deprecated. Use metastore.count.open.txns.interval instead. Default: 1s HiveServer2/ Metastore Time in seconds between checks to count open transactions (as of Hive 1.3.0 and 2.1.0).   hive.txn.retryable.sqlex.regex deprecated. Use metastore.txn.retryable.sqlex.regex instead. Default: \u0026quot;\u0026quot; (empty string) HiveServer2/ Metastore Comma separated list of regular expression patterns for SQL state, error code, and error message of retryable SQLExceptions, that\u0026rsquo;s suitable for the Hive metastore database (as of Hive 1.3.0 and 2.1.0).For an example, see Configuration Properties.   hive.compaction.merge.enabled Default: false HiveServer2 Enables merge-based compaction which is a compaction optimization when few ORC delta files are present   hive.compactor.initiator.duration.update.interval Default: 60s HiveServer2 Time in seconds that drives the update interval of compaction_initiator_duration metric.Smaller value results in a fine grained metric update.This updater can be turned off if its value less than or equals to zero.In this case the above metric will be update only after the initiator completed one cycle.The hive.compactor.initiator.on must be turned on (true) in-order to enable the Initiator,otherwise this setting has no effect.   hive.compactor.initiator.on deprecated. Use metastore.compactor.initiator.on instead. Default: falseValue required for transactions: true (for exactly one instance of the Thrift metastore service) Metastore Whether to run the initiator thread on this metastore instance. Prior to Hive 1.3.0 it\u0026rsquo;s critical that this is enabled on exactly one standalone metastore service instance (not enforced yet).As of Hive 1.3.0 this property may be enabled on any number of standalone metastore instances.   hive.compactor.cleaner.duration.update.interval Default: 60s HiveServer2 Time in seconds that drives the update interval of compaction_cleaner_duration metric.Smaller value results in a fine grained metric update.This updater can be turned off if its value less than or equals to zero.In this case the above metric will be update only after the cleaner completed one cycle.   hive.compactor.cleaner.on deprecated. Use metastore.compactor.cleaner.on instead. Default: falseValue required for transactions: true (for exactly one instance of the Thrift metastore service) Metastore Whether to run the cleaner thread on this metastore instance. Before Hive 4.0.0 Cleaner thread can be started/stopped with config hive.compactor.initiator.on. This config helps to enable/disable initiator/cleaner threads independently   hive.compactor.cleaner.threads.num Default: 1 HiveServer2 Enables parallelization of the cleaning directories after compaction, that includes many file related checks and may be expensive   hive.compactor.compact.insert.only Default: true HiveServer2 Whether the compactor should compact insert-only tables. A safety switch.   hive.compactor.crud.query.based Default: false HiveServer2 Means compaction on full CRUD tables is done via queries. Compactions on insert-only tables will always run via queries regardless of the value of this configuration.   hive.compactor.gather.stats Default: true HiveServer2 If set to true MAJOR compaction will gather stats if there are stats already associated with the table/partition.Turn this off to save some resources and the stats are not used anyway.This is a replacement for the HIVE_MR_COMPACTOR_GATHER_STATS config, and works both for MR and Query based compaction.   metastore.compactor.initiator.failed.retry.time Default: 7d Metastore Time after Initiator will ignore metastore.compactor.initiator.failed.compacts.threshold and retry with compaction again. This will try to auto heal tables with previous failed compaction without manual intervention. Setting it to 0 or negative value will disable this feature.   metastore.compactor.long.running.initiator.threshold.warning Default: 6h Metastore Initiator cycle duration after which a warning will be logged. Default time unit is: hours   metastore.compactor.long.running.initiator.threshold.error Default: 12h Metastore Initiator cycle duration after which an error will be logged. Default time unit is: hours   hive.compactor.worker.sleep.time *Default:*10800ms HiveServer2 Time in milliseconds for which a worker threads goes into sleep before starting another iteration in case of no launched job or error   hive.compactor.worker.max.sleep.time Default: 320000ms HiveServer2 Max time in milliseconds for which a worker threads goes into sleep before starting another iteration used for backoff in case of no launched job or error   hive.compactor.worker.threads deprecated. Use metastore.compactor.worker.threads instead. Default: 0Value required for transactions: \u0026gt; 0 on at least one instance of the Thrift metastore service Metastore How many compactor worker threads to run on this metastore instance.2   hive.compactor.worker.timeout Default: 86400s Metastore Time in seconds after which a compaction job will be declared failed and the compaction re-queued.   hive.compactor.cleaner.run.interval Default: 5000ms Metastore Time in milliseconds between runs of the cleaner thread. (Hive 0.14.0 and later.)   hive.compactor.check.interval Default: 300s Metastore Time in seconds between checks to see if any tables or partitions need to be compacted.3   hive.compactor.delta.num.threshold Default: 10 Metastore Number of delta directories in a table or partition that will trigger a minor compaction.   hive.compactor.delta.pct.threshold Default: 0.1 Metastore Percentage (fractional) size of the delta files relative to the base that will trigger a major compaction. 1 = 100%, so the default 0.1 = 10%.   hive.compactor.abortedtxn.threshold Default: 1000 Metastore Number of aborted transactions involving a given table or partition that will trigger a major compaction.   hive.compactor.aborted.txn.time.threshold Default: 12h Metastore Age of table/partition\u0026rsquo;s oldest aborted transaction when compaction will be triggered. Default time unit is: hours. Set to a negative number to disable.   hive.compactor.max.num.delta Default: 500 Metastore Maximum number of delta files that the compactor will attempt to handle in a single job (as of Hive 1.3.0).4   hive.compactor.job.queue Default: \u0026quot;\u0026quot; (empty string)  Metastore  Used to specify name of Hadoop queue to which Compaction jobs will be submitted. Set to empty string to let Hadoop choose the queue (as of Hive 1.3.0).   hive.compactor.request.queue Default: 1 HiveServer2 Enables parallelization of the checkForCompaction operation, that includes many file metadata checksand may be expensive   hive.split.grouping.mode Default: query (Allowed values: query, compactor) HiveServer2 This is set to compactor from within the query based compactor. This enables the Tez SplitGrouper to group splits based on their bucket number, so that all rows from different bucket files for the same bucket number can end up in the same bucket file after the compaction.   hive.txn.xlock.iow Default: true HiveServer2 Ensures commands with OVERWRITE (such as INSERT OVERWRITE) acquire Exclusive locks fortransactional tables. This ensures that inserts (w/o overwrite) running concurrentlyare not hidden by the INSERT OVERWRITE.   hive.txn.xlock.write Default: true HiveServer2 Manages concurrency levels for ACID resources. Provides better level of query parallelism by enabling shared writes and write-write conflict resolution at the commit step.- If true - exclusive writes are used: - INSERT OVERWRITE acquires EXCLUSIVE locks - UPDATE/DELETE acquire EXCL_WRITE locks - INSERT acquires SHARED_READ locks- If false - shared writes, transaction is aborted in case of conflicting changes: - INSERT OVERWRITE acquires EXCL_WRITE locks - INSERT/UPDATE/DELETE acquire SHARED_READ locks   metastore.acidmetrics.ext.on Default: true HiveServer2 Whether to collect additional acid related metrics outside of the acid metrics service. (metastore.metrics.enabled and/or hive.server2.metrics.enabled are also required to be set to true.)   Compaction History      hive.compactor.history.retention.succeeded deprecated. Use metastore.compactor.history.retention.succeeded instead Default: 3 Metastore Number of successful compaction entries to retain in history (per partition).   hive.compactor.history.retention.failed deprecated. Use metastore.compactor.history.retention.failed instead. Default: 3 Metastore Number of failed compaction entries to retain in history (per partition).   hive.compactor.history.retention.attempted deprecated. Use metastore.compactor.history.retention.did.not.initiate instead. Default: 2 Metastore Number of attempted compaction entries to retain in history (per partition).   hive.compactor.initiator.failed.compacts.threshold deprecated. Use metastore.compactor.initiator.failed.compacts.threshold instead. Default: 2 Metastore Number of of consecutive failed compactions for a given partition after which the Initiator will stop attempting to schedule compactions automatically. It is still possible to use ALTER TABLE to initiate compaction. Once a manually initiated compaction succeeds auto initiated compactions will resume. Note that this must be less than hive.compactor.history.retention.failed.   metastore.compactor.initiator.failed.compacts.threshold Default: 2 (Allowed between 1 and 20) Metastore Number of consecutive compaction failures (per table/partition) after which automatic compactions will not be scheduled any more. Note that this must be less than hive.compactor.history.retention.failed.   hive.compactor.history.reaper.interval deprecated. metastore.acid.housekeeper.interval handles it. Default: 2m Metastore Controls how often the process to purge historical record of compactions runs.   ACID metrics      metastore.acidmetrics.check.interval Default: 300s Metastore Time in seconds between acid related metric collection runs.   metastore.acidmetrics.thread.on Default: true Metastore Whether to run acid related metrics collection on this metastore instance.   metastore.deltametrics.delta.num.threshold Deafult: 100 Metastore The minimum number of active delta files a table/partition must have in order to be included in the ACID metrics report.   metastore.deltametrics.delta.pct.threshold Default: 0.01 Metastore Percentage (fractional) size of the delta files relative to the base directory. Deltas smaller than this threshold count as small deltas. Default 0.01 = 1%.)   metastore.deltametrics.max.cache.size Default: 100 (Allowed between 0 and 500) Metastore Size of the ACID metrics cache, i.e. max number of partitions and unpartitioned tables with the most deltas that will be included in the lists of active, obsolete and small deltas. Allowed range is 0 to 500.   metastore.deltametrics.obsolete.delta.num.threshold Default: 100 Metastore The minimum number of obsolete delta files a table/partition must have in order to be included in the ACID metrics report.    1metastore.txn.max.open.batch controls how many transactions streaming agents such as Flume or Storm open simultaneously. The streaming agent then writes that number of entries into a single file (per Flume agent or Storm bolt). Thus increasing this value decreases the number of delta files created by streaming agents. But it also increases the number of open transactions that Hive has to track at any given time, which may negatively affect read performance.\n 2Worker threads spawn MapReduce jobs to do compactions. They do not do the compactions themselves. Increasing the number of worker threads will decrease the time it takes tables or partitions to be compacted once they are determined to need compaction. It will also increase the background load on the Hadoop cluster as more MapReduce jobs will be running in the background. Each compaction can handle one partition at a time (or whole table if it\u0026rsquo;s unpartitioned). 3Decreasing this value will reduce the time it takes for compaction to be started for a table or partition that requires compaction. However, checking if compaction is needed requires several calls to the NameNode for each table or partition that has had a transaction done on it since the last major compaction. So decreasing this value will increase the load on the NameNode.\n4If the compactor detects a very high number of delta files, it will first run several partial minor compactions (currently sequentially) and then perform the compaction actually requested.\n5If the value is not the same active transactions may be determined to be \u0026ldquo;timed out\u0026rdquo; and consequently Aborted. This will result in errors like \u0026ldquo;No such transaction\u0026hellip;\u0026rdquo;, \u0026ldquo;No such lock \u0026hellip;\u0026rdquo;\nConfiguration Values to Set for Hive ACID (INSERT, UPDATE, DELETE) In addition to the new parameters listed above, some existing parameters need to be set to support *INSERT \u0026hellip; VALUES, UPDATE,*and DELETE.\n   Configuration key Must be set to     hive.support.concurrency true (default is false)   hive.enforce.bucketing true (default is false) (Not required as of Hive 2.0)   hive.exec.dynamic.partition.mode nonstrict (default is strict)    Configuration Values to Set for Compaction If the data in your system is not owned by the Hive user (i.e., the user that the Hive metastore runs as), then Hive will need permission to run as the user who owns the data in order to perform compactions. If you have already set up HiveServer2 to impersonate users, then the only additional work to do is assure that Hive has the right to impersonate users from the host running the Hive metastore. This is done by adding the hostname to hadoop.proxyuser.hive.hosts in Hadoop\u0026rsquo;s core-site.xml file. If you have not already done this, then you will need to configure Hive to act as a proxy user. This requires you to set up keytabs for the user running the Hive metastore and add hadoop.proxyuser.hive.hosts and hadoop.proxyuser.hive.groups to Hadoop\u0026rsquo;s core-site.xml file. See the Hadoop documentation on secure mode for your version of Hadoop (e.g., for Hadoop 2.5.1 it is at Hadoop in Secure Mode).\nCompaction pooling More in formation on compaction pooling can be found here: Compaction pooling\nTable Properties If a table is to be used in ACID writes (insert, update, delete) then the table property \u0026ldquo;transactional=true\u0026rdquo; must be set on that table. Note, once a table has been defined as an ACID table via TBLPROPERTIES (\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;true\u0026rdquo;), it cannot be converted back to a non-ACID table, i.e., changing TBLPROPERTIES (\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;false\u0026rdquo;) is not allowed. Also, hive.txn.manager must be set to org.apache.hadoop.hive.ql.lockmgr.DbTxnManager either in hive-site.xml or in the beginning of the session before any query is run. Without those, inserts will be done in the old style; updates and deletes will be prohibited. Operations on ACID tables without DbTxnManager are not allowed. Additional to \u0026ldquo;transactional=true\u0026rdquo;, \u0026ldquo;transactional_properties=insert_only\u0026rdquo; can be defined to allow only insert transactions.\nIf a table owner does not wish the system to automatically determine when to compact, then the table property \u0026ldquo;NO_AUTO_COMPACTION\u0026rdquo; can be set. This will prevent all automatic compactions. Manual compactions can still be done with Alter Table/Partition Compact statements.\nTable properties are set with the TBLPROPERTIES clause when a table is created or altered, as described in the Create Table and Alter Table Properties sections of Hive Data Definition Language. The \u0026ldquo;transactional\u0026rdquo; and \u0026ldquo;NO_AUTO_COMPACTION\u0026rdquo; table properties are case-insensitive.\nMore compaction related options can be set via TBLPROPERTIES. They can be set at both table-level via CREATE TABLE, and on request-level via ALTER TABLE/PARTITION COMPACT. These are used to override the Warehouse/table wide settings. For example, to override an MR property to affect a compaction job, one can add \u0026ldquo;compactor.=\u0026rdquo; in either CREATE TABLE statement or when launching a compaction explicitly via ALTER TABLE. The \u0026ldquo;=\u0026rdquo; will be set on JobConf of the compaction MR job. Similarly, \u0026ldquo;tblprops.=\u0026rdquo; can be used to set/override any table property which is interpreted by the code running on the cluster. Finally, \u0026ldquo;compactorthreshold.=\u0026rdquo; can be used to override properties from the \u0026ldquo;New Configuration Parameters for Transactions\u0026rdquo; table above that end with \u0026ldquo;.threshold\u0026rdquo; and control when compactions are triggered by the system. Examples:\nExample: Set compaction options in TBLPROPERTIES at table level\nCREATE TABLE table_name ( id int, name string ) CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES (\u0026quot;transactional\u0026quot;=\u0026quot;true\u0026quot;, \u0026quot;compactor.mapreduce.map.memory.mb\u0026quot;=\u0026quot;2048\u0026quot;, -- specify compaction map job properties \u0026quot;compactorthreshold.hive.compactor.delta.num.threshold\u0026quot;=\u0026quot;4\u0026quot;, -- trigger minor compaction if there are more than 4 delta directories \u0026quot;compactorthreshold.hive.compactor.delta.pct.threshold\u0026quot;=\u0026quot;0.5\u0026quot; -- trigger major compaction if the ratio of size of delta files to -- size of base files is greater than 50% ); Example: Set compaction options in TBLPROPERTIES at request level\nALTER TABLE table_name COMPACT 'minor' WITH OVERWRITE TBLPROPERTIES (\u0026quot;compactor.mapreduce.map.memory.mb\u0026quot;=\u0026quot;3072\u0026quot;); -- specify compaction map job properties ALTER TABLE table_name COMPACT 'major' WITH OVERWRITE TBLPROPERTIES (\u0026quot;tblprops.orc.compress.size\u0026quot;=\u0026quot;8192\u0026quot;); -- change any other Hive table properties Talks and Presentations The Art of Compaction by Kokila N at a Cloudera meetup.\nTransactional Operations In Hive by Eugene Koifman at Dataworks Summit 2017, San Jose, CA, USA\n Slides Video  DataWorks Summit 2018, San Jose, CA, USA - Covers Hive 3 and ACID V2 features\n Slides Video  Save\nSave\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/283118453/","tags":null,"title":"Apache Hive : Hive Transactions (Hive ACID)"},{"categories":null,"contents":"Apache Hive : Hive UDFs  Built-in Aggregate Functions (UDAF) Built-in Table-Generating Functions (UDTF) String Functions Date Functions Mathematical Functions Collection Functions Type Conversion Functions Conditional Functions Data Masking Functions Miscellaneous Functions Geospatial Creating Custom UDF\u0026rsquo;s  Hive User-Defined Functions (UDFs) are custom functions developed in Java and seamlessly integrated with Apache Hive. UDFs are routines designed to accept parameters, execute a specific action, and return the resulting value. The return value can either be a single scalar row or a complete result set, depending on the UDF\u0026rsquo;s code and the implemented interface. UDFs represent a powerful capability that enhances classical SQL functionality by allowing the integration of custom code, providing Hive users with a versatile toolset. Apache Hive comes equipped with a variety of built-in UDFs that users can leverage. Similar to other SQL-based solutions, Hive also offers functionality to expand its already rich set of UDFs by incorporating custom ones as needed.\nImportant\nEvery UDF\u0026rsquo;s evaluate method is one row at a time! This means if your UDFs has complex code, it could introduce performance issue in execution time.\nTo gain an understanding of UDFs, let\u0026rsquo;s begin with an example to comprehend their execution pattern.\nUDF\u0026rsquo;s in action\nSELECT length(string_col) FROM table_name; In this case, the length of thebuilt-in UDF evaluates each row of the string_col values. As we see how a built-in UDF works let\u0026rsquo;s see what kind of built-in UDFs the Apache Hive has.\nTipp\nAll Hive keywords are case-insensitive, including the names of Hive operators and functions. I.e: length and LENGTH are also accepted by the Hive.\nBuilt-in Aggregate Functions (UDAF) Hive Aggregate Functions (UDAF) take multiple rows as input and return a single row as output. Depending on the functionality it mostly aggregates the values and returns a single result. These functions are mainly used in the GROUP BY statements.\nThese functions can be used without GROUP BY as well.    Return Type Name(Signature) Description Source code     bigint count(*) count(expr) count(DISTINCT expr[, expr...]) count(*) - Returns the total number of retrieved rows, including rows containing NULL values.count(expr) - Returns the number of rows for which the supplied expression is non-NULL.count(DISTINCT expr[, expr]) - Returns the number of rows for which the supplied expression(s) are unique and non-NULL. Execution of this can be optimized with hive.optimize.distinct.rewrite. GenericUDAFCount   double sum(col), sum(DISTINCT col) Returns the sum of the elements in the group or the sum of the distinct values of the column in the group. MODIFIED GenericUDAFSum   double avg(col), avg(DISTINCT col) Returns the average of the elements in the group or the average of the distinct values of the column in the group. GenericUDAFAverage   double min(col) Returns the minimum of the column in the group. GenericUDAFMin   double max(col) Returns the maximum value of the column in the group. GenericUDAFMax   double variance(col), var_pop(col) Returns the variance of a numeric column in the group. MODIFIED GenericUDAFVariance   double var_samp(col) Returns the unbiased sample variance of a numeric column in the group. GenericUDAFVarianceSample   double stddev_pop(col) Returns the standard deviation of a numeric column in the group. GenericUDAFStd   double stddev_samp(col) Returns the unbiased sample standard deviation of a numeric column in the group. GenericUDAFStdSample   double covar_pop(col1, col2) Returns the population covariance of a pair of numeric columns in the group. GenericUDAFCovariance   double covar_samp(col1, col2) Returns the sample covariance of a pair of numeric columns in the group. GenericUDAFCovarianceSample   double corr(col1, col2) Returns the Pearson coefficient of correlation of a pair of numeric columns in the group. GenericUDAFCorrelation   double percentile(bigint col, p) Returns the exact pth percentile of a column in the group (does not work with floating point types). p must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral. UDAFPercentile   array percentile(bigint col, array(p1 [, p2]...)) Returns the exact percentiles p1, p2, \u0026hellip; of a column in the group (does not work with floating point types). pi must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral.    double percentile_approx(double col, p [, B]) Returns an approximate pth percentile of a numeric column (including floating point types) in the group. The B parameter controls approximation accuracy at the cost of memory. Higher values yield better approximations, and the default is 10,000. When the number of distinct values in col is smaller than B, this gives an exact percentile value. GenericUDAFPercentileApprox   array percentile_approx(double col, array(p1 [, p2]...) [, B]) Same as above, but accepts and returns an array of percentile values instead of a single one.    double regr_avgx(independent, dependent) Equivalent to avg(dependent).     double regr_avgy(independent, dependent) Equivalent to avg(independent).     double regr_count(independent, dependent) Returns the number of non-null pairs used to fit the linear regression line.     double regr_intercept(independent, dependent) Returns the y-intercept of the linear regression line, i.e. the value of b in the equation dependent = a * independent + b.    double regr_r2(independent, dependent) Returns the coefficient of determination for the regression.     double regr_slope(independent, dependent) Returns the slope of the linear regression line, i.e. the value of an in the equation dependent = a * independent + b.     double regr_sxx(independent, dependent) Equivalent to regr_count(independent, dependent) * var_pop(dependent).     double regr_sxy(independent, dependent) Equivalent to regr_count(independent, dependent) * covar_pop(independent, dependent).     double regr_syy(independent, dependent) Equivalent to regr_count(independent, dependent) * var_pop(independent).    array\u0026lt;struct {'x','y'}\u0026gt; histogram_numeric(col, b) Computes a histogram of a numeric column in the group using b non-uniformly spaced bins. The output is an array of size b of double-valued (x,y) coordinates that represent the bin centers and heights GenericUDAFHistogramNumeric   array collect_set(col) Returns a set of objects with duplicate elements eliminated. GenericUDAFCollectSet   array collect_list(col) Returns a list of objects with duplicates.  GenericUDAFCollectList   int ntile(integer x) Divides an ordered partition into x groups called buckets and assign a bucket number to each row in the partition. This allows easy calculation of tertiles, quartiles, deciles, percentiles, and other common summary statistics. GenericUDAFNTile    Most of the UDAFs ignore NULL values. Built-in Table-Generating Functions (UDTF) Normal user-defined functions, such as concat(), take in a single input row and output a single output row. In contrast, table-generating functions transform a single input row into multiple output rows. UDTF is one of the most advanced functions.\n   Row-set columns types Name(Signature) Description Source Code     T explode(ARRAY\u0026lt;T\u0026gt; a) Explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array. GenericUDTFExplode   Tkey,Tvalue explode(MAP\u0026lt;Tkey,Tvalue\u0026gt; m) Explodes a map to multiple rows. Returns a row-set with two columns (key,value) , one row for each key-value pair from the input map.    int,T posexplode(ARRAY\u0026lt;T\u0026gt; a) Explodes an array to multiple rows with an additional positional column ofint type (position of items in the original array, starting with 0). Returns a row-set with two columns (pos,val), one row for each element from the array. GenericUDTFPosExplode   T1,\u0026hellip;,Tn inline(ARRAY\u0026lt;STRUCT\u0026lt;f1:T1,...,fn:Tn\u0026gt;\u0026gt; a) Explodes an array of structs to multiple rows. Returns a row-set with N columns (N = number of top level elements in the struct), one row per struct from the array. GenericUDTFInline   T1,\u0026hellip;,Tn/r stack(int r,T1 V1,...,Tn/r Vn) Breaks up n values V1,\u0026hellip;,Vninto r rows. Each row will have n/r columns. r must be constant. GenericUDTFStack   string1,\u0026hellip;,stringn json_tuple(string jsonStr, string k1,...,string kn) Takes JSON string and a set of n keys, and returns a tuple of n values. This is a more efficient version of the get_json_object UDF because it can get multiple keys with just one call. GenericUDTFJSONTuple   string 1,\u0026hellip;,stringn parse_url_tuple(string urlStr, string p1,...,string pn) Takes URLstring and a set of n URL parts, and returns a tuple of n values. This is similar to the parse_url() UDF but can extract multiple parts at once out of a URL. Valid part names are HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:. GenericUDTFParseUrlTuple    String Functions There is no good engine without string manipulation functions. Apache Hive has rich built-instring functions.    Return Type Name(Signature) Description Source code     int ascii(string str) Returns the numeric value of the first character of str. UDFAscii   string base64(binary bin) Converts the argument from binary to a base64string. UDFBase64   int character_length(string str) Returns the number of UTF-8 characters contained in str. The function char_length is shorthand for this function. GenericUDFCharacterLength   string chr(bigint|double A) Returns the ASCII character having the binary equivalent to A. If A is larger than 256 the result is equivalent to chr(A % 256). Example: select chr(88); returns \u0026ldquo;X\u0026rdquo;. UDFChr   string concat(string|binary A,string|binary B...) Returns the string or bytes resulting from concatenating the strings or bytes passed in as parameters in order. For example, concat(\u0026lsquo;foo\u0026rsquo;, \u0026lsquo;bar\u0026rsquo;) results in \u0026lsquo;foobar\u0026rsquo;. Note that this function can take any number of input strings. GenericUDFConcat   array\u0026lt;struct\u0026lt;string,double\u0026raquo; context_ngrams(array\u0026lt;array\u0026lt;string\u0026gt;\u0026gt;, array\u0026lt;string\u0026gt;, int K, int pf) Returns the top-k contextual N-grams from a set of tokenized sentences, given a string of \u0026ldquo;context\u0026rdquo;. See StatisticsAndDataMining for more information. GenericUDAFContextNGrams   string concat_ws(string SEP, string A, string B...) Like concat() above, but with custom separator SEP. GenericUDFConcatWS   string concat_ws(string SEP, array\u0026lt;string\u0026gt;) Like concat_ws() above, but taking an array ofstrings.    string decode(binary bin, string charset) Decodes the first argument into a string using the provided character set (one of \u0026lsquo;US-ASCII\u0026rsquo;, \u0026lsquo;ISO-8859-1\u0026rsquo;, \u0026lsquo;UTF-8\u0026rsquo;, \u0026lsquo;UTF-16BE\u0026rsquo;, \u0026lsquo;UTF-16LE\u0026rsquo;, \u0026lsquo;UTF-16\u0026rsquo;). If either argument is null, the result will also be null. GenericUDFDecode   string elt(N int, str1 string, str2s tring, str3 string,...) Return string at index number. For example elt(2,\u0026lsquo;hello\u0026rsquo;,\u0026lsquo;world\u0026rsquo;) returns \u0026lsquo;world\u0026rsquo;. Returns NULL if N is less than 1 or greater than the number of arguments. GenericUDFElt   binary encode(string src, string charset) Encodes the first argument into a BINARY using the provided character set (one of \u0026lsquo;US-ASCII\u0026rsquo;, \u0026lsquo;ISO-8859-1\u0026rsquo;, \u0026lsquo;UTF-8\u0026rsquo;, \u0026lsquo;UTF-16BE\u0026rsquo;, \u0026lsquo;UTF-16LE\u0026rsquo;, \u0026lsquo;UTF-16\u0026rsquo;). If either argument is null, the result will also be null.  GenericUDFEncode   int field(val T, val1 T, val2 T, val3 T,...) Returns the index of val in the val1,val2,val3,\u0026hellip; list or 0 if not found. For example field(\u0026lsquo;world\u0026rsquo;,\u0026lsquo;say\u0026rsquo;,\u0026lsquo;hello\u0026rsquo;,\u0026lsquo;world\u0026rsquo;) returns 3.All primitive types are supported, arguments are compared using str.equals(x). If val is NULL, the return value is 0. GenericUDFField   int find_in_set(string str, string strList) Returns the first occurrence of str in strList where strList is a comma-delimited string. Returns null if either argument is null. Returns 0 if the first argument contains any commas. For example, find_in_set(\u0026lsquo;ab\u0026rsquo;, \u0026lsquo;abc,b,ab,c,def\u0026rsquo;) returns 3. UDFFindInSet   string format_number(number x, int d) Formats the number X to a format like \u0026lsquo;#,###,###.##\u0026rsquo;, rounded to D decimal places, and returns the result as a string. If D is 0, the result has no decimal point or fractional part. GenericUDFFormatNumber   string get_json_object(string json_string,string path) Extracts JSON object from a JSON string based on JSON path specified, and returns JSON string of the extracted JSON object. It will return null if the input JSON string is invalid.NoteThe json path can only have the characters [0-9a-z_], i.e., no upper-case or special characters. Also, the keys cannot start with numbers. This is due to restrictions on Hive column names. UDFJson   boolean in_file(string str, string filename) Returns true if the string str appears as an entire line in filename. GenericUDFInFile   int instr(string str, string substr) Returns the position of the first occurrence of substr in str. Returns null if either of the arguments are null and returns 0 if substr could not be found in str. Be aware that this is not zero-based. The first character in str has index 1. GenericUDFInstr   int length(string A) Returns the length of the string. GenericUDFLength   int locate(string substr,string str[,int pos]) Returns the position of the first occurrence of substr in str after position pos. GenericUDFLocate   string lower(string A) lcase(string A) Returns the string resulting from converting all characters of B to lowercase. For example, lower(\u0026lsquo;fOoBaR\u0026rsquo;) results in \u0026lsquo;foobar\u0026rsquo;. GenericUDFLower   string lpad(string str,int len,string pad) Returns str, left-padded with pad to a length of len. If str is longer than len, the return value is shortened to len characters. In the case of an empty padstring, the return value is null. GenericUDFLpad   string ltrim(string A) Returns the string resulting from trimming spaces from the beginning(left-hand side) of A. For example, ltrim(' foobar \u0026lsquo;) results in \u0026lsquo;foobar \u0026lsquo;. GenericUDFLTrim   array\u0026lt;struct\u0026lt;string,double\u0026raquo; ngrams(array\u0026lt;array\u0026lt;string\u0026gt;\u0026gt;,int N,int K,int pf) Returns the top-k N-grams from a set of tokenized sentences, such as those returned by the sentences() UDAF. See StatisticsAndDataMining for more information. GenericUDAFnGrams   int octet_length(string str) Returns the number of octets required to hold the string str in UTF-8 encoding. Note that octet_length(str) can be larger than character_length(str). GenericUDFOctetLength   string parse_url(string urlString,string partToExtract [,string keyToExtract]) Returns the specified part from the URL. Valid values for partToExtract include HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. For example, parse_url('http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1\u0026rsquo;, \u0026lsquo;HOST\u0026rsquo;) returns \u0026lsquo;facebook.com\u0026rsquo;. Also, a value of a particular key in QUERY can be extracted by providing the key as the third argument, for example, parse_url('http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1\u0026rsquo;, \u0026lsquo;QUERY\u0026rsquo;, \u0026lsquo;k1\u0026rsquo;) returns \u0026lsquo;v1\u0026rsquo;. UDFParseUrl   string printf(string format, Obj... args) Returns the input formatted according to printf-style formatstrings. GenericUDFPrintf   string quote(string text) Returns the quoted string. NEW Includes escape character for any single quotes in Apache Hive 4.0.0 GenericUDFQuote       Input Output     NULL NULL   DONT \u0026lsquo;DONT\u0026rsquo;   DON\u0026rsquo;T \u0026lsquo;DON'T\u0026rsquo;       Return Type Name(Signature) Description Source code     string regexp_extract(string subject, string pattern, int index) Returns the string extracted using the pattern. For example, regexp_extract(\u0026lsquo;foothebar\u0026rsquo;, \u0026lsquo;foo(.*?)(bar)\u0026rsquo;, 2) returns \u0026lsquo;bar.\u0026rsquo; Note that some care is necessary in using predefined character classes: using \u0026lsquo;\\s\u0026rsquo; as the second argument will match the letter s; \u0026lsquo;\\s\u0026rsquo; is necessary to match whitespace, etc. The \u0026lsquo;index\u0026rsquo; parameter is the Java regex Matcher group() method index.  UDFRegExpExtract   string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT) Returns the string resulting from replacing all substrings in INITIAL_STRING that match the java regular expression syntax defined in PATTERN with instances of REPLACEMENT. For example, regexp_replace(\u0026ldquo;foobar\u0026rdquo;, \u0026ldquo;oo|ar\u0026rdquo;, \u0026ldquo;\u0026quot;) returns \u0026lsquo;fb.\u0026rsquo; Note that some care is necessary in using predefined character classes: using \u0026lsquo;\\s\u0026rsquo; as the second argument will match the letter s; \u0026lsquo;\\s\u0026rsquo; is necessary to match whitespace, etc. UDFRegExpReplace   string repeat(string str, int n) Repeats str in n times. UDFRepeat   string replace(string A, string OLD, string NEW) Returns the string A with all non-overlapping occurrences of OLD replaced with NEW. Example: select replace(\u0026ldquo;ababab\u0026rdquo;, \u0026ldquo;abab\u0026rdquo;, \u0026ldquo;Z\u0026rdquo;); returns \u0026ldquo;Zab\u0026rdquo;. UDFReplace   string reverse(string A) Returns the reversed string. UDFReverse   string rpad(string str,int len,string pad) Returns str, right-padded with pad to a length of len. If str is longer than len, the return value is shortened to len characters. In the case of an empty padstring, the return value is null. GenericUDFRpad   string rtrim(string A) Returns the string resulting from trimming spaces from the end(right-hand side) of A. For example, rtrim(\u0026rsquo; foobar \u0026lsquo;) results in ' foobar\u0026rsquo;. GenericUDFRTrim   array\u0026lt;array\u0026gt; sentences(string str,string lang,string locale) Tokenizes a string of natural language text into words and sentences, where each sentence is broken at the appropriate sentence boundary and returned as an array of words. The \u0026lsquo;lang\u0026rsquo; and \u0026lsquo;locale\u0026rsquo; are optional arguments. For example, sentences(\u0026lsquo;Hello there! How are you?') returns ( (\u0026ldquo;Hello\u0026rdquo;, \u0026ldquo;there\u0026rdquo;), (\u0026ldquo;How\u0026rdquo;, \u0026ldquo;are\u0026rdquo;, \u0026ldquo;you\u0026rdquo;) ). GenericUDFSentences   string space(int n) Returns a string of n spaces. UDFSpace   array split(string str, string pat) Splits str around pat (pat is a regular expression). GenericUDFSplit   map\u0026lt;string,string\u0026gt; str_to_map(text[, delimiter1, delimiter2]) Splits text into key-value pairs using two delimiters. Delimiter1 separates text into K-V pairs, and Delimiter2 splits each K-V pair. Default delimiters are \u0026lsquo;,\u0026rsquo; for delimiter1 and \u0026lsquo;:\u0026rsquo; for delimiter2. GenericUDFStringToMap   string substr(string|binary A,int start) substring(string|binary A,int start) Returns the substring or slice of the byte array of A starting from start position till the end of string A. For example, substr(\u0026lsquo;foobar\u0026rsquo;, 4) results in \u0026lsquo;bar\u0026rsquo;. GenericUDFSubstringIndex   string substr(string|binary A,int start,int len) substring(string|binary A,int start,int len) Returns the substring or slice of the byte array of A starting from start position with length len. For example, substr(\u0026lsquo;foobar\u0026rsquo;, 4, 1) results in \u0026lsquo;b\u0026rsquo;.    string substring_index(string A,string delim,int count) Returns the substring from string A before count occurrences of the delimiter delim. If the count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. Substring_index performs a case-sensitive match when searching for delim. Example: substring_index('www.apache.org\u0026rsquo;, \u0026lsquo;.\u0026rsquo;, 2) = \u0026lsquo;www.apache\u0026rsquo;. GenericUDFSubstringIndex   string translate(string|char|varchar input,string|char|varchar from, string|char|varchar to) Translates the input string by replacing the characters present in the from string with the corresponding characters in the tostring. This is similar to the translate function in PostgreSQL. If any of the parameters to this UDF are NULL, the result is NULL as well.  GenericUDFTranslate   string trim(string A) Returns the string resulting from trimming spaces from both ends of A. For example, trim(' foobar \u0026lsquo;) results in \u0026lsquo;foobar\u0026rsquo; GenericUDFTrim   binary unbase64(string str) Converts the argument from a base64 string to BINARY. UDFUnbase64   string upper(string A) ucase(string A) Returns the string resulting from converting all characters of A to upper case. For example, upper(\u0026lsquo;fOoBaR\u0026rsquo;) results in \u0026lsquo;FOOBAR\u0026rsquo;. GenericUDFUpper   string initcap(string A) Returns string, with the first letter of each word in uppercase, and all other letters in lowercase. Words are delimited by whitespace. GenericUDFInitCap   int levenshtein(string A, string B) Returns the Levenshtein distance between two strings. Example: levenshtein(\u0026lsquo;kitten\u0026rsquo;, \u0026lsquo;sitting\u0026rsquo;) results in 3. GenericUDFLevenshtein   string soundex(string A) Returns the soundex code of the string. Example: soundex(\u0026lsquo;Miller\u0026rsquo;) results in M460. GenericUDFSoundex   string deserialize(base64 encoded message, compressionFormat) NEW Returns plain text string of given message which was compressed in compressionFormat and base64 encoded. Currently, Supports only \u0026lsquo;gzip\u0026rsquo; for Gzip compressed and base 64 encoded strings.Example: deserialize(\u0026lsquo;H4sIAAAAAAAA/ytJLS4BAAx+f9gEAAAA\u0026rsquo;, \u0026lsquo;gzip\u0026rsquo;) Result: test GenericUDFDeserialize    Date Functions In many analytical workloads Date is one of the most used built-in functions in Hive. The following list contains the supported built-in date functions in Hive.\n   Return Type Name(Signature) Description Source code     string from_unixtime(bigint unixtime[,string pattern]) Converts a number of seconds since epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current time zone(using config \u0026ldquo;hive.local.time.zone\u0026rdquo;) using the specified pattern. If the pattern is missing the default is used (\u0026lsquo;uuuu-MM-dd HH:mm:ss\u0026rsquo; or yyyy-MM-dd HH:mm:ss\u0026rsquo;). Example: from_unixtime(0)=1970-01-01 00:00:00 (hive.local.time.zone=Etc/GMT)MODIFIEDAs of Hive 4.0.0 the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation and as a consequence the accepted patterns and their behavior. Earlier versions used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter. GenericUDFFromUnixTime   bigint unix_timestamp() Gets the current Unix timestamp in seconds. This function is not deterministic and its value is not fixed for the scope of a query execution, therefore prevents proper optimization of queries - this has been deprecated since 2.0 in favor of CURRENT_TIMESTAMP constant. GenericUDFUnixTimeStamp   bigint unix_timestamp(string date) Converts a DateTimestring to unix time (seconds since epoch) using the default pattern(s). The default accepted patterns depend on the underlying formatter implementation. The DateTime string does not contain a timezone so the conversion uses the local time zone as specified by \u0026ldquo;hive.local.time.zone\u0026rdquo; property. Returns null when the conversion fails. Example: unix_timestamp(\u0026lsquo;2009-03-20 11:30:01\u0026rsquo;) = 1237573801MODIFIEDAs of Hive 4.0.0 the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation and as a consequence the accepted patterns and their behavior. Earlier versions used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter.    bigint unix_timestamp(string date,string pattern) Converts a date-time string to unix time (seconds since epoch) using the specified pattern. The accepted patterns and their behavior depend on the underlying formatter implementation. Returns null when the conversion fails. Example: unix_timestamp(\u0026lsquo;2009-03-20\u0026rsquo;, \u0026lsquo;uuuu-MM-dd\u0026rsquo;) = 1237532400MODIFIEDAs of Hive 4.0.0 the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation and as a consequence the accepted patterns and their behavior. Earlier versions used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter.    date to_date(string timestamp) Returns the date part of a timestamp date object. Example: to_date(\u0026ldquo;1970-01-01 00:00:00\u0026rdquo;)  GenericUDFDate   int year(string date) Returns the year part of a date or a timestamp string: year(\u0026ldquo;1970-01-01 00:00:00\u0026rdquo;) = 1970, year(\u0026ldquo;1970-01-01\u0026rdquo;) = 1970. UDFYear   int quarter(date/timestamp/string) Returns the quarter of the year for a date, timestamp, or string in the range 1 to 4. Example: quarter(\u0026lsquo;2015-04-08\u0026rsquo;) = 2. GenericUDFQuarter   int month(string date) Returns the month part of a date or a timestamp string. Example: month(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 11, month(\u0026ldquo;1970-11-01\u0026rdquo;) = 11. UDFMonth   int day(string date) dayofmonth(date) Returns the day part of a date or a timestamp string. Example: day(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 1, day(\u0026ldquo;1970-11-01\u0026rdquo;) = 1. UDFDayOfMonth   int hour(string date) Returns the hour of the timestamp: Example: hour(\u0026lsquo;2009-07-30 12:58:59\u0026rsquo;) = 12, hour(\u0026lsquo;12:58:59\u0026rsquo;) = 12. UDFHour   int minute(string date) Returns the minute of the timestamp. UDFMinute   int second(string date) Returns the second of the timestamp. UDFSecond   int weekofyear(string date) Returns the week number of a timestamp string. Example: weekofyear(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 44 or weekofyear(\u0026ldquo;1970-11-01\u0026rdquo;) = 44. UDFWeekOfYear   int extract(field FROM source) Retrieve fields such as days or hours from the source. The source must be a date, timestamp, interval, or string that can be converted into either a date or timestamp. Supported fields include day, dayofweek, hour, minute, month, quarter, second, week and year.Examples:1. select extract(month from \u0026ldquo;2016-10-20\u0026rdquo;) results in 10. 2. select extract(hour from \u0026ldquo;2016-10-20 05:06:07\u0026rdquo;) results in 5. 3. select extract(dayofweek from \u0026ldquo;2016-10-20 05:06:07\u0026rdquo;) results in 5. 4. select extract(month from interval \u0026lsquo;1-3\u0026rsquo; year to month) results in 3. 5. select extract(minute from interval \u0026lsquo;3 12:20:30\u0026rsquo; day to second) results in 20.    int datediff(string enddate,string startdate) Returns the number of days from startdate to end date. Example: datediff(\u0026lsquo;2009-03-01\u0026rsquo;, \u0026lsquo;2009-02-27\u0026rsquo;) = 2. GenericUDFDateDiff   date date_add(date/timestamp/string startdate, tinyint/smallint/int days) Adds a number of days to startdate. Example: date_add(\u0026lsquo;2008-12-31\u0026rsquo;, 1) = \u0026lsquo;2009-01-01\u0026rsquo;. GenericUDFDateAdd   date date_sub(date/timestamp/string startdate, tinyint/smallint/int days) Subtracts a number of days to startdate: date_sub(\u0026lsquo;2008-12-31\u0026rsquo;, 1) = \u0026lsquo;2008-12-30\u0026rsquo;. GenericUDFDateSub   timestamp from_utc_timestamp({any primitive type} ts,string timezone) Converts a timestamp* in UTC to a given timezone.* timestamp is a primitive type, including timestamp/date, tinyint/smallint/int/bigint, float/double and decimal.Fractional values are considered as seconds.integer values are considered as milliseconds. For example, from_utc_timestamp(2592000.0,\u0026lsquo;PST\u0026rsquo;), from_utc_timestamp(2592000000,\u0026lsquo;PST\u0026rsquo;) and from_utc_timestamp(timestamp \u0026lsquo;1970-01-30 16:00:00\u0026rsquo;,\u0026lsquo;PST\u0026rsquo;) all return the timestamp 1970-01-30 08:00:00. GenericUDFFromUtcTimestamp   timestamp to_utc_timestamp({any primitive type} ts,string timezone) Converts a timestamp* in a given timezone to UTC.* timestamp is a primitive type, including timestamp/date, tinyint/smallint/int/bigint, float/double and decimal.Fractional values are considered as seconds.integer values are considered as milliseconds. For example, to_utc_timestamp(2592000.0,\u0026lsquo;PST\u0026rsquo;), to_utc_timestamp(2592000000,\u0026lsquo;PST\u0026rsquo;) and to_utc_timestamp(timestamp \u0026lsquo;1970-01-30 16:00:00\u0026rsquo;,\u0026lsquo;PST\u0026rsquo;) all return the timestamp 1970-01-31 00:00:00. GenericUDFToUtcTimestamp   date current_date Returns the current date at the start of query evaluation. All calls of current_date within the same query return the same value. GenericUDFCurrentDate   timestamp current_timestamp Returns the current timestamp at the start of query evaluation. All calls of current_timestamp within the same query return the same value. GenericUDFCurrentTimestamp   string add_months(string start_date,int num_months, output_date_format) Returns the date that is num_months after start_date. start_date is a string, date or timestamp. num_months is an integer. If start_date is the last day of the month or if the resulting month has fewer days than the day component of start_date, then the result is the last day of the resulting month. Otherwise, the result has the same day component as start_date. The default output format is \u0026lsquo;yyyy-MM-dd\u0026rsquo;.MODIFIEDBefore Hive 4.0.0, the time part of the date is ignored. As of Hive 4.0.0, add_months supports an optional argument output_date_format, which accepts a string that represents a valid date format for the output. This allows to retain the time format in the output.For example :add_months(\u0026lsquo;2009-08-31\u0026rsquo;, 1) returns \u0026lsquo;2009-09-30\u0026rsquo;.add_months(\u0026lsquo;2017-12-31 14:15:16\u0026rsquo;, 2, \u0026lsquo;YYYY-MM-dd HH:mm:ss\u0026rsquo;) returns \u0026lsquo;2018-02-28 14:15:16\u0026rsquo;. GenericUDFAddMonths   string last_day(string date) Returns the last day of the month to which the date belongs. date is a string in the format \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo; or \u0026lsquo;yyyy-MM-dd\u0026rsquo;. The time part of the date is ignored! GenericUDFLastDay   string next_day(string start_date,string day_of_week) Returns the first date which is later than start_date and named as day_of_week. start_date is a string/date/timestamp. day_of_week is 2 letters, 3 letters or full name of the day of the week (e.g. Mo, tue, FRIDAY). The time part of start_date is ignored. Example: next_day(\u0026lsquo;2015-01-14\u0026rsquo;, \u0026lsquo;TU\u0026rsquo;) = 2015-01-20. GenericUDFNextDay   string trunc(string date,string format) Returns date truncated to the unit specified by the format. Supported formats: MONTH/MON/MM, YEAR/YYYY/YY. Example: trunc(\u0026lsquo;2015-03-17\u0026rsquo;, \u0026lsquo;MM\u0026rsquo;) = 2015-03-01. GenericUDFTrunc   double months_between(date1, date2) Returns the number of months between dates date1 and date2. If date1 is later than date2, then the result is positive. If date1 is earlier than date , then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always aninteger. Otherwise, the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2. date1 and date2 type can be date, timestamp or string in the format \u0026lsquo;yyyy-MM-dd\u0026rsquo; or \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;. The result is rounded to 8 decimal places. Example: months_between(\u0026lsquo;1997-02-28 10:30:00\u0026rsquo;, \u0026lsquo;1996-10-30\u0026rsquo;) = 3.94959677 GenericUDFMonthsBetween   string date_format(date/timestamp/string ts,string pattern) Converts a date/timestamp/string to a value of string using the specified pattern. The accepted patterns and their behavior depend on the underlying formatter implementation. The pattern argument should be constant. Example: date_format(\u0026lsquo;2015-04-08\u0026rsquo;, \u0026lsquo;y\u0026rsquo;) = \u0026lsquo;2015\u0026rsquo;.date_format can be used to implement other UDFs, e.g.:* dayname(date) is date_format(date, \u0026lsquo;EEEE\u0026rsquo;) dayofyear(date) is date_format(date, \u0026lsquo;D\u0026rsquo;) MODIFIEDAs of Hive 4.0.0 the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation and as a consequence the accepted patterns and their behavior. Earlier versions used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter. GenericUDFDateFormat    Mathematical Functions The following built-in mathematical functions are supported in Hive.    Return Type Name (Signature) Description Source code     double round(double a) Returns the rounded bigint value of a. GenericUDFRound   double round(double a,int d) Returns a rounded to d decimal places.    double bround(double a) Returns the rounded bigint value of a using HALF_EVEN rounding mode. Also known as Gaussian rounding or bankers' rounding. Example: bround(2.5) = 2, bround(3.5) = 4. GenericUDFBRound   double bround(double a,int d) Returns a rounded to d decimal places using HALF_EVEN rounding mode. Example: bround(8.25, 1) = 8.2, bround(8.35, 1) = 8.4.    bigint floor(double a) Returns the maximum bigint value that is equal to or less than a. GenericUDFFloor   bigint ceil(double a), ceiling(double a) Returns the minimum bigint value that is equal to or greater than a. GenericUDFCeil   double rand(), rand(INT seed) Returns a random number (that changes from row to row) that is distributed uniformly from 0 to 1. Specifying the seed will make sure the generated random number sequence is deterministic. UDFRand   double exp(double a), exp(decimal a) Returns ea where e is the base of the natural logarithm. UDFExp   double ln(double a), ln(decimal a) Returns the natural logarithm of the argument a. UDFLn   double log10(double a), log10(decimal a) Returns the base-10 logarithm of the argument a. UDFLog10   double log2(double a), log2(decimal a) Returns the base-2 logarithm of the argument a.  UDFLog2   double log(double base, double a) log(decimal base, decimal a) Returns the base-base logarithm of the argument a. UDFLog   double pow(double a, double p), power(double a, double p) Returns ap. GenericUDFPower   double sqrt(double a), sqrt(decimal a) Returns the square root of a. UDFSqrt   string bin(bigint a) Returns the number in binary format. UDFBin   string hex(bigint a) hex(string a) hex(binary a) If the argument is an int or binary, hex returns the number as a string in hexadecimal format. Otherwise, if the number is a string, it converts each character into its hexadecimal representation and returns the resulting string.  UDFHex   binary unhex(STRING a) Inverse of hex.interprets each pair of characters as a hexadecimal number and converts to the byte representation of the number. UDFUnhex   string conv(bigint num,int from_base,int to_base), conv(STRING num,int from_base,int to_base) Converts a number from a given base to another. UDFConv   double abs(double a) Returns the absolute value. GenericUDFAbs   int or double pmod(INT a,int b), pmod(double a, double b) Returns the positive value of a mod b. GenericUDFOPMod   double sin(double a), sin(decimal a) Returns the sine of a (a is in radians). UDFSin   double asin(double a), asin(decimal a) Returns the arc sin of a if -1\u0026lt;=a\u0026lt;=1 or NULL otherwise.  UDFAsin   double cos(double a), cos(decimal a) Returns the cosine of a (a is in radians).  UDFCos   double acos(double a), acos(decimal a) Returns the arccosine of a if -1\u0026lt;=a\u0026lt;=1 or NULL otherwise. UDFAcos   double tan(double a), tan(decimal a) Returns the tangent of a (a is in radians). UDFTan   double atan(double a), atan(decimal a) Returns the arctangent of a. UDFAtan   double degrees(double a), degrees(decimal a) Converts value of a from radians to degrees.  UDFDegrees   double radians(double a), radians(double a) Converts value of a from degrees to radians.  UDFRadians   int or double positive(INT a), positive(double a) Returns a. GenericUDFOPPositive   int or double negative(INT a), negative(double a) Returns -a. GenericUDFOPNegative   double or int sign(double a), sign(decimal a) Returns the sign of a as \u0026lsquo;1.0\u0026rsquo; (if a is positive) or \u0026lsquo;-1.0\u0026rsquo; (if a is negative), \u0026lsquo;0.0\u0026rsquo; otherwise. The decimal version returns int instead of double.  UDFSign   double e() Returns the value of e. UDFE   double pi() Returns the value of pi. UDFPI   bigint factorial(INT a) Returns the factorial of a Valid a is [0..20]. GenericUDFFactorial   double cbrt(double a) Returns the cube root of a double value. GenericUDFCbrt   int bigint shiftleft(TINYINT|SMALLINT|INT a,int b) shiftleft(bigint a,int b) Bitwise left shift. Shifts a b positions to the left.Returns int for tinyint, smallint andint a. Returns bigint for bigint a. UDFOPBitShiftLeft   int bigint shiftright(TINYINT|SMALLINT|INT a,int b) shiftright(bigint a,int b) Bitwise right shift. Shifts a b positions to the right.Returns int for tinyint, smallint andint a. Returns bigint for bigint a. UDFOPBitShiftRight   int bigint shiftrightunsigned(TINYINT|SMALLINT|INT a,int b), shiftrightunsigned(bigint a,int b) Bitwise unsigned right shift. Shifts a b positions to the right.Returns int for tinyint, smallint andint a. Returns bigint for bigint a. UDFOPBitShiftRightUnsigned   T greatest(T v1, T v2, ...) Returns the greatest value of the list of values. Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with \u0026ldquo;\u0026gt;\u0026rdquo; operator. GenericUDFGreatest   T least(T v1, T v2, ...) Returns the least value of the list of values. Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with \u0026ldquo;\u0026lt;\u0026rdquo; operator. GenericUDFLeast   int width_bucket(NUMERIC expr, NUMERIC min_value, NUMERIC max_value,int num_buckets) Returns an integer between 0 and num_buckets+1 by mapping expr into the ith equally sized bucket. Buckets are made by dividing [min_value, max_value]into equally sized regions. If expr \u0026lt; min_value, return 1, if expr \u0026gt; max_value return num_buckets+1. See https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions214.htm GenericUDFWidthBucket   double cosh(double x) NEW Returns the hyperbolic cosine of x, where x is in radians. Example: cosh(0) Result: 1 UDFCosh   double tanh(double x) NEW Returns the hyperbolic tangent of x, where x is in radians. Example: tanh(0) Result: 1 UDFTanh    Collection Functions The following built-in collection functions are supported in Hive.    Return Type Name(Signature) Description OSS     int size(Map\u0026lt;K.V\u0026gt;) Returns the number of elements in the map type.    int size(Array\u0026lt;T\u0026gt;) Returns the number of elements in the array type.    array map_keys(Map\u0026lt;K.V\u0026gt;) Returns an unordered array containing the keys of the input map.    array map_values(Map\u0026lt;K.V\u0026gt;) Returns an unordered array containing the values of the input map.    boolean array_contains(Array\u0026lt;T\u0026gt;, value) Returns TRUE if the array contains the provided parameter value.    array sort_array(Array\u0026lt;T\u0026gt;) Sorts the input array in ascending order according to the natural ordering of the array elements and returns it.    array array(obj1, obj2, .... objN) NEW The function returns an array of the same type as the input array with distinct values. Example: array(\u0026lsquo;b\u0026rsquo;, \u0026rsquo;d', \u0026rsquo;d', \u0026lsquo;a\u0026rsquo;) reurtns [\u0026lsquo;b\u0026rsquo;, \u0026rsquo;d', \u0026lsquo;a\u0026rsquo;]  GenericUDFArrayDistinct.java   array array_slice(array, start, length) NEW Returns the subset or range of elements. Example: array-slice(array(1, 2, 3, 4), 2 , 2) Result: 3,4 GenericUDFArraySlice    t array_min((array(obj1, obj2, obj3...)) NEW The function returns the minimum value in the array with elements for which order is supported. Example: array_min(array(1, 3, 0, NULL)) Result: 0 GenericUDFArrayMin   t array_max((array(obj1, obj2, obj3...)) NEW The function returns the maximum value in the array with elements for which order is supported. Example: array_max(array(1, 3, 0, NULL)) Result: 3 GenericUDFArrayMax   t array_distinct(array(obj1, obj2, obj3...)) NEW The function returns an array of the same type as the input array with distinct values. Example: array_distinct(array(\u0026lsquo;b\u0026rsquo;, \u0026rsquo;d', \u0026rsquo;d', \u0026lsquo;a\u0026rsquo;)) Result: [\u0026lsquo;b\u0026rsquo;, \u0026rsquo;d', \u0026lsquo;a\u0026rsquo;] GenericUDFArrayDistinct   string array_join(array, delimiter, replaceNull) NEW Concatenate the elements of an array with a specified delimiter. Example: array_join(array(1, 2, NULL, 4), \u0026lsquo;,\u0026rsquo;,':') Result: 1,2,:,4 GenericUDFArrayJoin   array array_expect(array1, array2) NEW Returns an array of the elements in array1 but not in array2. Example: array_expect(array(1, 2, 3,4), array(2,3)) Result: [1,4] GenericUDFArrayExcept   array array_intersect(array1, array2) NEW Returns an array of the elements in the intersection of array1 and array2, without duplicates. Example: array_intersect(array(1, 2, 3,4), array(1,2,3)) Result: [1,2,3] GenericUDFArrayIntersect   array array_union(array1, array2) NEW Returns an array of the elements in the union of array1 and array2 without duplicates. Example: array_union(array(1, 2, 2, 4), array(2, 3)) Result: [1, 2, 3, 4] GenericUDFArrayUnion   array array_remove(array, element) NEW Removes all occurrences of elements from the array. Example: array_remove(array(1, 2, 3, 4, 2), 2) Result: [1, 3, 4] GenericUDFArrayRemove    Type Conversion Functions The following built-in type conversion functions are supported in Hive.    Return Type Name(Signature) Description Source code     binary binary(string|binary) Casts the parameter into a binary. GenericUDFBaseBinary   Expected \u0026ldquo;=\u0026rdquo; to follow \u0026ldquo;type\u0026rdquo; cast(expr as \u0026lt;type\u0026gt;) Converts the results of the expression expr to \u0026lt;type\u0026gt;. For example, cast(\u0026lsquo;1\u0026rsquo; as bigint) will convert the string \u0026lsquo;1\u0026rsquo; to its integral representation. A null is returned if the conversion does not succeed. If cast(expr as boolean) Hive returns true for a non-empty string.     Conditional Functions The following built-in conditional functions are supported in Hive.\n   Return Type Name(Signature) Description     T if(boolean testCondition, T valueTrue, T valueFalseOrNull) Returns valueTrue when testCondition is true, returns valueFalseOrNull otherwise.   boolean isnull( a ) Returns true if a is NULL and false otherwise.   boolean isnotnull ( a ) Returns true if a is not NULL and false otherwise.   T nvl(T value, T default_value) Returns default value if value is null else returns value.   T COALESCE(T v1, T v2, ...) Returns the first v that is not NULL, or NULL if all v\u0026rsquo;s are NULL.   T CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END When a = b, returns c; when a = d, returns e; else returns f.   T CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END When a = true, returns b; when c = true, returns d; else returns e.   T nullif( a, b ) Returns NULL if a=b; otherwise returns a.Shorthand forCASE WHEN a = b then NULL else a   void assert_true(boolean condition) Throw an exception if \u0026lsquo;condition\u0026rsquo; is not true, otherwise, return null. For example, select assert_true (2\u0026lt;1).    Data Masking Functions The following built-in data masking functions are supported in Hive:\n   Return Type Name(Signature) Description Source code     string mask(string str[,string upper[,string lower[,string number]]]) Returns a masked version of str. By default, upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. Example: mask(\u0026ldquo;abcd-EFGH-8765-4321\u0026rdquo;) results in xxxx-XXXX-nnnn-nnnn. You can override the characters used in the mask by supplying additional arguments: the second argument controls the mask character for upper case letters, the third argument for lower case letters, and the fourth argument for numbers. For example, mask(\u0026ldquo;abcd-EFGH-8765-4321\u0026rdquo;, \u0026ldquo;U\u0026rdquo;, \u0026ldquo;l\u0026rdquo;, \u0026ldquo;#\u0026quot;) results in llll-UUUU-####-####. GenericUDFMask   string mask_first_n(string str[,int n]) Returns a masked version of str with the first n values masked. Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_first_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in nnnn-5678-8765-4321. GenericUDFMaskFirstN   string mask_last_n(string str[,int n]) Returns a masked version of str with the last n values masked. Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_last_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in 1234-5678-8765-nnnn. GenericUDFMaskLastN   string mask_show_first_n(string str[,int n]) Returns a masked version of str, showing the first n characters unmasked. Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_show_first_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in 1234-nnnn-nnnn-nnnn. GenericUDFMaskShowFirstN   string mask_show_last_n(string str[,int n]) Returns a masked version of str, showing the last n characters unmasked. Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_show_last_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in nnnn-nnnn-nnnn-4321. GenericUDFMaskShowLastN   string mask_hash(string|char|varchar str) Returns a hashed value based on str. The hash is consistent and can be used to join masked values together across tables. This function returns null for non-string types. GenericUDFMaskHash    Miscellaneous Functions In Hive several built-in functions do not belong to any categories above. These are the special functions that Hive has.\n   Return Type Name(Signature) Description Source code     varies java_method(class, method[, arg1[, arg2..]]) Synonym for reflect. GenericUDFReflect   varies reflect(class, method[, arg1[, arg2..]]) Calls a Java method by matching the argument signature, using reflection. See Reflect (Generic) UDF for examples. GenericUDFReflect   int hash(a1[, a2...]) Returns a hash value of the arguments.  GenericUDFHash   string current_user() Returns current user name from the configured authenticator manager. Could be the same as the user provided when connecting, but with some authentication managers (for example HadoopDefaultAuthenticator) it could be different. GenericUDFCurrentUser   string logged_in_user() Returns the current user name from the session state. This is the username provided when connecting to Hive. GenericUDFLoggedInUser   string current_database() Returns current database name. GenericUDFCurrentDatabase   string md5(string/binary) Calculates an MD5 128-bit checksum for the string or binary. The value is returned as a string of 32 hex digits, or NULL if the argument was NULL. Example: md5(\u0026lsquo;ABC\u0026rsquo;) = \u0026lsquo;902fbdd2b1df0c4f70b4a5d23525e932\u0026rsquo;. UDFMd5   string sha1(string/binary) sha(string/binary) Calculates the SHA-1 digest for string or binary and returns the value as a hex string. Example: sha1(\u0026lsquo;ABC\u0026rsquo;) = \u0026lsquo;3c01bdbb26f358bab27f267924aa2c9a03fcfdb8\u0026rsquo;. UDFSha1   bigint crc32(string/binary) Computes a cyclic redundancy check value for string or binary argument and returns bigint value. Example: crc32(\u0026lsquo;ABC\u0026rsquo;) = 2743272264. UDFCrc32   string sha2(string/binary,int) Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2(\u0026lsquo;ABC\u0026rsquo;, 256) = \u0026lsquo;b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78\u0026rsquo;. GenericUDFSha2   binary aes_encrypt(inputstring/binary, keystring/binary) Encrypt input using AES. Key lengths of 128, 192 or 256 bits can be used. 192 and 256 bits keys can be used if Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are installed. If either argument is NULL or the key length is not one of the permitted values, the return value is NULL. Example: base64(aes_encrypt(\u0026lsquo;ABC\u0026rsquo;, \u0026lsquo;1234567890123456\u0026rsquo;)) = \u0026lsquo;y6Ss+zCYObpCbgfWfyNWTw==\u0026rsquo;. GenericUDFAesEncrypt   binary aes_decrypt(input binary, keystring/binary) Decrypt input using AES. Key lengths of 128, 192 or 256 bits can be used. 192 and 256 bits keys can be used if Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are installed. If either argument is NULL or the key length is not one of the permitted values, the return value is NULL. Example: aes_decrypt(unbase64(\u0026lsquo;y6Ss+zCYObpCbgfWfyNWTw=='), \u0026lsquo;1234567890123456\u0026rsquo;) = \u0026lsquo;ABC\u0026rsquo;. GenericUDFAesDecrypt   string version() Returns the Hive version. The string contains 2 fields, the first being a build number and the second being a build hash. Example: \u0026ldquo;select version();\u0026rdquo; might return \u0026ldquo;2.1.0.2.5.0.0-1245 r027527b9c5ce1a3d7d0b6d2e6de2378fb0c39232\u0026rdquo;. Actual results will depend on your build. UDFVersion   bigint surrogate_key([write_id_bits, task_id_bits]) Automatically generate numerical Ids for rows as you enter data into a table. Can only be used as the default value for acid or insert-only tables. GenericUDFSurrogateKey   string typeof(x)  NEW Returns the type of the supplied argument GenericUDFTypeOf    Geospatial NEW\nHive 4.0 provides ESRI-based standard geospatial UDFs.\nThe ESRI UDFs constitute an extensive list, and it\u0026rsquo;s important to note that the Hive documentation does not duplicate the entirety of the function documentation. Here, you can find ESRI UDFs documentation.\nBlog about the Hive Geospatial support\nCreating Custom UDF\u0026rsquo;s Apache Hive offers a wide range of built-in User-Defined Functions (UDFs). However, for specific requirements not covered by the built-in functions, users can develop their own custom UDFs with just a basic understanding of Java.\nCustom UDFs in Hive are categorized into three types based on the number of input and output rows. Each type of UDF corresponds to a different interface that needs to be implemented.\n   UDF UDAF UDTF     It is a function that receives only a single row as an input and returns a single row as an output.Like: length, or round functions org.apache.hadoop.hive.ql.exec.UDF; It is a function that receives multiple rows as input and returns a single row as output.Like: Count, Min, Max It is a function that receives a single row as input and returns multiple rows - result set or table - as output.Like: exploed, parse_url_tuple    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-udfs_282102277/","tags":null,"title":"Apache Hive : Hive UDFs"},{"categories":null,"contents":"Apache Hive : HIVE-24543: Support SAML 2.0 authentication mode Description In cloud based deployments, it is common that the user identity is federated and managed externally by an identity provider (e.g Okta, PingIndentity, Azure AD). Integrating with such external identity providers (IDP) would help adoption and unlock use-cases where Hive is deployed in a cloud based environment and doesn\u0026rsquo;t need user managed authentication mechanisms (e.g Ldap, Kerberos). There are primarily two authentication protocols which are standardized with such external identity providers namely (SAML 2.0 and OpenID connect). This design specifically focuses on SAML 2.0 but it can be extended to support OpenID connect as well in the future.\nDesign SAML 2.0 In this approach, we rely on using SAML 2.0 protocol to exchange the authentication messages between the client and IDP and generate a SAML response object which is sent to the HiveServer2 from the IDP via browser as per the SAML specification. HiveServer2 can reuse the same port for its http connection and have a separate http path on it to receive the SAML Response. SAML 2.0 protocol is mainly browser dependent since it depends a lot on the redirect URLs which browser supports to control the SSO workflow. For the desktop based use-cases (JDBC/ODBC Driver running locally on a machine which has browser access) JDBC driver code will open a browser to execute this workflow. At the end of SSO workflow, the IDP will send back a signed SAML assertion to the HiveServer2’s assertion consumer service url (ACS). When the assertion is validated, HiveServer2 will generate a short lived token which is returned to the driver by redirecting the browser using a x-www-form-urlencoded response to a localhost port. This approach is similar to what is described in RFC8252 section 7.3. Driver will add the token to its authorization bearer header to make subsequent calls. HiveServer2 will validate the bearer token and the client identifier to allow the client to open a session.\nEstablishing trust with IDP In order to make sure that the SAML assertions received by HiveServer2 are valid, HiveServer2 needs to establish a trust relationship with the IDP which is going to sign the SAML assertions. This can be done by manual configuration in the HiveServer2. In order to configure the HiveServer2 with the IDP metadata, following steps will need to be done before the HiveServer2 is started.\n Fetch the IDP metadata xml file from the identity provider. The steps to extract the IDP metadata are specific for each IDP and users should follow the documentation of IDP to do that. HiveServer2 will expose a configuration which will point to the idp-metadata.xml. When it is started it loads the IDP metadata file which includes its public key, SSO url, and entityId of the IDP. This information is used to generate authentication requests and validate the authentication response received by HiveServer2. For the first version we will not support encrypted SAML assertions (since it would mean we need to exchange the public key of HiveServer2 with IDP) and will instead rely on the end-to-end TLS encryption for the connections between client and server. However, the SAML responses generated by IDP MUST be signed to validate the authenticity of the response.  SAML authentication workflow Authentication Flow Details  JDBC Driver: Receives a connection url from the user-facing clients (Beeline, Tableau). The connection URL uses http transport and has auth=browser JDBC Driver: Opens a port on the client machine to receive the server response. This port uses 127.0.0.1 as the bind address so that it doesn\u0026rsquo;t accept traffic from the internet. JDBC Driver: If the auth is browser, submits a HTTP post request to the HiveServer2. This could be a dummy Opensession() API call which will be redirected by HiveServer2 to SSO provider. HiveServer2: Detects that it is configured using SSO, generates a HTTP 302 redirect with the location as SSO URL pointing to the IDP provider. Additionally, it includes a client identifier to the response header which is used by the driver for subsequent requests. This URL also contains a relayState id which is used by the server to track the SSO request. JDBC Driver: Gets the URL, client identifier from the redirect response of the HiveServer2. JDBC Driver: Opens the browser and points it to the given SSO url. User: Authenticates with IDP using credentials and MFA as configured. IDP: If authentication is successful, IDP sends a SAML response to the SP’s assertion consumer service URL as per the SAML 2.0 specification. Else, after a configurable timeout both the client and server will clean their respective states. HiveServer2: Verifies if SAML assertion is valid. HiveServer2: If the SAML assertion is valid, generates a response which includes short-lived token, status, and an informative message and respond to the browser with a form which submits on page load to the http://localhost:port from step 2 in a x-www-form-urlencoded format. If the assertion is not valid, generates a failure response without token and includes a descriptive message. JDBC Driver: Parses the response from the server. If the response is successful extracts the token from the response and adds it to the “Authorization: Bearer “ header of the subsequent request. Else, displays the appropriate error message to the user. HiveServer2: Receives a post request which contains the bearer token. Validates the token, the client identifier and allows the connection to proceed. It generates a cookie which is used for subsequent requests.  Implementation Details New configurations Following new configurations will be added to the hive-site.xml which would need to be configured by the clients.\nThis configuration will be set to SAML to indicate that the server will use SAML 2.0 protocol to authenticate the user. This configuration will provide a path to the IDP metadata xml file.\nThis configuration should be same the service provider entity id as configured in the IDP. Some identity providers require this to be same as the ACS URL.\nThis configuration will be used to map the SAML attribute in the response to the groups of the user. This should be configured in the identity provider as the attribute name for the group information.\nThis configuration will be used to configure the allowed group names.\nThe http URL endpoint where the SAML assertion is posted back by the IDP. Currently this must be on the same port as HiveServer2’s http endpoint and must be TLS enabled (https) on secure setups.\nJDBC Driver changes On the driver side the following new jdbc connection parameters will be defined/allowed.\n New values of browser and token which will be allowed for the existing connection parameter auth. [TBD] In case the auth is set to token, the value for token will be passed as the value of new keyword token.  For example, in case of browser the URL will look like\njdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=browser A token based URL will look like:\njdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=token;token=\u0026lt;token_string\u0026gt; The Jdbc connection parameters will be passed in over the connection URL. SSO mode URL validations Driver makes sure that the SSO integration is only allowed when TLS enabled between driver and the server. If the SSL is not enabled, the driver should error out (or optionally warn the users based on a configuration override). Currently, both these modes will only be supported in http mode.\nBrowser mode When the auth is browser the Driver will open a available port on the machine where the driver is running. Driver will make a http post request with this port number in the request header (it could be the same as the OpenSessionReq). This request will get a HTTP 302 response with the location set to the SSO URI. Additionally, the server will set a client identifier in the response header. This client identifier header must be sent by the driver in all the subsequent requests to the server.\nOnce the SSO URI is available, Driver must take the following actions:\n Open a browser window and point it to the SSO URI. If there is no active session with the IDP, the user is expected to enter the credentials on the browser and complete the single-sign on workflow along with the MFA if configured with the IDP. When the browser completes the SSO workflow, the IDP forwards the SAML response to the Assertion consumer service (ACS) URL of HiveServer2. The ACS will validate the response and return a HTTP form in the response which is posted to the http://localhost:port using application/x-www-form-urlencoded encoding.  If the status is success, the token must be present and the driver will add the token as a bearer authorization HTTP header for the subsequent connection to open session with the key “Authorization: Bearer :”\nIf the status is error, the message field will include an appropriate error message which can be logged by the driver.\nThe driver will have a configurable timeout value which will be used to error out the connection attempt in case no response is received on the localhost:port until the given timeout. This configurable timeout value could be provided as a connection parameter. Additionally, the connection URL can provide a specific port to bind to on the localhost on the connection URL. For example, following URL will open a port 12345 and have a timeout of 120 seconds to complete the SSO flow.\njdbc:hive2://HiveServer2-host:10001/default;transportMode=http;httpPath=cliservice;auth=browser;samlResponsePort=12345;samlResponseTimeout=120 Token Expiry and renewal In the initial version the token returned by the server will be used for a one-time validation within the default period of 30 seconds (token will be valid for only 30 seconds) which could be configurable. The token will be used by the server to set a cookie which will be used for further requests. However, this is a server side implementation detail which client does not need to be aware of. When the session expires the server will return a HTTP 401 status code which will be used by the clients to re authenticate using the browser flow again. Unfortunately, RFC 7231 does have an explicit status code for session expiration. However, the clients can detect if the response code of 401 is due to session expiry by keeping track of whether we are sending a token in the header in the previous request or not. In such case the JDBC Application can choose to close the connection or re-authenticate as per the application logic.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/170266662/","tags":null,"title":"Apache Hive : HIVE-24543: Support SAML 2.0 authentication mode"},{"categories":null,"contents":"Apache Hive : Hive-Iceberg Integration Apache Hive starting from 4.0 out of the box supports the Iceberg table format, the iceberg tables can be created like regular hive external or ACID tables, without adding any extra jars.\nCreating an Iceberg Table\nAn iceberg table can be created using STORED BY ICEBERG keywords while creating a table.\n Creating an Iceberg table using normal create command  CREATE TABLE TBL_ICE (ID INT) STORED BY ICEBERG; The above creates an iceberg table named \u0026lsquo;TBL_ICE\u0026rsquo;\n Creating an Iceberg Table using CTAS  CREATE TABLE TABLE_CTAS AS SELECT * FROM SRCTABLE STORED BY ICEBERG; The above creates an iceberg table named \u0026lsquo;TABLE_CTAS\u0026rsquo; with schema \u0026amp; records as in \u0026lsquo;SRCTABLE\u0026rsquo;  Creating an Iceberg Table using CTLT  CREATE TABLE_CTLT LIKE SRCTABLE STORED BY ICEBERG; The above creates an iceberg table named \u0026lsquo;TABLE_CTLT\u0026rsquo; with schema same as SRCTABLE\nFormat Versions:\nThe iceberg tables support both v1 \u0026amp; v2 tables, the tables by default are created as v1 table unless explicitly specified. To create a v2 table (\u0026lsquo;format-version\u0026rsquo;=\u0026lsquo;2\u0026rsquo;) can be specified as table property while creating the table\nExample:\nCREATE TABLE V2_TABLE (ID INT) STORED BY ICEBERG TBLPROPERTIES ('format-version'='2'); File Formats:\nThe iceberg table currently supports three file formats: PARQUET, ORC \u0026amp; AVRO. The default file format is Parquet. The file format can be explicitily provided by using STORED AS while creating the table\nExample-1:\nCREATE TABLE ORC_TABLE (ID INT) STORED BY ICEBERG STORED AS ORC; The above creates a v1 iceberg table named \u0026lsquo;ORC_TABLE\u0026rsquo; of ORC file format.\nExample-2:\nCREATE TABLE V2_ORC_TABLE (ID INT) STORED BY ICEBERG STORED AS ORC TBLPROPERTIES ('format-version'='2'); The above creates a v2 iceberg table named \u0026lsquo;V2_ORC_TABLE\u0026rsquo; of ORC file format.\nSimilarly we can specify any of the supported file formats while creating the table,\nDelete Modes:\nHive for delete, update \u0026amp; merge queries support both Copy-on-Write and Merge-on-Read, by default the tables are created with Merge-on-Read mode. The delete mode can be configured using the following TBLPROPERTIES:\nCREATE TABLE tbl_x (id int) STORED BY ICEBERG TBLPROPERTIES ( 'write.delete.mode'='copy-on-write', 'write.update.mode'='copy-on-write', 'write.merge.mode'='copy-on-write' ); Migrating existing tables to Iceberg Tables\nAny Hive external table can be converted into an iceberg tables, without actually rewriting the data files again. We can use ALTER TABLE CONVERT TO ICEBERG [TBLPROPERTIES] to convert any existing external table to an iceberg table.\nALTER TABLE TABLE1 CONVERT TO ICEBERG TBLPROPERTIES ('format-version'='2'); The above converts an existing external table \u0026lsquo;TABLE1\u0026rsquo; into a v2 Iceberg table, specifying the TBLPROPERTIES \u0026amp; format-version is option, if not specified the table will be converted into a v1 iceberg table.\nQuerying an Iceberg Table\nIceberg tables support all query statements similar to any other hive table.\nExample:\nSELECT * FROM TBL_ICE WHERE ID \u0026gt; 5; Writing data into iceberg tables\nIceberg tables supports all data ingestion methods supported with hive\n Insert-Into  INSERT INTO TBL_ICE VALUES (1),(2),(3),(4);  Insert-Overwrite  INSERT OVERWRITE TBL_ICE SELECT * FROM TABLE1;  Delete  DELETE FROM TBL_ICE WHERE ID=5;  Update  UPDATE TBL_ICE WHERE ID=8 SET ID=2;  LOAD DATA  LOAD DATA LOCAL INPATH '/data/files/doctors.avro' OVERWRITE INTO TABLE ice_avro; If the config “hive.load.data.use.native.api” is set to “false”, hive will rather than using the Append api, will launch a Tez job to do the LOAD.\nMetadata tables:\nHive supports the following metadata tables for Iceberg tables The metadata tables can be queried using the syntax \u0026lt;DATABASE NAME\u0026gt;.\u0026lt;TABLE NAME\u0026gt;.\u0026lt;METADATA TABLE\u0026gt;\nENTRIES SELECT * from db_name.tbl_name.entries; FILES SELECT * from db_name.tbl_name.files; HISTORY SELECT * from db_name.tbl_name.history; SNAPSHOTS SELECT * from db_name.tbl_name.snapshots; MANIFESTS SELECT * from db_name.tbl_name.manifests; PARTITIONS SELECT * from db_name.tbl_name.partitions; ALL_DATA_FILES SELECT * from db_name.tbl_name.all_data_files; ALL_MANIFESTS SELECT * from db_name.tbl_name.all_manifests; ALL_ENTRIES SELECT * from db_name.tbl_name.all_entries; DATA_FILES SELECT * from db_name.tbl_name.data_files; DELETE_FILES SELECT * from db_name.tbl_name.delete_files; METADATA_LOG_ENTRIES SELECT * from db_name.tbl_name.metadata_log_entries; REFS SELECT * from db_name.tbl_name.refs; ALL_DELETE_FILES SELECT * from db_name.tbl_name.all_delete_files; ALL_FILES SELECT * from db_name.tbl_name.all_files; Branches \u0026amp; Tags:\nIceberg tables supports branches \u0026amp; tags, the details around the feature can be read here\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-iceberg-integration_282102247/","tags":null,"title":"Apache Hive : Hive-Iceberg Integration"},{"categories":null,"contents":"Apache Hive : Hive-Tez Compatibility This is derived from the pom files of the respective releases. Other releases with compatibility are listed in parenthesis.\n   Hive (Works with) Tez     0.13 0.4.0-incubating   0.14 0.5.2+, (through 0.7.0)   1.0 0.5.2, (through 0.7.0)   1.1 0.5.2, (through 0.7.0)   1.2* 0.5.3, (through 0.7.0)   2.0 0.8.2    *Hive-1.2 is the latest release of Hive as of 07/2015.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hive-tez-compatibility_59689974/","tags":null,"title":"Apache Hive : Hive-Tez Compatibility"},{"categories":null,"contents":"Apache Hive : HiveAmazonElasticMapReduce Amazon Elastic MapReduce and Hive Amazon Elastic MapReduce is a web service that makes it easy to launch managed, resizable Hadoop clusters on the web-scale infrastructure of Amazon Web Services (AWS). Elastic Map Reduce makes it easy for you to launch a Hive and Hadoop cluster, provides you with flexibility to choose different cluster sizes, and allows you to tear them down automatically when processing has completed. You pay only for the resources that you use with no minimums or long-term commitments.\nAmazon Elastic MapReduce simplifies the use of Hive clusters by:\n Handling the provisioning of Hadoop clusters of up to thousands of EC2 instances Installing Hadoop across the master and slave nodes of your cluster and configuring Hadoop based on your chosen hardware Installing Hive on the master node of your cluster and configuring it for communication with the Hadoop JobTracker and NameNode Providing a simple API, a web UI, and purpose-built tools for managing, monitoring, and debugging Hadoop tasks throughout the life of the cluster Providing deep integration, and optimized performance, with AWS services such as S3 and EC2 and AWS features such as Spot Instances, Elastic IPs, and Identity and Access Management (IAM)  Please refer to the following link to view the Amazon Elastic MapReduce Getting Started Guide:\nhttp://docs.amazonwebservices.com/ElasticMapReduce/latest/GettingStartedGuide/\nAmazon Elastic MapReduce provides you with multiple clients to run your Hive cluster. You can launch a Hive cluster using the AWS Management Console, the Amazon Elastic MapReduce Ruby Client, or the AWS Java SDK. You may also install and run multiple versions of Hive on the same cluster, allowing you to benchmark a newer Hive version alongside your previous version. You can also install a newer Hive version directly onto an existing Hive cluster.\nSupported versions:          Hadoop Version Hive Version   0.18 0.4   0.20 0.5, 0.7, 0.7.1    Hive Defaults Thrift Communication port          Hive Version Thrift port   0.4 10000   0.5 10000   0.7 10001   0.7.1 10002    Log File          Hive Version Log location   0.4 /mnt/var/log/apps/hive.log   0.5 /mnt/var/log/apps/hive_05.log   0.7 /mnt/var/log/apps/hive_07.log   0.7.1 /mnt/var/log/apps/hive_07_1.log    MetaStore By default, Amazon Elastic MapReduce uses MySQL, preinstalled on the Master Node, for its Hive metastore. Alternatively, you can use the Amazon Relational Database Service (Amazon RDS) to ensure the metastore is persisted beyond the life of your cluster. This also allows you to share the metastore between multiple Hive clusters. Simply override the default location of the MySQL database to the external persistent storage location.\nHive CLI EMR configures the master node to allow SSH access. You can log onto the master node and execute Hive commands using the Hive CLI. If you have multiple versions of Hive installed on the cluster you can access each one of them via a separate command:\n         Hive Version Hive command   0.4 hive   0.5 hive-0.5   0.7 hive-0.7   0.7.1 hive-0.7.1    EMR sets up a separate Hive metastore and Hive warehouse for each installed Hive version on a given cluster. Hence, creating tables using one version does not interfere with the tables created using another version installed. Please note that if you point multiple Hive tables to same location, updates to one table become visible to other tables.\nHive Server EMR runs a Thrift Hive server on the master node of the Hive cluster. It can be accessed using any JDBC client (for example, squirrel SQL) via Hive JDBC drivers. The JDBC drivers for different Hive versions can be downloaded via the following links:\n         Hive Version Hive JDBC   0.5 http://aws.amazon.com/developertools/0196055244487017   0.7 http://aws.amazon.com/developertools/1818074809286277   0.7.1 http://aws.amazon.com/developertools/8084613472207189    Here is the process to connect to the Hive Server using a JDBC driver:\nhttp://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/UsingEMR_Hive.html#HiveJDBCDriver\nRunning Batch Queries You can also submit queries from the command line client remotely. Please note that currently there is a limit of 256 steps on each cluster. If you have more than 256 steps to execute, it is recommended that you run the queries directly using the Hive CLI or submit queries via a JDBC driver.\nHive S3 Tables An Elastic MapReduce Hive cluster comes configured for communication with S3. You can create tables and point them to your S3 location and Hive and Hadoop will communicate with S3 automatically using your provided credentials.\nOnce you have moved data to an S3 bucket, you simply point your table to that location in S3 in order to read or process data via Hive. You can also create partitioned tables in S3. Hive on Elastic MapReduce provides support for dynamic partitioning in S3.\nHive Logs Hive application logs: All Hive application logs are redirected to /mnt/var/log/apps/ directory.\nHadoop daemon logs: Hadoop daemon logs are available in /mnt/var/log/hadoop/ folder.\nHadoop task attempt logs are available in /mnt/var/log/hadoop/userlogs/ folder on each slave node in the cluster.\nTutorials The following Hive tutorials are available for you to get started with Hive on Elastic MapReduce:\n Finding trending topics using Google Books n-grams data and Apache Hive on Elastic MapReduce  http://aws.amazon.com/articles/Elastic-MapReduce/5249664154115844   Contextual Advertising using Apache Hive and Amazon Elastic MapReduce with High Performance Computing instances  http://aws.amazon.com/articles/Elastic-MapReduce/2855   Operating a Data Warehouse with Hive, Amazon Elastic MapReduce and Amazon SimpleDB  http://aws.amazon.com/articles/Elastic-MapReduce/2854   Running Hive on Amazon ElasticMap Reduce  http://aws.amazon.com/articles/2857    In addition, Amazon provides step-by-step video tutorials:\n http://aws.amazon.com/articles/2862  Support You can ask questions related to Hive on Elastic MapReduce on Elastic MapReduce forums at:\nhttps://forums.aws.amazon.com/forum.jspa?forumID=52\nPlease also refer to the EMR developer guide for more information:\nhttp://docs.amazonwebservices.com/ElasticMapReduce/latest/DeveloperGuide/\nContributed by: Vaibhav Aggarwal\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveamazonelasticmapreduce_27825646/","tags":null,"title":"Apache Hive : HiveAmazonElasticMapReduce"},{"categories":null,"contents":"Apache Hive : HiveAws = Hive and Amazon Web Services =\nBackground This document explores the different ways of leveraging Hive on Amazon Web Services - namely S3, EC2 and Elastic Map-Reduce.\nHadoop already has a long tradition of being run on EC2 and S3. These are well documented in the links below which are a must read:\n Hadoop and S3 Amazon and EC2  The second document also has pointers on how to get started using EC2 and S3. For people who are new to S3 - there\u0026rsquo;s a few helpful notes in S3 for n00bs section below. The rest of the documentation below assumes that the reader can launch a hadoop cluster in EC2, copy files into and out of S3 and run some simple Hadoop jobs.\nIntroduction to Hive and AWS There are three separate questions to consider when running Hive on AWS:\n Where to run the Hive CLI from and store the metastore db (that contains table and schema definitions). How to define Hive tables over existing datasets (potentially those that are already in S3) How to dispatch Hive queries (which are all executed using one or more map-reduce programs) to a Hadoop cluster running in EC2.  We walk you through the choices involved here and show some practical case studies that contain detailed setup and configuration instructions.\nRunning the Hive CLI The CLI takes in Hive queries, compiles them into a plan (commonly, but not always, consisting of map-reduce jobs) and then submits them to a Hadoop Cluster. While it depends on Hadoop libraries for this purpose - it is otherwise relatively independent of the Hadoop cluster itself. For this reason the CLI can be run from any node that has a Hive distribution, a Hadoop distribution, a Java Runtime Engine. It can submit jobs to any compatible hadoop cluster (whose version matches that of the Hadoop libraries that Hive is using) that it can connect to. The Hive CLI also needs to access table metadata. By default this is persisted by Hive via an embedded Derby database into a folder named metastore_db on the local file system (however state can be persisted in any database - including remote mysql instances).\nThere are two choices on where to run the Hive CLI from:\n  Run Hive CLI from within EC2 - the Hadoop master node being the obvious choice. There are several problems with this approach:\n Lack of comprehensive AMIs that bundle different versions of Hive and Hadoop distributions (and the difficulty in doing so considering the large number of such combinations). Cloudera provides some AMIs that bundle Hive with Hadoop - although the choice in terms of Hive and Hadoop versions may be restricted. Any required map-reduce scripts may also need to be copied to the master/Hive node. If the default Derby database is used - then one has to think about persisting state beyond the lifetime of one hadoop cluster. S3 is an obvious choice - but the user must restore and backup Hive metadata at the launch and termination of the Hadoop cluster.    Run Hive CLI remotely from outside EC2. In this case, the user installs a Hive distribution on a personal workstation, - the main trick with this option is connecting to the Hadoop cluster - both for submitting jobs and for reading and writing files to HDFS. The section on Running jobs from a remote machine details how this can be done. Case Study 1 goes into the setup for this in more detail. This option solves the problems mentioned above:\n   Stock Hadoop AMIs can be used. The user can run any version of Hive on their workstation, launch a Hadoop cluster with the desired Hadoop version etc. on EC2 and start running queries. Map-reduce scripts are automatically pushed by Hive into Hadoop\u0026rsquo;s distributed cache at job submission time and do not need to be copied to the Hadoop machines. Hive Metadata can be stored on local disk painlessly.  However - the one downside of Option 2 is that jar files are copied over to the Hadoop cluster for each map-reduce job. This can cause high latency in job submission as well as incur some AWS network transmission costs. Option 1 seems suitable for advanced users who have figured out a stable Hadoop and Hive (and potentially external libraries) configuration that works for them and can create a new AMI with the same.\nLoading Data into Hive Tables It is useful to go over the main storage choices for Hadoop/EC2 environment:\n  S3 is an excellent place to store data for the long term. There are a couple of choices on how S3 can be used:\n Data can be either stored as files within S3 using tools like aws and s3curl as detailed in S3 for n00bs section. This suffers from the restriction of 5G limit on file size in S3. But the nice thing is that there are probably scores of tools that can help in copying/replicating data to S3 in this manner. Hadoop is able to read/write such files using the S3N filesystem. Alternatively Hadoop provides a block based file system using S3 as a backing store. This does not suffer from the 5G max file size restriction. However - Hadoop utilities and libraries must be used for reading/writing such files.    HDFS instance on the local drives of the machines in the Hadoop cluster. The lifetime of this is restricted to that of the Hadoop instance - hence this is not suitable for long lived data. However it should provide data that can be accessed much faster and hence is a good choice for intermediate/tmp data.\n  Considering these factors, the following makes sense in terms of Hive tables:\n  For long-lived tables, use S3 based storage mechanisms\n  For intermediate data and tmp tables, use HDFS\n  Case Study 1 shows you how to achieve such an arrangement using the S3N filesystem.\nIf the user is running Hive CLI from their personal workstation - they can also use Hive\u0026rsquo;s \u0026lsquo;load data local\u0026rsquo; commands as a convenient alternative (to dfs commands) to copy data from their local filesystems (accessible from their workstation) into tables defined over either HDFS or S3.\nSubmitting jobs to a Hadoop cluster This applies particularly when Hive CLI is run remotely. A single Hive CLI session can switch across different hadoop clusters (especially as clusters are bought up and terminated). Only two configuration variables:\n fs.default.name mapred.job.tracker  need to be changed to point the CLI from one Hadoop cluster to another. Beware though that tables stored in previous HDFS instance will not be accessible as the CLI switches from one cluster to another. Again - more details can be found in Case Study 1.\nCase Studies  Querying files in S3 using EC2, Hive and Hadoop  Appendix \u0026laquo;Anchor(S3n00b)\u0026raquo;\nS3 for n00bs One of the things useful to understand is how S3 is used as a file system normally. Each S3 bucket can be considered as a root of a File System. Different files within this filesystem become objects stored in S3 - where the path name of the file (path components joined with \u0026lsquo;/') become the S3 key within the bucket and file contents become the value. Different tools like [S3Fox|https:\u0026ndash;addons.mozilla.org-en-US-firefox-addon-3247] and native S3 !FileSystem in Hadoop (s3n) show a directory structure that\u0026rsquo;s implied by the common prefixes found in the keys. Not all tools are able to create an empty directory. In particular - S3Fox does (by creating a empty key representing the directory). Other popular tools like aws, s3cmd and s3curl provide convenient ways of accessing S3 from the command line - but don\u0026rsquo;t have the capability of creating empty directories.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveaws_27362103/","tags":null,"title":"Apache Hive : HiveAws"},{"categories":null,"contents":"Apache Hive : HiveAws HivingS3nRemotely = Querying S3 files from your PC (using EC2, Hive and Hadoop) =\nUsage Scenario The scenario being covered here goes as follows:\n A user has data stored in S3 - for example Apache log files archived in the cloud, or databases backed up into S3. The user would like to declare tables over the data sets here and issue SQL queries against them These SQL queries should be executed using computed resources provisioned from EC2. Ideally, the compute resources can be provisioned in proportion to the compute costs of the queries Results from such queries that need to be retained for the long term can be stored back in S3  This tutorial walks through the steps required to accomplish this. Please send email to the hive-users mailing list in case of any problems with this tutorial.\nRequired Software On the client side (PC), the following are required:\n Any version of Hive that incorporates HIVE-467. (As of this writing - the relevant patches are not committed. For convenience sake - a Hive distribution with this patch can be downloaded from here.) A version of Hadoop ec2 scripts (src/contrib/ec2/bin) with a fix for here]. Again - since the relevant patches are not committed yet - a version of Hadoop-19 ec2 scripts with the relevant patches applied can be downloaded from [[http:\u0026ndash;jsensarma.com-downloads-hadoop-0.19-ec2-remote.tar.gz]. These scripts must be used to launch hadoop clusters in EC2.  Hive requires a local directory of Hadoop to run (specified using environment variable HADOOP_HOME). This can be a version of Hadoop compatible with the one running on the EC2 clusters. This recipe has been tried with hadoop distribution created from from branch-19.\nIt is assumed that the user can successfully launch Hive CLI (bin/hive from the Hive distribution) at this point.\nHive Setup A few Hadoop configuration variables are required to be specified for all Hive sessions. These can be set using the hive cli as follows:\n hive\u0026gt; set hadoop.socks.server=localhost:2600; hive\u0026gt; set hadoop.rpc.socket.factory.class.default=org.apache.hadoop.net.SocksSocketFactory; hive\u0026gt; set hadoop.job.ugi=root,root; hive\u0026gt; set mapred.map.tasks=40; hive\u0026gt; set mapred.reduce.tasks=-1; hive\u0026gt; set fs.s3n.awsSecretAccessKey=2GAHKWG3+1wxcqyhpj5b1Ggqc0TIxj21DKkidjfz hive\u0026gt; set fs.s3n.awsAccessKeyId=1B5JYHPQCXW13GWKHAG2 The values assigned to s3n keys are just an example and need to be filled in by the user as per their account details. Explanation for the rest of the values can be found in Configuration Guide section below.\nInstead of specifying these command lines each time the CLI is bought up - we can store these persistently within hive-site.xml in the conf/ directory of the Hive installation (from where they will be picked up each time the CLI is launched.\nExample Public Data Sets Some example data files are provided in the S3 bucket data.s3ndemo.hive. We will use them for the sql examples in this tutorial:\n s3n://data.s3ndemo.hive/kv - Key Value pairs in a text file s3n://data.s3ndemo.hive/pkv - Key Value pairs in a directories that are partitioned by date s3n://data.s3ndemo.hive/tpch/* - Eight directories containing data corresponding to the eight tables used by TPCH benchmark. The data is generated for a scale 10 (approx 10GB) database using the standard dbgen utility provided by TPCH.  Setting up tables (DDL Statements) In this example - we will use HDFS as the default table store for Hive. We will make Hive tables over the files in S3 using the external tables functionality in Hive. Executing DDL commands does not require a functioning Hadoop cluster (since we are just setting up metadata):\n Declare a simple table containing key-value pairs:     hive\u0026gt; create external table kv (key int, values string) location \u0026lsquo;s3n://data.s3ndemo.hive/kv\u0026rsquo;;\n* Declare a partitioned table over a nested directory containing key-value pairs and associate table partitions with dirs: * ``` hive\u0026gt; create external table pkv (key int, values string) partitioned by (insertdate string); hive\u0026gt; alter table pkv add partition (insertdate='2008-01-01') location 's3n://data.s3ndemo.hive/pkv/2008-01-01';  Declare a table over a TPCH table:     hive\u0026gt; create external table lineitem ( l_orderkey int, l_partkey int, l_suppkey int, l_linenumber int, l_quantity double, l_extendedprice double, l_discount double, l_tax double, l_returnflag string, l_linestatus string, l_shipdate string, l_commitdate string, l_receiptdate string, l_shipinstruct string, l_shipmode string, l_comment string) row format delimited fields terminated by \u0026lsquo;|\u0026rsquo; location \u0026lsquo;s3n://data.s3ndemo.hive/tpch/lineitem\u0026rsquo;;\n The TPCH DDL statements are slightly modified versions of the original TPCH statements (since Hive does not support all the data types used in TPCH). All the TPCH DDL statements for Hive can be be found ^TpchDdlForHive.sql ## Executing Queries Hive can execute some queries without a Hadoop cluster. For example: hive\u0026gt; select * from kv limit 10;\n `select *` queries with limit clauses can be performed locally on the Hive CLI itself. If you are doing this - please note that: * `fs.default.name` should be set to `[file:///![](images/icons/linkext7.gif)](https://hive.apache.org/)` in case CLI is not configured to use a working Hadoop cluster * **Please Please do not select all the rows from large data sets**. This will cause large amount of data to be downloaded from S3 to outside AWS and incur charges on the host account for these data sets! Of course - the real fun is in doing some non-trivial queries using map-reduce. For this we will need a Hadoop cluster (finally!): 1. Start a Hadoop cluster on EC2 (using directions from [Hadoop-EC2 tutorial![](images/icons/linkext7.gif)](http://wiki.apache.org/hadoop/AmazonEC2) - but making sure to use a version of ec2 scripts with HADOOP-5839 applied! User is free to allocate any number of nodes they wish - although this tutorial was tried out with 10 nodes. 2. Note down the public hostnames of the master node. For example, the public hostname maybe something like: * `ec2-12-34-56-78.compute-1.amazonaws.com` 1.#3 Point the Hive CLI to use this Hadoop cluster by executing: * ``` hive\u0026gt; set fs.default.name=hdfs://ec2-12-34-56-78.compute-1.amazonaws.com:50001; hive\u0026gt; set mapred.job.tracker=ec2-12-34-56-78.compute-1.amazonaws.com:50002; 1.#4 Set up a ssh tunnel via port 2600 to the Hadoop master. This can be done by executing the following from another terminal/window:\n $ ssh -i \u0026lt;path to Hadoop private key path\u0026gt; -D 2600 ec2-12-34-56-78.compute-1.amazonaws.com  Now we are all setup. The sample query from TPCH (1.sql) can be tried as follows:\n hive\u0026gt; insert overwrite directory '/tmp/tpcresults-1.sql' select l_returnflag, l_linestatus, sum ( l_quantity ) as sum_qty, sum ( l_extendedprice ) as sum_base_price, sum ( l_extendedprice * ( 1 - l_discount )) as sub_disc_price, sum ( l_extendedprice * ( 1 - l_discount ) * ( 1 + l_tax )) as sum_charge, avg ( l_quantity ) as avg_qty, avg ( l_extendedprice ) as avg_price, avg ( l_discount ) as avg_disc, count ( 1 ) as count_order from lineitem where l_shipdate \u0026lt;= to_date('1998-12-01') group by l_returnflag, l_linestatus; This launches one map-reduce job and on 10 nodes with default hadoop/hive settings - this took about 10 minutes. The results in this case are stored in HDFS and can be obtained by doing a dfs -cat /tmp/tpcresults/1-2.sql/* - either from bin/hadoop or from hive CLI. The query above differs from the TPCH query in skipping the order by clause - since it\u0026rsquo;s not implemented by Hive currently.\nStoring results back in S3 The results could also have been stored as a file in S3 directly, for example, we could alter the previous insert clause to read as:\n hive\u0026gt; insert overwrite directory 's3n://target-bucket/tpcresults-1.sql'; As another alternative, one could have created an external table over S3 and stored the results directly in it, for example:\n hive\u0026gt; create external table t1 (flag string, status string, double ...) location 's3n://jssarma/tpcresults-1.sql'; hive\u0026gt; insert overwrite table t1 select ...; Similarly, one could have stored the results back in a partition in an partitioned external table as well.\nUsing tmp tables in HDFS Currently, Hive does not have any explicit support tmp tables. But tables defined over HDFS in EC2 are like tmp tables since they only last for the duration of the Hadoop cluster. Since they are likely to be much faster than accessing S3 directly - they can be used to stage data that may be accessed repeatedly during a session. For example - for the TPCH dataset - one may want to do some analysis of customer attributes against order details - and it would be first beneficial to materialize the join of these data sets and then do repeated queries against it. Here\u0026rsquo;s some example sql that would do the same:\n hive\u0026gt; create table cust_order (nationkey string, acctbal double, mktsegment string, orderstatus string, totalprice double); hive\u0026gt; from customer c left outer join orders o on (c.c_custkey = o.o_custkey) insert overwrite table cust_order select c.c_nationkey, c.c_acctbal, c.c_mktsegment, o.o_orderstatus, o.o_totalprice; Appendix \u0026laquo;Anchor(ConfigHell)\u0026raquo;\nConfiguration Guide The socket related options allow Hive CLI to communicate with the Hadoop cluster using a ssh tunnel (that will be established later). The job.ugi is specified to avoid issues with permissions on HDFS. mapred.map.tasks specification is a hack that works around HADOOP-5861 and may need to be set higher for large clusters. mapred.reduce.tasks is specified to let Hive determine the number of reducers (see HIVE-490).\nLinks  Unknown macro: {link-to} Hive and AWS  presents general landscape and alternative on running Hive queries in AWS.\n On issues and lessons learned during this integration effort  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveaws-hivings3nremotely_27362102/","tags":null,"title":"Apache Hive : HiveAws HivingS3nRemotely"},{"categories":null,"contents":"Apache Hive : HiveClient  Command Line JDBC  JDBC Client Sample Code  Running the JDBC Sample Code   JDBC Client Setup for a Secure Cluster   Python PHP ODBC Thrift  Thrift Java Client Thrift C++ Client Thrift Node Clients Thrift Ruby Client    This page describes the different clients supported by Hive. The command line client currently only supports an embedded server. The JDBC and Thrift-Java clients support both embedded and standalone servers. Clients in other languages only support standalone servers.\nFor details about the standalone server see Hive Server or HiveServer2.\nCommand Line Operates in embedded mode only, that is, it needs to have access to the Hive libraries. For more details see Getting Started and Hive CLI.\nJDBC This document describes the JDBC client for the original Hive Server (sometimes called Thrift server or HiveServer1). For information about the HiveServer2 JDBC client, see JDBC in the HiveServer2 Clients document. HiveServer2 use is recommended; the original HiveServer has several concurrency issues and lacks several features available in HiveServer2.\nVersion information\nThe original Hive Server was removed from Hive releases starting in version 1.0.0. See HIVE-6977.\nFor embedded mode, uri is just \u0026ldquo;jdbc:hive://\u0026rdquo;. For standalone server, uri is \u0026ldquo;jdbc:hive://host:port/dbname\u0026rdquo; where host and port are determined by where the Hive server is run. For example, \u0026ldquo;jdbc:hive://localhost:10000/default\u0026rdquo;. Currently, the only dbname supported is \u0026ldquo;default\u0026rdquo;.\nJDBC Client Sample Code import java.sql.SQLException; import java.sql.Connection; import java.sql.ResultSet; import java.sql.Statement; import java.sql.DriverManager; public class HiveJdbcClient { private static String driverName = \u0026quot;org.apache.hadoop.hive.jdbc.HiveDriver\u0026quot;; public static void main(String[] args) throws SQLException { try { Class.forName(driverName); } catch (ClassNotFoundException e) { // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); } Connection con = DriverManager.getConnection(\u0026quot;jdbc:hive://localhost:10000/default\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;\u0026quot;); Statement stmt = con.createStatement(); String tableName = \u0026quot;testHiveDriverTable\u0026quot;; stmt.executeQuery(\u0026quot;drop table \u0026quot; + tableName); ResultSet res = stmt.executeQuery(\u0026quot;create table \u0026quot; + tableName + \u0026quot; (key int, value string)\u0026quot;); // show tables String sql = \u0026quot;show tables '\u0026quot; + tableName + \u0026quot;'\u0026quot;; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); if (res.next()) { System.out.println(res.getString(1)); } // describe table sql = \u0026quot;describe \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1) + \u0026quot;\\t\u0026quot; + res.getString(2)); } // load data into table // NOTE: filepath has to be local to the hive server // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line String filepath = \u0026quot;/tmp/a.txt\u0026quot;; sql = \u0026quot;load data local inpath '\u0026quot; + filepath + \u0026quot;' into table \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); // select * query sql = \u0026quot;select * from \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(String.valueOf(res.getInt(1)) + \u0026quot;\\t\u0026quot; + res.getString(2)); } // regular hive query sql = \u0026quot;select count(1) from \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1)); } } } Running the JDBC Sample Code # Then on the command-line $ javac HiveJdbcClient.java # To run the program in standalone mode, we need the following jars in the classpath # from hive/build/dist/lib # hive_exec.jar # hive_jdbc.jar # hive_metastore.jar # hive_service.jar # libfb303.jar # log4j-1.2.15.jar # # from hadoop/build # hadoop-*-core.jar # # To run the program in embedded mode, we need the following additional jars in the classpath # from hive/build/dist/lib # antlr-runtime-3.0.1.jar # derby.jar # jdo2-api-2.1.jar # jpox-core-1.2.2.jar # jpox-rdbms-1.2.2.jar # # as well as hive/build/dist/conf $ java -cp $CLASSPATH HiveJdbcClient # Alternatively, you can run the following bash script, which will seed the data file # and build your classpath before invoking the client. #!/bin/bash HADOOP_HOME=/your/path/to/hadoop HIVE_HOME=/your/path/to/hive echo -e '1\\x01foo' \u0026gt; /tmp/a.txt echo -e '2\\x01bar' \u0026gt;\u0026gt; /tmp/a.txt HADOOP_CORE={{ls $HADOOP_HOME/hadoop-*-core.jar}} CLASSPATH=.:$HADOOP_CORE:$HIVE_HOME/conf for i in ${HIVE_HOME}/lib/*.jar ; do CLASSPATH=$CLASSPATH:$i done java -cp $CLASSPATH HiveJdbcClient JDBC Client Setup for a Secure Cluster To configure Hive on a secure cluster, add the directory containing hive-site.xml to the CLASSPATH of the JDBC client.\nPython Operates only on a standalone server. Set (and export) PYTHONPATH to build/dist/lib/py.\nThe python modules imported in the code below are generated by building hive.\nPlease note that the generated python module names have changed in hive trunk.\n#!/usr/bin/env python import sys from hive import ThriftHive from hive.ttypes import HiveServerException from thrift import Thrift from thrift.transport import TSocket from thrift.transport import TTransport from thrift.protocol import TBinaryProtocol try: transport = TSocket.TSocket('localhost', 10000) transport = TTransport.TBufferedTransport(transport) protocol = TBinaryProtocol.TBinaryProtocol(transport) client = ThriftHive.Client(protocol) transport.open() client.execute(\u0026quot;CREATE TABLE r(a STRING, b INT, c DOUBLE)\u0026quot;) client.execute(\u0026quot;LOAD TABLE LOCAL INPATH '/path' INTO TABLE r\u0026quot;) client.execute(\u0026quot;SELECT * FROM r\u0026quot;) while (1): row = client.fetchOne() if (row == None): break print row client.execute(\u0026quot;SELECT * FROM r\u0026quot;) print client.fetchAll() transport.close() except Thrift.TException, tx: print '%s' % (tx.message) PHP Operates only on a standalone server.\n\u0026lt;?php // set THRIFT_ROOT to php directory of the hive distribution $GLOBALS['THRIFT_ROOT'] = '/lib/php/'; // load the required files for connecting to Hive require_once $GLOBALS['THRIFT_ROOT'] . 'packages/hive_service/ThriftHive.php'; require_once $GLOBALS['THRIFT_ROOT'] . 'transport/TSocket.php'; require_once $GLOBALS['THRIFT_ROOT'] . 'protocol/TBinaryProtocol.php'; // Set up the transport/protocol/client $transport = new TSocket('localhost', 10000); $protocol = new TBinaryProtocol($transport); $client = new ThriftHiveClient($protocol); $transport-\u0026gt;open(); // run queries, metadata calls etc $client-\u0026gt;execute('SELECT * from src'); var_dump($client-\u0026gt;fetchAll()); $transport-\u0026gt;close(); ODBC Operates only on a standalone server. The Hive ODBC client provides a set of C-compatible library functions to interact with Hive Server in a pattern similar to those dictated by the ODBC specification. See Hive ODBC Driver.\nThrift Thrift Java Client Operates both in embedded mode and on standalone server.\nThrift C++ Client Operates only on a standalone server. In the works.\nThrift Node Clients Thrift Node clients are available on github at https://github.com/wdavidw/node-thrift-hive and https://github.com/forward/node-hive.\nThrift Ruby Client A Thrift Ruby client is available on github at https://github.com/forward3d/rbhive.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveclient_27362101/","tags":null,"title":"Apache Hive : HiveClient"},{"categories":null,"contents":"Apache Hive : HiveContributorsMinutes100601 Notes provided by Namit Jain.\nThe following people were present:\n Facebook (Paul Yang; Ning Zhang; Yongqiang He; Ahmed Aly; John Sichi; Ashish Thusoo; Namit Jain) Netflix (Eva Tse; Jerome Boulon) Cloudera (Arvind Prabhakar; Vinithra Varadharajan; Carl Steinbach) Yahoo (Alan Gates)  The following were the main meeting minutes:\n  We should have these meetings more often, say every month. Cloudera will host the next meeting.\n  We should try to have a release every 4 months. We should try to push out 0.6 before end of June, For the new release, Cloudera will take a lead on the release management issues and also help with documentation. Documentation for Hive leaves a lot to be desired.\n  The test framework is pretty brittle, and it is pretty difficult for new people to do big contributions without having a very sound test-plan. Ideally, facebook should host a test cluster so that everyone can run tests there.\n  A lot of external customers are asking for ODBC/JDBC support on top of Hive. Cloudera will take the lead on that.\n  The process of making a new committer should be more transparent. In order to grow the community, it would be very desirable to add more committers outside Facebook.\n  Create new components for Drivers (ODBC/JDBC) and UDFs.\n  Yahoo will take the lead of making Hive work on top of Zebra\n  Some new tasks were identified, but they can change if new priorities come in.\n Carl will focusing on \u0026lsquo;having\u0026rsquo; support and co-related sub-queries.\n  Arvind will be focusing on the cost-based optimizer\n  The main idea was that we should meet more often and share our ideas. Time-based release will be very desirable.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivecontributorsminutes100601_27362064/","tags":null,"title":"Apache Hive : HiveContributorsMinutes100601"},{"categories":null,"contents":"Apache Hive : HiveContributorsMinutes100706 Attendees: Amr Awadallah, John Sichi, Paul Yang, Olga Natkovich, Ajay Kidave, Yongqiang He, Basab Malik, Vinithra Varadharajan, bc Wong, Arvind Prabhakar, Carl Steinbach\n bc Wong gave a live demo of Cloudera\u0026rsquo;s Hue framework and the Beeswax Hive web interface.  Slides from this talk are available here: http://www.slideshare.net/cwsteinbach/cloudera-huebeeswax Hue was recently released as open source. The code is available on Github here: http://github.com/cloudera/hue   Olga Natkovich gave a whiteboard talk on HOwl.  HOwl = Hive !MetaStore + Owl = shared metadata system between Pig, Hive, and Map Reduce HOwl will likely leverage the !MetaStore schema and ORM layer. A somewhat outdated Owl design document is available here: http://wiki.apache.org/pig/owl   Carl gave an update on progress with the 0.6.0 release.  There was a discussion about the plan to move the documentation off of the wiki and into version control. Several people voiced concerns that developers/users are less likely to update the documentation if doing so requires them to submit a patch. The new proposal for documentation reached at the meeting is as follows:  The trunk version of the documentation will be maintained on the wiki. As part of the release process the documentation will be copied off of the wiki and converted to xdoc, and then checked into svn. HTML documentation generated from the xdoc will be posted to the Hive webpage when the new release is posted.   Carl is going to investigate the feasibility of writing a tool that converts documentation directly from !MoinMoin wiki markup to xdoc.   John agreed to host the next contributors meeting at Facebook.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivecontributorsminutes100706_27362065/","tags":null,"title":"Apache Hive : HiveContributorsMinutes100706"},{"categories":null,"contents":"Apache Hive : HiveCounters Task counters created by Hive during query execution\n For Tez execution, %context is set to the mapper/reducer name. For other execution engines it is not included in the counter name.\n   Counter Name Description     RECORDS_IN[_%context] Input records read   RECORDS_OUT[_%context] Output records written   RECORDS_OUT_INTERMEDIATE[_%context] Records written as intermediate records to ReduceSink (which become input records to other tasks)   CREATED_FILES Number of files created   DESERIALIZE_ERRORS Deserialization errors encountered while reading data    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivecounters_67636835/","tags":null,"title":"Apache Hive : HiveCounters"},{"categories":null,"contents":"Apache Hive : HiveDerbyServerMode Hive Using Derby in Server Mode  Hive Using Derby in Server Mode  Download Derby Set Environment Starting Derby Configure Hive to Use Network Derby Copy Derby Jar Files Start Up Hive The Result    Hive in embedded mode has a limitation of one active user at a time. You may want to run Derby as a Network Server, this way multiple users can access it simultaneously from different systems.\nSee Metadata Store and Embedded Metastore for more information.\nDownload Derby It is suggested you download the version of Derby that ships with Hive. If you have already run Hive in embedded mode, the first line of derby.log contains the version.\nMy structure looks like this:\n/opt/hadoop/hadoop-0.17.2.1 /opt/hadoop/db-derby-10.4.1.3-bin /opt/hadoop/hive cd /opt/hadoop \u0026lt;download\u0026gt; tar -xzf db-derby-10.4.1.3-bin.tar.gz mkdir db-derby-10.4.1.3-bin/data Set Environment The variable to set has changed over the years. DERBY_HOME is now the proper name. I set this and the legacy name.\n/etc/profile.d/derby.sh\nDERBY_INSTALL=/opt/hadoop/db-derby-10.4.1.3-bin DERBY_HOME=/opt/hadoop/db-derby-10.4.1.3-bin export DERBY_INSTALL export DERBY_HOME Hive also likes to know where Hadoop is installed:\n/etc/profile.d/hive.sh\nHADOOP=/opt/hadoop/hadoop-0.17.2.1/bin/hadoop export HADOOP Starting Derby Likely you are going to want to run Derby when Hadoop starts up. An interesting place for this other than as an lsb-init-script might be alongside Hadoop scripts like start-dfs. By default Derby will create databases in the directory it was started from.\ncd /opt/hadoop/db-derby-10.4.1.3-bin/data # If you are using JDK 1.7u51+, you'll need to either specify an ephemeral port (typically between 49152 and 65535) # or add a grant to your JDK version's java.policy file. # See http://stackoverflow.com/questions/21154400/unable-to-start-derby-database-from-netbeans-7-4 for details. nohup /opt/hadoop/db-derby-10.4.1.3-bin/startNetworkServer -h 0.0.0.0 \u0026amp; Configure Hive to Use Network Derby Edit /opt/hadoop/hive/conf/hive-site.xml as follows. Note that \u0026ldquo;hadoop1\u0026rdquo; should be replaced with the hostname or IP address where the Derby network server can be found.\n/opt/hadoop/hive/conf/hive-site.xml\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:derby://hadoop1:1527/metastore_db;create=true\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;JDBC connect string for a JDBC metastore\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.derby.jdbc.ClientDriver\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Driver class name for a JDBC metastore\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; /opt/hadoop/hive/conf/jpox.properties\n Version: JPOX properties are NOT used in Hive 5.0 or later.\nJPOX properties can be specified in hive-site.xml. Normally jpox.properties changes are not required.\njavax.jdo.PersistenceManagerFactoryClass=org.jpox.PersistenceManagerFactoryImpl org.jpox.autoCreateSchema=false org.jpox.validateTables=false org.jpox.validateColumns=false org.jpox.validateConstraints=false org.jpox.storeManagerType=rdbms org.jpox.autoCreateSchema=true org.jpox.autoStartMechanismMode=checked org.jpox.transactionIsolation=read_committed javax.jdo.option.DetachAllOnCommit=true javax.jdo.option.NontransactionalRead=true javax.jdo.option.ConnectionDriverName=org.apache.derby.jdbc.ClientDriver javax.jdo.option.ConnectionURL=jdbc:derby://hadoop1:1527/metastore_db;create=true javax.jdo.option.ConnectionUserName=APP javax.jdo.option.ConnectionPassword=mine Copy Derby Jar Files Now since there is a new client you MUST make sure Hive has these jar files in the lib directory or in the classpath. The same would be true if you used MySQL or some other DB.\ncp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbyclient.jar /opt/hadoop/hive/lib cp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbytools.jar /opt/hadoop/hive/lib If you receive the error \u0026ldquo;javax.jdo.JDOFatalInternalException: Error creating transactional connection factory\u0026rdquo; where the stack trace originates at \u0026ldquo;org.datanucleus.exceptions.ClassNotResolvedException: Class 'org.apache.derby.jdbc.ClientDriver' was not found in the CLASSPATH. Please check your specification and your CLASSPATH\u0026rdquo;, you may benefit from putting the Derby jar files directly in the Hadoop lib directory:\ncp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbyclient.jar /opt/hadoop/hadoop-0.17.2.1/lib cp /opt/hadoop/db-derby-10.4.1.3-bin/lib/derbytools.jar /opt/hadoop/hadoop-0.17.2.1/lib Start Up Hive The metastore will not be created until the first query hits it.\ncd /opt/hadoop/hive bin/hive hive\u0026gt; show tables; A directory should be created: /opt/hadoop/db-derby-10.4.1.3-bin/data/metastore_db .\nThe Result Now you can run multiple Hive instances working on the same data simultaneously and remotely.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivederbyservermode_27362068/","tags":null,"title":"Apache Hive : HiveDerbyServerMode"},{"categories":null,"contents":"Apache Hive : HiveDeveloperFAQ  Developing  How do I move some files?   Building  Maven settings How to build all source? How do I import into Eclipse? How to generate tarball? How to generate protobuf code? How to generate Thrift code?   HIVE-26769  How to compile ODBC? How do I publish Hive artifacts to my local Maven repository?   Testing  How do I run precommit tests on a patch? How do I rerun precommit tests over the same patch? How do I run a single test? How do I run all of the unit tests? How do I run all of the unit tests except for a certain few tests? How do I run the clientpositive/clientnegative unit tests? Why isn\u0026rsquo;t the itests pom connected to the root pom? Why does a test fail with a NullPointerException in MiniDFSCluster?   Debugging  How do I debug into a single test in Eclipse? How do I debug my queries in Hive?      Developing How do I move some files? Post a patch for testing purposes which simply does add and deletes. SVN will not understand these patches are actually moves, therefore you should actually upload the following, in order so the last upload is the patch for testing purposes:\n A patch which has only the non-move changes for commit e.g. HIVE-XXX-for-commit.patch A script of of commands required to make the moves HIVE-XXX-moves.sh A patch for testing purposes HIVE-XXX.patch  The script should be a set of svn mv commands along with any perl commands required for find/replace. For example:\n$ svn mv MyCLass.java MyClass.java $ perl -i -pe 's\u0026lt;at:var at:name=\u0026quot;MyCLass\u0026quot; /\u0026gt;MyClass@g' MyClass.java Building  See Getting Started: Building Hive from Source for detailed information about building Hive releases 0.13 and later with Maven. See Installing from Source Code (Hive 0.12.0 and Earlier) for detailed information about building Hive 0.12 and earlier with Ant.  Maven settings You might have to set the following Maven options on certain systems to get build working: Set MAVEN_OPTS to \u0026ldquo;-Xmx2g -XX:MaxPermSize=256M\u0026rdquo;.\nHow to build all source? The way Maven is set up differs between the master branch and branch-1. In branch-1, since both Hadoop 1.x and 2.x are supported, you need to specify whether you want to build Hive against Hadoop 1.x or 2.x. This is done via Maven profiles. There is a profile for each version of Hadoop, hadoop-1 and hadoop-2. For most Maven operations one of these profiles needs to be specified or the build will fail.\nIn master, only Hadoop 2.x is supported, thus there is no need to specify a Maven profile for most build operations.\nIn master:\nmvn clean install -DskipTests cd itests mvn clean install -DskipTests In branch-1, MVN:\nmvn clean install -DskipTests -Phadoop-2 cd itests mvn clean install -DskipTests -Phadoop-2 To build against Hadoop 1.x, switch the above to -Phadoop-1.\nFor the remainder of this page we will assume master and not show the profiles. However, if you are working on branch-1 remember that you will need to add in the appropriate profile.\nHow do I import into Eclipse? Build and generate Eclipse files (the conservative method).\nFor master – without using a local Maven repository: $ mkdir workspace $ cd workspace $ git clone https://github.com/apache/hive.git $ cd hive $ mvn clean package eclipse:clean eclipse:eclipse -Pitests -DskipTests -DdownloadSources -DdownloadJavadocs NOTE: The presence of the package target is crucial in successfully generating the project files.\nNOTE: It is recommended to wipe out or rename the org/apache/hive subtree of the local m2 repository prior to generating the project files; and you may want to check after its execution that it does not contain any files – if it does contain something, then your project files may not be fully independent of downloaded Maven artifacts.\nThis will work on all branches; but keep in mind that in this case you are installing the Hive artifacts into your local repository.\n$ mkdir workspace $ cd workspace $ git clone https://github.com/apache/hive.git $ cd hive $ mvn clean install -DskipTests $ mvn eclipse:clean $ mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs $ cd itests $ mvn clean install -DskipTests $ mvn eclipse:clean $ mvn eclipse:eclipse -DdownloadSources -DdownloadJavadocs In Eclipse define M2_REPO in Preferences -\u0026gt; Java -\u0026gt; Build Path -\u0026gt; Classpath Variables to either:\nMac Example\n/Users/$USER/.m2/repository Linux Example\n/home/$USER/.m2/repository Windows Example\nC:/users/$USER/.m2/repository Then import the workspaces. If you get an error about \u0026ldquo;restricted use of Signal\u0026rdquo; for Beeline and CLI, follow these instructions.\nNote that if you use the Hive git base directory as the Eclipse workspace, then it does not pick the right project names (for example, picks \u0026lsquo;ant\u0026rsquo; instead of \u0026lsquo;hive-ant\u0026rsquo;). Therefore it\u0026rsquo;s recommended to have the workspace directory one up from the git directory. For example workspaces/hive-workspace/hive where hive-workspace is the Eclipse workspace and hive is the git base directory.\nHow to generate tarball? mvn clean package -DskipTests -Pdist It will then be located in the packaging/target/ directory.\nHow to generate protobuf code? cd ql mvn clean install -DskipTests -Pprotobuf How to generate Thrift code? mvn clean install -Pthriftif -DskipTests -Dthrift.home=/usr/local Don’t forget to update hive_metastore.proto when changing `hive_metastore.thrift\nHIVE-26769 [TRACKING] gRPC support for Hive metastore Open`### How to run findbugs after a change?\nmvn site -Pfindbugs Note: Available in Hive 1.1.0 onward (see HIVE-8327).\nHow to compile ODBC? cd odbc mvn compile -Podbc -Dthrift.home=/usr/local -Dboost.home=/usr/local How do I publish Hive artifacts to my local Maven repository? mvn clean install -DskipTests cd itests mvn clean install -DskipTests Testing For general information, see Unit Tests and Debugging in the Developer Guide.\nHow do I run precommit tests on a patch? A Jenkins job will start when you create a pull request on GitHub.\nHow do I rerun precommit tests over the same patch? You can ask a committer to rerun it, or do git commit --amend and push it.\nHow do I run a single test? ITests\nNote that any test in the itests directory needs to be executed from within the itests directory. The pom is disconnected from the parent project for technical reasons.Single test class:\nmvn test -Dtest=ClassName Single test method:\nmvn test -Dtest=ClassName#methodName Note that a pattern can also be supplied to -Dtests to run multiple tests matching the pattern:\nmvn test -Dtest='org.apache.hive.beeline.*' For more usage see the documentation for the Maven Surefire Plugin.\nHow do I run all of the unit tests? mvn test cd itests mvn test Note that you need to have previously built and installed the jars:\nmvn clean install -DskipTests cd itests mvn clean install -DskipTests How do I run all of the unit tests except for a certain few tests? Similar to running all tests, but define test.excludes.additional to specify a test/pattern to exclude from the test run. For example the following will run all tests except for the CliDriver tests:\ncd itests mvn test -Dtest.excludes.additional='**/Test*CliDriver.java' How do I run the clientpositive/clientnegative unit tests? See this page.\nWhy isn\u0026rsquo;t the itests pom connected to the root pom? It would be great to have it connected, but it would make it harder to use mvn test locally. The best option would be to utilize the failsafe plugin for integration testing; but it needs a bit different setup, and it\u0026rsquo;s harder to use for now\u0026hellip;. If you\u0026rsquo;d like to give that a try, by all means, go ahead.\nThere is an option to attach all the itest subprojects to the main project by enabling this with -Pitests (HIVE-13490).\nThere are some good and bad sides using this, it\u0026rsquo;s introduced as a profile to clearly communicate that the integration tests are attached to the main project.\nThe good side is that you may freely use -Pitests to run integration tests from the root project without the need of mvn install.\nmvn test -q -Pitests -Dtest=TestCliDriver -Dqtest=sample2.q The bad side is that a simple mvn test -Pitestswill start executing all integration tests.\nWhy does a test fail with a NullPointerException in MiniDFSCluster? If any test fails with the error below it means you have an inappropriate umask setting. It should be set to 0022.\njava.lang.NullPointerException: null at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:426) at org.apache.hadoop.hdfs.MiniDFSCluster.\u0026lt;init\u0026gt;(MiniDFSCluster.java:284) at org.apache.hadoop.hdfs.MiniDFSCluster.\u0026lt;init\u0026gt;(MiniDFSCluster.java:124) Debugging How do I debug into a single test in Eclipse? You can debug into a single JUnit test in Eclipse by first making sure you\u0026rsquo;ve built the Eclipse files and imported the project into Eclipse as described here. Then set one or more breakpoints, highlight the method name of the JUnit test method you want to debug into, and do Run-\u0026gt;Debug.\nAnother useful method to debug these tests is to attach a remote debugger. When you run the test, enable the debug mode for surefire by passing in \u0026ldquo;-Dmaven.surefire.debug\u0026rdquo;. Additional details on how to turning on debugging for surefire can be found here. Now when you run the tests, it will wait with a message similar to\nListening for transport dt_socket at address: 5005 Note that this assumes that you are still using the default port 5005 for surefire. Otherwise you might see a different port. Once you see this message, in Eclipse right click on the project you want to debug, go to \u0026ldquo;Debug As -\u0026gt; Debug Configurations -\u0026gt; Remote Java Application\u0026rdquo; and hit the \u0026ldquo;+\u0026rdquo; sign on far left top. This should bring up a dialog box. Make sure that the host is \u0026ldquo;localhost\u0026rdquo; and the port is \u0026ldquo;5005\u0026rdquo;. Before you start debugging, make sure that you have set appropriate debug breakpoints in the code. Once ready, hit \u0026ldquo;Debug\u0026rdquo;. Now if you go back to the terminal, you should see the tests running and they will stop at the breakpoints that you set for debugging.\nHow do I debug my queries in Hive? You can also interactively debug your queries in Hive by attaching a remote debugger. To do so, start Beeline with the \u0026ldquo;--debug\u0026rdquo; option.\n$ beeline --debug Listening for transport dt_socket at address: 8000 Once you see this message, in Eclipse right click on the project you want to debug, go to \u0026ldquo;Debug As -\u0026gt; Debug Configurations -\u0026gt; Remote Java Application\u0026rdquo; and hit the \u0026ldquo;+\u0026rdquo; sign on far left top. This should bring up a dialog box. Make sure that the host is the host on which the Beeline CLI is running and the port is \u0026ldquo;8000\u0026rdquo;. Before you start debugging, make sure that you have set appropriate debug breakpoints in the code. Once ready, hit \u0026ldquo;Debug\u0026rdquo;. The remote debugger should attach to Beeline and proceed.\n$ beeline --debug Listening for transport dt_socket at address: 8000 Beeline version 1.2.0 by Apache Hive beeline\u0026gt; At this point, run your queries as normal and it should stop at the breakpoints that you set so that you can start debugging.\nThis method should work great if your queries are simple fetch queries that do not kick off MapReduce jobs. If a query runs in a distributed mode, it becomes very hard to debug. Therefore, it is advisable to run in a \u0026ldquo;local\u0026rdquo; mode for debugging. In order to run Hive in local mode, do the following:\nMRv1:\nSET mapred.job.tracker=local MRv2 (YARN):\nSET mapreduce.framework.name=local At this point, attach the remote debugger as mentioned before to start debugging your queries.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivedeveloperfaq_27823747/","tags":null,"title":"Apache Hive : HiveDeveloperFAQ"},{"categories":null,"contents":"Apache Hive : HiveJDBCInterface Hive JDBC Driver  Hive JDBC Driver  Integration with Pentaho Integration with SQuirrel SQL Client    The current JDBC interface for Hive only supports running queries and fetching results. Only a small subset of the metadata calls are supported.\nTo see how the JDBC interface can be used, see sample code.\nIntegration with Pentaho  Download pentaho report designer from the pentaho website. Overwrite report-designer.sh with the code provided below.  #!/bin/sh HADOOP_CORE={{ls $HADOOP_HOME/hadoop-*-core.jar}} CLASSPATH=.:$HADOOP_CORE:$HIVE_HOME/conf for i in ${HIVE_HOME}/lib/*.jar ; do CLASSPATH=$CLASSPATH:$i done CLASSPATH=$CLASSPATH:launcher.jar echo java -XX:MaxPermSize=512m -cp $CLASSPATH -jar launcher.jar java -XX:MaxPermSize=512m -cp $CLASSPATH org.pentaho.commons.launcher.Launcher Build and start the hive server with instructions from HiveServer. Compile and run the Hive JDBC client code to load some data (I haven\u0026rsquo;t figured out how to do this in report designer yet). See sample code for loading the data. Run the report designer (note step 2).  $ sh reporter-designer.sh Select \u0026lsquo;Report Design Wizard\u0026rsquo;. Select a template - say \u0026lsquo;fall template\u0026rsquo; - next. Create a new data source - JDBC (custom), Generic database. Provide Hive JDBC parameters. Give the connection a name \u0026lsquo;hive\u0026rsquo;.   URL: jdbc:hive://localhost:10000/default Driver name: org.apache.hadoop.hive.jdbc.HiveDriver Username and password are empty Click on \u0026lsquo;Test\u0026rsquo;. The test should succeed. Edit the query: select \u0026lsquo;Sample Query\u0026rsquo;, click edit query, click on the connection \u0026lsquo;hive\u0026rsquo;. Create a new query. Write a query on the table testHiveDriverTable, for example, select * from testHiveDriverTable. Click next. Layout Step: Add PageOfPages to Group Items By. Add key and value as Selected Items. Click next. And Finish. Change the Report header to \u0026lsquo;hive-pentaho-report\u0026rsquo;. Change the type of the header to \u0026lsquo;html\u0026rsquo;. Run the report and generate pdf. You should get something like the report attached here.  Integration with SQuirrel SQL Client   Download, install and start the SQuirrel SQL Client from the SQuirrel SQL website.\n  Select \u0026lsquo;Drivers -\u0026gt; New Driver\u0026hellip;\u0026rsquo; to register the Hive JDBC driver.\n Enter the driver name and example URL:   Name: Hive Example URL: jdbc:hive://localhost:10000/default ``\n  Select \u0026lsquo;Extra Class Path -\u0026gt; Add\u0026rsquo; to add the following jars from your local Hive and Hadoop distribution.\n   HIVE_HOME/build/dist/lib/*.jar HADOOP_HOME/hadoop-*-core.jar Select \u0026lsquo;List Drivers\u0026rsquo;. This will cause SQuirrel to parse your jars for JDBC drivers and might take a few seconds. From the \u0026lsquo;Class Name\u0026rsquo; input box select the Hive driver:   org.apache.hadoop.hive.jdbc.HiveDriver Click \u0026lsquo;OK\u0026rsquo; to complete the driver registration. Select \u0026lsquo;Aliases -\u0026gt; Add Alias\u0026hellip;\u0026rsquo; to create a connection alias to your Hive server.  Give the connection alias a name in the \u0026lsquo;Name\u0026rsquo; input box. Select the Hive driver from the \u0026lsquo;Driver\u0026rsquo; drop-down. Modify the example URL as needed to point to your Hive server. Leave \u0026lsquo;User Name\u0026rsquo; and \u0026lsquo;Password\u0026rsquo; blank and click \u0026lsquo;OK\u0026rsquo; to save the connection alias.   To connect to the Hive server, double-click the Hive alias and click \u0026lsquo;Connect\u0026rsquo;.  When the connection is established you will see errors in the log console and might get a warning that the driver is not JDBC 3.0 compatible. These alerts are due to yet-to-be-implemented parts of the JDBC metadata API and can safely be ignored. To test the connection enter SHOW TABLES in the console and click the run icon.\nAlso note that when a query is running, support for the \u0026lsquo;Cancel\u0026rsquo; button is not yet available.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivejdbcinterface_27362100/","tags":null,"title":"Apache Hive : HiveJDBCInterface"},{"categories":null,"contents":"Apache Hive : HiveODBC Hive ODBC Driver  Hive ODBC Driver  Introduction  Suggested Reading Software Requirements Driver Architecture   Building and Setting Up ODBC Components  Hive Client Build/Setup unixODBC API Wrapper Build/Setup Connecting the Driver to a Driver Manager Testing with ISQL Build libodbchive.so for 3rd Party Driver Manager Troubleshooting   Current Status    These instructions are for the Hive ODBC driver available in Hive for HiveServer1.\nThere is no ODBC driver available for HiveServer2 as part of Apache Hive. There are third party ODBC drivers available from different vendors, and most of them seem to be free.\nHiveServer was removed from Hive releases starting with Hive 1.0.0. See HIVE-6977. Please switch over to HiveServer2.\nIntroduction The Hive ODBC Driver is a software library that implements the Open Database Connectivity (ODBC) API standard for the Hive database management system, enabling ODBC compliant applications to interact seamlessly (ideally) with Hive through a standard interface. This driver will NOT be built as a part of the typical Hive build process and will need to be compiled and built separately according to the instructions below.\nSuggested Reading This guide assumes you are already familiar with the following:\n Hive Hive Server Thrift ODBC API unixODBC  Software Requirements The following software components are needed for the successful compilation and operation of the Hive ODBC driver:\n Hive Server – a service through which clients may remotely issue Hive commands and requests. The Hive ODBC driver depends on Hive Server to perform the core set of database interactions. Hive Server is built as part of the Hive build process. More information regarding Hive Server usage can be found here. Apache Thrift – a scalable cross-language software framework that enables the Hive ODBC driver (specifically the Hive client) to communicate with the Hive Server. See this link for the details on Thrift Installation. The Hive ODBC driver was developed with Thrift trunk version r790732, but the latest revision should also be fine. Make sure you note the Thrift install path during the Thrift build process as this information will be needed during the Hive client build process. The Thrift install path will be referred to as THRIFT_HOME.  Driver Architecture Internally, the Hive ODBC Driver contains two separate components: Hive client, and the unixODBC API wrapper.\n Hive client – provides a set of C-compatible library functions to interact with Hive Server in a pattern similar to those dictated by the ODBC specification. However, Hive client was designed to be independent of unixODBC or any ODBC specific headers, allowing it to be used in any number of generic cases beyond ODBC. unixODBC API wrapper – provides a layer on top of Hive client that directly implements the ODBC API standard. The unixODBC API wrapper will be compiled into a shared object library, which will be the final form of the Hive ODBC driver. The wrapper files will remain a file attachment on the associated JIRA until it can be checked into the unixODBC code repository: HIVE-187, HIVE-1101.  Building and Setting Up ODBC Components NOTE: Hive client needs to be built and installed before the unixODBC API wrapper can compile successfully.\nHive Client Build/Setup In order to build and install the Hive client:\n Checkout and setup the latest version of Apache Hive from the Subversion or Git source code repository. For more details, see Getting Started with Hive. From this point onwards, the path to the Hive root directory will be referred to as HIVE_SRC_ROOT.  Using a tarball source release\nIf you are compiling against source code contained in the tarball release package then HIVE_SRC_ROOT refers to the \u0026lsquo;src\u0026rsquo; subdirectory.\nThe ODBC driver is broken on trunk!\nCurrently the C++ Thrift client library that the ODBC driver depends on will not build on trunk. This issue is being tracked in HIVE-4433. If you are using trunk prior to release 0.12 check the status of this ticket before proceeding. Also see HIVE-4492. 2. Build the Hive client by running the following command from HIVE_SRC_ROOT. This will compile and copy the libraries and header files to HIVE_SRC_ROOT/build/odbc/. Please keep in mind that all paths should be fully specified (no relative paths). If you encounter an \u0026ldquo;undefined reference to vtables\u0026rdquo; error, make sure that you have specified the absolute path for thrift.home.\n $ ant compile-cpp -Dthrift.home=\u0026lt;THRIFT_HOME\u0026gt; MVN:\n$ cd odbc $ mvn compile -Podbc,hadoop-1 -Dthrift.home=/usr/local -Dboost.home=/usr/local You can optionally force Hive client to compile into a non-native bit architecture by specifying the additional parameter (assuming you have the proper compilation libraries):\n $ ant compile-cpp -Dthrift.home=\u0026lt;THRIFT_HOME\u0026gt; -Dword.size=\u0026lt;32 or 64\u0026gt;  You can verify the entire Hive compilation by running the Hive test suite from HIVE_SRC_ROOT. Specifying the argument \u0026lsquo;-Dthrift.home=\u0026lt;THRIFT_HOME\u0026gt;\u0026rsquo; will enable the tests for the Hive client. If you do NOT specify thrift.home, the Hive client tests will not be run and will just return successful.   $ ant test -Dthrift.home=\u0026lt;THRIFT_HOME\u0026gt; MVN:\n$ cd odbc $ mvn test -Podbc,hadoop-1 -Dthrift.home=/usr/local -Dboost.home=/usr/local You can specifically execute the Hive client tests by running the above command from HIVE_SRC_ROOT/odbc/. NOTE: Hive client tests require that a local Hive Server be operating on port 10000. 2. To install the Hive client libraries onto your machine, run the following command from HIVE_SRC_ROOT/odbc/. NOTE: The install path defaults to /usr/local. While there is no current way to change this default directory from the ant build process, a manual install may be performed by skipping the command below and copying out the contents of HIVE_SRC_ROOT/build/odbc/lib and HIVE_SRC_ROOT/build/odbc/include into their local file system counterparts.\n $ sudo ant install -Dthrift.home=\u0026lt;THRIFT_HOME\u0026gt; NOTE: The compiled static library, libhiveclient.a, requires linking with stdc++ as well as thrift libraries to function properly.\nNOTE: Currently, there is no way to specify non-system library and header directories to the unixODBC build process. Thus, the Hive client libraries and headers MUST be installed to a default system location in order for the unixODBC build process to detect these files. This issue may be remedied in the future.\nunixODBC API Wrapper Build/Setup After you have built and installed the Hive client, you can now install the unixODBC API wrapper:\n In the unixODBC root directory, run the following command:   $ ./configure --enable-gui=no --prefix=\u0026lt;unixODBC_INSTALL_DIR\u0026gt; If you encounter the the errors: \u0026ldquo;redefinition of 'struct _hist_entry'\u0026rdquo; or \u0026ldquo;previous declaration of 'add_history' was here\u0026rdquo; then re-execute the configure with the following command:\n $ ./configure --enable-gui=no --enable-readline=no --prefix=\u0026lt;unixODBC_INSTALL_DIR\u0026gt; To force the compilation of the unixODBC API wrapper into a non-native bit architecture, modify the CC and CXX environment variables to include the appropriate flags. For example:\n $ CC=\u0026quot;gcc -m32\u0026quot; CXX=\u0026quot;g++ -m32\u0026quot; ./configure --enable-gui=no --enable-readline=no --prefix=\u0026lt;unixODBC_INSTALL_DIR\u0026gt; Compile the unixODBC API wrapper with the following:   $ make If you want to completely install unixODBC and all related drivers, run the following from the unixODBC root directory:\n $ sudo make install If your system complains about undefined symbols during unixODBC testing (such as with isql or odbcinst) after installation, try running ldconfig to update your dynamic linker\u0026rsquo;s runtime libraries.\nIf you only want to obtain the Hive ODBC driver shared object library: 3. After compilation, the driver will be located at \u0026lt;unixODBC_BUILD_DIR\u0026gt;/Drivers/hive/.libs/libodbchive.so.1.0.0.\nThis may be copied to any other location as desired. Keep in mind that the Hive ODBC driver has a dependency on the Hive client shared object library: libhiveclient.so and libthrift.so.0.\nYou can manually install the unixODBC API wrapper by doing the following:\n $ cp \u0026lt;unixODBC_BUILD_DIR\u0026gt;/Drivers/hive/.libs/libodbchive.so.1.0.0 \u0026lt;SYSTEM_INSTALL_DIR\u0026gt; $ cd \u0026lt;SYSTEM_INSTALL_DIR\u0026gt; $ ln -s libodbchive.so.1.0.0 libodbchive.so $ ldconfig Connecting the Driver to a Driver Manager This portion assumes that you have already built and installed both the Hive client and the unixODBC API wrapper shared libraries on the current machine. To connect the Hive ODBC driver to a previously installed Driver Manager (such as the one provided by unixODBC or a separate application):\n  Locate the odbc.ini file associated with the Driver Manager (DM).\n If you are installing the driver on the system DM, then you can run the following command to print the locations of DM configuration files.   $ odbcinst -j unixODBC 2.2.14 DRIVERS............: /usr/local/etc/odbcinst.ini SYSTEM DATA SOURCES: /usr/local/etc/odbc.ini FILE DATA SOURCES..: /usr/local/etc/ODBCDataSources USER DATA SOURCES..: /home/ehwang/.odbc.ini SQLULEN Size.......: 8 SQLLEN Size........: 8 SQLSETPOSIROW Size.: 8 `` 2. If you are installing the driver on an application DM, then you have to help yourself on this one . Hint: try looking in the installation directory of your application. * Keep in mind that an application\u0026rsquo;s DM can exist simultaneously with the system DM and will likely use its own configuration files, such as odbc.ini. * Also, note that some applications do not have their own DMs and simply use the system DM.\n  Add the following section to the DM\u0026rsquo;s corresponding odbc.ini:\n   [Hive] Driver = \u0026lt;path_to_libodbchive.so\u0026gt; Description = Hive Driver v1 DATABASE = default HOST = \u0026lt;Hive_server_address\u0026gt; PORT = \u0026lt;Hive_server_port\u0026gt; FRAMED = 0 Testing with ISQL Once you have installed the necessary Hive ODBC libraries and added a Hive entry in your system\u0026rsquo;s default odbc.ini, you will be able to interactively test the driver with isql:\n$ isql -v Hive If your system does not have isql, you can obtain it by installing the entirety of unixODBC. If you encounter an error saying that the shared libraries cannot be opened by isql, use the ldd tool to ensure that all dynamic library dependencies are resolved and use the file tool to ensure that isql and all necessary libraries are compiled into the same architecture (32 or 64 bit).\nBuild libodbchive.so for 3rd Party Driver Manager If you want to build libodbchive.so for other Driver Manager (for example, MicroStrategy uses DataDirect ODBC libraries which contains its own Driver Manager), you need to configure and build libodbchive.so against that Driver Manager (libodbc.so and libodbcinst.so).\nIf you have the 3rd party Driver Manager installed, the easiest way to do that is to find the installation directory containing libodbc.so and libodbcinst.so, and set that directory to LD_LIBRARY_PATH. Then you need to run configure and make for the Hive ODBC driver. After you get the libodbchive.so, make sure the 3rd party application can access the dynamic library libodbchive.so, libthrift.so and libhiveclient.so (through LD_LIBRARY_PATH or ldconfig).\nIf you build libodbchive.so for the 3rd party Driver Manager, isql may not work with the same set of .so files. So you may need to compile a different libodbchive.so for each Driver Manager.\nTroubleshooting  Hive client build process  \u0026ldquo;libthrift.a: could not read symbols: Bad value\u0026rdquo; or \u0026ldquo;relocation R_X86_64_32 against `a local symbol' can not be used when making a shared object\u0026rdquo;?  Try recompiling your Apache Thrift libraries with the -fPIC option for your C++ compiler   \u0026ldquo;undefined reference to vtable\u0026rdquo; ?  Make sure that your Apache Thrift libraries are being included from the proper Thrift directory and that it has the same architecture (32 or 64 bit) as the Hive client. Also, check to make sure you are providing a fully qualified path for the thrift.home parameter.   In general, ldd, file, and nm are essential unix tools for debugging problems with shared object libraries. If you don\u0026rsquo;t know what they are, use man to get more details.    Current Status  Comments: Please keep in mind that this is still an initial version and is still very rough around the edges. However, it provides basic ODBC 3.51 API support for connecting, executing queries, fetching, etc. This driver has been successfully tested on 32-bit and 64-bit linux machines with iSQL. It has also been tested with partial success on enterprise applications such as MicroStrategy. Due to licensing reasons, the unixODBC API wrapper files will be uploaded as a separate JIRA attachment that will not be part of this code repository. Limitations:  Only support for Linux operating systems No support for Unicode No support for asynchronous execution of queries Does not support pattern matching for functions such as SQLColumns and SQLTables; requires exact matches. Hive Server is currently not thread safe (see JIRA HIVE-80: https://issues.apache.org/jira/browse/HIVE-80). This will prevent the driver from safely making multiple connections to the same Hive Server. We need to resolve this issue to allow the driver to operate properly. Hive Server\u0026rsquo;s getSchema() function seems to have trouble with certain types of queries (such as \u0026ldquo;SELECT * \u0026hellip;\u0026rdquo; or \u0026ldquo;EXPLAIN\u0026rdquo;), and so the Hive ODBC driver sometimes has difficulties with these queries as well.   ODBC API Function Support (does anyone know how to remove the linking from the function names?):           SQLAllocConnect supported   SQLAllocEnv supported   SQLAllocHandle supported   SQLAllocStmt supported   SQLBindCol supported   SQLBindParameter NOT supported   SQLCancel NOT supported   SQLColAttribute supported   SQLColumns supported   SQLConnect supported   SQLDescribeCol supported   SQLDescribeParam NOT supported   SQLDisconnect supported   SQLDriverConnect supported   SQLError supported   SQLExecDirect supported   SQLExecute supported   SQLExtendedFetch NOT supported   SQLFetch supported   SQLFetchScroll NOT supported   SQLFreeConnect supported   SQLFreeEnv supported   SQLFreeHandle supported   SQLFreeStmt supported   SQLGetConnectAttr NOT supported   SQLGetData supported (however, SQLSTATE not returning values)   SQLGetDiagField NOT supported   SQLGetDiagRec supported   SQLGetInfo partially supported; (to get MSTR v9 running)   SQLMoreResults NOT supported   SQLNumParams NOT supported   SQLNumResultCols supported   SQLParamOptions NOT supported   SQLPrepare supported; but does not permit parameter markers   SQLRowCount NOT supported   SQLSetConnectAttr NOT supported   SQLSetConnectOption NOT supported   SQLSetEnvAttr Limited support   SQLSetStmtAttr NOT supported   SQLSetStmtOption NOT supported   SQLTables supported   SQLTransact NOT supported    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveodbc_27362099/","tags":null,"title":"Apache Hive : HiveODBC"},{"categories":null,"contents":"Apache Hive : HivePlugins  Creating Custom UDFs Deploying Jars for User Defined Functions and User Defined SerDes  Creating Custom UDFs First, you need to create a new class that extends UDF, with one or more methods named evaluate.\npackage com.example.hive.udf; import org.apache.hadoop.hive.ql.exec.UDF; import org.apache.hadoop.io.Text; public final class Lower extends UDF { public Text evaluate(final Text s) { if (s == null) { return null; } return new Text(s.toString().toLowerCase()); } } (Note that there\u0026rsquo;s already a built-in function for this, it\u0026rsquo;s just an easy example).\nAfter compiling your code to a jar, you need to add this to the Hive classpath. See the section below on deploying jars.\nOnce Hive is started up with your jars in the classpath, the final step is to register your function as described in Create Function:\ncreate temporary function my_lower as 'com.example.hive.udf.Lower'; Now you can start using it:\nhive\u0026gt; select my_lower(title), sum(freq) from titles group by my_lower(title); ... Ended Job = job_200906231019_0006 OK cmo\t13.0 vp\t7.0 For a more involved example, see this page.\nAs of Hive 0.13, you can register your function as a permanent UDF either in the current database or in a specified database, as described in Permanent Functions. For example:\ncreate function my_db.my_lower as 'com.example.hive.udf.Lower'; Deploying Jars for User Defined Functions and User Defined SerDes In order to start using your UDF, you first need to add the code to the classpath:\nhive\u0026gt; add jar my_jar.jar; Added my_jar.jar to class path By default, it will look in the current directory. You can also specify a full path:\nhive\u0026gt; add jar /tmp/my_jar.jar; Added /tmp/my_jar.jar to class path Your jar will then be on the classpath for all jobs initiated from that session. To see which jars have been added to the classpath you can use:\nhive\u0026gt; list jars; my_jar.jar See Hive CLI for full syntax and more examples.\nAs of Hive 0.13, UDFs also have the option of being able to specify required jars in the CREATE FUNCTION statement:\nCREATE FUNCTION myfunc AS 'myclass' USING JAR 'hdfs:///path/to/jar'; This will add the jar to the classpath as if ADD JAR had been called on that jar. ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveplugins_27362098/","tags":null,"title":"Apache Hive : HivePlugins"},{"categories":null,"contents":"Apache Hive : HiveQL This page is deprecated\nPlease see the HiveQL Language Manual\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveql_27362097/","tags":null,"title":"Apache Hive : HiveQL"},{"categories":null,"contents":"Apache Hive : HiveReplicationDevelopment  Introduction  Purposes of Replication  Disaster Recovery Load Balancing   Replication Taxonomy  Transaction Source  Primary-Copy Update-Anywhere   Synchronization Strategy  Eager Lazy       Design  Taxonomy Design Choices  Primary-Copy vs Update-Anywhere Eager vs Lazy   Other Design Choices Basic Approach   Implementation  Events Event IDs, State IDs, and Sequencing of Exports/Imports Handling of Events   Future Features References  Introduction Replication in the context of databases and warehouses is the process of duplication of entities from one warehouse to another. This can be at the broader level of an entire database, or at a smaller level such as a table or partition. The goal of replication is to have a replica which changes whenever the base entity changes.\nIn Hive, replication (introduced in Hive 1.2.0) focuses on disaster recovery, using a lazy, primary-copy model. It uses notifications and export/import statements to implement an API for replication that can then be executed by other tools such as Falcon.\nSee Hive Replication for usage information.\nVersion 2 of Hive Replication This document describes the initial version of Hive Replication. A second version is also available: see HiveReplicationv2Development for details.\nPurposes of Replication Replication is the process of making a duplicate copy of some object such as a database, table, or partition. There are two main use cases for replication: disaster recovery and load balancing.\nDisaster Recovery  Allows recovery of data after a disaster such as a server crashing unrecoverably or natural disasters such as fires and earthquakes. Prioritizes the safety of the data/metadata with no loss. Speed of availability is not as important. Hot-cold backup is often sufficient.  Load Balancing  Allows the splitting of users across multiple systems to manage a heavier load than the system would normally be able to handle. Prioritizes speed and access. Tends to be read-only for most uses. Hot-warm backup is usually desirable.  Replication Taxonomy Replication systems are frequently classified by their transaction source (“where”) and their synchronization strategy (“when”) [1].\nTransaction Source Primary-Copy  Unilateral copy from primary to replica. Pros: Simple concurrency control (no locks introduced by the need for replication). Cons: Poor for non-read load balancing.  Update-Anywhere  Allows bidirectional updates. Pros: Good for load balancing. Cons: Complex concurrency control.  Synchronization Strategy Eager  Creates a singular sequential stream of events. Pros: Guarantees strong consistency. Cons: Possible performance problems with long response times due to the requirement that no event may be processed until the replication of the current event has been staged.  Lazy  Allows some reordering of events to optimize use of resources. Pros: Quick response time to user. Cons: Can have stale data on destination and temporary inconsistency.  Design Taxonomy Design Choices Replication in Hive uses lazy, primary-copy replication for a number of reasons as discussed below.\nPrimary-Copy vs Update-Anywhere Since replication in Hive focuses on disaster recovery, the read-only load balancing offered by primary-copy replication is sufficient. The cost of complex concurrency control in update-anywhere replication is too high. Additionally, primary-copy replication can be isolated to be unidirectional per object, which allows for scenarios where two different objects can be replicated in different directions if necessary. This might be sufficient for some load-balancing requirements.\nEager vs Lazy Eager replication requires a guaranteed delta log for every update on the primary. This poses a problem when external tables are used in Hive. External tables allow data to be managed completely outside of Hive, and therefore may not provide Hive with a complete and accurate delta log. With ACID tables, such a log does exist and will be made use of in future development, but currently replication in Hive works only with traditional tables.\nInstead, Hive uses lazy replication. Unlike eager replication, lazy replication is asynchronous and non-blocking which allows for better resource utilization. This is prioritized in Hive with the acknowledgement that there is some complexity involved with events being processed out of order, including destructive events such as DROP.\nOther Design Choices In addition to the taxonomy choices listed above, a number of other factors influenced the design of replication in Hive:\n Hive’s primary need for replication is disaster recovery. Hive supports a notion of export/import. There are already some implementations of manual replication and time-automated replication with Perl/Python scripts using export/import and DistCp. Hive already has the ability to send notifications to trigger actions based on an event, such as an Oozie job triggered by a partition being loaded. UI is not intended as there are other data management tools (such as Falcon) that already cover data movement and include some form of feed replication. Hive should allow these tools to plug into the replication system. Hive handles the logic and provides APIs for external tools to handle the mechanics (execution, data management, user interaction). Timestamp/feed-based replication is solved by other tools and therefore Hive does not need to re-solve this problem. HDFS replication and backing metastore database replication are also possible using broad DFS copies and native replication tools of backing databases and therefore Hive does not need to re-solve these problems. Additionally, attempting to solve replication by these methods would lead to multiple sources of truth and cause synchronization problems as well as not easily allowing for a finer granularity of approach (i.e., they would not easily support replication of only hot tables or partitions).  Basic Approach Hive already supports EXPORT and IMPORT commands which can be used to dump out tables, DistCp them to another cluster, and import/create from that. A mechanism which automates exports/imports would establish a base on which replication could be developed. With the aid of HiveMetaStoreEventHandler mechanisms, such automation can be developed to generate notifications when certain changes are committed to the metastore and then translate those notifications to export actions, DistCp actions, and import actions.\nThis already partially exists with the notification system that is part of the hcatalog-server-extensions jar. Initially, this was developed to be able to trigger a JMS notification, which an Oozie workflow could use to start off actions keyed on the finishing of a job that used HCatalog to write to a table. While this currently lives under HCatalog, the primary reason for its existence has a scope well past HCatalog alone and can be used as-is without the use of HCatalog IF/OF. This can be extended with the help of a library which does that aforementioned translation of notifications to actions.\nImplementation Hive’s implementation of replication combines the notification event subsystem with a mapping of an event to an actual task to be performed that helps automate the replication process. Hive provides an API for replication which other tools can call to handle the execution aspects of replication.\nEvents Events are triggered when any change happens to the warehouse. Hive provides an API for tools to read these events. Each event has an associated event sequence ID (monotonically increasing). Once read, these events are then translated to replication tasks by means of a ReplicationTaskFactory.\nThus, the base replication mechanism in Hive is not tied exclusively to the export/import (EXIM) mechanism described previously, which instead serves only as a default event-to-task mapping mechanism. It is possible for an organization or a project integrating with Hive to plug in some other ReplicationTaskFactory implementation to achieve the same goal, which would allow modifications such as using additional bookkeeping or security aspects or using a different mechanism to move data from one warehouse to another. It is also theoretically possible to use a different implementation to achieve some other form of replication, even across database technologies, such as Hive \u0026ndash;\u0026gt; MySQL, for instance. However, this document will focus on the details of the EXIM mechanism as implemented by EximReplicationTaskFactory.\nExecution of events occurs after the mapping from event to task is completed. All aspects of execution, such as managing appropriate credentials, optimum cluster use, and scheduling, are outside of the scope of Hive and are therefore performed by replication tools such as HiveDR in Falcon.\nEvent IDs, State IDs, and Sequencing of Exports/Imports As mentioned above, each event is tagged with an event sequence ID. In addition to event IDs, exports are tagged with the current state ID of the primary warehouse at the time when the export occurs (which will be a later time than the event itself) since replication is done asynchronously. This allows replication to determine if the state of the object at the destination is newer or older than that being exported from the source and make a decision accordingly at import time as to whether or not to accept the import. If the destination’s current state of the object is newer than the state of the object being exported from the source, it will not copy that change. This allows for robustness in the face of replication event repeats or restarts after failure.\nHandling of Events Each event is handled differently depending on its event type, which is a combination of the object (database, table, partition) and the operation (create, add, alter, insert, drop). Each event may include a source command, a copy, and a destination command. The following chart describes the ten event types and how they are handled with descriptions below.\n   Event Source Command Needs Copy? Destination Command     CreateDatabase No-op No No-op   DropDatabase No-op No DROP DATABASE CASCADE   AlterDatabase (not implemented) (not implemented) (not implemented)   CreateTable EXPORT … FOR REPLICATION Yes IMPORT   DropTable No-op No DROP TABLE … FOR REPLICATION(‘id’)   AlterTable EXPORT … FOR METADATA REPLICATION Yes (metadata only) IMPORT   AddPartition (multi) EXPORT … FOR REPLICATION Yes (multi) IMPORT   DropPartition No-op No (multi) ALTER TABLE … DROP PARTITION(…) FOR REPLICATION(‘id’)   AlterPartition EXPORT … FOR METADATA REPLICATION Yes (metadata only) IMPORT   Insert EXPORT … FOR REPLICATION Yes (dumb copy) IMPORT    CreateDatabase For security reasons, Hive does not currently allow the replication of a database before it exists since that may enable a replication of private data unintentionally. Accordingly, everything is set to no-op. However, this may be revisited in the future and therefore the implementation includes an API point.\nDropDatabase Drop does not require any action on the source side or any copying. On the destination side, it requires a drop database cascade. Although this is a dangerous command, the current requirement for a database to already exist before it can be replicated mitigates the risk by not allowing any replication definitions to be set up before the object exists and therefore this is considered an intended action.\nAlterDatabase This has not currently been implemented as Hive considers a database to be a container object. The only meaningful alteration would therefore be the HDFS location, which has no need for replication. Therefore there is no API point for this.\nCreateTable This requires an export on the source side, a copying of data, and an import on the destination side. This import will only execute if the import’s state ID is newer than the replica object’s state ID. Otherwise, it will be a no-op.\nDropTable Drop does not require any action on the source side or any copying. On the destination side, the table will be dropped if the state ID included in the drop command is newer than the state ID of the replica object. Otherwise, it will be a no-op.\nAlterTable This requires the export, copying, and import of metadata only. The import will only execute if its state ID is newer than the replica object’s state ID. Otherwise, it will be a no-op.\nAddPartition Multiple partitions can be added atomically, resulting in multiple export commands bundled into one event and a copying of the data. The imports will only execute if their state ID is newer than the replica object’s state ID. Otherwise, it will be a no-op.\nDropPartition Drop does not require any action on the source side or any copying. On the destination side, there can be multiple commands for one event, as with AddPartition. These are only executed if their state ID is newer than the replica object’s state ID. Otherwise, it will be a no-op.\nAlterPartition This requires the export, copying, and import of metadata only. Unlike AddPartition and DropPartition, AlterPartition only modifies one partition at a time. The import will only execute if its state ID is newer than the replica object’s state ID. Otherwise, it will be a no-op.\nInsert This requires the export, copying, and import of data (not metadata). Currently it is a dumb copy of the entire object (rather than applying only the changes made), although this is an area to improve and optimize in the future. The import will only execute if its state ID is newer than the replica object’s state ID. Otherwise, it will be a no-op.\nFuture Features In the future, additional work should be done in the following areas:\n Enable replication to work with ACID tables, especially with relation to load-balancing use cases. Load-balancing might also be needed only for specific hot tables, allowing for a more surgical use of replication. Limit unnecessary copies by allowing for event nullification/collapsing. Look at storing events generated on message queues like Kafka, or on-disk on HDFS, rather than in the metastore. Support replication of more Hive objects such as roles, users, etc.  HIVE-7973 tracks progress on developing replication in Hive.\nReferences [1] Kemme, B., et al., \u0026ldquo;Database Replication: A Tutorial,\u0026rdquo; in Replication: Theory and Practice, B. Charron-Bost et al., Eds. Berlin, Germany: Springer, 2010, pp. 219-252.\nSave\nSave\nSave\nSave\nSave\nSave\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivereplicationdevelopment_55155632/","tags":null,"title":"Apache Hive : HiveReplicationDevelopment"},{"categories":null,"contents":"Apache Hive : HiveReplicationv2Development  Issues with the Current Replication System  Slowness Requiring Staging Directories with Full Copies (4xcopy Problem) Unsuitable for Load-Balancing Use Cases Incompatibility with ACID Dependency on External Tools To Do a Lot Support for a Hub-Spoke Model   Rubberbanding Change Management  _files Solution for Rubber Banding _metadata   A Need for Bootstrap New Commands  REPL DUMP  Syntax: Return values: Note:   REPL LOAD  Return values:   REPL STATUS  Return values:     Bootstrap, Revisited Metastore notification API security Setup/Configuration  This document describes the second version of Hive Replication. Please refer to the first version of Hive Replication for details on prior implementation.\nThis work is under development and interfaces are subject to change. This has been designed for use in conjunction with external orchestration tools, which would be responsible for co-ordinating the right sequence of commands between source and target clusters, fault tolerance/failure handling, and also providing correct configuration options that are necessary to be able to do cross cluster replication.\nAs of Hive 3.0.0 release : only managed table replication where Hive user owns the table contents is supported. External tables, ACID tables, statistics and constraint replication are not supported.\nIssues with the Current Replication System Some of the observed issues with the current replication implementation are as follows:\n Slowness Requiring staging dirs with full copies (4xcopy problem) Unsuitability for load-balancing use-cases Incompatibility with ACID Dependency on external tools to do a lot (staging dir mgmt, manage state info, etc.)  We will thus first try to understand why each of these occurs and what we can do about them.\nSlowness Why is the first version of Hive Replication slow?\nThe primary reason for its slowness is that it depends on state transfer, rather than delta-replay. This means that the amount of data being funneled across to the destination is much larger than it otherwise would be. This is especially a problem with frequent updates/inserts. (Creates cannot be delta-optimized since the original state is null, and deletes are instantaneous.)\nThe secondary reason is that the original implementation was designed to ensure \u0026ldquo;correctness\u0026rdquo; in terms of resilience. We were planning optimizations that would drastically reduce the number of events processed, but these have not yet been implemented. The optimizations would have worked by processing a window of events at a time and skipping the processing of some of the events when a future event nullified the effect of processing the first event (as in cases where an insert followed an insert, or a drop followed a create, etc.). Thus, our current implementation can be seen as a naive implementation where the window size is 1.\nRequiring Staging Directories with Full Copies (4xcopy Problem) Again, this problem comes down to needing to do a state transfer, and using export and import to do it. The first copy is the source table, which is then exported to a staging directory. This is the second copy. It has to be dist-cp-ed over to the destination cluster, which then forms the third copy. Then, upon import, it impresses the data on to the destination table, becoming the fourth copy.\nNow, two of these copies, the source table and the destination table, are necessary from the very nomenclature of replication - two copies are needed. The chief issue is that two additional copies are required temporarily in staging directories. For clusters without much temporary overflow space, this becomes a major constraint.\nLet us examine each of these copies and the design reasoning behind each. Firstly, on the source, the reason the export is done is to give a stable point that will be resilient when we\u0026rsquo;re trying to distcp to destination, in case the table should be modified during the attempt to copy it over. If we do not make a copy, the critical section of the source-side work is a much longer section, and any replay/recovery semantics get much more complex.\nOn the destination side, optimizations are certainly possible, and in fact, are actually done, so that we don\u0026rsquo;t have an extra copy here. On import, import actually moves the files to the destination table, thus, our 4xcopy problem is actually a 3x copy problem. However, we\u0026rsquo;ve taken to calling this the 4xcopy problem since that was the first problem we hit and then solved.\nHowever, what this optimization does mean is that if we fail during import after moving, then the redo of that import will actually require a redo of the export side as well, and thus, is not trivially retriable. This was a conscious decision as the likelihood of this happening is low, in comparison to the other failure points we have. If we were to desire to make the import resiliently retriable as well, we will have a 4x copy problem.\nUnsuitable for Load-Balancing Use Cases By forcing an \u0026ldquo;export\u0026rdquo; early, we handle DR use cases, so that even if the source hive wh should be compromised, we will not suffer unduly, and can replay our exported commands on the destination and recover. However, in doing so, we treat each table and partition as independent objects, for which the only important consideration is that we save the latest state each, without consideration to how they got there.\nThus, any cross-table relationships can be temporarily lost (an object can exist in one table that refers to something in another table which does not have the update yet), and queries that run across tables can produce results on the destination that never existed in the source. All this makes it so that the current implementation of replication is entirely unsuitable for load balancing.\nEssentially the primary problem is that the state transfer approach means that each object is considered independent, and each object can \u0026ldquo;rubber-band\u0026rdquo; to the latest version of itself. If all events have been processed, we will be in a stable state which is identical to the source, and thus, this can work for load balancing for users that have a pronounced \u0026ldquo;loading period\u0026rdquo; on their warehouse that is separate from their \u0026ldquo;reading period\u0026rdquo;, which allows us time in the middle to catch up and process all events. This is also true at a table level. This can work for many of the traditional data warehousing use cases, but fails for many analytics-like expectations.\nWe will delve further into this rubber-banding problem in a separate section later, since it is a primary problem we attempt to solve in Replv2.\nIncompatibility with ACID Personally, I think that notions of replication should be developed on top of a robust transactional system(and it is easier to develop that too), and trying to manage replications without such concepts is what leads to problems like the ones we face in hive, where we have to do the best we can with what we have. Hive\u0026rsquo;s flexibility in usage being one of it\u0026rsquo;s major usability points, and having to solve the needs of the majority of the install base is what leads us to try to develop replication without assuming ACID.\nAnd, having said that, due to the nature of compactions, our current approach of simply copying data and re-impressing metadata does not work without the need to copy over transaction objects, translate them and recompact on the destination as well, which does not work well without an idea of distributed transactions as well.\nThus, it is ironic that in our implementation of replication, we do not support ACID tables. We\u0026rsquo;ve considered what we would need to do to replicate ACID tables, and in most discussions, a popular notion seems to be one of using streaming to send deltas over to the destination, rather than to copy over the files and trying to fudge around with the transaction metadata. This, however, will require quite some more work, and thus, is not something we\u0026rsquo;re planning on addressing in replv2 either. It is likely to be a major push/focus of the next batch of work we put into replication.\nDependency on External Tools To Do a Lot Our current implementation assumes that we extend how EXPORT and IMPORT work, allow a Notification and ReplicationTask/Command based api that an external tool can use to implement replication on top of us. However, this means that they are the ones that have to manage staging directories, and in addition, have to manage the notion of what state each of our destination tables/dbs are in, and over time, there is a possibility of extensive hive logic bleeding into them. Apache Falcon has a tool called HiveDR, which has implemented these interfaces, and they\u0026rsquo;ve expressed a desire that hive take on some more of the management aspects for a cleaner interface.\nTo this end, one of the goals of replv2 would be that we manage our own staging directories, and instead of replication tools being the ones that move data over, we step in more proactively to pull the data from the source to the destination.\nSupport for a Hub-Spoke Model One more piece of feedback we got was the desire to support a hub-spoke model for replication. While there is nothing in the current design of replication that prevents the deployment of a hub-spoke model, the current implementations by third party tools on top of Hive Replication did not explicitly support a 1:n replication, since they wind up needing to do far too much book-keeping. Now that we take on more of the responsibilities of replication on to hive, we should not have a situation whereby we introduce design artiacts that make hub-spoke replication harder.\nOne of the primary ways this consideration affects us is that we drifted towards a pull model where the destinations pull the necessary info from the source, instead of a push model. We might choose to revisit aspects of this, but in the meanwhile, this is worth keeping in mind as we design the rest of the system.\nRubberbanding Consider the following series of operations:\nCREATE TABLE blah (a int) PARTITIONED BY (p string); INSERT INTO TABLE blah [PARTITION (p=\u0026quot;a\u0026quot;) VALUES 5; INSERT INTO TABLE blah [PARTITION (p=\u0026quot;b\u0026quot;) VALUES 10; INSERT INTO TABLE blah [PARTITION (p=\u0026quot;a\u0026quot;) VALUES 15; Now, for each operation that occurs, a monotonically increasing state-id is provided by DbNotificationListener, so that we have an ability to order those events by when they occurred. For the sake of simplicity, let\u0026rsquo;s say they occurred at states 10,20,30,40 respectively, in order.\nNow, if there were another thread running \u0026ldquo;SELECT * from blah;\u0026rdquo; from another thread, then depending on when the SELECT command ran, it would have differing results:\n If it ran before 10, then the table does not yet exist, and will return an error. If it ran between 10 \u0026amp; 20, then the table exists, but has no contents, and thus, it will return an empty set. If it ran between 20 \u0026amp; 30, then it will return { (a,5) } If it ran between 30 \u0026amp; 40, then it will return { (a,5) , (b,10) } , i.e. 2 rows. If it ran after 40, then it will return { (a,5) , (b,10) , (a,15) }.  Now, the problem with the state transfer approach of current replication as it occurs is that if we replicate these sequence of events from source to a destination warehouse, it is entirely likely that the very first time EXPORT runs on the table is quite a while after the event has occurred, say at around event 500. At this point of time, if it tries to export the state of the partition (p=\u0026ldquo;a\u0026rdquo;), then it will capture all the changes that have occurred till that point.\nLet us denote the event of processing an event E to replicate E from source to destination as PROC(E). Thus, PROC(10) would denote the processing of event 10 from source to destination.\nNow, let us look at the same select * behaviour we observed on the source as it occurs on the destination.\n If the select * runs before PROC(10), then we get an error, since the table has not yet been created. If the select * runs between PROC(10) \u0026amp; PROC(20), then it will result in the partition p=\u0026ldquo;a\u0026rdquo;) being impressed over.  If PROC(20) occurs before 40 has occurred, then it will return { (a,5) } If PROC(20) occurs after 40 has occurred, then it will return { (a,5) , (a,15) } - This is because the partition state captured by PROC(20) will occur after 40, and thus contain (a,15), but partition p=\u0026ldquo;b\u0026rdquo; has not yet been re-impressed because we haven\u0026rsquo;t yet re-impressed that partition, which will occur only at PROC(30).    We stop our examination at this point, because we see one possible outcome from the select * on the destination which was impossible at the source. This is the problem introduced by state-transfer that we term rubber-banding - nomenclature coming in from online games which deal with each individual player having different latencies, and the server having to reconcile updates in a staggered/stuttering fashion.\nAn example:\nState-transfer has a few good things going for it, such as being resilient and idempotent, but it introduces this problem of temporary states that are possible which never existed in the source, and this is a big no-no for load-balancing use-cases where the destination db is not simply a cold backup but a db that is actively being used for reads.\nChange Management Let us now consider a base part of a replication workflow. It would need to have the following parts:\n An event happens on the source that causes a change (at, say, t1) A notification event is generated for it (at, say, t2) That notification event is then processed on source to \u0026ldquo;ready\u0026rdquo; an actionable task to do on the destination to replicate this. (at, say, t3) The requisite data is copied over from the source wh to the destination warehouse The destination then performs whatever task is needed to restate  Now, so far, our primary problem seems to be that we can only capture \u0026ldquo;latest\u0026rdquo; state, and not the original state at the time the event occurred. That is to say that at the time we process the notification, we get the state of the object at that time, t3, instead of the state of the object at time t1. In the time between t1 and t3, the object may have changed substantially, and if we go ahead and take the state at t3, and then apply to destination in an idempotent fashion, always taking only updates, we get our current implementation, with the rubberbanding problem.\nFundamentally, this is the core of our problem. To not have rubberbanding, one of the following must be true of that time period between t1 \u0026amp; t3:\n No other change must have happenned to the object - which means that we do the equivalent of locking the object in question from t1 to t3. Such an approach is possible if t1 \u0026amp; t3 occur in the same transaction interval. If changes to the object between t1 \u0026amp; t3 are inevitable, we must have a way of recording each state change, so that when t3 rolls around, we still have the original t1 state available somewhere.  Route (1) is how we should approach ACID tables, and should be the way hopefully all hive tables are accessed at some point in the future. The benefit of the transactional route is that we would have exactly the delta/change that we\u0026rsquo;re applying, and we would save that delta to pass on to the other side.\nIn the meanwhile, however, we must try to solve (2) as well. To this end, our goal with replv2 is to make sure that if there is any hive access that makes any change to an object, we capture the original state. There are two aspects to the original state - the metadata and the data. The metadata is easily solvable, since t1 \u0026amp; t2 can be done in the context of a single hive operation, and we can impress the metadata for the notification and our change to the metadata in the same metastore transaction. This now leaves us the question of what happens with the backing filesystem data for the object.\nNow, in addition to this problem that we\u0026rsquo;re solving of tracking what the filesystem state was at the time we did our dump, we have one more problem we want to solve, and that is that of the 4x copy problem. We\u0026rsquo;ve already solved the problem with the extra copy on the destination. Now, we need to somehow prevent the extra copy on the source to make this work. Essentially, what we need, to prevent making an extra copy of the entire data on the source, we need to have a \u0026ldquo;stable\u0026rdquo; way of determining what the FS backing state for the object was at the time the event occurred.\nBoth of these problems, that of the 4x copy problem, and that of making sure that we know what FS state existed at t1 to prevent rubberbanding, are then solvable if we have a snapshot of the source filesystem at the time the event occurred. At first, this, to us, led us to looking at HDFS snapshots as the way to solve this problem. Unfortunately, HDFS snapshots, while they would solve our problem, are, per discussion with HDFS folks, not something we can create a large number of, and we might very well likely need a snapshot for every single event that comes along.\nHowever, the idea behind the snapshot is still what we really want, and if HDFS cannot support the number of snapshots that we would create, it is possible for us to do a pseudo-snapshot, so that for all files that are backing hive objects, if we detect any hive operation would move them away or modify them, we retain the original in a separate directory, similar to how we manage Trash. This pseudo-trash like capturing behaviour is what we refer to as the \u0026ldquo;change-management\u0026rdquo; piece and is the main piece that needs to be in place to solve the rubberbanding problem as well as the 4x copy problem.\n_files Currently, when we do an EXPORT of a table, the directory structure created in this dump has, at its root, a _metadata file that contains all the metadata state to be impressed, and then has directory structures for each partition to be impressed.\nTo populate each of the partition directories, it runs a CopyTask that copies the files of each of the partitions over. Now, to make sure that we do not do secondary copies, our design is very simple - instead of a CopyTask, we use a ReplCopyTask, which, will, instead of copying the files to the destination directory, will instead create a file called _files in the destination directory with a list of each of the filenames of the original files.\nThus, instead of partition directories with actual data, we will instead have partition directories with _files files that then contain the location of the original files. (We will discuss and handle what happens when the original files get moved away or deleted later, for now, it is sufficient to assume that these urls will be stable urls to the state of the files at the time we did the dump, as if it were a pseudo-snapshot.)\nNow, when this export dump is imported, we need to make sure that for each _files file loaded, we go through the contents of the _files, and apply the copy instead to the underlying file. Also, we will wind up invoking DistCp automatically from hive when we try to copy files over from a remote cluster. (Again, this can be optimized and will be discussed in detail later, but for now, it suffices that we are able to access it.)\nWith this notion of EXPORT creating _files as indirections to the actual files, and IMPORT loading _files to locate the actual files needing copying, we solve the 4x copy problem.\nSolution for Rubber Banding Here is a possible solution to the rubber banding problem described earlier:\nFor each metastore event for which a notification is generated, store the metadata object (e.g. table, partition etc), the location of the files (associated with the event) and the checksum of each affected file (the reason for storing the checksum is explained shortly). In case of events which delete files (e.g. drop table/partition), move the deleted files to a configurable location on the file system (let\u0026rsquo;s call it $cmroot for purpose of this discussion) instead of deleting them.\nConsider the following sequence of commands for illustration:\nEvent 100: ALTER TABLE tbl ADD PARTITION (p=1) SET LOCATION \u0026lt;location\u0026gt;; Event 110: ALTER TABLE tbl DROP PARTITION (p=1); Event 120: ALTER TABLE tbl ADD PARTITION (p=1) SET LOCATION \u0026lt;location\u0026gt;; When loading the dump on the destination side (at a much later point), when the event 100 is replayed, the load task on the destination will try to pull the files from the (the _files contains the path of ), which may contain new or different data. To replicate the exact state of the source at the time event 100 occurred at the source, we do the following:\n When Event 100 occurs at the source, in the notification event, we store the checksum of the file(s) in the newly added partition along with the file path(s). When Event 110 occurs at the source, we move the files of the dropped partition to $cmroot/database/tbl/p=1 instead of purging them. When Event 120 occurs at the source, in the notification event, we again store the checksum of the file(s) in the newly added partition along with the file path(s).  Now when Event 100 is replayed at the destination at a later point, the destination calculates the checksum for file(s) in the partition path. If the checksum differs (for example in this case due to Event 110 and Event 120 that have occurred at the source), the destination looks for those files in $cmroot/database/table/p=1, and pulls them from this location to replicate the state of the source, at the time Event 100 had occurred on the source.\n_metadata Currently, when we EXPORT a table or partition, we generate a _metadata file, which contains a snapshot of the metadata state of the object in question. This _metadata is generated at the point the EXPORT is done. For the purposes of solving rubberbanding, we now also have a need to be able to capture the metadata state of the object in question at the time an event happens. Thus, DbNotificationListener is being enhanced to also store a snapshot of the object itself, rather than just the name of the object, and at event-export time, it takes the metadata not from the metastore, but from the event data.\nThis then allows us to generate the appropriate object on the destination at the time the destination needs updating to that state, and not earlier. This, in conjunction with the file-pseudo-snapshotting that we introduce, allows us to replay state on the destination for both metadata and data.\nA Need for Bootstrap One of the requests we got was that by offloading too much of the requirements of replication, we push too much \u0026ldquo;hive knowledge\u0026rdquo; over to the tools that integrate with us, asking them to essentially bootstrap the destination warehouse to a point where it is capable of receiving incremental updates. Currently, we recommend that users run a manual \u0026ldquo;EXPORT \u0026hellip; FOR REPLICATION\u0026rdquo; on all tables involved, set up any dbs needed and IMPORT these dumps as needed, etc, to prepare a destination for replicating into. We need to introduce a mechanism by which we can set up a replication dump at a larger scale than just tables, say, at a DB level. For this purpose, the best fit seemed to be a new tool or command, similar to mysqldump.\n(Note, in this section, I constantly refer to mysql and mysqldump, not because this is the only solution out there but because I\u0026rsquo;m a little familiar with it. Other dbs have equivalent tools)\nThere are a couple of major differences, however, between expectations we have of something like mysqldump, and a command we implement:\n The scale of the data involved in an initial dump is orders more for a hive warehouse as compared to a typical mysql db. Transactional isolation \u0026amp; log-based approaches means that mysqldump can have a stable snapshot of the entire db/metadata during which it proceeds to dump out all dbs and tables. So, even if it takes a while to dump them out, it need not worry about the objects changing while it gets dumped. We, on the other hand, need to handle that.  The first point can be solved by using our change-management semantics that we\u0026rsquo;re developing, and using the lazy _files approach rather than a CopyTask.\nThe second part is a little more involved, and needs to do some consolidation during the dump generation. We will discuss this in short order, after a brief detour of new commands we introduce to manage the replication dump and reload.\nNew Commands The current implementation of replication is built upon existing commands EXPORT and IMPORT. These commands are semantically more suited to the task of exporting and importing, than of a direct notion of an applicable event log. The notion of a lazy _files behaviour on EXPORT is not a good fit, since EXPORTs are done with the understanding that they need to be a stable copy irrespective of cleanup policies on the source. In addition, EXPORTing \u0026ldquo;events\u0026rdquo; is something that is more tenuous. EXPORTing a CREATE event is easy enough, but it is a semantic stretch to export a DROP event. Thus, to fit our needs better, and to not have to keep making the existing EXPORT and IMPORT way more complex, we introduce a new REPL command, with three modes of operation: REPL DUMP, REPL LOAD and REPL STATUS.\nREPL DUMP Syntax: REPL DUMP \u0026lt;repl_policy\u0026gt; {REPLACE \u0026lt;old_repl_policy\u0026gt;} {FROM \u0026lt;init-evid\u0026gt; {TO \u0026lt;end-evid\u0026gt;} {LIMIT \u0026lt;num-evids\u0026gt;} } {WITH ('key1'='value1', 'key2'='value2')};\nReplication policy: \u0026lt;dbname\u0026gt;{{.[\u0026lt;comma_separated_include_tables_regex_list\u0026gt;]}{.[\u0026lt;comma_separated_exclude_tables_regex_list\u0026gt;]}}\nThis is better described via various examples of each of the pieces of the command syntax, as follows:\n(a) REPL DUMP sales; REPL DUMP sales.['.*?']Replicates out sales database for bootstrap, from =0 (bootstrap case) to =with a batch size of 0, i.e. no batching.\n(b) REPL DUMP sales.[\u0026lsquo;T3\u0026rsquo;, \u0026lsquo;[a-z]+'];\nSimilar to case (a), but sets up db-level replication that includes only table/view \u0026lsquo;T3\u0026rsquo; and any table/view names with just alphabets of any length such as \u0026lsquo;orders\u0026rsquo;, \u0026lsquo;stores\u0026rsquo; etc. (c) REPL DUMP sales.[\u0026rsquo;.*?'].[\u0026lsquo;T[0-9]+\u0026rsquo;, \u0026lsquo;Q4\u0026rsquo;];\nSimilar to case(a), but sets up db-level replication that excludes table/view \u0026lsquo;Q4\u0026rsquo; and all table/view names that have prefix \u0026lsquo;T\u0026rsquo; and numeric suffix of any length. For example, \u0026lsquo;T3\u0026rsquo;, \u0026lsquo;T400\u0026rsquo;, \u0026lsquo;t255\u0026rsquo; etc. The table/view names are case-insensitive in nature and hence table/view name with prefix \u0026rsquo;t' would also be excluded from dump.\n(d) REPL DUMP sales.[];\nThis sets up db-level replication that excludes all the tables/views but includes only functions.\n(e) REPL DUMP sales FROM 200 TO 1400;\nThe presence of a FROM tag makes this dump not a bootstrap, but a dump which looks at the event log to produce a delta dump. FROM 200 TO 1400 is self-evident in that it will go through event ids 200 to 1400 looking for events from the relevant db.\n(f) REPL DUMP sales FROM 200;\nSimilar to above, but with an implicit assumed as being the current event id at the time the command is run.\n(g) REPL DUMP sales FROM 200 to 1400 LIMIT 100;REPL DUMP sales FROM 200 LIMIT 100;\nSimilar to cases (d) \u0026amp; (e), with the addition of a batch size of =100. This causes us to stop processing if we reach 100 events, and return at that point. Note that this does not mean that we stop processing at event id = 300, since we began at 200 - it means that we will stop processing events when we have processed 100 events in the event stream (that has unrelated events) belonging to this replication-definition, i.e. of a relevant db or db.table, then we stop.\n(h) REPL DUMP sales.['[a-z]+'] REPLACE sales FROM 200;\n REPL DUMP sales.['[a-z]+', \u0026lsquo;Q5\u0026rsquo;] REPLACE sales.['[a-z]+'] FROM 500;\nThis is an example of changing the replication policy/scope dynamically during incremental replication cycle.\nIn first case, a full DB replication policy \u0026ldquo;sales\u0026rdquo; is changed to a replication policy that includes only table/view names with only alphabets \u0026ldquo;sales.['[a-z]+']\u0026rdquo; such as \u0026ldquo;stores\u0026rdquo;, \u0026ldquo;products\u0026rdquo; etc. The REPL LOAD using this dump would intelligently drops the tables which are excluded as per the new policy. For instance, table with name \u0026lsquo;T5\u0026rsquo; would be automatically dropped during REPL LOAD if it is already there in target cluster.\nIn second case, policy is again changed to include table/view \u0026lsquo;Q5\u0026rsquo; and in this case, Hive would intelligently bootstrap the table/view \u0026lsquo;Q5\u0026rsquo; in the current incremental dump. The same is applicable for table/view renames where (i) REPL DUMP sales WITH (\u0026lsquo;hive.repl.include.external.tables\u0026rsquo;=\u0026lsquo;false\u0026rsquo;, \u0026lsquo;hive.repl.dump.metadata.only\u0026rsquo;=\u0026lsquo;true\u0026rsquo;);\nThe REPL DUMP command has an optional WITH clause to set command-specific configurations to be used when trying to dump. These configurations are only used by the corresponding REPL DUMP command and won\u0026rsquo;t be used for other queries running in the same session. In this example, we set the configurations to exclude external tables and also include only metadata and don\u0026rsquo;t dump data. Return values:  Error codes returned as return error codes (and over jdbc if with HS2) Returns 2 columns in the ResultSet:  - the directory to which it has dumped info. - the last event-id associated with this dump, which might be the end-evid, or the curr-evid, as the case may be.    Note: Now, the dump generated will be similar to the kind of dumps generated by EXPORTs, in that it will contain a _metadata file, but it will not contain the actual data files, instead using a _files file as an indirection to the actual files. One more aspect of REPL DUMP is that it does not take a directory as an argument on where to dump into. Instead, it creates its own dump directory inside a root dir specified by a new HiveConf parameter, hive.repl.rootdir , which will configure a root directory for dumps, and returns the dumped directory as part of the return value from it. It is intended also that we will introduce a replication dumpdir cleaner which will periodically clean it up.\nThis call is intended to be synchronous, and expects the caller to wait for the result.\nIf HiveConf parameter [hive.in](http://hive.in).test is false, REPL DUMP will not use a new dump location, thus it will garble an existing dump. Hence before taking an incremental dump, clear the bootstrap dump location if [hive.in](http://hive.in).test is false.\nBootstrap note : The FROM clause means that we read the event log to determine what to dump. For bootstrapping, we would not use FROM.\nWhen bootstrap dump is in progress, it blocks rename table/partition operations on any tables of the dumped database and throws HiveException. Once bootstrap dump is completed, rename operations are enabled and will work as normal. If HiveServer2 crashes when bootstrap dump in progress, then rename operations will continue to throw HiveException even after HiveServer2 is restored with no REPL DUMP in progress. This abnormal state should be manually fixed using following work around. Look up the HiveServer logs for below pair of log messages.\n REPL DUMP:: Set property for Database: \u0026lt;db_name\u0026gt;, Property: \u0026lt;bootstrap.dump.state.xxxx\u0026gt;, Value: ACTIVE\nREPL DUMP:: Reset property for Database: \u0026lt;db_name\u0026gt;, Property: \u0026lt;bootstrap.dump.state.xxxx\u0026gt;\n If Reset property log is not found for the corresponding Set property log, then user need to manually reset the database property \u0026lt;bootstrap.dump.state.xxxx\u0026gt; with value as \u0026ldquo;IDLE\u0026rdquo; using ALTER DATABASE command.\nREPL LOAD REPL LOAD {\u0026lt;dbname\u0026gt;} FROM \u0026lt;dirname\u0026gt; {WITH ('key1'='value1', 'key2'='value2')};\nThis causes a REPL DUMP present in (which is to be a fully qualified HDFS URL) to be pulled and loaded. If is specified, and the original dump was a database-level dump, this allows Hive to do db-rename-mapping on import. If dbname is not specified, the original dbname as recorded in the dump would be used.The REPL LOAD command has an optional WITH clause to set command-specific configurations to be used when trying to copy from the source cluster. These configurations are only used by the corresponding REPL LOAD command and won\u0026rsquo;t be used for other queries running in the same session.\nReturn values:  Error codes returned as normal. Does not return anything in ResultSet, expects user to run REPL STATUS to check.  REPL STATUS REPL STATUS \u0026lt;dbname\u0026gt;;\nWill return the same output that REPL LOAD returns, allows REPL LOAD to be run asynchronously. If no knowledge of a replication associated with that db is present, i.e., there are no known replications for that, we return an empty set. Note that for cases where a destination db or table exists, but no known repl exists for it, this should be considered an error condition for tools calling REPL LOAD to pass on to the end-user, to alert them that they may be overwriting an existing db with another.\nReturn values:  Error codes returned as normal. Returns the last replication state (event ID) for the given database.  Bootstrap, Revisited When we introduced the notion of a need for bootstrap, we said that the problem of time passing during the bootstrap was something of a problem that needed solving separately.\nLet us say that we begin the dump at evid=170, and by the time we finish the dump of all objects contained in our dump, it is now evid=230. For a consistent picture of the dump, we now also have to consolidate the information included in events 170-230 into our dump before we can pass the baton over to incremental replication.\nLet us consider the case of a table T1, which was dumped out around evid=200. Now, let us say that the following operations have occurred on the two tables during the time the dump has been proceeding:\n   event id operation     184 ALTER TABLE T1 DROP PARTITION(Px)   196 ALTER TABLE T1 ADD PARTITION(Pa)   204 ALTER TABLE T1 ADD PARTITION(Pb)   216 ALTER TABLE T1 DROP PARTITION(Py)    Basically, let us try to understand what happens when partitions are added(Pa \u0026amp; Pb) and dropped(Px \u0026amp; Py) both before and after a table is dumped. So, for our bootstrap, we go through 2 phases - first an object dump of all the objects we\u0026rsquo;re expected to dump, and then a consolidation phase where we go through all the events that occurred during our object dump.\nIf the table T1 was dumped at around evid=200, then, it will not contain partition Px, since the drop would have been processed before the dump occurred, and it will contain the partition Pa, since that partition was added before the object dump occurred. In contrast, partition Pb will not be present in the dump, since Pb will have not yet been added, and also, it will still contain partition Py, since that partition will not yet have been dropped.\nSo, given this disparity, we need to consolidate this somehow. There are a couple of ways of consolidation.\nApproach 1 : Consolidate at destination.\nNow, one approach to handle this would be to simply say that we say that the dump is of the minimum state for the whole object, say 170, and let the various events apply on the destination as long as they are applicable, and ignore errors (such as when we try to drop a partition Px from a replicated table T1 which already does not have Px in it.)\nWhile this can work, the problem with this approach is that the destination can now have tables at differing states as a result of the dump - i.e. a table T2 that was dumped at about evid=220 will have newer info than T1 that was dumped about evid=200, and this is a sort of mini-rubberbanding in itself, since different parts of a whole are at different states. This problem is actually a little worse, since different partitions of a table can actually be at different states. Thus, we will not follow this approach.\nApproach 2 : Consolidate at source.\nThe alternate approach, then, is to go through each of the events from evid=170 to evid=230 in our example, which are the current-event-ids at the beginning of the object dump phase and the end of the object dump phase respectively, and to use that to modify the object dumps that we\u0026rsquo;ve just made. Any drops will result in the dumped object being changed/deleted, and any creates will result in additional dumped objects being added. Alters will result in dumped objects being replaced by their newer equivalent. At the end of this consolidation, all objects dumped should be capable of being restored on the destination as if the state for them was 230, and incremental replication can then take over, processing event 230 onwards.\nThis is the approach we expect to take. One further modification this will require from the current export semantics, is that currently, export exports only 1 _metadata file per table, which contains the list of all the partitions inside it in the _metadata file itself. Instead, now, we propose to split that up so that the _metadata level at an object level will contain only metadata for that object. Thus, _metadata at a table level will contain only the table object, and the individual directories inside it will contain all the required partitions, and each of those dirs will have a partition level _metadata.\nMetastore notification API security We want to secure DbNotificationListener related metastore APIs listed below by adding an authorization logic (other APIs not affected). These three APIs are mainly used by replication operations, so are allowed to be used by admin/superuser only:\n get_next_notification get_current_notificationEventId get_notification_events_count  The related hive config parameter is \u0026ldquo;hive.metastore.event.db.notification.api.auth\u0026rdquo;, which is set to true by default.\nThe auth mechanism works as below:\n Skip auth in embedded metasore mode regardless of \u0026ldquo;hive.metastore.event.db.notification.api.auth\u0026rdquo; setting\nThe reason is that we know the metastore calls are made from hive as opposed to other un-authorized processes that are running metastore client. Enable auth in remote metastore mode if \u0026ldquo;hive.metastore.event.db.notification.api.auth\u0026rdquo; set to true\nThe UGI of the remote metastore client is always set on metastore server. We retrieve this user info and check if this user has proxy privilege according to the proxy user settings. For example, the UGI is user \u0026ldquo;hive\u0026rdquo; and \u0026ldquo;hive\u0026rdquo; been configured to have the proxy privilege against a list of hosts. Then the auth will pass for the notification related calls from those hosts. If a user \u0026ldquo;foo\u0026rdquo; is performing repl operations (e.g. through HS2 with doAs=true), then the auth will fail unless user \u0026ldquo;foo\u0026rdquo; is configured to have the proxy privilege.  Setup/Configuration The following parameters need to be setup in source cluster -\nhive.metastore.transactional.event.listeners = org.apache.hive.hcatalog.listener.DbNotificationListener\nhive.metastore.dml.events = true\n//\u0026ldquo;Turn on ChangeManager, so delete files will go to cmrootdir.\u0026rdquo;\nhive.repl.cm.enabled = true\nThere are additional replication related parameters (with their default values). These are relevant only to cluster that acts as the source cluster. The defaults should work for these in most cases - REPLDIR(\u0026ldquo;hive.repl.rootdir\u0026rdquo;,\u0026quot;/user/hive/repl/\u0026quot;, \u0026ldquo;HDFS root dir for all replication dumps.\u0026quot;),\nREPLCMDIR(\u0026ldquo;hive.repl.cmrootdir\u0026rdquo;,\u0026quot;/user/hive/cmroot/\u0026quot;, \u0026ldquo;Root dir for ChangeManager, used for deleted files.\u0026quot;),\nREPLCMRETIAN(\u0026quot;hive.repl.cm.retain\u0026rdquo;,\u0026ldquo;24h\u0026rdquo;, new TimeValidator(TimeUnit.HOURS),\u0026ldquo;Time to retain removed files in cmrootdir.\u0026quot;),\nREPLCMINTERVAL(\u0026quot;hive.repl.cm.interval\u0026rdquo;,\u0026ldquo;3600s\u0026rdquo;,new TimeValidator(TimeUnit.SECONDS),\u0026ldquo;Inteval for cmroot cleanup thread.\u0026quot;),\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hivereplicationv2development_66850051/","tags":null,"title":"Apache Hive : HiveReplicationv2Development"},{"categories":null,"contents":"Apache Hive : HiveServer Thrift Hive Server HiveServer is an optional service that allows a remote client to submit requests to Hive, using a variety of programming languages, and retrieve results. HiveServer is built on Apache ThriftTM (http://thrift.apache.org/), therefore it is sometimes called the Thrift server although this can lead to confusion because a newer service named HiveServer2 is also built on Thrift. Since the introduction of HiveServer2, HiveServer has also been called HiveServer1.\nWARNING!\nHiveServer cannot handle concurrent requests from more than one client. This is actually a limitation imposed by the Thrift interface that HiveServer exports, and can\u0026rsquo;t be resolved by modifying the HiveServer code.\nHiveServer2 is a rewrite of HiveServer that addresses these problems, starting with Hive 0.11.0. Use of HiveServer2 is recommended.\nHiveServer was removed from Hive releases starting in Hive 1.0.0 (formerly called 0.14.1). Please switch over to HiveServer2.\nPreviously its removal had been scheduled for Hive 0.15 (now called 1.1.0). See HIVE-6977.\nThrift\u0026rsquo;s interface definition language (IDL) file for HiveServer is hive_service.thrift, which is installed in $HIVE_HOME/service/if/.\nOnce Hive has been built using steps in Getting Started, the Thrift server can be started by running the following:\n0.8 and Later\n$ build/dist/bin/hive --service hiveserver --help usage: hiveserver -h,--help Print help information --hiveconf \u0026lt;property=value\u0026gt; Use value for given property --maxWorkerThreads \u0026lt;arg\u0026gt; maximum number of worker threads, default:2147483647 --minWorkerThreads \u0026lt;arg\u0026gt; minimum number of worker threads, default:100 -p \u0026lt;port\u0026gt; Hive Server port number, default:10000 -v,--verbose Verbose mode $ bin/hive --service hiveserver 0.7 and Earlier\n$ build/dist/bin/hive --service hiveserver --help usage HIVE_PORT=xxxx ./hive --service hiveserver HIVE_PORT : Specify the server port $ bin/hive --service hiveserver After starting the server, to test if the server is working well, run the hiveserver and jdbc tests in \u0026lsquo;standalone\u0026rsquo; mode. The HIVE_PORT is assumed to be 10000 on localhost for this case.\n$ ant test -Dtestcase=TestJdbcDriver -Dstandalone=true $ ant test -Dtestcase=TestHiveServer -Dstandalone=true The service supports clients in multiple languages. For more details see Hive Client.\nTroubleshooting: Connection Error\nHive server and clients communicate through Thrift and FB303 services. In some distributions, both the Hadoop and Hive distributions have different versions of libthrift.jar and libfb303.jar. If they are incompatible, it may cause a Thrift connection error when running the unit test on standalone mode. The solution is to remove the Hadoop\u0026rsquo;s version of libthrift.jar and libfb303.jar.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveserver_27362111/","tags":null,"title":"Apache Hive : HiveServer"},{"categories":null,"contents":"Apache Hive : HiveServer2 Clients  Beeline – Command Line Shell  Beeline Example Beeline Commands Beeline Properties Beeline Hive Commands Beeline Command Options Output Formats  table vertical xmlattr xmlelements json jsonfile Separated-Value Output Formats  csv2, tsv2, dsv  Quoting in csv2, tsv2 and dsv Formats   csv, tsv     HiveServer2 Logging Cancelling the Query Background Query in Terminal Script   JDBC  Connection URLs  Connection URL Format Connection URL for Remote or Embedded Mode Connection URL When HiveServer2 Is Running in HTTP Mode Connection URL When SSL Is Enabled in HiveServer2 Connection URL When ZooKeeper Service Discovery Is Enabled Named Connection URLs Reconnecting Using hive-site.xml to automatically connect to HiveServer2 Using beeline-site.xml to automatically connect to HiveServer2 Using JDBC  JDBC Client Sample Code Running the JDBC Sample Code     JDBC Data Types JDBC Client Setup for a Secure Cluster  Multi-User Scenarios and Programmatic Login to Kerberos KDC  Using Kerberos with a Pre-Authenticated Subject     JDBC Fetch Size   Python Client Ruby Client Integration with SQuirrel SQL Client Integration with SQL Developer Integration with DbVisSoftware\u0026rsquo;s DbVisualizer Advanced Features for Integration with Other Tools  Supporting Cookie Replay in HTTP Mode Using 2-way SSL in HTTP Mode Passing HTTP Header Key/Value Pairs via JDBC Driver Passing Custom HTTP Cookie Key/Value Pairs via JDBC Driver    This page describes the different clients supported by HiveServer2. Other documentation for HiveServer2 includes:\n HiveServer2 Overview Setting Up HiveServer2 Hive Configuration Properties: HiveServer2  Version\nIntroduced in Hive version 0.11. See HIVE-2935.\nBeeline – Command Line Shell HiveServer2 supports a command shell Beeline that works with HiveServer2. It\u0026rsquo;s a JDBC client that is based on the SQLLine CLI (http://sqlline.sourceforge.net/). There’s detailed documentation of SQLLine which is applicable to Beeline as well.\nReplacing the Implementation of Hive CLI Using Beeline\nThe Beeline shell works in both embedded mode as well as remote mode. In the embedded mode, it runs an embedded Hive (similar to Hive CLI) whereas remote mode is for connecting to a separate HiveServer2 process over Thrift. Starting in Hive 0.14, when Beeline is used with HiveServer2, it also prints the log messages from HiveServer2 for queries it executes to STDERR. Remote HiveServer2 mode is recommended for production use, as it is more secure and doesn\u0026rsquo;t require direct HDFS/metastore access to be granted for users.\nIn remote mode HiveServer2 only accepts valid Thrift calls – even in HTTP mode, the message body contains Thrift payloads.\nBeeline Example % bin/beeline Hive version 0.11.0-SNAPSHOT by Apache beeline\u0026gt; !connect jdbc:hive2://localhost:10000 scott tiger !connect jdbc:hive2://localhost:10000 scott tiger Connecting to jdbc:hive2://localhost:10000 Connected to: Hive (version 0.10.0) Driver: Hive (version 0.10.0-SNAPSHOT) Transaction isolation: TRANSACTION_REPEATABLE_READ 0: jdbc:hive2://localhost:10000\u0026gt; show tables; show tables; +-------------------+ | tab_name | +-------------------+ | primitives | | src | | src1 | | src_json | | src_sequencefile | | src_thrift | | srcbucket | | srcbucket2 | | srcpart | +-------------------+ 9 rows selected (1.079 seconds) You can also specify the connection parameters on command line. This means you can find the command with the connection string from your UNIX shell history. % beeline -u jdbc:hive2://localhost:10000/default -n scott -w password_file Hive version 0.11.0-SNAPSHOT by Apache Connecting to jdbc:hive2://localhost:10000/default Beeline with NoSASL connection\nIf you\u0026rsquo;d like to connect via NOSASL mode, you must specify the authentication mode explicitly:\n% bin/beeline beeline\u0026gt; !connect jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;auth=noSasl hiveuser pass Beeline Commands    Command Description     ! List of SQLLine commands available at http://sqlline.sourceforge.net/.Example: !quit exits the Beeline client.   !delimiter Set the delimiter for queries written in Beeline. Multi-character delimiters are allowed, but quotation marks, slashes, and \u0026ndash; are not allowed. Defaults to ;Usage: !delimiter $$Version: 3.0.0 (HIVE-10865)    Beeline Properties    Property Description     fetchsize Standard JDBC enables you to specify the number of rows fetched with each database round-trip for a query, and this number is referred to as the fetch size.Setting the fetch size in Beeline overrides the JDBC driver\u0026rsquo;s default fetch size and affects subsequent statements executed in the current session.1. A value of -1 instructs Beeline to use the JDBC driver\u0026rsquo;s default fetch size (default)   2. A value of zero or more is passed to the JDBC driver for each statement    3. Any other negative value will throw an ExceptionUsage: !set fetchsize 200     Version: 4.0.0 (HIVE-22853) |\nBeeline Hive Commands Hive specific commands (same as Hive CLI commands) can be run from Beeline, when the Hive JDBC driver is used.\nUse \u0026ldquo;;\u0026rdquo; (semicolon) to terminate commands. Comments in scripts can be specified using the \u0026ldquo;--\u0026rdquo; prefix.\n   Command Description     reset Resets the configuration to the default values.   reset  Resets the value of a particular configuration variable (key) to the default value.Note: If you misspell the variable name, Beeline will not show an error.   set = Sets the value of a particular configuration variable (key). Note: If you misspell the variable name, Beeline will not show an error.   set Prints a list of configuration variables that are overridden by the user or Hive.   set -v Prints all Hadoop and Hive configuration variables.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * Adds one or more files, jars, or archives to the list of resources in the distributed cache. See Hive Resources for more information.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * As of Hive 1.2.0, adds one or more files, jars or archives to the list of resources in the distributed cache using an Ivy URL of the form ivy://group:module:version?query_string. See Hive Resources for more information.   list FILE[S] list JAR[S] list ARCHIVE[S] Lists the resources already added to the distributed cache. See Hive Resources for more information. (As of Hive 0.14.0: HIVE-7592).   list FILE[S] * list JAR[S] * list ARCHIVE[S] * Checks whether the given resources are already added to the distributed cache or not. See Hive Resources for more information.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * Removes the resource(s) from the distributed cache.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * As of Hive 1.2.0, removes the resource(s) which were added using the from the distributed cache. See Hive Resources for more information.   reload As of Hive 0.14.0, makes HiveServer2 aware of any jar changes in the path specified by the configuration parameter hive.reloadable.aux.jars.path (without needing to restart HiveServer2). The changes can be adding, removing, or updating jar files.   dfs  Executes a dfs command.    Executes a Hive query and prints results to standard output.    Beeline Command Options The Beeline CLI supports these command line options:\n   Option Description     -u  The JDBC URL to connect to. Special characters in parameter values should be encoded with URL encoding if needed.Usage: beeline -u db_URL    -r Reconnect to last used URL (if a user has previously used !connect to a URL and used !save to a beeline.properties file).Usage: beeline -rVersion: 2.1.0 (HIVE-13670)   -n  The username to connect as.Usage: beeline -n valid_user   -p  The password to connect as.Usage: beeline -p valid_passwordOptional password mode:Starting Hive 2.2.0 (HIVE-13589) the argument for -p option is optional.Usage : beeline -p [valid_password]If the password is not provided after -p Beeline will prompt for the password while initiating the connection. When password is provided Beeline uses it initiate the connection without prompting.   -d  The driver class to use.Usage: beeline -d driver_class   -e  Query that should be executed. Double or single quotes enclose the query string. This option can be specified multiple times.Usage: beeline -e \u0026quot;query_string\u0026ldquo;Support to run multiple SQL statements separated by semicolons in a single query_string: 1.2.0 (HIVE-9877)Bug fix (null pointer exception): 0.13.0 (HIVE-5765)Bug fix (\u0026ndash;headerInterval not honored): 0.14.0 (HIVE-7647)Bug fix (running -e in background): 1.3.0 and 2.0.0 (HIVE-6758); workaround available for earlier versions    -f  Script file that should be executed.Usage: beeline -f filepathVersion: 0.12.0 (HIVE-4268)Note: If the script contains tabs, query compilation fails in version 0.12.0. This bug is fixed in version 0.13.0 (HIVE-6359).Bug fix (running -f in background): 1.3.0 and 2.0.0 (HIVE-6758); workaround available for earlier versions    -i (or) \u0026ndash;init  The init files for initializationUsage: beeline -i */tmp/initfile*Single file:Version: 0.14.0 (HIVE-6561)Multiple files:Version: 2.1.0 (HIVE-11336)   -w (or) \u0026ndash;password-file  The password file to read password from.Version: 1.2.0 (HIVE-7175)   -a (or) \u0026ndash;authType  The authentication type passed to the jdbc as an auth propertyVersion: 0.13.0 (HIVE-5155)   \u0026ndash;property-file  File to read configuration properties fromUsage: beeline --property-file */tmp/a*Version: 2.2.0 (HIVE-13964)   \u0026ndash;hiveconf property*=*value Use value for the given configuration property. Properties that are listed in hive.conf.restricted.list cannot be reset with hiveconf (see Restricted List and Whitelist).Usage: beeline --hiveconf prop1=value1Version: 0.13.0 (HIVE-6173)   \u0026ndash;hivevar name*=*value Hive variable name and value. This is a Hive-specific setting in which variables can be set at the session level and referenced in Hive commands or queries.Usage: beeline --hivevar var1=value1   \u0026ndash;color=[true/false] Control whether color is used for display. Default is false.Usage: beeline --color=true(Not supported for Separated-Value Output formats. See HIVE-9770)   \u0026ndash;showHeader=[true/false] Show column names in query results (true) or not (false). Default is true.Usage: beeline --showHeader=false   **\u0026ndash;headerInterval=**ROWS The interval for redisplaying column headers, in number of rows, when outputformat is table. Default is 100.Usage: beeline --headerInterval=50(Not supported for Separated-Value Output formats. See HIVE-9770)   \u0026ndash;fastConnect=[true/false] When connecting, skip building a list of all tables and columns for tab-completion of HiveQL statements (true) or build the list (false). Default is true.Usage: beeline --fastConnect=false   \u0026ndash;autoCommit=[true/false] Enable/disable automatic transaction commit. Default is false.Usage: beeline --autoCommit=true   \u0026ndash;verbose=[true/false] Show verbose error messages and debug information (true) or do not show (false). Default is false.Usage: beeline --verbose=true   \u0026ndash;showWarnings=[true/false] Display warnings that are reported on the connection after issuing any HiveQL commands. Default is false.Usage: beeline --showWarnings=true   \u0026ndash;showDbInPrompt=[true/false] Display the current database name in prompt. Default is false.Usage: beeline --showDbInPrompt=trueVersion: 2.2.0 (HIVE-14123)   \u0026ndash;showNestedErrs=[true/false] Display nested errors. Default is false.Usage: beeline --showNestedErrs=true   \u0026ndash;numberFormat=[pattern] Format numbers using a DecimalFormat pattern.Usage: beeline --numberFormat=\u0026quot;#,###,##0.00\u0026quot;   \u0026ndash;force=[true/false] Continue running script even after errors (true) or do not continue (false). Default is false.Usage: beeline--force=true   **\u0026ndash;maxWidth=**MAXWIDTH The maximum width to display before truncating data, in characters, when outputformat is table. Default is to query the terminal for current width, then fall back to 80.Usage: beeline --maxWidth=150   **\u0026ndash;maxColumnWidth=**MAXCOLWIDTH The maximum column width, in characters, when outputformat is table. Default is 50 in Hive version 2.2.0+ (see HIVE-14135) or 15 in earlier versions.Usage: beeline --maxColumnWidth=25   \u0026ndash;silent=[true/false] Reduce the amount of informational messages displayed (true) or not (false). It also stops displaying the log messages for the query from HiveServer2 (Hive 0.14 and later) and the HiveQL commands (Hive 1.2.0 and later). Default is false.Usage: beeline --silent=true   \u0026ndash;autosave=[true/false] Automatically save preferences (true) or do not autosave (false). Default is false.Usage: beeline --autosave=true   \u0026ndash;outputformat=[table/vertical/csv/tsv/dsv/csv2/tsv2] Format mode for result display. Default is table. See Separated-Value Output Formats below for description of recommended sv options.Usage: beeline --outputformat=tsvVersion: dsv/csv2/tsv2 added in 0.14.0 (HIVE-8615)   **\u0026ndash;**truncateTable=[true/false] If true, truncates table column in the console when it exceeds console length.Version: 0.14.0 (HIVE-6928)   \u0026ndash;delimiterForDSV= DELIMITER The delimiter for delimiter-separated values output format. Default is '   **\u0026ndash;isolation=**LEVEL Set the transaction isolation level to TRANSACTION_READ_COMMITTED or TRANSACTION_SERIALIZABLE. See the \u0026ldquo;Field Detail\u0026rdquo; section in the Java Connection documentation.Usage: beeline --isolation=TRANSACTION_SERIALIZABLE   \u0026ndash;nullemptystring=[true/false] Use historic behavior of printing null as empty string (true) or use current behavior of printing null as NULL (false). Default is false.Usage: beeline --nullemptystring=falseVersion: 0.13.0 (HIVE-4485)   \u0026ndash;incremental=[true/false] Defaults to true from Hive 2.3 onwards, before it defaulted to false``. When set to false, the entire result set is fetched and buffered before being displayed, yielding optimal display column sizing. When set to true, result rows are displayed immediately as they are fetched, yielding lower latency and memory usage at the price of extra display column padding. Setting --incremental=true is recommended if you encounter an OutOfMemory on the client side (due to the fetched result set size being large).   ****\u0026ndash;incrementalBufferRows=****NUMROWS The number of rows to buffer when printing rows on stdout, defaults to 1000; only applicable if --incremental=true and --outputformat=tableUsage: beeline --incrementalBufferRows=1000Version: 2.3.0 (HIVE-14170)   \u0026ndash;maxHistoryRows**=******NUMROWS The maximum number of rows to store Beeline history.Version: 2.3.0 (HIVE-15166)   \u0026ndash;delimiter=; Set the delimiter for queries written in Beeline. Multi-char delimiters are allowed, but quotation marks, slashes, and \u0026ndash; are not allowed. Defaults to ;Usage: beeline --delimiter=$$Version: 3.0.0 (HIVE-10865)   \u0026ndash;convertBinaryArrayToString=[true/false] Display binary column data as a string using the platform\u0026rsquo;s default character set.The default behavior (false) is to display binary data using: Arrays.toString(byte[] columnValue)Version: 3.0.0 (HIVE-14786)Display binary column data as a string using the UTF-8 character set.The default behavior (false) is to display binary data using Base64 encoding without padding.Version: 4.0.0 (HIVE-23856)Usage: beeline --convertBinaryArrayToString=true   \u0026ndash;help Display a usage message.Usage: beeline --help    Output Formats In Beeline, the result can be displayed in different formats. The format mode can be set with the outputformat option.\nThe following output formats are supported:\n table vertical xmlattr xmlelements HiveServer2 Clients#json HiveServer2 Clients#jsonfile separated-value formats (csv, tsv, csv2, tsv2, dsv)  table The result is displayed in a table. A row of the result corresponds to a row in the table and the values in one row are displayed in separate columns in the table.\nThis is the default format mode.\nExampleResult of the query select id, value, comment from test_table\n+-----+---------+-----------------+ | id | value | comment | +-----+---------+-----------------+ | 1 | Value1 | Test comment 1 | | 2 | Value2 | Test comment 2 | | 3 | Value3 | Test comment 3 | +-----+---------+-----------------+ vertical Each row of the result is displayed in a block of key-value format, where the keys are the names of the columns.\nExampleResult of the query select id, value, comment from test_table\nid 1 value Value1 comment Test comment 1 id 2 value Value2 comment Test comment 2 id 3 value Value3 comment Test comment 3 xmlattr The result is displayed in an XML format where each row is a \u0026ldquo;result\u0026rdquo; element in the XML.\nThe values of a row are displayed as attributes on the \u0026ldquo;result\u0026rdquo; element. The names of the attributes are the names of the columns.\nExampleResult of the query select id, value, comment from test_table\n\u0026lt;resultset\u0026gt; \u0026lt;result id=\u0026quot;1\u0026quot; value=\u0026quot;Value1\u0026quot; comment=\u0026quot;Test comment 1\u0026quot;/\u0026gt; \u0026lt;result id=\u0026quot;2\u0026quot; value=\u0026quot;Value2\u0026quot; comment=\u0026quot;Test comment 2\u0026quot;/\u0026gt; \u0026lt;result id=\u0026quot;3\u0026quot; value=\u0026quot;Value3\u0026quot; comment=\u0026quot;Test comment 3\u0026quot;/\u0026gt; \u0026lt;/resultset\u0026gt; xmlelements The result is displayed in an XML format where each row is a \u0026ldquo;result\u0026rdquo; element in the XML. The values of a row are displayed as child elements of the result element.\nExampleResult of the query select id, value, comment from test_table\n\u0026lt;resultset\u0026gt; \u0026lt;result\u0026gt; \u0026lt;id\u0026gt;1\u0026lt;/id\u0026gt; \u0026lt;value\u0026gt;Value1\u0026lt;/value\u0026gt; \u0026lt;comment\u0026gt;Test comment 1\u0026lt;/comment\u0026gt; \u0026lt;/result\u0026gt; \u0026lt;result\u0026gt; \u0026lt;id\u0026gt;2\u0026lt;/id\u0026gt; \u0026lt;value\u0026gt;Value2\u0026lt;/value\u0026gt; \u0026lt;comment\u0026gt;Test comment 2\u0026lt;/comment\u0026gt; \u0026lt;/result\u0026gt; \u0026lt;result\u0026gt; \u0026lt;id\u0026gt;3\u0026lt;/id\u0026gt; \u0026lt;value\u0026gt;Value3\u0026lt;/value\u0026gt; \u0026lt;comment\u0026gt;Test comment 3\u0026lt;/comment\u0026gt; \u0026lt;/result\u0026gt; \u0026lt;/resultset\u0026gt; json (Hive 4.0) The result is displayed in JSON format where each row is a \u0026ldquo;result\u0026rdquo; element in the JSON array \u0026ldquo;resultset\u0026rdquo;.\nExampleResult of the query select String, Int, Decimal, Bool, Null, Binary from test_table\n{\u0026quot;resultset\u0026quot;:[{\u0026quot;String\u0026quot;:\u0026quot;aaa\u0026quot;,\u0026quot;Int\u0026quot;:1,\u0026quot;Decimal\u0026quot;:3.14,\u0026quot;Bool\u0026quot;:true,\u0026quot;Null\u0026quot;:null,\u0026quot;Binary\u0026quot;:\u0026quot;SGVsbG8sIFdvcmxkIQ\u0026quot;},{\u0026quot;String\u0026quot;:\u0026quot;bbb\u0026quot;,\u0026quot;Int\u0026quot;:2,\u0026quot;Decimal\u0026quot;:2.718,\u0026quot;Bool\u0026quot;:false,\u0026quot;Null\u0026quot;:null,\u0026quot;Binary\u0026quot;:\u0026quot;RWFzdGVyCgllZ2cu\u0026quot;}]} jsonfile (Hive 4.0) The result is displayed in JSON format where each row is a distinct JSON object. This matches the expected format for a table created as JSONFILE format.\nExampleResult of the query select String, Int, Decimal, Bool, Null, Binary from test_table\n{\u0026quot;String\u0026quot;:\u0026quot;aaa\u0026quot;,\u0026quot;Int\u0026quot;:1,\u0026quot;Decimal\u0026quot;:3.14,\u0026quot;Bool\u0026quot;:true,\u0026quot;Null\u0026quot;:null,\u0026quot;Binary\u0026quot;:\u0026quot;SGVsbG8sIFdvcmxkIQ\u0026quot;} {\u0026quot;String\u0026quot;:\u0026quot;bbb\u0026quot;,\u0026quot;Int\u0026quot;:2,\u0026quot;Decimal\u0026quot;:2.718,\u0026quot;Bool\u0026quot;:false,\u0026quot;Null\u0026quot;:null,\u0026quot;Binary\u0026quot;:\u0026quot;RWFzdGVyCgllZ2cu\u0026quot;} Separated-Value Output Formats The values of a row are separated by different delimiters.\nThere are five separated-value output formats available: csv, tsv, csv2, tsv2 and dsv.\ncsv2, tsv2, dsv Starting with Hive 0.14 there are improved SV output formats available, namely dsv, csv2 and tsv2.\nThese three formats differ only with the delimiter between cells, which is comma for csv2, tab for tsv2, and configurable for dsv.\nFor the dsv format, the delimiter can be set with the delimiterForDSV option. The default delimiter is \u0026lsquo;|\u0026rsquo;.\nPlease be aware that only single character delimiters are supported.\nExampleResult of the query select id, value, comment from test_table\ncsv2\nid,value,comment 1,Value1,Test comment 1 2,Value2,Test comment 2 3,Value3,Test comment 3 tsv2\nid\tvalue\tcomment 1\tValue1\tTest comment 1 2\tValue2\tTest comment 2 3\tValue3\tTest comment 3 dsv (the delimiter is |)\nid|value|comment 1|Value1|Test comment 1 2|Value2|Test comment 2 3|Value3|Test comment 3 Quoting in csv2, tsv2 and dsv Formats If quoting is not disabled, double quotes are added around a value if it contains special characters (such as the delimiter or double quote character) or spans multiple lines.\nEmbedded double quotes are escaped with a preceding double quote.\nThe quoting can be disabled by setting the disable.quoting.for.sv system variable to true.\nIf the quoting is disabled, no double quotes are added around the values (even if they contains special characters) and the embedded double quotes are not escaped.\nBy default, the quoting is disabled.\nExample Result of the query select id, value, comment from test_table\ncsv2, quoting is enabled\nid,value,comment 1,\u0026quot;Value,1\u0026quot;,Value contains comma 2,\u0026quot;Value\u0026quot;\u0026quot;2\u0026quot;,Value contains double quote 3,Value'3,Value contains single quote csv2, quoting is disabled\nid,value,comment 1,Value,1,Value contains comma 2,Value\u0026quot;2,Value contains double quote 3,Value'3,Value contains single quote csv, tsv These two formats differ only with the delimiter between values, which is comma for csv and tab for tsv.\nThe values are always surrounded with single quote characters, even if the quoting is disabled by the disable.quoting.for.sv system variable.\nThese output formats don\u0026rsquo;t escape the embedded single quotes.\nPlease be aware that these output formats are deprecated and only maintained for backward compatibility.\nExample Result of the query select id, value, comment from test_table\ncsv\n'id','value','comment' '1','Value1','Test comment 1' '2','Value2','Test comment 2' '3','Value3','Test comment 3' tsv\n'id'\t'value'\t'comment' '1'\t'Value1'\t'Test comment 1' '2'\t'Value2'\t'Test comment 2' '3'\t'Value3'\t'Test comment 3' HiveServer2 Logging Starting with Hive 0.14.0, HiveServer2 operation logs are available for Beeline clients. These parameters configure logging:\n hive.server2.logging.operation.enabled hive.server2.logging.operation.log.location hive.server2.logging.operation.verbose (Hive 0.14 to 1.1) hive.server2.logging.operation.level (Hive 1.2 onward)   HIVE-11488 (Hive 2.0.0) adds the support of logging queryId and sessionId to HiveServer2 log file. To enable that, edit/add %X{queryId} and %X{sessionId} to the pattern format string of the logging configuration file.\nCancelling the Query When a user enters CTRL+C on the Beeline shell, if there is a query which is running at the same time then Beeline attempts to cancel the query while closing the socket connection to HiveServer2. This behavior is enabled only when [hive.server2.close.session.on.disconnect](#hive-server2-close-session-on-disconnect) is set to true. Starting from Hive 2.2.0 (HIVE-15626) Beeline does not exit the command line shell when the running query is being cancelled as a user enters CTRL+C. If the user wishes to exit the shell they can enter CTRL+C for the second time while the query is being cancelled. However, if there is no query currently running, the first CTRL+C will exit the Beeline shell. This behavior is similar to how the Hive CLI handles CTRL+C.\n!quit is the recommended command to exit the Beeline shell.\nBackground Query in Terminal Script Beeline can be run disconnected from a terminal for batch processing and automation scripts using commands such as nohup and disown.\nSome versions of Beeline client may require a workaround to allow the nohup command to correctly put the Beeline process in the background without stopping it. See HIVE-11717, HIVE-6758.\nThe following environment variable can be updated:\nexport HADOOP_CLIENT_OPTS=\u0026quot;$HADOOP_CLIENT_OPTS -Djline.terminal=jline.UnsupportedTerminal\u0026quot; Running with nohangup (nohup) and ampersand (\u0026amp;) will place the process in the background and allow the terminal to disconnect while keeping the Beeline process running. nohup beeline --silent=true --showHeader=true --outputformat=dsv -f query.hql \u0026lt;/dev/null \u0026gt; /tmp/output.log 2\u0026gt; /tmp/error.log \u0026amp; JDBC HiveServer2 has a JDBC driver. It supports both embedded and remote access to HiveServer2. Remote HiveServer2 mode is recommended for production use, as it is more secure and doesn\u0026rsquo;t require direct HDFS/metastore access to be granted for users.\nConnection URLs Connection URL Format The HiveServer2 URL is a string with the following syntax:\njdbc:hive2://\u0026lt;host1\u0026gt;:\u0026lt;port1\u0026gt;,\u0026lt;host2\u0026gt;:\u0026lt;port2\u0026gt;/dbName;initFile=\u0026lt;file\u0026gt;;sess_var_list?hive_conf_list#hive_var_list\nwhere\n *\u0026lt;host1\u0026gt;*:*\u0026lt;port1\u0026gt;*,*\u0026lt;host2\u0026gt;*:*\u0026lt;port2\u0026gt;* is a server instance or a comma separated list of server instances to connect to (if dynamic service discovery is enabled). If empty, the embedded server will be used. dbName is the name of the initial database.  is the path of init script file (Hive 2.2.0 and later). This script file is written with SQL statements which will be executed automatically after connection. This option can be empty. sess_var_list is a semicolon separated list of key=value pairs of session variables (e.g., user=foo;password=bar). hive_conf_list is a semicolon separated list of key=value pairs of Hive configuration variables for this session hive_var_list is a semicolon separated list of key=value pairs of Hive variables for this session.  Special characters in *sess_var_list, hive_conf_list, hive_var_list*parameter values should be encoded with URL encoding if needed.\nConnection URL for Remote or Embedded Mode The JDBC connection URL format has the prefix jdbc:hive2:// and the Driver class is org.apache.hive.jdbc.HiveDriver. Note that this is different from the old HiveServer.\n For a remote server, the URL format is jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;initFile=\u0026lt;file\u0026gt; (default port for HiveServer2 is 10000). For an embedded server, the URL format is jdbc:hive2:///;initFile=``(no host or port).  The initFile option is available in Hive 2.2.0 and later releases.\nConnection URL When HiveServer2 Is Running in HTTP Mode JDBC connection URL: jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;, where:\n \u0026lt;http_endpoint\u0026gt; is the corresponding HTTP endpoint configured in hive-site.xml. Default value is cliservice. Default port for HTTP transport mode is 10001.  Versions earlier than 0.14\nIn versions earlier than 0.14 these parameters used to be called [hive.server2.transport.mode](#hive-server2-transport-mode) and [hive.server2.thrift.http.path](#hive-server2-thrift-http-path) respectively and were part of the hive_conf_list. These versions have been deprecated in favour of the new versions (which are part of the sess_var_list) but continue to work for now.\nConnection URL When SSL Is Enabled in HiveServer2 JDBC connection URL: jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;ssl=true;sslTrustStore=\u0026lt;trust_store_path\u0026gt;;trustStorePassword=\u0026lt;trust_store_password\u0026gt;, where:\n \u0026lt;trust_store_path\u0026gt; is the path where client\u0026rsquo;s truststore file lives. \u0026lt;trust_store_password\u0026gt; is the password to access the truststore.  In HTTP mode: jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;ssl=true;sslTrustStore=\u0026lt;trust_store_path\u0026gt;;trustStorePassword=\u0026lt;trust_store_password\u0026gt;;transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;.\nFor versions earlier than 0.14, see the version note above.\nConnection URL When ZooKeeper Service Discovery Is Enabled ZooKeeper-based service discovery introduced in Hive 0.14.0 (HIVE-7935) enables high availability and rolling upgrade for HiveServer2. A JDBC URL that specifies needs to be used to make use of these features. That is, at least in hive-site.xml or other configuration files for HiveServer2, hive.server2.support.dynamic.service.discovery should be set to true, and hive.zookeeper.quorum should be defined to point to several started Zookeeper Servers. Reference Configuration Properties .\nThe minimal configuration example is as follows.\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot; encoding=\u0026quot;UTF-8\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.support.dynamic.service.discovery\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.zookeeper.quorum\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;127.0.0.1:2181\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; With further changes in Hive 2.0.0 and 1.3.0 (unreleased, HIVE-11581), none of the additional configuration parameters such as authentication mode, transport mode, or SSL parameters need to be specified, as they are retrieved from the ZooKeeper entries along with the hostname.\nThe JDBC connection URL: jdbc:hive2://\u0026lt;zookeeper quorum\u0026gt;/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2 .\nThe is the same as the value of hive.zookeeper.quorum configuration parameter in hive-site.xml/hivserver2-site.xml used by HiveServer2.\nAdditional runtime parameters needed for querying can be provided within the URL as follows, by appending it as a ?as before.\nThe JDBC connection URL: jdbc:hive2://\u0026lt;zookeeper quorum\u0026gt;/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2?tez.queue.name=hive1\u0026amp;hive.server2.thrift.resultset.serialize.in.tasks=true\nNamed Connection URLs As of Hive 2.1.0 (HIVE-13670), Beeline now also supports named URL connect strings via usage of environment variables. If you try to do a !connect to a name that does not look like a URL, then Beeline will attempt to see if there is an environment variable called BEELINE_URL_. For instance, if you specify !connect blue, it will look for BEELINE_URL_BLUE, and use that to connect. This should make it easier for system administrators to specify environment variables for users, and users need not type in the full URL each time to connect.\nReconnecting Traditionally, !reconnect has worked to refresh a connection that has already been established. It is not able to do a fresh connect after !close has been run. As of Hive 2.1.0 (HIVE-13670), Beeline remembers the last URL successfully connected to in a session, and is able to reconnect even after a !close has been run. In addition, if a user does a !save, then this is saved in the beeline.properties file, which then allows !reconnect to connect to this saved last-connected-to URL across multiple Beeline sessions. This also allows the use of beeline -r from the command line to do a reconnect on startup.\nUsing hive-site.xml to automatically connect to HiveServer2 As of Hive 2.2.0 (HIVE-14063), Beeline adds support to use the hive-site.xml present in the classpath to automatically generate a connection URL based on the configuration properties in hive-site.xml and an additional user configuration file. Not all the URL properties can be derived from hive-site.xml and hence in order to use this feature user must create a configuration file called “beeline-hs2-connection.xml” which is a Hadoop XML format file. This file is used to provide user-specific connection properties for the connection URL. Beeline looks for this configuration file in ${user.home}/.beeline/ (Unix based OS) or ${user.home}\\beeline\\ directory (in case of Windows). If the file is not found in the above locations Beeline looks for it in ${HIVE_CONF_DIR} location and /etc/hive/conf (check HIVE-16335 which fixes this location from /etc/conf/hive in Hive 2.2.0) in that order. Once the file is found, Beeline uses beeline-hs2-connection.xml in conjunction with the hive-site.xml in the class path to determine the connection URL.\nThe URL connection properties in beeline-hs2-connection.xml must have the prefix “beeline.hs2.connection.” followed by the URL property name. For example in order to provide the property ssl the property key in the beeline-hs2-connection.xml should be “beeline.hs2.connection.ssl”. The sample beeline.hs2.connection.xml below provides the value of user and password for the Beeline connection URL. In this case the rest of the properties like HS2 hostname and port information, Kerberos configuration properties, SSL properties, transport mode, etc., are picked up using the hive-site.xml in the class path. If the password is empty beeline.hs2.connection.password property should be removed. In most cases the below configuration values in beeline-hs2-connection.xml and the correct hive-site.xml in classpath should be sufficient to make the connection to the HiveServer2.\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026quot;text/xsl\u0026quot; href=\u0026quot;configuration.xsl\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.user\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.password\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; In case of properties which are present in both beeline-hs2-connection.xml and hive-site.xml, the property value derived from beeline-hs2-connection.xml takes precedence. For example in the below beeline-hs2-connection.xml file provides the value of principal for Beeline connection in a Kerberos enabled environment. In this case the property value for beeline.hs2.connection.principal overrides the value of HiveConf.ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL from hive-site.xml as far as connection URL is concerned.\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026quot;text/xsl\u0026quot; href=\u0026quot;configuration.xsl\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;localhost:10000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.principal\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive/dummy-hostname@domain.com\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; In case of properties beeline.hs2.connection.hosts, beeline.hs2.connection.hiveconf and beeline.hs2.connection.hivevar the property value is a comma-separated list of values. For example the following beeline-hs2-connection.xml provides the hiveconf and hivevar values in a comma separated format.\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026quot;text/xsl\u0026quot; href=\u0026quot;configuration.xsl\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.user\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.hiveconf\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive.cli.print.current.db=true, hive.cli.print.header=true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.hivevar\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;testVarName1=value1, testVarName2=value2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; When the beeline-hs2-connection.xml is present and when no other arguments are provided, Beeline automatically connects to the URL generated using configuration files. When connection arguments (-u, -n or -p) are provided, Beeline uses them and does not use beeline-hs2-connection.xml to automatically connect. Removing or renaming the beeline-hs2-connection.xml disables this feature.\nUsing beeline-site.xml to automatically connect to HiveServer2 In addition to the above method of using hive-site.xml and beeline-hs2-connection.xml for deriving the JDBC connection URL to use when connecting to HiveServer2 from Beeline, a user can optionally add beeline-site.xml to their classpath, and within beeline-site.xml, she can specify complete JDBC URLs. A user can also specify multiple named URLs and use beeline -c \u0026lt;named_url\u0026gt; to connect to a specific URL. This is particularly useful when the same cluster has multiple HiveServer2 instances running with different configurations. One of the named URLs is treated as default (which is the URL that gets used when the user simply types beeline). An example beeline-site.xml is shown below:\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026quot;text/xsl\u0026quot; href=\u0026quot;configuration.xsl\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.jdbc.url.tcpUrl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:hive2://localhost:10000/default;user=hive;password=hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.jdbc.url.httpUrl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:hive2://localhost:10000/default;user=hive;password=hive;transportMode=http;httpPath=cliservice\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.jdbc.url.default\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;tcpUrl\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; In the above example, simply typing beeline opens a new JDBC connection to jdbc:hive2://localhost:10000/default;user=hive;password=hive. If both beeline-site.xml and beeline-hs2-connection.xml are present in the classpath, the final URL is created by applying the properties specified in beeline-hs2-connection.xml on top of the URL properties derived from beeline-site.xml. As an example consider the following beeline-hs2-connection.xml:\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026quot;text/xsl\u0026quot; href=\u0026quot;configuration.xsl\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.user\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.connection.password\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Consider the following beeline-site.xml:\n\u0026lt;?xml version=\u0026quot;1.0\u0026quot;?\u0026gt; \u0026lt;?xml-stylesheet type=\u0026quot;text/xsl\u0026quot; href=\u0026quot;configuration.xsl\u0026quot;?\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.jdbc.url.tcpUrl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:hive2://localhost:10000/default\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.jdbc.url.httpUrl\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;beeline.hs2.jdbc.url.default\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;tcpUrl\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; In the above example, simply typing beeline opens a new JDBC connection to j``dbc:hive2://localhost:10000/default;user=hive;password=hive. When the user types beeline -c httpUrl, a connection is opened to jdbc:hive2://localhost:10000/default;transportMode=http;httpPath=cliservice;user=hive;password=hive.\nUsing JDBC You can use JDBC to access data stored in a relational database or other tabular format.\n Load the HiveServer2 JDBC driver. As of 1.2.0 applications no longer need to explicitly load JDBC drivers using Class.forName().  For example:\nClass.forName(\u0026quot;org.apache.hive.jdbc.HiveDriver\u0026quot;); Connect to the database by creating a Connection object with the JDBC driver.  For example:\nConnection cnct = DriverManager.getConnection(\u0026quot;jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026quot;, \u0026quot;\u0026lt;user\u0026gt;\u0026quot;, \u0026quot;\u0026lt;password\u0026gt;\u0026quot;); The default \u0026lt;port\u0026gt; is 10000. In non-secure configurations, specify a \u0026lt;user\u0026gt; for the query to run as. The \u0026lt;password\u0026gt; field value is ignored in non-secure mode.\nConnection cnct = DriverManager.getConnection(\u0026quot;jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;\u0026quot;, \u0026quot;\u0026lt;user\u0026gt;\u0026quot;, \u0026quot;\u0026quot;); In Kerberos secure mode, the user information is based on the Kerberos credentials. 3. Submit SQL to the database by creating a Statement object and using its executeQuery() method.\nFor example:\nStatement stmt = cnct.createStatement(); ResultSet rset = stmt.executeQuery(\u0026quot;SELECT foo FROM bar\u0026quot;); Process the result set, if necessary.  These steps are illustrated in the sample code below.\nJDBC Client Sample Code import java.sql.SQLException; import java.sql.Connection; import java.sql.ResultSet; import java.sql.Statement; import java.sql.DriverManager; public class HiveJdbcClient { private static String driverName = \u0026quot;org.apache.hive.jdbc.HiveDriver\u0026quot;; /** * @param args * @throws SQLException */ public static void main(String[] args) throws SQLException { try { Class.forName(driverName); } catch (ClassNotFoundException e) { // TODO Auto-generated catch block e.printStackTrace(); System.exit(1); } //replace \u0026quot;hive\u0026quot; here with the name of the user the queries should run as Connection con = DriverManager.getConnection(\u0026quot;jdbc:hive2://localhost:10000/default\u0026quot;, \u0026quot;hive\u0026quot;, \u0026quot;\u0026quot;); Statement stmt = con.createStatement(); String tableName = \u0026quot;testHiveDriverTable\u0026quot;; stmt.execute(\u0026quot;drop table if exists \u0026quot; + tableName); stmt.execute(\u0026quot;create table \u0026quot; + tableName + \u0026quot; (key int, value string)\u0026quot;); // show tables String sql = \u0026quot;show tables '\u0026quot; + tableName + \u0026quot;'\u0026quot;; System.out.println(\u0026quot;Running: \u0026quot; + sql); ResultSet res = stmt.executeQuery(sql); if (res.next()) { System.out.println(res.getString(1)); } // describe table sql = \u0026quot;describe \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1) + \u0026quot;\\t\u0026quot; + res.getString(2)); } // load data into table // NOTE: filepath has to be local to the hive server // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line String filepath = \u0026quot;/tmp/a.txt\u0026quot;; sql = \u0026quot;load data local inpath '\u0026quot; + filepath + \u0026quot;' into table \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); stmt.execute(sql); // select * query sql = \u0026quot;select * from \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(String.valueOf(res.getInt(1)) + \u0026quot;\\t\u0026quot; + res.getString(2)); } // regular hive query sql = \u0026quot;select count(1) from \u0026quot; + tableName; System.out.println(\u0026quot;Running: \u0026quot; + sql); res = stmt.executeQuery(sql); while (res.next()) { System.out.println(res.getString(1)); } } } Running the JDBC Sample Code # Then on the command-line $ javac HiveJdbcClient.java # To run the program using remote hiveserver in non-kerberos mode, we need the following jars in the classpath # from hive/build/dist/lib # hive-jdbc*.jar # hive-service*.jar # libfb303-0.9.0.jar # libthrift-0.9.0.jar # log4j-1.2.16.jar # slf4j-api-1.6.1.jar # slf4j-log4j12-1.6.1.jar # commons-logging-1.0.4.jar # # # To run the program using kerberos secure mode, we need the following jars in the classpath # hive-exec*.jar # commons-configuration-1.6.jar (This is not needed with Hadoop 2.6.x and later). # and from hadoop # hadoop-core*.jar (use hadoop-common*.jar for Hadoop 2.x) # # To run the program in embedded mode, we need the following additional jars in the classpath # from hive/build/dist/lib # hive-exec*.jar # hive-metastore*.jar # antlr-runtime-3.0.1.jar # derby.jar # jdo2-api-2.1.jar # jpox-core-1.2.2.jar # jpox-rdbms-1.2.2.jar # and from hadoop/build # hadoop-core*.jar # as well as hive/build/dist/conf, any HIVE_AUX_JARS_PATH set, # and hadoop jars necessary to run MR jobs (eg lzo codec) $ java -cp $CLASSPATH HiveJdbcClient Alternatively, you can run the following bash script, which will seed the data file and build your classpath before invoking the client. The script adds all the additional jars needed for using HiveServer2 in embedded mode as well.\n#!/bin/bash HADOOP_HOME=/your/path/to/hadoop HIVE_HOME=/your/path/to/hive echo -e '1\\x01foo' \u0026gt; /tmp/a.txt echo -e '2\\x01bar' \u0026gt;\u0026gt; /tmp/a.txt HADOOP_CORE=$(ls $HADOOP_HOME/hadoop-core*.jar) CLASSPATH=.:$HIVE_HOME/conf:$(hadoop classpath) for i in ${HIVE_HOME}/lib/*.jar ; do CLASSPATH=$CLASSPATH:$i done java -cp $CLASSPATH HiveJdbcClient JDBC Data Types The following table lists the data types implemented for HiveServer2 JDBC.\n   Hive Type Java Type Specification     TINYINT byte signed or unsigned 1-byte integer   SMALLINT short signed 2-byte integer   INT int signed 4-byte integer   BIGINT long signed 8-byte integer   FLOAT double single-precision number (approximately 7 digits)   DOUBLE double double-precision number (approximately 15 digits)   DECIMAL java.math.BigDecimal fixed-precision decimal value   BOOLEAN boolean a single bit (0 or 1)   STRING String character string or variable-length character string   TIMESTAMP java.sql.Timestamp date and time value   BINARY String binary data   Complex Types     ARRAY String – json encoded values of one data type   MAP String – json encoded key-value pairs   STRUCT String – json encoded structured values    JDBC Client Setup for a Secure Cluster When connecting to HiveServer2 with Kerberos authentication, the URL format is:\njdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;principal=\u0026lt;Server_Principal_of_HiveServer2\u0026gt;\nThe client needs to have a valid Kerberos ticket in the ticket cache before connecting.\nNOTE: If you don\u0026rsquo;t have a \u0026ldquo;/\u0026rdquo; after the port number, the jdbc driver does not parse the hostname and ends up running HS2 in embedded mode . So if you are specifying a hostname, make sure you have a \u0026ldquo;/\u0026rdquo; or \u0026ldquo;/\u0026rdquo; after the port number.\nIn the case of LDAP, CUSTOM or PAM authentication, the client needs to pass a valid user name and password to the JDBC connection API.\nTo use sasl.qop, add the following to the sessionconf part of your Hive JDBC hive connection string, e.g. jdbc:hive://hostname/dbname;sasl.qop=auth-int For more information, see Setting Up HiveServer2.\nMulti-User Scenarios and Programmatic Login to Kerberos KDC In the current approach of using Kerberos you need to have a valid Kerberos ticket in the ticket cache before connecting. This entails a static login (using kinit, key tab or ticketcache) and the restriction of one Kerberos user per client. These restrictions limit the usage in middleware systems and other multi-user scenarios, and in scenarios where the client wants to login programmatically to Kerberos KDC.\nOne way to mitigate the problem of multi-user scenarios is with secure proxy users (see HIVE-5155). Starting in Hive 0.13.0, support for secure proxy users has two components:\n Direct proxy access for privileged Hadoop users (HIVE-5155). This enables a privileged user to directly specify an alternate session user during the connection. If the connecting user has Hadoop level privilege to impersonate the requested userid, then HiveServer2 will run the session as that requested user. Delegation token based connection for Oozie (OOZIE-1457). This is the common mechanism for Hadoop ecosystem components.\nProxy user privileges in the Hadoop ecosystem are associated with both user names and hosts. That is, the privilege is available for certain users from certain hosts. Delegation tokens in Hive are meant to be used if you are connecting from one authorized (blessed) machine and later you need to make a connection from another non-blessed machine. You get the delegation token from a blessed machine and connect using the delegation token from a non-blessed machine. The primary use case is Oozie, which gets a delegation token from the server machine and then gets another connection from a Hadoop task node.  If you are only making a JDBC connection as a privileged user from a single blessed machine, then direct proxy access is the simpler approach. You can just pass the user you need to impersonate in the JDBC URL by using the hive.server2.proxy.user= parameter.\nSee examples in ProxyAuthTest.java.\nSupport for delegation tokens with HiveServer2 binary transport mode hive.server2.transport.mode has been available starting 0.13.0; support for this feature with HTTP transport mode was added in HIVE-13169, which should be part of Hive 2.1.0.\nThe other way is to use a pre-authenticated Kerberos Subject (see HIVE-6486). In this method, starting with Hive 0.13.0 the Hive JDBC client can use a pre-authenticated subject to authenticate to HiveServer2. This enables a middleware system to run queries as the user running the client.\nUsing Kerberos with a Pre-Authenticated Subject To use a pre-authenticated subject you will need the following changes.\n Add hive-exec*.jar to the classpath in addition to the regular Hive JDBC jars (commons-configuration-1.6.jar and hadoop-core*.jar are not required). Add auth=kerberos and kerberosAuthType=fromSubject JDBC URL properties in addition to having the “principal\u0026rdquo; url property. Open the connection in Subject.doAs().  The following code snippet illustrates the usage (refer to HIVE-6486 for a complete test case):\nstatic Connection getConnection( Subject signedOnUserSubject ) throws Exception{ Connection conn = (Connection) Subject.doAs(signedOnUserSubject, new PrivilegedExceptionAction\u0026lt;Object\u0026gt;() { public Object run() { Connection con = null; String JDBC_DB_URL = \u0026quot;jdbc:hive2://HiveHost:10000/default;\u0026quot; || \u0026quot;principal=hive/localhost.localdomain@EXAMPLE.COM;\u0026quot; || \u0026quot;kerberosAuthType=fromSubject\u0026quot;; try { Class.forName(JDBC_DRIVER); con = DriverManager.getConnection(JDBC_DB_URL); } catch (SQLException e) { e.printStackTrace(); } catch (ClassNotFoundException e) { e.printStackTrace(); } return con; } }); return conn; } JDBC Fetch Size Gives the JDBC driver a hint as to the number of rows that should be fetched from the database when more rows are needed by the client. The default value, used for every statement, can be specified through the JDBC connection string. This default value may subsequently be overwritten, per statement, with the JDBC API. If no value is specified within the JDBC connection string, then the default fetch size is retrieved from the HiveServer2 instance as part of the session initiation operation.\njdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;fetchsize=\u0026lt;value\u0026gt;\nHive Version 4.0\nThe Hive JDBC driver will receive a preferred fetch size from the instance of HiveServer2 it has connected to. This value is specified on the server by the hive.server2.thrift.resultset.default.fetch.size configuration.\nThe JDBC fetch size is only a hint and the server will attempt to respect the client\u0026rsquo;s requested fetch size though with some limits. HiveServer2 will cap all requests at a maximum value specified by the hive.server2.thrift.resultset.max.fetch.size configuration value regardless of the client\u0026rsquo;s requested fetch size.\nWhile a larger fetch size may limit the number of round-trips between the client and server, it does so at the expense of additional memory requirements on the client and server.\nThe default JDBC fetch size value may be overwritten, per statement, with the JDBC API:\n Setting a value of 0 instructs the driver to use the fetch size value preferred by the server Setting a value greater than zero will instruct the driver to fetch that many rows, though the actual number of rows returned may be capped by the server If no fetch size value is explicitly set on the JDBC driver\u0026rsquo;s statement then the driver\u0026rsquo;s default value is used  If the fetch size value is specified within the JDBC connection string, this is the default value If the fetch size value is absent from the JDBC connection string, the server\u0026rsquo;s preferred fetch size is used as the default value    Python Client A Python client driver is available on github. For installation instructions, see Setting Up HiveServer2: Python Client Driver.\nRuby Client A Ruby client driver is available on github at https://github.com/forward3d/rbhive.\nIntegration with SQuirrel SQL Client   Download, install and start the SQuirrel SQL Client from the SQuirrel SQL website.\n  Select \u0026lsquo;Drivers -\u0026gt; New Driver\u0026hellip;\u0026rsquo; to register Hive\u0026rsquo;s JDBC driver that works with HiveServer2.\n Enter the driver name and example URL:   Name: Hive Example URL: jdbc:hive2://localhost:10000/default ``\n  Select \u0026lsquo;Extra Class Path -\u0026gt; Add\u0026rsquo; to add the following jars from your local Hive and Hadoop distribution.\n   HIVE_HOME/lib/hive-jdbc-*-standalone.jar HADOOP_HOME/share/hadoop/common/hadoop-common-*.jar Version information\nHive JDBC standalone jars are used in Hive 0.14.0 onward (HIVE-538); for previous versions of Hive, use HIVE_HOME/build/dist/lib/*.jar instead.\nThe hadoop-common jars are for Hadoop 2.0; for previous versions of Hadoop, use HADOOP_HOME/hadoop-*-core.jar instead. 4. Select \u0026lsquo;List Drivers\u0026rsquo;. This will cause SQuirrel to parse your jars for JDBC drivers and might take a few seconds. From the \u0026lsquo;Class Name\u0026rsquo; input box select the Hive driver for working with HiveServer2:\n org.apache.hive.jdbc.HiveDriver Click \u0026lsquo;OK\u0026rsquo; to complete the driver registration. Select \u0026lsquo;Aliases -\u0026gt; Add Alias\u0026hellip;\u0026rsquo; to create a connection alias to your HiveServer2 instance.  Give the connection alias a name in the \u0026lsquo;Name\u0026rsquo; input box. Select the Hive driver from the \u0026lsquo;Driver\u0026rsquo; drop-down. Modify the example URL as needed to point to your HiveServer2 instance. Enter \u0026lsquo;User Name\u0026rsquo; and \u0026lsquo;Password\u0026rsquo; and click \u0026lsquo;OK\u0026rsquo; to save the connection alias. To connect to HiveServer2, double-click the Hive alias and click \u0026lsquo;Connect\u0026rsquo;.    When the connection is established you will see errors in the log console and might get a warning that the driver is not JDBC 3.0 compatible. These alerts are due to yet-to-be-implemented parts of the JDBC metadata API and can safely be ignored. To test the connection enter SHOW TABLES in the console and click the run icon.\nAlso note that when a query is running, support for the \u0026lsquo;Cancel\u0026rsquo; button is not yet available.\nIntegration with SQL Developer Integration with Oracle SQLDeveloper is available using JDBC connection.\nhttps://community.hortonworks.com/articles/1887/connect-oracle-sql-developer-to-hive.html\nIntegration with DbVisSoftware\u0026rsquo;s DbVisualizer  Download, install and start DbVisualizer free or purchase DbVisualizer Pro from https://www.dbvis.com/. Follow instructions on github.  Advanced Features for Integration with Other Tools Supporting Cookie Replay in HTTP Mode Version 1.2.0 and later\nThis option is available starting in Hive 1.2.0.\nHIVE-9709 introduced support for the JDBC driver to enable cookie replay. This is turned on by default so that incoming cookies can be sent back to the server for authentication. The JDBC connection URL when enabled should look like this:\njdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;?transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;;cookieAuth=true;cookieName=\u0026lt;cookie_name\u0026gt;\n cookieAuth is set to true by default. cookieName: If any of the incoming cookies' keys match the value of cookieName, the JDBC driver will not send any login credentials/Kerberos ticket to the server. The client will just send the cookie alone back to the server for authentication. The default value of cookieName is hive.server2.auth (this is the HiveServer2 cookie name). To turn off cookie replay, cookieAuth=false must be used in the JDBC URL. Important Note: As part of HIVE-9709, we upgraded Apache http-client and http-core components of Hive to 4.4. To avoid any collision between this upgraded version of HttpComponents and other any versions that might be present in your system (such as the one provided by Apache Hadoop 2.6 which uses http-client and http-core components version of 4.2.5), the client is expected to set CLASSPATH in such a way that Beeline-related jars appear before HADOOP lib jars. This is achieved via setting HADOOP_USER_CLASSPATH_FIRST=true before using hive-jdbc. In fact, in bin/beeline.sh we do this!  Using 2-way SSL in HTTP Mode Version 1.2.0 and later\nThis option is available starting in Hive 1.2.0.\nHIVE-10447 enabled the JDBC driver to support 2-way SSL in HTTP mode. Please note that HiveServer2 currently does not support 2-way SSL. So this feature is handy when there is an intermediate server such as Knox which requires client to support 2-way SSL.\nJDBC connection URL:\njdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;ssl=true;twoWay=true;``sslTrustStore=\u0026lt;trust_store_path\u0026gt;;trustStorePassword=\u0026lt;trust_store_password\u0026gt;;sslKeyStore=\u0026lt;key_store_path\u0026gt;;keyStorePassword=\u0026lt;key_store_password\u0026gt;;``transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;\n \u0026lt;trust_store_path\u0026gt; is the path where the client\u0026rsquo;s truststore file lives. This is a mandatory non-empty field. \u0026lt;trust_store_password\u0026gt; is the password to access the truststore. \u0026lt;key_store_path\u0026gt; is the path where the client\u0026rsquo;s keystore file lives. This is a mandatory non-empty field. \u0026lt;key_store_password\u0026gt; is the password to access the keystore.  For versions earlier than 0.14, see the version note above.\nIn the environment where exposing trustStorePassword and keyStorePassword in the connection URL is a security concern, a new option storePasswordPath is introduced with HIVE-27308 that can be used in URL instead of trustStorePassword and keyStorePassword. storePasswordPath value hold the path to the local keystore file storing the trustStorePassword and keyStorePassword aliases. When the existing trustStorePassword or keyStorePassword is present in URL along with storePasswordPath, respective password is directly obtained from password option. Otherwise, fetches the particular alias from local keystore file(i.e., existing password options are preferred over storePasswordPath).\nJDBC connection URL with storePasswordPath:\njdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;ssl=true;twoWay=true;``sslTrustStore=\u0026lt;trust_store_path\u0026gt;;sslKeyStore=\u0026lt;key_store_path\u0026gt;;storePasswordPath=store_password_path\u0026gt;**;**``transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;\nA local keystore file can be created leveraging hadoop credential command with trustStorePassword and keyStorePassword aliases like below. And this file can be passed with storePasswordPath option in the connection URL.\nhadoop credential create trust****StorePassword -value mytruststorepassword -provider localjceks://file/tmp/client_creds.jceks\nhadoop credential create keyStorePassword -value mykeystorepassword -provider localjceks://file/tmp/client_creds.jceks\nPassing HTTP Header Key/Value Pairs via JDBC Driver Version 1.2.0 and later\nThis option is available starting in Hive 1.2.0.\nHIVE-10339 introduced an option for clients to provide custom HTTP headers that can be sent to the underlying server (Hive 1.2.0 and later).\nJDBC connection URL:\njdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;;http.header.\u0026lt;name1\u0026gt;=\u0026lt;value1\u0026gt;;http.header.\u0026lt;name2\u0026gt;=\u0026lt;value2\u0026gt;\nWhen the above URL is specified, Beeline will call underlying requests to add an HTTP header set to  and  and another HTTP header set to  and . This is helpful when the end user needs to send identity in an HTTP header down to intermediate servers such as Knox via Beeline for authentication, for example http.header.USERNAME=\u0026lt;value1\u0026gt;;http.header.PASSWORD=\u0026lt;value2\u0026gt;.\nFor versions earlier than 0.14, see the version note above. Passing Custom HTTP Cookie Key/Value Pairs via JDBC Driver In Hive version 3.0.0 HIVE-18447 introduced an option for clients to provide custom HTTP cookies that can be sent to the underlying server. Some authentication mechanisms, like Single Sign On, need the ability to pass a cookie to some intermediate authentication service like Knox via the JDBC driver. JDBC connection URL: jdbc:hive2://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;db\u0026gt;;transportMode=http;httpPath=\u0026lt;http_endpoint\u0026gt;;http.cookie.\u0026lt;name1\u0026gt;=\u0026lt;value1\u0026gt;;http.cookie.\u0026lt;name2\u0026gt;=\u0026lt;value2\u0026gt;\nWhen the above URL is specified, Beeline will call underlying requests to add HTTP cookie in the request header, and will set it to = and =. Save\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveserver2-clients_30758725/","tags":null,"title":"Apache Hive : HiveServer2 Clients"},{"categories":null,"contents":"Apache Hive : HiveServer2 Overview  Introduction HS2 Architecture  Server Transport Protocol Processor   Dependencies of HS2 JDBC Client Source Code Description  Server Side Client Side Interaction between Client and Server   Resources  Introduction HiveServer2 (HS2) is a service that enables clients to execute queries against Hive. HiveServer2 is the successor to HiveServer1 which has been deprecated. HS2 supports multi-client concurrency and authentication. It is designed to provide better support for open API clients like JDBC and ODBC.\nHS2 is a single process running as a composite service, which includes the Thrift-based Hive service (TCP or HTTP) and a Jetty web server for web UI. HS2 Architecture The Thrift-based Hive service is the core of HS2 and responsible for servicing the Hive queries (e.g., from Beeline). Thrift is an RPC framework for building cross-platform services. Its stack consists of 4 layers: Server, Transport, Protocol, and Processor. You can find more details about the layers at https://thrift.apache.org/docs/concepts.\nThe usage of those layers in the HS2 implementation is described below.\nServer HS2 uses a TThreadPoolServer (from Thrift) for TCP mode, or a Jetty server for the HTTP mode. The TThreadPoolServer allocates one worker thread per TCP connection. Each thread is always associated with a connection even if the connection is idle. So there is a potential performance issue resulting from a large number of threads due to a large number of concurrent connections. In the future HS2 might switch to another server type for TCP mode, for example TThreadedSelectorServer. Here is an article about a performance comparison between different Thrift Java servers. Transport HTTP mode is required when a proxy is needed between the client and server (for example, for load balancing or security reasons). That is why it is supported, as well as TCP mode. You can specify the transport mode of the Thrift service through the Hive configuration property hive.server2.transport.mode.\nProtocol The Protocol implementation is responsible for serialization and deserialization. HS2 is currently using TBinaryProtocol as its Thrift protocol for serialization. In the future other protocols may be considered, such as TCompactProtocol, based on more performance evaluation.\nProcessor Process implementation is the application logic to handle requests. For example, the ThriftCLIService.ExecuteStatement() method implements the logic to compile and execute a Hive query.\nDependencies of HS2  Metastore\nThe metastore can be configured as embedded (in the same process as HS2) or as a remote server (which is a Thrift-based service as well). HS2 talks to the metastore for the metadata required for query compilation. Hadoop cluster\nHS2 prepares physical execution plans for various execution engines (MapReduce/Tez/Spark) and submits jobs to the Hadoop cluster for execution.  You can find a diagram of the interactions between HS2 and its dependencies here.\nJDBC Client The JDBC driver is recommended for the client side to interact with HS2. Note that there are some use cases (e.g., Hadoop Hue) where the Thrift client is used directly and JDBC is bypassed.\nHere is a sequence of API calls involved to make the first query:\n The JDBC client (e.g., Beeline) creates a HiveConnection by initiating a transport connection (e.g., TCP connection) followed by an OpenSession API call to get a SessionHandle. The session is created from the server side. The HiveStatement is executed (following JDBC standards) and an ExecuteStatement API call is made from the Thrift client. In the API call, SessionHandle information is passed to the server along with the query information. The HS2 server receives the request and asks the driver (which is a CommandProcessor) for query parsing and compilation. The driver kicks off a background job that will talk to Hadoop and then immediately returns a response to the client. This is an asynchronous design of the ExecuteStatement API. The response contains an OperationHandle created from the server side. The client uses the OperationHandle to talk to HS2 to poll the status of the query execution.  Source Code Description The following sections help you locate some basic components of HiveServer2 in the source code.\nServer Side  Thrift IDL file for TCLIService: https://github.com/apache/hive/blob/master/service-rpc/if/TCLIService.thrift. TCLIService.Iface implemented by: org.apache.hive.service.cli.thrift.ThriftCLIService class. ThriftCLIService subclassed by: org.apache.hive.service.cli.thrift.ThriftBinaryCLIServiceand org.apache.hive.service.cli.thrift.ThriftHttpCLIService for TCP mode and HTTP mode respectively*.* org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService class: Embedded mode for HS2. Don\u0026rsquo;t get confused with embedded metastore, which is a different service (although the embedded mode concept is similar). org.apache.hive.service.cli.session.HiveSessionImpl class: Instances of this class are created on the server side and managed by an org.apache.accumulo.tserver.TabletServer.SessionManager instance*.* org.apache.hive.service.cli.operation.Operation class: Defines an operation (e.g., a query). Instances of this class are created on the server and managed by an org.apache.hive.service.cli.operation.OperationManager instance*.* org.apache.hive.service.auth.HiveAuthFactory class: A helper used by both HTTP and TCP mode for authentication. Refer to Setting Up HiveServer2 for various authentication options, in particular Authentication/Security Configuration and Cookie Based Authentication.  Client Side  org.apache.hive.jdbc.HiveConnection class:**** Implements thejava.sql.Connection interface (part of JDBC*)**.* An instance of this class holds a reference to a SessionHandle instance which is retrieved when making Thrift API calls to the server*.* org.apache.hive.jdbc.HiveStatement class: Implements the java.sql.Statement interface (part of JDBC). The client (e.g., Beeline) calls the HiveStatement.execute() method for the query. Inside the execute() method, the Thrift client is used to make API calls. org.apache.hive.jdbc.HiveDriver class: Implements the java.sql.Driver interface (part of JDBC). The core method is connect() which is used by the JDBC client to initiate a SQL connection.  Interaction between Client and Server  **org.apache.hive.service.cli.SessionHandle class:Session identifier.Instances of this class are returned from the server and used by the client as input for Thrift API calls. org.apache.hive.service.cli.OperationHandle class: Operation identifier. Instances of this class are returned from the server and used by the client to poll the execution status of an operation.  Resources How to set up HS2: Setting Up HiveServer2\nHS2 clients: HiveServer2 Clients\nUser interface: Web UI for HiveServer2\nMetrics: Hive Metrics\nCloudera blog on HS2: http://blog.cloudera.com/blog/2013/07/how-hiveserver2-brings-security-and-concurrency-to-apache-hive/\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveserver2-overview_65147648/","tags":null,"title":"Apache Hive : HiveServer2 Overview"},{"categories":null,"contents":"Apache Hive : HiveServer2 Thrift API Introduction This document is a proposal for a new HiveServer2 Thrift API.\nMotivations Concurrency Many users have reported that the current HiveServer implementation has concurrency bugs (for example, see HIVE-80). In fact, it\u0026rsquo;s impossible for HiveServer to support concurrent connections using the current Thrift API, a result of the fact that Thrift doesn\u0026rsquo;t provide server-side access to connection handles. Since the current API does not provide explicit support for sessions or connections, HiveServer has no way of mapping incoming requests to client sessions, which makes it impossible for HiveServer to maintain session state in between calls.\nHiveServer currently attempts to maintain session state using thread-local variables and relies on Thrift to consistently map the same connection to the same Thrift worker thread, but this isn\u0026rsquo;t a valid assumption to make. For example, if a client executes \u0026ldquo;set mapred.reduce.tasks=1\u0026rdquo; followed by \u0026ldquo;select \u0026hellip;..\u0026rdquo;, it is incorrect to assume that both of these statements will be executed by the same worker thread. Furthermore, the Thrift API doesn\u0026rsquo;t provide any mechanism for detecting client disconnects (see THRIFT-1195), which results in incorrect behavior like this:\n% hive -h localhost -p 10000 [localhost:10000] hive\u0026gt; set x=1; set x=1; [localhost:10000] hive\u0026gt; set x; set x; x=1 [localhost:10000] hive\u0026gt; quit; quit; % hive -h localhost -p 10000 [localhost:10000] hive\u0026gt; set x; set x; x=1 [localhost:10000] hive\u0026gt; quit; quit; In this example the user opened a connection to a HiveServer process, modified the connection\u0026rsquo;s session state by setting x=1, and then closed the connection. The same user then created a new connection and printed the value of x. Since this statement was executed in a new connection/session we expect x to be undefined. However, we see that x actually has the value that was set in the previous session. This incorrect behavior occurs because Thrift has assigned the same worker thread to service the second connection, and since Thrift doesn\u0026rsquo;t provide a mechanism for detecting client disconnects, HiveServer was unable to clear the thread-local session state associated with that worker thread before Thrift reassigned it to the second connection.\nWhile it\u0026rsquo;s tempting to try to solve these problems by modifying Thrift to provide direct access to the connection handle (which would allow HiveServer to map client connection handles to session state), this approach makes it really hard to support HA since it depends on the physical connection lasting as long as the user session, which isn\u0026rsquo;t a fair assumption to make in the context of queries that can take many hours to complete.\nInstead, the approach we\u0026rsquo;re taking with HiveServer2 is to provide explicit support for sessions in the client API, e.g every RPC call references a session ID which the server then maps to persistent session state. This makes it possible for any worker thread to service any request from any client connection, and also the avoids the need to tightly couple physical connections to logical sessions.\nImproved Support for ODBC/JDBC It has been a struggle to implement ODBC and JDBC drivers on top of the HiveServer Thrift API. This is largely a result of missing support for sessions, asynchronous query execution, the ability to cancel running queries, and methods for retrieving information about the capabilities of the remote server. We have attempted to address all of these issues with the HiveServer2 API.\nProposed HiveServer2 Thrift API // Licensed to the Apache Software Foundation (ASF) under one // or more contributor license agreements. See the NOTICE file // distributed with this work for additional information // regarding copyright ownership. The ASF licenses this file // to you under the Apache License, Version 2.0 (the // \u0026quot;License\u0026quot;); you may not use this file except in compliance // with the License. You may obtain a copy of the License at // // http://www.apache.org/licenses/LICENSE-2.0 // // Unless required by applicable law or agreed to in writing, software // distributed under the License is distributed on an \u0026quot;AS IS\u0026quot; BASIS, // WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. // See the License for the specific language governing permissions and // limitations under the License. // required for GetQueryPlan() include \u0026quot;ql/if/queryplan.thrift\u0026quot; namespace java org.apache.hive.service namespace cpp Apache.Hive // List of protocol versions. A new token should be // added to the end of this list every time a change is made. enum ProtocolVersion { HIVE_SERVER2_PROTOCOL_V1 } enum TType { BOOLEAN_TYPE, TINYINT_TYPE, SMALLINT_TYPE, INT_TYPE, BIGINT_TYPE, FLOAT_TYPE, DOUBLE_TYPE, STRING_TYPE, TIMESTAMP_TYPE, BINARY_TYPE, ARRAY_TYPE, MAP_TYPE, STRUCT_TYPE, UNION_TYPE, USER_DEFINED_TYPE } const set\u0026lt;TType\u0026gt; PRIMITIVE_TYPES = [ TType.BOOLEAN_TYPE TType.TINYINT_TYPE TType.SMALLINT_TYPE TType.INT_TYPE TType.BIGINT_TYPE TType.FLOAT_TYPE TType.DOUBLE_TYPE TType.STRING_TYPE TType.TIMESTAMP_TYPE TType.BINARY_TYPE ] const set\u0026lt;TType\u0026gt; COMPLEX_TYPES = [ TType.ARRAY_TYPE TType.MAP_TYPE TType.STRUCT_TYPE TType.UNION_TYPE TType.USER_DEFINED_TYPE ] const set\u0026lt;TType\u0026gt; COLLECTION_TYPES = [ TType.ARRAY_TYPE TType.MAP_TYPE ] const map\u0026lt;TType,string\u0026gt; TYPE_NAMES = { TType.BOOLEAN_TYPE: \u0026quot;BOOLEAN\u0026quot;, TType.TINYINT_TYPE: \u0026quot;TINYINT\u0026quot;, TType.SMALLINT_TYPE: \u0026quot;SMALLINT\u0026quot;, TType.INT_TYPE: \u0026quot;INT\u0026quot;, TType.BIGINT_TYPE: \u0026quot;BIGINT\u0026quot;, TType.FLOAT_TYPE: \u0026quot;FLOAT\u0026quot;, TType.DOUBLE_TYPE: \u0026quot;DOUBLE\u0026quot;, TType.STRING_TYPE: \u0026quot;STRING\u0026quot;, TType.TIMESTAMP_TYPE: \u0026quot;TIMESTAMP\u0026quot;, TType.BINARY_TYPE: \u0026quot;BINARY\u0026quot;, TType.ARRAY_TYPE: \u0026quot;ARRAY\u0026quot;, TType.MAP_TYPE: \u0026quot;MAP\u0026quot;, TType.STRUCT_TYPE: \u0026quot;STRUCT\u0026quot;, TType.UNION_TYPE: \u0026quot;UNIONTYPE\u0026quot; } // Thrift does not support recursively defined types or forward declarations, // which makes it difficult to represent Hive's nested types. // To get around these limitations TTypeDesc employs a type list that maps // integer \u0026quot;pointers\u0026quot; to TTypeEntry objects. The following examples show // how different types are represented using this scheme: // // \u0026quot;INT\u0026quot;: // TTypeDesc { // types = [ // TTypeEntry.primitive_entry { // type = INT_TYPE // } // ] // } // // \u0026quot;ARRAY\u0026lt;INT\u0026gt;\u0026quot;: // TTypeDesc { // types = [ // TTypeEntry.array_entry { // object_type_ptr = 1 // }, // TTypeEntry.primitive_entry { // type = INT_TYPE // } // ] // } // // \u0026quot;MAP\u0026lt;INT,STRING\u0026gt;\u0026quot;: // TTypeDesc { // types = [ // TTypeEntry.map_entry { // key_type_ptr = 1 // value_type_ptr = 2 // }, // TTypeEntry.primitive_entry { // type = INT_TYPE // }, // TTypeEntry.primitive_entry { // type = STRING_TYPE // } // ] // } typedef i32 TTypeEntryPtr // Type entry for a primitive type. struct TPrimitiveTypeEntry { // The primitive type token. This must satisfy the condition // that type is in the PRIMITIVE_TYPES set. 1: required TType type } // Type entry for an ARRAY type. struct TArrayTypeEntry { 1: required TTypeEntryPtr object_type_ptr } // Type entry for a MAP type. struct TMapTypeEntry { 1: required TTypeEntryPtr key_type_ptr 2: required TTypeEntryPtr value_type_ptr } // Type entry for a STRUCT type. struct TStructTypeEntry { 1: required map\u0026lt;string, TTypeEntryPtr\u0026gt; name_to_type_ptr } // Type entry for a UNIONTYPE type. struct TUnionTypeEntry { 1: required map\u0026lt;string, TTypeEntryPtr\u0026gt; name_to_type_ptr } struct TUserDefinedTypeEntry { // The fully qualified name of the class implementing this type. 1: required string typeClassName } // We use a union here since Thrift does not support inheritance. union TTypeEntry { 1: TPrimitiveTypeEntry primitive_entry 2: TArrayTypeEntry array_entry 3: TMapTypeEntry map_entry 4: TStructTypeEntry struct_entry 5: TUnionTypeEntry union_entry 6: TUserDefinedTypeEntry user_defined_type_entry } // Type descriptor for columns. struct TTypeDesc { // The \u0026quot;top\u0026quot; type is always the first element of the list. // If the top type is an ARRAY, MAP, STRUCT, or UNIONTYPE // type, then subsequent elements represent nested types. 1: required list\u0026lt;TTypeEntry\u0026gt; types } // A result set column descriptor. struct TColumnDesc { // The name of the column 1: required string col_name // The type descriptor for this column 2: required TTypeDesc type_desc // The ordinal position of this column in the schema 3: required i32 position 4: optional string comment } // Metadata used to describe the schema (column names, types, comments) // of result sets. struct TTableSchema { 1: required list\u0026lt;TColumnDesc\u0026gt; columns } // A single column value in a result set. // Note that Hive's type system is richer than Thrift's, // so in some cases we have to map multiple Hive types // to the same Thrift type. On the client-side this is // disambiguated by looking at the Schema of the // result set. union TColumnValue { 1: bool bool_val // BOOLEAN 2: byte byte_val // TINYINT 3: i16 i16_val // SMALLINT 4: i32 i32_val // INT 5: i64 i64_val // BIGINT, TIMESTAMP 6: double double_val // FLOAT, DOUBLE 7: string string_val // STRING, LIST, MAP, STRUCT, UNIONTYPE, BINARY } // Represents a row in a rowset. struct TRow { 1: list\u0026lt;TColumnValue\u0026gt; colVals } // Represents a rowset struct TRowSet { // The starting row offset of this rowset. 1: i64 start_row_offset 2: list\u0026lt;TRow\u0026gt; rows } // The return status code contained in each response. enum TStatusCode { SUCCESS, SUCCESS_WITH_INFO, SQL_STILL_EXECUTING, ERROR, INVALID_HANDLE } // The return status of a remote request struct TStatus { 1: required TStatusCode status_code // If status is SUCCESS_WITH_INFO, info_msgs may be populated with // additional diagnostic information. 2: optional list\u0026lt;string\u0026gt; info_msgs // If status is ERROR, then the following fields may be set 3: optional string sql_state // as defined in the ISO/IEF CLI specification 4: optional i32 error_code // internal error code 5: optional string error_message } // The state of an operation (i.e. a query or other // asynchronous operation that generates a result set) // on the server. enum TOperationState { // The operation is running. In this state the result // set is not available. RUNNING, // The operation has completed. When an operation is in // this state it's result set may be fetched. FINISHED, // The operation was canceled by a client CANCELED, // The operation failed due to an error ERROR } // A string identifier. This is interpreted literally. typedef string TIdentifier // A search pattern. // // Valid search pattern characters: // '_': Any single character. // '%': Any sequence of zero or more characters. // '\\': Escape character used to include special characters, // e.g. '_', '%', '\\'. If a '\\' precedes a non-special // character it has no special meaning and is interpreted // literally. typedef string TPattern // A search pattern or identifier. Used as input // parameter for many of the catalog functions. union TPatternOrIdentifier { 1: TIdentifier identifier 2: TPattern pattern } struct THandleIdentifier { // 16 byte globally unique identifier // This is the public ID of the handle and // can be used for reporting. 1: binary guid, // 16 byte secret generated by the server // and used to verify that the handle is not // being hijacked by another user. 2: binary secret, } // Client-side handle to persistent // session information on the server-side. struct TSessionHandle { 1: required THandleIdentifier sess_id } // The subtype of an OperationHandle. enum TOperationType { EXECUTE_STATEMENT, GET_TYPE_INFO, GET_TABLES, GET_COLUMNS, GET_FUNCTIONS, } // Client-side reference to a task running // asynchronously on the server. struct TOperationHandle { 1: required THandleIdentifier op_id 2: required TOperationType op_type } // OpenSession() // // Open a session (connection) on the server against // which operations may be executed. struct TOpenSessionReq { // The version of the HiveServer2 protocol that the client is using. 1: required ProtocolVersion client_protocol = ProtocolVersion.HIVE_SERVER2_PROTOCOL_V1 // Username and password for authentication. // Depending on the authentication scheme being used, // this information may instead be provided by a lower // protocol layer, in which case these fields may be // left unset. 2: optional string username 3: optional string password // Configuration overlay which is applied when the session is // first created. 4: optional map\u0026lt;string, string\u0026gt; configuration } struct TOpenSessionResp { 1: required TStatus status // The protocol version that the server is using. 2: ProtocolVersion server_protocol = ProtocolVersion.HIVE_SERVER2_PROTOCOL_V1 // Session Handle 3: TSessionHandle session_handle // The configuration settings for this session. 4: map\u0026lt;string, string\u0026gt; configuration } // CloseSession() // // Closes the specified session and frees any resources // currently allocated to that session. Any open // operations in that session will be canceled. struct TCloseSessionReq { 1: required TSessionHandle session_handle } struct TCloseSessionResp { 1: required TStatus status } // GetInfo() // // This function is based on ODBC's SQLGetInfo() function. // The function returns general information about the data source // using the same keys as ODBC. struct TGetInfoReq { // The sesssion to run this request against 1: required TSessionHandle session_handle // List of keys for which info is requested. If unset or empty, // the response message will contain the values of all Info // properties. 2: optional list\u0026lt;string\u0026gt; info_keys } struct TGetInfoResp { 1: required TStatus status // key/value pairs representing Info values 2: map\u0026lt;string, string\u0026gt; info } // ExecuteStatement() // // Execute a statement. // The returned OperationHandle can be used to check on the // status of the statement, and to fetch results once the // statement has finished executing. struct TExecuteStatementReq { // The session to exexcute the statement against 1: required TSessionHandle session // The statement to be executed (DML, DDL, SET, etc) 2: required string statement // Configuration properties that are overlayed on top of the // the existing session configuration before this statement // is executed. These properties apply to this statement // only and will not affect the subsequent state of the Session. 3: map\u0026lt;string, string\u0026gt; conf_overlay } struct TExecuteStatementResp { 1: required TStatus status 2: TOperationHandle op_handle } // GetTypeInfo() // // Get information about types supported by the HiveServer instance. // The information is returned as a result set which can be fetched // using the OperationHandle provided in the response. // // Refer to the documentation for ODBC's SQLGetTypeInfo function for // for the format of the result set. struct TGetTypeInfoReq { // The session to run this request against. 1: required TSessionHandle session } struct TGetTypeInfoResp { 1: required TStatus status 2: TOperationHandle op_handle } // GetTables() // // Returns a list of tables with catalog, schema, and table // type information. The information is returned as a result // set which can be fetched using the OperationHandle // provided in the response. // // Result Set Columns: // // col1 // name: TABLE_CAT // type: STRING // desc: Catalog name. NULL if not applicable. // // col2 // name: TABLE_SCHEM // type: STRING // desc: Schema name. // // col3 // name: TABLE_NAME // type: STRING // desc: Table name. // // col4 // name: TABLE_TYPE // type: STRING // desc: The table type, e.g. \u0026quot;TABLE\u0026quot;, \u0026quot;VIEW\u0026quot;, etc. // // col5 // name: REMARKS // type: STRING // desc: Comments about the table // struct TGetTablesReq { // Session to run this request against 1: required TSessionHandle session // Name of the catalog or a search pattern. 2: optional TPatternOrIdentifier catalog_name // Name of the schema or a search pattern. 3: required TPatternOrIdentifier schema_name // Name of the table or a search pattern. 4: required TPatternOrIdentifier table_name // List of table types to match // e.g. \u0026quot;TABLE\u0026quot;, \u0026quot;VIEW\u0026quot;, \u0026quot;SYSTEM TABLE\u0026quot;, \u0026quot;GLOBAL TEMPORARY\u0026quot;, // \u0026quot;LOCAL TEMPORARY\u0026quot;, \u0026quot;ALIAS\u0026quot;, \u0026quot;SYNONYM\u0026quot;, etc. 5: list\u0026lt;string\u0026gt; table_types } struct TGetTablesResp { 1: required TStatus status 2: TOperationHandle op_handle } // GetColumns() // // Returns a list of columns in the specified tables. // The information is returned as a result set which can be fetched // using the OperationHandle provided in the response. // // Result Set Columns are the same as those for the ODBC SQLColumns // function. // struct TGetColumnsReq { // Session to run this request against 1: required TSessionHandle session // Name of the catalog. Must not contain a search pattern. 2: optional TIdentifier catalog_name // Schema name or search pattern 3: required TPatternOrIdentifier schema_name // Table name or search pattern 4: required TPatternOrIdentifier table_name // Column name or search pattern 5: required TPatternOrIdentifier column_name } struct TGetColumnsResp { 1: required TStatus status 2: optional TOperationHandle op_handle } // GetFunctions() // // Returns a list of functions supported by the data source. // // Result Set Columns: // // col1 // name: FUNCTION_NAME // type: STRING // desc: The name of the function. // // col2 // name: DESCRIPTION // type: STRING // desc: A description of the function. // // col3 // name: FUNCTION_TYPE // type: STRING // desc: The function type: \u0026quot;UDF\u0026quot;, \u0026quot;UDAF\u0026quot;, or \u0026quot;UDTF\u0026quot;. // // col4 // name: FUNCTION_CLASS // type: STRING // desc: The fully qualified name of the class // that implements the specified function. // struct TGetFunctionsReq { // Session to run this request against 1: required TSessionHandle session } struct TGetFunctionsResp { 1: required TStatus status 2: optional TOperationHandle op_handle } // GetOperationStatus() // // Get the status of an operation running on the server. struct TGetOperationStatusReq { // Session to run this request against 1: required TOperationHandle op_handle } struct TGetOperationStatusResp { 1: required TStatus status 2: TOperationState op_state } // CancelOperation() // // Cancels processing on the specified operation handle and // frees any resources which were allocated. struct TCancelOperationReq { // Operation to cancel 1: required TOperationHandle op_handle } struct TCancelOperationResp { 1: required TStatus status } // GetQueryPlan() // // Given an OperationHandle corresponding to an ExecuteStatement // request, this function returns the queryplan for that // statement annotated with counter information. struct TGetQueryPlanReq { 1: required TOperationHandle op_handle } struct TGetQueryPlanResp { 1: required TStatus status 2: optional queryplan.QueryPlan query_plan } // CloseOperation() // // Given an operation in the FINISHED, CANCELED, // or ERROR states, CloseOperation() will free // all of the resources which were allocated on // the server to service the operation. struct TCloseOperationReq { 1: required TOperationHandle op_handle } struct TCloseOperationResp { 1: required TStatus status } // GetResultSetMetadata() // // Retrieves schema information for the specified operation struct TGetResultSetMetadataReq { // Operation for which to fetch result set schema information 1: required TOperationHandle op_handle } struct TGetResultSetMetadataResp { 1: required TStatus status 2: optional TTableSchema schema } enum TFetchOrientation { // Get the next rowset. The fetch offset is ignored. FETCH_NEXT, // Get the previous rowset. The fetch offset is ignored. // NOT SUPPORTED FETCH_PRIOR, // Return the rowset at the given fetch offset relative // to the curren rowset. // NOT SUPPORTED FETCH_RELATIVE, // Return the rowset at the specified fetch offset. // NOT SUPPORTED FETCH_ABSOLUTE, // Get the first rowset in the result set. FETCH_FIRST, // Get the last rowset in the result set. // NOT SUPPORTED FETCH_LAST } // FetchResults() // // Fetch rows from the server corresponding to // a particular OperationHandle. struct TFetchResultsReq { // Operation from which to fetch results. 1: required TOperationHandle op_handle // The fetch orientation. For V1 this must be either // FETCH_NEXT or FETCH_FIRST. Defaults to FETCH_NEXT. 2: TFetchOrientation orientation = TFetchOrientation.FETCH_NEXT // Max number of rows that should be returned in // the rowset. 3: i64 max_rows } struct TFetchResultsResp { 1: required TStatus status // TRUE if there are more rows left to fetch from the server. 2: bool has_more_rows // The rowset. This is optional so that we have the // option in the future of adding alternate formats for // representing result set data, e.g. delimited strings, // binary encoded, etc. 3: optional TRowSet results } service TSQLService { TOpenSessionResp OpenSession(1:TOpenSessionReq req); TCloseSessionResp CloseSession(1:TCloseSessionReq req); TGetInfoResp GetInfo(1:TGetInfoReq req); TExecuteStatementResp ExecuteStatement(1:TExecuteStatementReq req); TGetTypeInfoResp GetTypeInfo(1:TGetTypeInfoReq req); TGetTablesResp GetTables(1:TGetTablesReq req); TGetColumnsResp GetColumns(1:TGetColumnsReq req); TGetFunctionsResp GetFunctions(1:TGetFunctionsReq req); TGetOperationStatusResp GetOperationStatus(1:TGetOperationStatusReq req); TCancelOperationResp CancelOperation(1:TCancelOperationReq req); TGetQueryPlanResp GetQueryPlan(1:TGetQueryPlanReq req); TGetResultSetMetadataResp GetResultSetMetadata(1:TGetResultSetMetadataReq req); TFetchResultsResp FetchResults(1:TFetchResultsReq req); } ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/hiveserver2-thrift-api_27843687/","tags":null,"title":"Apache Hive : HiveServer2 Thrift API"},{"categories":null,"contents":"Apache Hive : Home  Apache Hive Hive Documentation  General Information about Hive User Documentation Administrator Documentation HCatalog and WebHCat Documentation Resources for Contributors   Hive Versions and Branches  Apache Hive The Apache Hive™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax.\nBuilt on top of Apache Hadoop™, Hive provides the following features:\n Tools to enable easy access to data via SQL, thus enabling data warehousing tasks such as extract/transform/load (ETL), reporting, and data analysis. A mechanism to impose structure on a variety of data formats Access to files stored either directly in Apache HDFS™**** or in other data storage systems such as Apache HBase™**** Query execution via Apache Tez™, Apache Spark™, or MapReduce Procedural language with HPL-SQL Sub-second query retrieval via Hive LLAP, Apache YARN and Apache Slider.  Hive provides standard SQL functionality, including many of the later SQL:2003, SQL:2011, and SQL:2016 features for analytics.\nHive\u0026rsquo;s SQL can also be extended with user code via user defined functions (UDFs), user defined aggregates (UDAFs), and user defined table functions (UDTFs).\nThere is not a single \u0026ldquo;Hive format\u0026rdquo; in which data must be stored. Hive comes with built in connectors for comma and tab-separated values (CSV/TSV) text files, Apache Parquet™, Apache ORC™, and other formats. Users can extend Hive with connectors for other formats. Please see File Formats and Hive SerDe in the Developer Guide for details.\nHive is not designed for online transaction processing (OLTP) workloads. It is best used for traditional data warehousing tasks.\nHive is designed to maximize scalability (scale out with more machines added dynamically to the Hadoop cluster), performance, extensibility, fault-tolerance, and loose-coupling with its input formats.\nComponents of Hive include HCatalog and WebHCat.\n HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — including Pig and MapReduce — to more easily read and write data on the grid. WebHCat provides a service that you can use to run Hadoop MapReduce (or YARN), Pig, Hive jobs. You can also perform Hive metadata operations using an HTTP (REST style) interface.  Hive Documentation The links below provide access to the Apache Hive wiki documents. This list is not complete, but you can navigate through these wiki pages to find additional documents. For more information, please see the official Hive website.\nGeneral Information about Hive  Getting Started Books about Hive Presentations and Papers about Hive Sites and Applications Powered by Hive Related Projects FAQ Hive Users Mailing List Hive IRC Channel: #hive on irc.freenode.net  User Documentation  Hive Tutorial Hive SQL Language Manual: Commands, CLIs, Data Types,\nDDL (create/drop/alter/truncate/show/describe), Statistics (analyze), Indexes, Archiving,\nDML (load/insert/update/delete/merge, import/export, explain plan),\nQueries (select), Operators and UDFs, Locks, Authorization File Formats and Compression: RCFile, Avro, ORC, Parquet; Compression, LZO Procedural Language: Hive HPL/SQL Hive Configuration Properties Hive Clients  Hive Client (JDBC, ODBC, Thrift) HiveServer2: Overview, HiveServer2 Client and Beeline, Hive Metrics   Hive SerDes: Avro SerDe, Parquet SerDe, CSV SerDe, JSON SerDe Hive Accumulo Integration Hive HBase Integration Druid Integration Kudu Integration Hive Transactions, Streaming Data Ingest, and Streaming Mutation API Hive Counters Using TiDB as the Hive Metastore database StarRocks Integration  Administrator Documentation  Installing Hive Configuring Hive Setting Up Metastore  Hive Schema Tool   Setting Up Hive Server (JDBC, ODBC, Thrift, HiveServer2) Hive Replication Hive on Amazon Web Services Hive on Amazon Elastic MapReduce Hive on Spark: Getting Started  HCatalog and WebHCat Documentation  HCatalog WebHCat (Templeton)  Resources for Contributors  How to Contribute Hive Contributors Meetings Hive Developer Docs  Hive Developer Guide (code organization, compile and run Hive, unit tests, debug, pluggable interfaces) Hive Developer FAQ (move files, build Hive, test Hive Plugin Developer Kit Writing UDTFs Hive APIs Overview   Hive Testing Docs  FAQ: Testing Developer Guide: Unit Tests Unit Testing Hive SQL Running Yetus MetaStore API Tests   Hive Performance Hive Architecture Overview Hive Design Docs: Completed; In Progress; Proposed; Incomplete, Abandoned, Other Roadmap/Call to Add More Features Becoming a Committer How to Commit How to Release  Hive Versions and Branches Recent versions of Hive are available on the Downloads page of the Hive website. For each version, the page provides the release date and a link to the change log. If you want a change log for an earlier version (or a development branch), use the Configure Release Notes page.\nThe Apache Hive JIRA keeps track of changes to Hive code, documentation, infrastructure, etc. The version number or branch for each resolved JIRA issue is shown in the \u0026ldquo;Fix Version/s\u0026rdquo; field in the Details section at the top of the issue page. For example, HIVE-5107 has a fix version of 0.13.0.\nSometimes a version number changes before the release. When that happens, the original number might still be found in JIRA, wiki, and mailing list discussions. For example:\n   Release Number Original Number     1.0.0 0.14.1   1.1.0 0.15.0   2.3.0 2.2.0    More information about Hive branches is available in How to Contribute: Understanding Hive Branches.\nApache Hive, Apache Hadoop, Apache HBase, Apache HDFS, Apache, the Apache feather logo, and the Apache Hive project logo are trademarks of The Apache Software Foundation.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/home_27362069/","tags":null,"title":"Apache Hive : Home"},{"categories":null,"contents":"Apache Hive : Howl This page collects some pointers to resources about Howl (an effort to create a metastore for all of Hadoop) and how its first incarnation is being built by reusing and extending Hive\u0026rsquo;s metastore and CLI.\n Howl wiki Yahoo group for Howl developers (including mailing list archive) Howl source code at github Howl CLI functional spec Original plans for Owl (predecessor to Howl)  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/howl_27362109/","tags":null,"title":"Apache Hive : Howl"},{"categories":null,"contents":"Apache Hive : HowToCommit Guide for Hive Committers  Guide for Hive Committers  New committers Review Reject PreCommit runs, and committing patches Commit  Committing Documentation   Backporting commits to previous branches Dialog    This page contains guidelines for committers of the Apache Hive project. (If you\u0026rsquo;re currently a contributor, and are interested in how we add new committers, read BecomingACommitter)\nNew committers New committers are encouraged to first read Apache\u0026rsquo;s generic committer documentation:\n Apache New Committer Guide ASF Project Security for committers Apache Committer FAQ  The first act of a new core committer is typically to add their name to the credits page. This requires changing the source in https://github.com/apache/hive-site/blob/main/content/community/people.md\nCommitters are strongly encouraged to subscribe to the security@hive.apache.org list with their Apache email and help addressing security vulnerability reports.\nReview Hive committers should, as often as possible, attempt to review Pull Requests submitted by others. Ideally every submitted PR will get reviewed by a committer within a few days. If a committer reviews a PR they\u0026rsquo;ve not authored, and believe it to be of sufficient quality, then they can merge the PR, otherwise the PR should be cancelled with a clear explanation for why it was rejected.\nThe list of open Pull Requests can be found here: Hive Open Pull Requests. This is ordered by time of creating. Committers should scan the list from bottom-to-top, looking for Pull Requests that they feel qualified to review and possibly merge.\nHive committers can not +1/Approve their own Pull Requests, i.e. you are allowed to commit/merge your own changes only if the Pull Request first receives a +1 vote from another committer. In the past this rule has typically been ignored when making small changes to the website (e.g. adding a new committer to the credits page), but you should follow the standard process for anything else.\nReject Pull Requests should be rejected which do not adhere to the guidelines in HowToContribute. Committers should always be polite to contributors and try to instruct and encourage them to contribute better Pull Requests. If a committer wishes to improve an unacceptable change, then he/she drop review comments and ask the contributor to update.\nPreCommit runs, and committing patches  Run Pre-Commit tests on a Pull Request before committing. If the test run is clean (and there\u0026rsquo;s a +1 from a committer), the Pull Request can be merged/committed. Test runs may not be clean due to issues in the PR itself, or due to flaky tests. These issues must be fixed and PR should not be committed until the run is clean.  If a commit introduces new test failures, the preferred process is to revert the patch, rather than opening a new JIRA to fix the new failures.\nCommit When you commit/merge a Pull Request, please:\n Ensure that the Pull Request has a +1 vote, and that 24 hours have elapsed since the first +1 vote was cast on the Pull Request. Note that this rule appears in the Hive Bylaws. Do not ignore it. Include the Jira issue id in the commit message, along with a short description of the change. Be sure to get the issue id right, in order to have proper links between Jira and GitHub. Append the Pull Request id, in the commit subject to track the relation between the commit and the respective Pull Request. Add Co-authored-by: Ayush Saxena \u0026lt;ayushsaxena@apache.org\u0026gt; in the body of the message if multiple people contributed to the Pull Request. Resolve the JIRA issue as fixed, and set the \u0026ldquo;Fix Version\u0026rdquo; to the release in which the change will appear. If a patch is backported to a point release (such as 1.0.2) then multiple fix versions should be set so that the automated release notes can list the Jira issue for the point release as well as the primary release. Thank the contributor(s), the reviewers, and the reporter of the issue (if different from the contributor). It is easier to thank the people in GitHub by mentioning their GitHub ids under the respective Pull Request.  Below you can find a sample commit message that adheres to the guidelines outlined here.\nHIVE-27424: Display dependency:tree in GitHub actions (#5756) Committing Documentation Hive\u0026rsquo;s official documentation is hosted at Github-Hive-Site. To commit major documentation changes you must raise a Pull Request to the hive-site repo.\nChanges committed to the hive site repo will automatically get published on: https://hive.apache.org/\nBackporting commits to previous branches If a patch needs to be backported to previous branches, follow these steps.\n Raise a Pull Request directed toward the target branch with the actual Jira Id/message \u0026amp; commit id. If the build is green, the PR can be merged to the desired branch  Dialog Committers/Contributors can hang out in the #hive channel in Apache Slack workspace for real-time discussions. However any substantive discussion (as with any off-list project-related discussion) should be re-iterated in Jira or on the developer list.\nNote: Committers or individuals with Apache Id can directly join the #hive slack channel on Apache Workspace, any other individual if interested should drop a mail to hive dev mailing list with his email id and any existing member of the #hive apache channel should be able to send him the invite to join the group.\nInstructions to add folks to ASF hive channel: https://infra.apache.org/slack.html\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/howtocommit_27362108/","tags":null,"title":"Apache Hive : HowToCommit"},{"categories":null,"contents":"Apache Hive : HowToContribute How to Contribute to Apache Hive This page describes the mechanics of how to contribute software to Apache Hive. For ideas about what you might contribute, please see open tickets in Jira.\n Getting the Source Code Becoming a Contributor Making Changes  Coding Conventions Understanding Maven Understanding Hive Branches Hadoop Dependencies Unit Tests Add a Unit Test Submitting a PR Fetching a PR from Github   Contributing Your Work JIRA  Guidelines   Generating Thrift Code See Also    Getting the Source Code First of all, you need the Hive source code.\nGet the source code on your local drive using git. See Understanding Hive Branches below to understand which branch you should be using.\ngit clone https://github.com/apache/hive Setting Up Eclipse Development Environment (Optional)\nThis is an optional step. Eclipse has a lot of advanced features for Java development, and it makes the life much easier for Hive developers as well.\nHow do I import into eclipse?\nBecoming a Contributor This checklist tells you how to create accounts and obtain permissions needed by Hive contributors. See the Hive website for additional information.\n Request an Apache Software Foundation JIRA account, if you do not already have one.  The ASF JIRA system dashboard is here. The Hive JIRA is here.   To review patches check the open pull requests on GitHub To contribute to the Hive wiki, follow the instructions in About This Wiki. To edit the Hive website, follow the instructions in How to edit the website. Join the Hive mailing lists to receive email about issues and discussions.  Making Changes If you\u0026rsquo;re a newcomer, feel free to contribute by working on a newbie task.\nBefore you start, send a message to the Hive developer mailing list, or file a bug report in JIRA. Describe your proposed changes and check that they fit in with what others are doing and have planned for the project. Be patient, it may take folks a while to understand your requirements.\nModify the source code and add some features using your favorite IDE.\nCoding Conventions Please take care about the following points.\n All public classes and methods should have informative Javadoc comments.  Do not use @author tags.   Code should be formatted according to Sun\u0026rsquo;s conventions, with two exceptions:  Indent two (2) spaces per level, not four (4). Line length limit is 120 chars, instead of 80 chars.    An Eclipse formatter is provided in the dev-support folder – this can be used with both Eclipse and Intellij. Please consider importing this before editing the source code.\n   For Eclipse: - Go to Preferences -\u0026gt; Java -\u0026gt; Code Style -\u0026gt; Formatter; Import eclipse-styles.xml; Apply. - In addition update save actions: Java -\u0026gt; Editor -\u0026gt; Save Actions; Check the following: Perform the following actions on save; Format Source Code; Format edited lines.  For Intellij:  Go to Settings -\u0026gt; Editor -\u0026gt; Code Style -\u0026gt; Java -\u0026gt; Scheme; Click manage; Import eclipse-styles.xml; Apply.       Contributions should not introduce new Checkstyle violations.  Check for new Checkstyle violations by running mvn checkstyle:checkstyle-aggregate, and then inspect the results in the target/site directory. It is possible to run the checks for a specific module, if the mvn command is issued in the root directory of the module. If you use Eclipse you should install the eclipse-cs Checkstyle plugin. This plugin highlights violations in your code and is also able to automatically correct some types of violations.   Contributions should pass existing unit tests. New unit tests should be provided to demonstrate bugs and fixes. JUnit is our test framework:  You should create test classes for junit4, whose class name must start with a \u0026lsquo;Test\u0026rsquo; prefix. You can run all the unit tests with the command mvn test, or you can run a specific unit test with the command mvn test -Dtest=\u0026lt;class name without package prefix\u0026gt; (for example: mvn test -Dtest=TestFileSystem). After uploading your patch, it might worthwhile to check if your new test has been executed in the precommit job.    Understanding Maven Hive is a multi-module Maven project. If you are new to Maven, the articles below may be of interest:\n Maven in Five Minutes Maven getting started Maven by example - a multi-module project  Additionally, Hive actually has two projects, \u0026ldquo;core\u0026rdquo; and \u0026ldquo;itests\u0026rdquo;. The reason that itests is not connected to the core reactor is that itests requires the packages to be built.\nThe actual Maven commands you will need are discussed on the HiveDeveloperFAQ page.\nUnderstanding Hive Branches Hive has a few \u0026ldquo;main lines\u0026rdquo;, master and branch-X.\nAll new feature work and bug fixes in Hive are contributed to the master branch. Releases are done from branch-X. Major versions like 2.x versions are not necessarily backwards compatible with 1.x versions.backwards compatibility will be accepted on branch-1.\nIn addition to these main lines Hive has two types of branches, release branches and feature branches.\nRelease branches are made from branch-1 (for 1.x) or master (for 2.x) when the community is preparing a Hive release. Release branches match the number of the release (e.g., branch-1.2 for Hive 1.2). For patch releases the branch is made from the existing release branch (to avoid picking up new features from the main line). For example, if a 1.2.1 release was being made branch-1.2.1 would be made from the tip of branch-1.2. Once a release branch has been made, inclusion of additional patches on that branch is at the discretion of the release manager. After a release has been made from a branch, additional bug fixes can still be applied to that branch in anticipation of the next patch release. Any bug fix applied to a release branch must first be applied to master (and branch-1 if applicable).\nFeature branches are used to develop new features without destabilizing the rest of Hive. The intent of a feature branch is that it will be merged back into master once the feature has stabilized.\nFor general information about Hive branches, see Hive Versions and Branches.\nHadoop Dependencies Hadoop dependencies are handled differently in master and branch-1.\nbranch-1 In branch-1 both Hadoop 1.x and 2.x are supported. The Hive build downloads a number of different Hadoop versions via Maven in order to compile \u0026ldquo;shims\u0026rdquo; which allow for compatibility with these Hadoop versions. However, the rest of Hive is only built and tested against a single Hadoop version.\nThe Maven build has two profiles, hadoop-1 for Hadoop 1.x and hadoop-2 for Hadoop 2.x. When building, you must specify which profile you wish to use via Maven\u0026rsquo;s -P command line option (see How to build all source).\nbranch-2 Hadoop 1.x is no longer supported in Hive\u0026rsquo;s master branch. There is no need to specify a profile for most Maven commands, as Hadoop 2.x will always be chosen.\nHadoop Version Information\nOn this page we assume you are building from the master branch and do not include the profile in the example Maven commands. If you are building on branch-1 you will need to select the appropriate profile for the version of Hadoop you are building against.\nUnit Tests When submitting a patch it\u0026rsquo;s highly recommended you execute tests locally which you believe will be impacted in addition to any new tests. The full test suite can be executed by hive-precommit on Jenkins. Hive Developer FAQ describes how to execute a specific set of tests.\nmvn clean install -DskipTests mvn test -Dtest=SomeTest Unit tests take a long time (several hours) to run sequentially even on a very fast machine.\nAdd a Unit Test There are two kinds of unit tests that can be added: those that test an entire component of Hive, and those that run a query to test a feature.\nJava Unit Test To test a particular component of Hive:\n Add a new class (name must start with Test) in the component\u0026rsquo;s */src/test/java directory. To test only the new testcase, run mvn test -Dtest=TestAbc (where TestAbc is the name of the new class), which will be faster than mvn test which tests all testcases.  Query File Test(qtest) You can run end-to-end integration tests using LLAP, Tez, Iceberg, etc.\nSubmitting a PR There are many excellent howtos about how to submit pullrequests for github projects. The following is one way to approach it:\nSetting up a repo with 2 remotes; I would recommend to use the github user as the remote name - as it may make things easier if you need to add someone else\u0026rsquo;s repo as well.\n# clone the apache/hive repo from github git clone --origin apache https://github.com/apache/hive cd hive # add your own fork as a remote git remote add GITHUB_USER git@github.com:GITHUB_USER/hive You will need a separate branch to make your changes; you need to this for every PR you are doing.\n# fetch all changes - so you will create your feature branch on top of the current master git fetch --all # create a feature branch This branch name can be anything - including the ticket id may help later on identifying the branch. git branch HIVE-9999-something apache/master git checkout HIVE-9999-something # push your feature branch to your github fork - and set that branch as upstream to this branch git push GITHUB_USER -u HEAD Make your change\n# make your changes; you should include the ticketid + message in the first commit message git commit -m 'HIVE-9999: Something' -a # a simple push will deliver your changes to the github branch git push If you think your changes are ready to be tested and reviewed - you could open a PR request on the https://github.com/apache/hive page.\nIf you need to make changes you just need to push further changes to the branch.\nPlease do not:\n reformat code unrelated to the issue being fixed: formatting changes should be separate patches/commits; comment out code that is now obsolete: just remove it; insert comments around each change, marking the change: folks can use git to figure out what\u0026rsquo;s changed and by whom; make things public that are not required by end-users.  Please do:\n try to adhere to the coding style of files you edit; comment code whose function or rationale is not obvious; add one or more unit tests (see Add a Unit Test above); update documentation (such as Javadocs including package.html files and this wiki).  Fetching a PR from Github you could do that using:\ngit fetch origin pull/ID/head:BRANCHNAME Suppose you want to pull the changes of PR-1234 into a local branch named \u0026ldquo;radiator\u0026rdquo;\ngit fetch origin pull/1234/head:radiator Contributing Your Work You should open a JIRA ticket about the issue you are about to fix.\nUpload your changes to your github fork and open a PR against the hive repo.\nIf your patch creates an incompatibility with the latest major release, then you must set the Incompatible change flag on the issue\u0026rsquo;s JIRA and fill in the Release Note field with an explanation of the impact of the incompatibility and the necessary steps users must take.\nIf your patch implements a major feature or improvement, then you must fill in the Release Note field on the issue\u0026rsquo;s JIRA with an explanation of the feature that will be comprehensible by the end user.\nThe Release Note field can also document changes in the user interface (such as new HiveQL syntax or configuration parameters) prior to inclusion in the wiki documentation.\nA committer should evaluate the patch within a few days and either: commit it; or reject it with an explanation.\nPlease be patient. Committers are busy people too. If no one responds to your patch after a few days, please make friendly reminders. Please incorporate others' suggestions into your patch if you think they\u0026rsquo;re reasonable. Finally, remember that even a patch that is not committed is useful to the community.\nShould your patch receive a \u0026ldquo;-1\u0026rdquo; select Resume Progress on the issue\u0026rsquo;s JIRA, upload a new patch with necessary fixes, and then select the Submit Patch link again.\nCommitters: for non-trivial changes, it is best to get another committer to review your patches before commit. Use the Submit Patch link like other contributors, and then wait for a \u0026ldquo;+1\u0026rdquo; from another committer before committing. Please also try to frequently review things in the patch queue.\nJIRA Hive uses JIRA for issues/case management. You must have a JIRA account in order to log cases and issues.\nRequests for the creation of new accounts can be submitted via the following form: https://selfserve.apache.org/jira-account.html\nGuidelines Please comment on issues in JIRA, making your concerns known. Please also vote for issues that are a high priority for you.\nPlease refrain from editing descriptions and comments if possible, as edits spam the mailing list and clutter JIRA\u0026rsquo;s \u0026ldquo;All\u0026rdquo; display, which is otherwise very useful. Instead, preview descriptions and comments using the preview button (icon below the comment box) before posting them.\nKeep descriptions brief and save more elaborate proposals for comments, since descriptions are included in JIRA\u0026rsquo;s automatically sent messages. If you change your mind, note this in a new comment, rather than editing an older comment. The issue should preserve this history of the discussion.\nTo open a JIRA issue, click the Create button on the top line of the Hive summary page or any Hive JIRA issue.\nPlease leave Fix Version/s empty when creating the issue – it should not be tagged until an issue is closed, and then, it is tagged by the committer closing it to indicate the earliest version(s) the fix went into. Instead of Fix Version/s, use Target Version/s to request which versions the new issue\u0026rsquo;s patch should go into. (Target Version/s was added to the Create Issue form in November 2015. You can add target versions to issues created before that with the Edit button, which is in the upper left corner.)\nConsider using bi-directional links when referring to other tickets. It is very common and convenient to refer to other tickets by adding the HIVE-XXXXX pattern in summary, description, and comments. The pattern allows someone to navigate quickly to an older JIRA from the current one but not the other way around. Ideally, along with the mention (HIVE-XXXXX) pattern, it helps to add an explicit link (relates to, causes, depends upon, etc.) so that the relationship between tickets is visible from both ends.\nAdd the \u0026ldquo;backward-incompatible\u0026rdquo; label to tickets changing the behavior of some component or introduce modifications to public APIs. There are various other labels available for similar purposes but this is the most widely used across projects so it is better to stick to it to keep things uniform.\nWhen in doubt about how to fill in the Create Issue form, take a look at what was done for other issues. Here are several Hive JIRA issues that you can use as examples:\n bug: HIVE-8485, HIVE-8600, HIVE-9438, HIVE-11174 new feature: HIVE-6806,HIVE-7088,HIVE-11103 improvement: HIVE-7685,HIVE-9858, HIVE-10165 test: HIVE-8601, HIVE-10637 wish: HIVE-4563, HIVE-10427 task: HIVE-7111, HIVE-7789  Many examples of uncommitted issues are available in the \u0026ldquo;Added recently\u0026rdquo; list on the issues panel.\nGenerating Thrift Code Some portions of the Hive code are generated by Thrift. For most Hive changes, you don\u0026rsquo;t need to worry about this, but if you modify any of the Thrift IDL files (e.g., standalone-metastore/metastore-common/src/main/thrift/hive_metastore.thrift and service-rpc/if/TCLIService.thrift), then you\u0026rsquo;ll also need to regenerate these files and submit their updated versions as part of your patch.\nHere are the steps relevant to hive_metastore.thrift:\n  Don\u0026rsquo;t make any changes to hive_metastore.thrift until instructed below.\n  Use the approved version of Thrift. This is currently thrift-0.14.1, which you can obtain from http://thrift.apache.org/.\n For Mac via Homebrew (since the version we need is not available by default):  brew tap-new $USER/local-tap brew extract --version='0.14.1' thrift $USER/local-tap brew install thrift@0.14.1 mkdir -p /usr/local/share/fb303/if cp /usr/local/Cellar/thrift@0.14.1/0.14.1/share/fb303/if/fb303.thrift /usr/local/share/fb303/if `` 2. For Mac, building from sources:\nwget http://archive.apache.org/dist/thrift/0.14.1/thrift-0.14.1.tar.gz tar xzf thrift-0.14.1.tar.gz brew install libtool brew install automake #If configure fails with \u0026quot;syntax error near unexpected token `QT5\u0026quot;, then run \u0026quot;brew install pkg-config\u0026quot; ./bootstrap.sh sudo ./configure --with-openssl=/usr/local/Cellar/openssl@1.1/1.1.1j --without-erlang --without-nodejs --without-python --without-py3 --without-perl --without-php --without-php_extension --without-ruby --without-haskell --without-go --without-swift --without-dotnetcore --without-qt5 brew install openssl sudo ln -s /usr/local/opt/openssl/include/openssl/ /usr/local/include/ sudo make sudo make install mkdir -p /usr/local/share/fb303/if cp path/to/thrift-0.14.1/contrib/fb303/if/fb303.thrift /usr/local/share/fb303/if/fb303.thrift # or alternatively the following command curl -o /usr/local/share/fb303/if/fb303.thrift https://raw.githubusercontent.com/apache/thrift/master/contrib/fb303/if/fb303.thrift `` 3. For Linux:\ncd /path/to/thrift-0.14.1 /configure -without-erlang --without-nodejs --without-python --without-py3 --without-perl --without-php --without-php_extension --without-ruby --without-haskell --without-go --without-swift --without-dotnetcore --without-qt5 sudo make sudo make install sudo mkdir -p /usr/local/share/fb303/if sudo cp /path/to/thrift-0.14.1/contrib/fb303/if/fb303.thrift /usr/local/share/fb303/if/fb303.thrift ``\n  Before proceeding, verify that which thrift returns the build of Thrift you just installed (typically /usr/local/bin on Linux); if not, edit your PATH and repeat the verification. Also verify that the command \u0026lsquo;thrift -version\u0026rsquo; returns the expected version number of Thrift.\n  Now you can run the Maven \u0026lsquo;thriftif\u0026rsquo; profile to generate the Thrift code:\n cd /path/to/hive/    mvn clean install -Pthriftif -DskipTests -Dthrift.home=/usr/local\n  Verify that the code generation was a no-op, which should be the case if you have the correct Thrift version and everyone has been following these instructions. You may use git status for the same. If you can\u0026rsquo;t figure out what is going wrong, ask for help from a committer.\n  Now make your changes to hive_metastore.thrift, and then run the compiler again, from /path/to/hive/\u0026lt;hive_metastore.thrift\u0026rsquo;s module\u0026gt;:\n    mvn clean install -Pthriftif -DskipTests -Dthrift.home=/usr/local\n  Now use git status and git diff to verify that the regenerated code corresponds only to the changes you made to hive_metastore.thrift. You may also need git add if new files were generated (and or git rm if some files are now obsoleted).\n  cd /path/to/hive\n  mvn clean package -DskiptTests (at the time of writing also \u0026quot;-Dmaven.javadoc.skip\u0026quot; is needed)\n  Verify that Hive is still working correctly with both embedded and remote metastore configurations.\n  Stay Involved\nContributors should join the Hive mailing lists. In particular the dev list (to join discussions of changes) and the user list (to help others).\nSee Also  Apache contributor documentation Apache voting documentation GitHub repository of this website  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/howtocontribute_27362107/","tags":null,"title":"Apache Hive : HowToContribute"},{"categories":null,"contents":"Apache Hive : HowToRelease This page is prepared for Hive committers. You need committer rights to create a new Hive release.\n   Storage API Release  Storage API Prepare Master Branch Storage API Branching Making Storage API Release Artifacts Publishing the Storage API Artifacts Preparing Branch for further development Cleaning Up Storage API Artifacts   Hive Release  Preparation Branching Updating Release Branch Building Voting Verifying the Release Candidate Publishing Archive old releases Preparing Branch for Future Maintenance Release   See Also  Hadoop Version Warning\nThis page assumes you are releasing from the master branch, and thus omits the use of Maven profiles to determine which version of Hadoop you are building against. If you are releasing from branch-1, you will need to add -Phadoop-2 to most of your Maven commands.\nStorage API Release The Hive projects has two products that are released separately:\n Storage-API – the vectorization and predicate push down classes Hive – the rest of Hive  Most Hive releases will require a new storage-api release and the storage-api currently releases faster than Hive, so has higher version numbers.\nStorage API Prepare Master Branch Skip this section if this is NOT the first release in a series (i.e., release X.Y.0).\n Check out the master branch  git checkout master Increment the value of the version property in the storage-api/pom.xml file. For example, if the current value is 2.5.0-SNAPSHOT, the new value should be 2.6.0-SNAPSHOT. Please note that the SNAPSHOT suffix is required in order to indicate that this is an unreleased development branch. Update the storage-api.version property in the root pom.xml and standalone-metastore/pom.xml to the new value from the step above. Verify that the build is working with changes. Commit these changes to master with a comment \u0026ldquo;Preparing for storage-api X.Y+1.0 development\u0026rdquo;.  Storage API Branching Skip this section if this is NOT the first release in a series (i.e., release X.Y.0).\n Notify developers on the #hive IRC channel and dev@hive mailing lists that you are about to branch a release. Create a branch for the release series:   git checkout -b storage-branch-X.Y origin/master Update the version property value in the storage-api/pom.xml file. You should remove the SNAPSHOT suffix and set version equal to X.Y.Z where Z is the point release number in this release series (0 for the first one, in which case this step is a no-op since you already did this above when creating the branch). Use Maven\u0026rsquo;s Versions plugin to do this as follows: Verify that the build is working with changes. Commit these changes with a comment \u0026ldquo;Preparing for storage-api X.Y.Z release\u0026rdquo;. Tag the release candidate (R is the release candidate number, and also starts from 0):  git commit -a -m \u0026quot;Preparing for storage-api X.Y.Z release\u0026quot; git push -u origin storage-branch-X.Y git tag -a storage-release-X.Y.Z-rcR -m \u0026quot;Hive Storage API X.Y.Z-rcR release.\u0026quot; git push origin storage-release-X.Y.Z-rcR Making Storage API Release Artifacts  Make sure your release notes have been updated for any new commits, and go through the previous steps if necessary. Create and publish the tag:  git tag storage-release-X.Y.Z-rcR -m \u0026quot;Hive Storage API X.Y.Z-rcR release.\u0026quot; git push origin storage-release-X.Y.Z-rcR Build the release (binary and source versions) after running unit tests. Manually create the sha file.  % wget https://github.com/apache/hive/archive/storage-release-X.Y.Z-rcR.tar.gz % tar xzvf storage-release-X.Y.Z-rcR.tar.gz % mv storage-release-X.Y.Z-rcR/storage-api hive-storage-X.Y.Z % tar czvf hive-storage-X.Y.Z-rcR.tar.gz hive-storage-X.Y.Z % shasum -a 256 hive-storage-X.Y.Z-rcR.tar.gz \u0026gt; hive-storage-X.Y.Z-rcR.tar.gz.sha256  Setup your PGP keys for signing the release, if you don\u0026rsquo;t have them already.\n See https://www.apache.org/dev/release-signing.html, https://www.apache.org/dev/openpgp.html.    Sign the release (see Step-By-Step Guide to Mirroring Releases for more information).\n  % gpg --armor --detach-sig hive-storage-X.Y.Z-rcR.tar.gz Check the signatures.  % shasum -c hive-storage-X.Y.Z-rcR.tar.gz.sha256 hive-storage-X.Y.Z-rcR.tar.gz: OK % gpg hive-storage-X.Y.Z-rcR.tar.gz.asc gpg: assuming signed data in `hive-storage-X.Y.Z-rcR.tar.gz' gpg: Signature made Fri Apr 28 12:50:03 2017 PDT using RSA key ID YOUR-KEY-ID gpg: Good signature from \u0026quot;Your Name \u0026lt;YOUR-APACHE-ID@apache.org\u0026gt;\u0026quot; Copy release files to a public place.  % sftp YOUR-APACHE-ID@home.apache.org sftp\u0026gt; cd public_html sftp\u0026gt; mkdir hive-storage-X.Y.Z sftp\u0026gt; cd hive-storage-X.Y.Z sftp\u0026gt; put hive-storage-X.Y.Z-rcR.tar.gz* sftp\u0026gt; quit Send email to dev@hive.apache.org calling the vote.  Publishing the Storage API Artifacts  After the release vote passes, push the artifacts to Nexus. (If you get an error gpg: signing failed: Inappropriate ioctl for device trying doing export GPG_TTY=$(tty) .)  % git checkout storage-release-X.Y.Z-rcR % cd storage-api % mvn -Papache-release -DskipTests clean deploy Login to Nexus and close the repository. Mark the repository as released. Create the final tag (be very careful, tags in \u0026ldquo;rel/\u0026rdquo; are not changeable).  % git checkout storage-release-X.Y.Z-rcR % git tag -s rel/storage-release-X.Y.Z -m \u0026quot;Hive Storage API X.Y.Z\u0026quot; % git push origin rel/storage-release-X.Y.Z Add the artifacts to Hive\u0026rsquo;s dist area. There might be a problem with the size of the artifact.  INFRA-23055 Authenticate to see issue details\nsolved the issue.\n% svn checkout https://dist.apache.org/repos/dist/release/hive hive-dist % cd hive-dist % mkdir hive-storage-X.Y.Z % cd hive-storage-X.Y.Z % wget https://home.apache.org/~YOUR-APACHE-ID/hive-storage-X.Y.Z/hive-storage-X.Y.Z-rcR.tar.gz{,.sha256,.asc} % svn add . % svn commit Preparing Branch for further development  Edit storage-api/pom.xml to change version to X.Y.Z+1-SNAPSHOT. Edit pom.xml to change storage-api.version to X.Y.Z+1-SNAPSHOT. Commit the changes back  % git commit -a -s -m 'Preparing for development post-X.Y.Z.' % git push origin storage-branch-X.Y Cleaning Up Storage API Artifacts  Delete the storage-release-X.Y.Z-rcR tags. Delete the artifacts from home.apache.org.  Hive Release Preparation  Bulk update Jira to unassign from this release all issues that are open non-blockers and send follow-up notification to the developer list that this was done. There are two kinds of JIRAs that need to be taken care of:  Unresolved JIRAs with Target Version/s or Fix Version/s (legacy) set to the release in question. Resolved/closed(!) JIRAs with Target Version/s, but not Fix Version/s set to the release in question (e.g. a JIRA targets 2.0.0 and 1.3.0, but was only committed to 2.0.0).   Run \u0026lsquo;mvn clean apache-rat:check\u0026rsquo; and examine the generated report for any files, especially .java files which should all have Apache license headers. Note also, that each individual component will have a rat.txt inside it when you run this – be sure to check ql/target/rat.txt, for example. Add the license header to any file that is missing it (open a jira and submit a patch). Update copyright date in NOTICE. If any components mentioned in them have updated versions, you would need to update the copyright dates for those. (Thejas comment: It sounds like entries are needed in NOTICE only if the license requires such attribution. See https://www.apache.org/legal/src-headers.html#notice.)  Branching Skip this section if this is NOT the first release in a series (i.e., release X.Y.0).\n Notify developers on the #hive IRC channel and dev@hive mailing lists that you are about to branch a release. Create a branch for the release series:   git checkout -b branch-X.Y origin/master git push -u origin branch-X.Y Increment the value of the version property in all pom.xml files. For example, if the current value is 0.7.0-SNAPSHOT, the new value should be 0.8.0-SNAPSHOT. Please note that the SNAPSHOT suffix is required in order to indicate that this is an unreleased development branch. This can be accomplished with a single command using Maven\u0026rsquo;s Versions plugin as follows:   mvn versions:set -DnewVersion=X.Y.0-SNAPSHOT -DgenerateBackupPoms=false Make changes to metastore upgrade scripts. See HIVE-6555 on how this was done for HIVE 0.13. Verify that the build is working with changes. Commit these changes to master with a comment \u0026ldquo;Preparing for X.Y+1.0 development\u0026rdquo;.  Updating Release Branch These operations take place in the release branch.\n Check out the release branch with:  git clone https://git-wip-us.apache.org/repos/asf/hive.git/ \u0026lt;hive_src_dir\u0026gt; cd \u0026lt;hive_src_dir\u0026gt; git checkout branch-X.Y Update the version property value in all pom.xml files. You should remove the SNAPSHOT suffix and set version equal to hive-X.Y.Z where Z is the point release number in this release series (0 for the first one, in which case this step is a no-op since you already did this above when creating the branch). Use Maven\u0026rsquo;s Versions plugin to do this as follows:  mvn versions:set -DnewVersion=0.7.0 -DgenerateBackupPoms=false Make sure to update the version property in standalone-metastore/pom.xml and upgrade-acid/pom.xml. 3. Remove the storage-api from the list of modules to build in the top level pom.xml. Set the storage-api.version property to the release of storage-api that you are using for your release. Make sure to set the storage-api.version property in standalone-metastore/pom.xml as well. 4. Update the the value of the TRACKING_BRANCH field in the .reviewboardrc file to point to the origin/branch-X.Y. 5. Verify that the build is working with changes. 6. Commit these changes with a comment \u0026ldquo;Preparing for X.Y.Z release\u0026rdquo;. 7. If not already done, merge desired patches from trunk into the branch and commit these changes. Avoid usage of \u0026ldquo;git merge\u0026rdquo; to avoid too many merge commits. Either request the committer who committed that patch in master to commit to this branch, or commit it yourself, or try doing a git cherry-pick for trivial patches. Specifics of this step can be laid down by the release manager. 8. You probably also want to commit a patch (on both trunk and branch) which updates README.txt to bring it up to date (at a minimum, search+replacing references to the version number). Also check NOTICE to see if anything needs to be updated for recent library dependency changes or additions. 1. Select all of the JIRAs for the current release that aren\u0026rsquo;t FIXED and do bulk update to clear the \u0026lsquo;Fixed Version\u0026rsquo; field. 2. Likewise, use JIRA\u0026rsquo;s Release Notes link to generate content for the RELEASE_NOTES.txt file. Be sure to select \u0026lsquo;Text\u0026rsquo; format. (It\u0026rsquo;s OK to do this with a direct commit rather than a patch.) 3. Update the release notes in trunk with the release notes in branch. 9. Tag the release candidate (R is the release candidate number, and also starts from 0):\ngit tag -a release-X.Y.Z-rcR -m \u0026quot;Hive X.Y.Z-rcR release.\u0026quot; git push origin release-X.Y.Z-rcR Building  Make sure your release notes have been updated for any new commits, and go through the previous steps if necessary. Build the release (binary and source versions) after running unit tests. Manually create the sha256 files.  % mvn install -Pdist,iceberg -DskipTests -Dmaven.javadoc.skip=true -DcreateChecksum=true % cd packaging/target % shasum -a 256 apache-hive-X.Y.Z-bin.tar.gz \u0026gt; apache-hive-X.Y.Z-bin.tar.gz.sha256 % shasum -a 256 apache-hive-X.Y.Z-src.tar.gz \u0026gt; apache-hive-X.Y.Z-src.tar.gz.sha256 Note: If you build from the existing project, make sure there are no empty directories or the \u0026ldquo;*.iml\u0026rdquo; files in the apache-hive-X.Y.Z-src.tar.gz. 3. Verify that the SHA 256 checksums are valid:\n% shasum -a 256 -c apache-hive-X.Y.Z-bin.tar.gz.sha256 apache-hive-X.Y.Z-bin.tar.gz: OK % shasum -a 256 -c apache-hive-X.Y.Z-src.tar.gz.sha256 apache-hive-X.Y.Z-src.tar.gz: OK Check that release file looks ok \u0026ndash; e.g., install it and run examples from tutorial. Setup your PGP keys for signing the release, if you don\u0026rsquo;t have them already.  See https://www.apache.org/dev/release-signing.html, https://www.apache.org/dev/openpgp.html.    % gpg --full-generate-key % gpg --keyserver hkp://keyserver.ubuntu.com --send-keys \u0026lt;PUB_KEY\u0026gt; % svn co --depth files https://dist.apache.org/repos/dist/release/hive % cd hive % (gpg --list-sigs \u0026lt;NAME\u0026gt; \u0026amp;\u0026amp; gpg --armor --export \u0026lt;NAME\u0026gt;) \u0026gt;\u0026gt; KEYS % svn add KEYS % svn commit -m 'Adding \u0026lt;FullName\u0026gt;'s key' Sign the release (see Step-By-Step Guide to Mirroring Releases for more information).  % gpg --armor --output apache-hive-X.Y.Z-bin.tar.gz.asc --detach-sig apache-hive-X.Y.Z-bin.tar.gz % gpg --armor --output apache-hive-X.Y.Z-src.tar.gz.asc --detach-sig apache-hive-X.Y.Z-src.tar.gz Follow instructions in https://www.apache.org/dev/release-publishing.html#distribution to push the new release artifacts (tar.gz, tar.gz.asc, tar.gz.sha256) to the SVN staging area of the project (https://dist.apache.org/repos/dist/dev/hive/). Make sure to create a new directory for the release candidate. You may need PMC privileges to do this step – if you do not have such privileges, please ping a PMC member to do this for you.  svn co --depth empty https://dist.apache.org/repos/dist cd dist svn update --set-depth empty dev svn update --set-depth empty dev/hive mkdir dev/hive/hive-X.Y.Z/ cp \u0026lt;hive-source-dir\u0026gt;/packaging/target/apache-hive-X.Y.Z*.tar.gz* dev/hive/hive-X.Y.Z/ svn add dev/hive/hive-X.Y.Z svn commit -m \u0026quot;Hive X.Y.Z release\u0026quot; Publish Maven artifacts to the Apache staging repository. Make sure to have this setup for Apache releases. Use committer setting.xml.  Note: If you get an error gpg: signing failed: Inappropriate ioctl for device, try doing export GPG_TTY=$(tty)\nNote: if you have multiple gpg keys, you may need to specify which key to use via -Dgpg.keyname=\u0026lt;PRIV_KEY\u0026gt;\n% mvn deploy -DskipTests -Papache-release,iceberg -Dmaven.javadoc.skip=true Login to the Apache Nexus server and \u0026ldquo;close\u0026rdquo; the staged repository. This makes the artifacts available at a temporary URL.  Voting  Call a release vote on dev at hive.apache.org.  From: you@apache.org To: dev@hive.apache.org Subject: [VOTE] Apache Hive X.Y.Z Release Candidate N Apache Hive X.Y.Z Release Candidate N is available here: https://people.apache.org/~you/hive-X.Y.Z-candidate-N The checksums are these: - ff60286044d2f3faa8ad1475132cdcecf4ce9ed8faf1ed4e56a6753ebc3ab585 apache-hive-4.0.0-alpha-1-bin.tar.gz - 07f30371df5f624352fa1d0fa50fd981a4dec6d4311bb340bace5dd7247d3015 apache-hive-4.0.0-alpha-1-src.tar.gz Maven artifacts are available here: https://repository.apache.org/content/repositories/orgapachehive-121/ The tag release-X.Y.Z-rcR has been applied to the source for this release in github, you can see it at https://github.com/apache/hive/tree/release-X.Y.Z-rcR The git commit hash is: https://github.com/apache/hive/commit/357d4906f5c806d585fd84db57cf296e12e6049b Voting will conclude in 72 hours. Hive PMC Members: Please test and vote. Thanks. Verifying the Release Candidate  Verifying the PGP signature:  #get the hive committers keys file wget https://www.apache.org/dist/hive/KEYS or wget https://people.apache.org/keys/group/hive.asc gpg --import \u0026lt;keys file\u0026gt; gpg --verify hive-X.Y.Z-bin.tar.gz.asc hive-X.Y.Z-bin.tar.gz gpg --verify hive-X.Y.Z.tar.gz.asc hive-X.Y.Z.tar.gz Verifying the sha256 checksum:\nSee the step under Building.  Publishing Once three PMC members have voted for a release, it may be published.\n Tag the release and delete the release candidate tag. Do it from the release branch:  git tag -s rel/release-X.Y.Z -m \u0026quot;HiveX.Y.Z release.\u0026quot; git push origin rel/release-X.Y.Z git tag -d release-X.Y.Z-rcR git push origin :release-X.Y.Z-rcR If errors happen while \u0026ldquo;git tag -s\u0026rdquo;, try to configure the git signing key by \u0026ldquo;git config user.signingkey your_gpg_key_id\u0026rdquo; then rerun the command. 2. Move the release artifacts to the release area of the project (https://dist.apache.org/repos/dist/release/hive/). Using svn mv command is important otherwise you may hit size limitations applying to artifacts(\nINFRA-23055 Authenticate to see issue details\n)\nsvn mv https://dist.apache.org/repos/dist/dev/hive/hive-X.Y.Z https://dist.apache.org/repos/dist/release/hive/hive-X.Y.Z -m \u0026quot;Move hive-X.Y.Z release from dev to release\u0026quot; Wait till the release propagates to the mirrors and appears under: https://dlcdn.apache.org/hive/ In your base hive source directory, generate javadocs as follows:  mvn clean install javadoc:javadoc javadoc:aggregate -DskipTests -Pjavadoc,iceberg After you run this, you should have javadocs present in your \u0026lt;hive_source_dir\u0026gt;/target/site/apidocs 5. Check out the javadocs svn repository as follows:\nsvn co --depth empty https://svn.apache.org/repos/infra/websites/production/hive/content/javadocs Copy the generated javadocs from the source repository to the javadocs repository, add and commit:  mkdir \u0026lt;hive_javadocs_repo_dir\u0026gt;/rX.Y.Z/ cd \u0026lt;hive_javadocs_repo_dir\u0026gt; cp -r \u0026lt;hive_source_dir\u0026gt;/target/site/apidocs ./rX.Y.Z/api svn add rX.Y.Z svn commit If this is a bugfix release, svn rm the obsoleted version. (For eg., when committing javadocs for r0.13.1, r0.13.0 would have been removed) 7. Prepare to edit the website.\ngit clone https://github.com/apache/hive-site.git Edit files content/downloads.mdtext and javadoc.mdtext to appropriately add entries for the new release in the appropriate location. For example, for 1.2.0, the entries made were as follows:  ./downloads.md:### 18 May 2015 : release 1.2.0 available ./downloads.md:You can look at the complete [JIRA change log for this release][HIVE_1_2_0_CL]. ./downloads.md:[HIVE_1_2_0_CL]: https://issues.apache.org/jira/secure/ReleaseNote.jspa?version=12329345\u0026amp;styleName=Text\u0026amp;projectId=12310843 ./javadoc.md: * [Hive 1.2.0 Javadocs][r1.2.0] ./javadoc.md:[r1.2.0]: /javadocs/r1.2.0/api/index.html As you can see, you will need a release note link for this release as created previously for this section. 9. Push your changes to the https://github.com/apache/hive-site/tree/gh-pages branch, and you can preview the results at https://apache.github.io/hive-site/. If everything is ok, then you can push your changes to https://github.com/apache/hive-site/tree/main branch and see the results at https://hive.apache.org/ site. 10. Update JIRA 1. Ensure that only issues in the \u0026ldquo;Fixed\u0026rdquo; state have a \u0026ldquo;Fix Version\u0026rdquo; set to release X.Y.Z. 2. Release the version. Visit the releases page. Select the version number you are releasing, and hit the release button. You need to have the \u0026ldquo;Admin\u0026rdquo; role in Hive\u0026rsquo;s Jira for this step and the next. 3. Close issues resolved in the release. Disable mail notifications for this bulk change. 11. Login to the Apache Nexus server and mark the release candidate artifacts as released. 12. Add the release in Apache Committee Report Helper for the next board report to pick that up automatically. 13. Check whether the Docker image for the release is present or not. 14. Send a release announcement to Hive user and dev lists as well as the Apache announce list. This email should be sent from your Apache email address:\nFrom: you@apache.org To: user@hive.apache.org, dev@hive.apache.org, announce@apache.org Subject: [ANNOUNCE] Apache Hive X.Y.Z Released The Apache Hive team is proud to announce the release of Apache Hive version X.Y.Z. The Apache Hive (TM) data warehouse software facilitates querying and managing large datasets residing in distributed storage. Built on top of Apache Hadoop (TM), it provides, among others: * Tools to enable easy data extract/transform/load (ETL) * A mechanism to impose structure on a variety of data formats * Access to files stored either directly in Apache HDFS (TM) or in other data storage systems such as Apache HBase (TM) * Massively parallel query execution via Apache Tez For Hive release details and downloads, please visit: https://hive.apache.org/downloads.html Hive X.Y.Z Release Notes are available here: [UPDATE THIS LINK] https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12310843\u0026amp;version=12316178 For the Docker image, check: https://hub.docker.com/r/apache/hive/tags We would like to thank the many contributors who made this release possible. Regards, The Apache Hive Team Archive old releases According to the INFRA archival guidelines old releases should be removed from the main download site of the project following. Check the respective guidelines and perform the necessary cleanup.\nsvn del -m \u0026quot;Archiving release Apache Hive 4.0.0-beta-1\u0026quot; https://dist.apache.org/repos/dist/release/hive/hive-4.0.0-beta-1/ Preparing Branch for Future Maintenance Release After the release has been completed, prepare the branch for the next development cycle.\n Check out the release branch with:  git clone https://git-wip-us.apache.org/repos/asf/hive.git/ \u0026lt;hive_src_dir\u0026gt; cd \u0026lt;hive_src_dir\u0026gt; git checkout branch-X.Y Increment the version property value in all pom.xml files and add the SNAPSHOT suffix. For example, if the released version was 0.7.0, the new value should be 0.7.1-SNAPSHOT. Please note that the SNAPSHOT suffix is required in order to indicate that this is an unreleased development branch. Use Maven\u0026rsquo;s Versions plugin to do this as follows:  mvn versions:set -DnewVersion=0.7.1-SNAPSHOT -DgenerateBackupPoms=false If the release number you are preparing moves the major (first) or minor (second) number, update the Hive version name in the poms. In both pom.xml and standalone-metastore/pom.xml search for the property hive.version.shortname. This should match the new version number. For example, if you are working on branch-3 and have just released Hive 3.2 and are preparing the branch for Hive 3.3 development, you need to update both poms to have \u0026lt;hive.version.shortname\u0026gt;3.3.0\u0026lt;/hive.shortname.version\u0026gt;. If however you are working on branch-3.1 and have just released Hive 3.1.2 and are preparing the branch for 3.1.3 development, this is not necessary. Verify that the build is working with changes. Commit these changes with a comment \u0026ldquo;Preparing for X.Y.Z+1 development\u0026rdquo;.  See Also  Apache Releases FAQ  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/howtorelease_27362106/","tags":null,"title":"Apache Hive : HowToRelease"},{"categories":null,"contents":"Apache Hive : Hybrid Hybrid Grace Hash Join, v1.0  Overview Scope Notation and Assumptions Brief Review on Hash Join Algorithms  Simple Hash Join GRACE Hash Join Hybrid GRACE Hash Join Hash Join in Hive   Motivation for “Hybrid Hybrid GRACE Hash Join” Algorithm Recursive Hashing and Spilling Skewed Data Distribution Bloom Filter References  Overview We are proposing an enhanced hash join algorithm called “hybrid hybrid grace hash join”. We can benefit from this feature as illustrated below:\n The query will not fail even if the estimated memory requirement is slightly wrong. Expensive garbage collection overhead can be avoided when hash table grows. Join execution using a Map join operator even though the small table doesn\u0026rsquo;t fit in memory, as spilling some data from the build and probe sides will still be cheaper than having to shuffle the large fact table.  The design was based on Hadoop’s parallel processing capability and significant amount of memory available.\nSee HIVE-9277 for the current status of this work.\nScope This new join algorithm will only work with Tez. It does not support Map Reduce currently.\nNotation and Assumptions The goal is to compute the equi-join result of two relations labeled R and S. We assume R is the small table with smaller cardinality, and S is the big table with bigger cardinality.\nBrief Review on Hash Join Algorithms Simple Hash Join Also known as Classic Hash Join, it is used when the hash table for R can entirely fit into the memory.\nIn general, all hash join algorithms described here have two phases: Build phase and Probe phase. During the build phase, a hash table is built into memory based on the joining column(s) from the small table R. During the probe phase, the big table S is scanned sequentially and for each row of S, the hash table is probed for matching rows. If a match is found, output the pair, otherwise drop the row from S and continue scanning S.\nWhen the hash table for R cannot wholly fit in the memory, only part of the hash table for R will be put in memory first and S is scanned against the partial hash table. After the scan on S is completed, the memory is cleared and another part of hash table of R is put in memory, and S is scanned again. This process can repeat more times if there are more parts of the hash table.\nGRACE Hash Join Apparently when the size of small table R is much bigger than memory, we end up having many partial hash tables of R loaded in the memory one by one, and the big table S being scanned multiple times, which is very expensive.\nGRACE hash join brings rounds of scanning the big table from many times down to just twice. One for partitioning, the other for row matching. Similarly, the small table will also be scanned twice.\nHere is the detailed process.\n Small table R is scanned, and during the scan a hash function is used to distribute the rows into different output buffers (or partitions). The size of each output buffer should be specified as close as possible to the memory limit but no more than that. After R has been completely scanned, all output buffers are flushed to disk. Similarly, big table S is scanned, partitioned and flushed to disk the same way. The hash function used here is the same one as in the previous step. Load one partition of R into memory and build a hash table for it. This is the build phase. Hash each row of the corresponding partition of S, and probe for a match in R’s hash table. If a match is found, output the pair to the result, otherwise, proceed with the next row in the current S partition. This is the probe phase. Repeat loading and building hash table for partitions of R and probing partitions of S, until both are exhausted.  It can be seen there are extra partitioning steps in this algorithm (1 \u0026amp; 2). The rest of the steps are the same as Classic Hash Join, i.e., building and probing. One assumption here is all partitions of R can completely fit into the memory.\nHybrid GRACE Hash Join It is a hybrid of Classic Hash Join and GRACE Hash Join. The idea is to build an in-memory hash table for the first partition of R during the partitioning phase, without the need to write this partition to disk. Similarly, while partitioning S, the first partition does not have to be put to the disk since probing can be directly done against the in-memory first partition of R. So at the end of the partitioning phase, the join of the first pair of partitions of R and S has already been done.\nComparing to GRACE hash join, Hybrid GRACE hash join has an advantage of avoiding writing the first partitions of R and S to disk during the partitioning phase and reading them back in again during the probing phase.\nHash Join in Hive It is also known as replicated join, map-side join or mapjoin. When one side of the data is small enough to fit in memory of a mapper, it can be copied to all the mappers and the join will be performed only in the map phase. There is no reduce phase needed.\nMotivation for “Hybrid Hybrid GRACE Hash Join” Current implementation of hash join in Hive is the Classic Hash Join, which can only handle the case when the small table can be entirely fit in memory. Otherwise hash join cannot be performed. This feature is trying to lift that limitation by making hash join more general, so that even if the size of small table is larger than memory, we can still do the hash join, in an elegant way.\nIt’s obvious that GRACE Hash Join uses main memory as a staging area during the partitioning phase, which unconditionally scans both relations from disk, then partitions and puts them back to disk (Hybrid GRACE Hash Join puts one pair less), and finally reads them back to find matches.\nThis feature tries to avoid the unnecessary write-back of partitions to disk as much as possible, and will only do that when necessary. The idea is to fully utilize the main memory to hold existing partitions of hash tables.\nThe key factor that will impact the performance of this algorithm is whether the data can be evenly distributed into different hash partitions. If we have skewed values, which will result in a few very big partitions, then an extra partitioning step is needed to divide the big partitions down to many. This can happen recursively. Refer to Recursive Hashing and Spilling below for more details. Algorithm Like other hash joins, there are a build phase and a probe phase. But there are several new terms to be defined.\n Partition. Instead of putting all keys of the small table into a single hash table, they will be put into many hash tables, based on their hash value. Those hash tables are known as partitions. Partitions can be either in memory when initially created, or spilled to disk when the memory is used up. Sidefile. It is a row container on disk to hold rows from the small table targeted to a specific partition that has been already spilled to disk. There can be zero or one sidefile for each partition spilled to disk. Matchfile. It is a row container on disk to hold rows from the big table that have possible matching keys with partitions spilled to disk. It also has a one-to-one relationship to the partitions.  When the entire small table can fit in memory, everything is similar to Classic Hash Join, except there are multiple hash tables instead of one in memory.\nWhen the small table cannot fit in memory, things are a little more complicated.\nSmall table keys keep getting hashed into different partitions until at some point the memory limit is reached. Then the biggest partition in memory will be moved to disk so that memory is freed to some extent. The new key that should have been put into that partition will be put in the corresponding sidefile.\nDuring probing, if matches are found from in-memory partitions, they will be directly put into the result.\nIf a possible match is found, i.e., the key hash of big table falls into an on-disk partition, it will be put into an on-disk matchfile. No matching is done at this moment, but will be done later.\nAfter the big table has been exhausted, all the in-memory partitions are no longer needed and purged. Purging is appropriate because those keys that should match have been output to result, and those that don’t match won’t be relevant to the on-disk structures.\nNow there may be many on-disk triplets (partition, sidefile and matchfile). For each triplet, merge the partition and sidefile and put them back in memory to form a new hash table. Then scan the matchfile against the newly created hash table looking for matches. The logic here is similar to Classic Hash Join.\nRepeat until all on-disk triplets have been exhausted. At this moment, the join between small table and big table is completed.\nRecursive Hashing and Spilling There are cases when the hash function is not working well for distributing values evenly among the hash partitions, or the values themselves are skewed, which will result in very big sidefiles. At the time when the on-disk partition and sidefile are to be merged back into memory, the size of the newly combined hash partition will exceed the memory limit. In that case, another round of hashing and spilling is necessary. The process is illustrated below.\nAssume there’s only 1GB memory space available for the join. Hash function h1 distributes keys into two partitions HT1 and HT2. At some point in time, the memory is full and HT1 is moved to disk. Assume all the future keys all hash to HT1, so they go to Sidefile1.\n\nAs normal, during the probe phase big table values will be either matched and put into result or not matched and put into a Matchfile.\n\nAfter the big table is exhausted, all the values from big table should be either output (matched) to the result, or staged into matchfiles. Then it is time to merge back pairs of on-disk hash partition and sidefile, and build a new in-memory hash table, and probe the corresponding matchfile against it.\nIn this example, since the predicted size of the merge is greater than the memory limit (800MB + 300MB \u0026gt; 1GB), we cannot simply merge them back to memory. Instead, we need to rehash them by using a different hash function h2.\n\n Now we probe using Matchfile 1 against HT 3 (in memory) and HT 4 (on disk). Matching values for HT 3 go into result. Possibly matching values for HT 4 go to Matchfile 4.\nThis process can continue recursively if the size of HT 4 plus size of Sidefile 4 is still greater than the memory limit, i.e., hashing HT 4 and Sidefile 4 using a third hash function h3, and probing Matchfile 4 using h3. In this example, the size of HT 4 plus size of Sidefile 4 is smaller than memory limit, so we are done.\nSkewed Data Distribution Sometimes it happens that all or most rows in one hash partition share the same value. In that case, no matter how we change the hash function, it won’t help anyway.\nSeveral approaches can be considered to handle this problem. If we have reliable statistics about the data, like detailed histograms, we can process rows with the same value using a new join algorithm (using or dropping rows sharing the same value all in one shot). Or we can divide the rows into pieces such that each piece can fit into memory, and perform the probing in several passes. This probably will bring performance impact. Further, we can resort to regular shuffle join as a fallback option once we figure out Mapjoin cannot handle this situation.\nBloom Filter As of Hive 2.0.0, a cheap Bloom filter is built during the build phase of the Hybrid hashtable, which is consulted against before spilling a row into the matchfile. The goal is to minimize the number of records which end up being spilled to disk, which may not have any matches in the spilled hashtables. The optimization also benefits left outer joins since the row which entered the hybrid join can be immediately generated as output with appropriate nulls indicating a lack of match, while without the filter it would have to be serialized onto disk only to be reloaded without a match at the end of the probe.\nReferences  Hybrid Hybrid Grace Hash Join presentation by Mostafa MapJoinOptimization https://cwiki.apache.org/confluence/display/Hive/MapJoinOptimization HIVE-1641 add map joined table to distributed cache HIVE-1642 Convert join queries to map-join based on size of table/row Database Management Systems, 3rd ed Kitsuregawa, M. Application of Hash to Data Base Machine and Its Architecture Shapiro, L. D. Join Processing in Database Systems with Large Main Memories Dewitt, David J. Implementation techniques for main memory database systems Jimmy Lin and Chris Dyer Data-Intensive Text Processing with MapReduce  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/50860526/","tags":null,"title":"Apache Hive : Hybrid Hybrid Grace Hash Join, v1.0"},{"categories":null,"contents":"Apache Hive : IndexDev Indexes  Indexes  Indexing Is Removed since 3.0 Introduction Scope CREATE INDEX Metastore Model Metastore Upgrades REBUILD DROP INDEX Plugin Interface Reference Implementation TBD Current Status (JIRA)    Indexing Is Removed since 3.0 There are alternate options which might work similarily to indexing:\n Materialized views with automatic rewriting can result in very similar results. Hive 2.3.0 adds support for materialzed views. Using columnar file formats (Parquet, ORC) – they can do selective scanning; they may even skip entire files/blocks.  Indexing has been removed in version 3.0 (HIVE-18448).\nIntroduction This document explains the proposed design for adding index support to Hive (HIVE-417). Indexing is a standard database technique, but with many possible variations. Rather than trying to provide a \u0026ldquo;one-size-fits-all\u0026rdquo; index implementation, the approach we are taking is to define indexing in a pluggable manner (related to StorageHandlers) and provide one concrete indexing implementation as a reference, leaving it open for contributors to plug in other indexing schemes as time goes by. No index support will be available until Hive 0.7.\nScope Only single-table indexes are supported. Others (such as join indexes) may be more appropriately expressed as materialized views once Hive has support for those.\nThis document currently only covers index creation and maintenance. A follow-on will explain how indexes are used to optimize queries (building on FilterPushdownDev).\nCREATE INDEX CREATE INDEX index_name ON TABLE base_table_name (col_name, ...) AS 'index.handler.class.name' [WITH DEFERRED REBUILD] [IDXPROPERTIES (property_name=property_value, ...)] [IN TABLE index_table_name] [PARTITIONED BY (col_name, ...)] [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] [LOCATION hdfs_path] [TBLPROPERTIES (...)] [COMMENT \u0026quot;index comment\u0026quot;] For the details of the various clauses such as ROW FORMAT, see Create Table.\nBy default, index partitioning matches the partitioning of the base table. The PARTITIONED BY clause may be used to specify a subset of the table\u0026rsquo;s partitioning columns (this column list may be empty to indicate that the index spans all partitions of the table). For example, a table may be partitioned by date+region even though the index is partitioned by date alone (each index partition spanning all regions).\nIndexes cannot be created on views. We will (eventually) support them on non-native tables (in cases where the corresponding storage handler indicates that it supports them).\nIndex handlers may require that the base table being indexed have a particular format.\nQuestion: should we allow indexes on EXTERNAL tables? What does this mean for implicit DROP when the table is dropped? Is there a concept of an EXTERNAL index?\nIf the index handler stores its representation in tabular form, then index_table_name can be used to control the name of the \u0026ldquo;index table\u0026rdquo; automatically created for this purpose. The index table storage format can be controlled using STORED AS (e.g. RCFILE or SEQUENCFILE) or STORED BY (e.g. to store the index table in a non-native table such as HBase), although some index handlers may require usage of a specific storage format. Not all index handlers store their representation in tabular form; some may use non-table files, and others may use structures maintained completely outside of Hive (e.g. a persistent key/value store).\nMetastore Model The diagram below shows the new metastore schema with index support:\n\u0026lt;http://issues.apache.org/jira/secure/attachment/12449601/idx2.png\u0026gt;\nThe new IDXS table in the metastore schema contains one entry per index created. It has two relationships with the TBLS table:\n ORIG_TBL_ID is a mandatory foreign key referencing the ID of the base table containing the data to be indexed. IDX_TBL_ID is an optional foreign key referencing the ID of a table containing the index representation. It is optional because not all index implementations use a table for storage. For indexes which do use a table for storage, the implicitly created table will have its TBL_TYPE set to INDEX_TABLE.  So, given the following DDL:\nCREATE TABLE t(i int, j int); CREATE INDEX x ON TABLE t(j) AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'; The TBLS table in the metastore will have two entries:\n one for base table t one for the index table, automatically named as default+t_x+  In the IDXS entry for x, ORIG_TBL_ID will reference the TBL_ID of x, and IDX_TBL_ID will reference the TBL_ID of default+t_x+.\nTo avoid the generated name, a user-specified name such as t_x can be supplied instead:\nCREATE INDEX x ON TABLE t(j) AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' IN TABLE t_x; Note that index names are qualified by the containing base table (like partitions), so the same index name can be used across two different tables. However, names of index tables are in the same namespace as all other tables and views, so they must be unique within the same database.\nAn index has a storage descriptor which includes the subset of columns from the original table covered by the index. If the index representation is stored in a table, most of the other fields in the index\u0026rsquo;s own storage descriptor (e.g. LOCATION) will be irrelevant.\nTBD:\n change IDX_TYPE to IDX_HANDLER what does LAST_ACCESS_TIME mean? last time the optimizer used this index? need LAST_REBUILD_TIME? how do we track it at partition-level? it should be in the metastore (not just HDFS) in the case where the index partitioning is a subset of the base table partitioning, we need a way to model this in the metastore  Metastore Upgrades Here are the MySQL metastore upgrade statements.\nDROP TABLE IF EXISTS {{IDXS}}; CREATE TABLE {{IDXS}} ( {{INDEX_ID}} bigint(20) NOT NULL, {{CREATE_TIME}} int(11) NOT NULL, {{DEFERRED_REBUILD}} bit(1) NOT NULL, {{INDEX_HANDLER_CLASS}} varchar(256) DEFAULT NULL, {{INDEX_NAME}} varchar(128) DEFAULT NULL, {{INDEX_TBL_ID}} bigint(20) DEFAULT NULL, {{LAST_ACCESS_TIME}} int(11) NOT NULL, {{ORIG_TBL_ID}} bigint(20) DEFAULT NULL, {{SD_ID}} bigint(20) DEFAULT NULL, PRIMARY KEY ({{INDEX_ID}}), UNIQUE KEY {{UNIQUEINDEX}} ({{INDEX_NAME}},{{ORIG_TBL_ID}}), KEY {{IDXS_FK1}} ({{SD_ID}}), KEY {{IDXS_FK2}} ({{INDEX_TBL_ID}}), KEY {{IDXS_FK3}} ({{ORIG_TBL_ID}}), CONSTRAINT {{IDXS_FK3}} FOREIGN KEY ({{ORIG_TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}}), CONSTRAINT {{IDXS_FK1}} FOREIGN KEY ({{SD_ID}}) REFERENCES {{SDS}} ({{SD_ID}}), CONSTRAINT {{IDXS_FK2}} FOREIGN KEY ({{INDEX_TBL_ID}}) REFERENCES {{TBLS}} ({{TBL_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; -- -- Table structure for table {{INDEX_PARAMS}} -- DROP TABLE IF EXISTS {{INDEX_PARAMS}}; CREATE TABLE {{INDEX_PARAMS}} ( {{INDEX_ID}} bigint(20) NOT NULL, {{PARAM_KEY}} varchar(256) NOT NULL, {{PARAM_VALUE}} varchar(767) DEFAULT NULL, PRIMARY KEY ({{INDEX_ID}},{{PARAM_KEY}}), CONSTRAINT {{INDEX_PARAMS_FK1}} FOREIGN KEY ({{INDEX_ID}}) REFERENCES {{IDXS}} ({{INDEX_ID}}) ) ENGINE=InnoDB DEFAULT CHARSET=latin1; REBUILD ALTER INDEX index_name ON table_name [PARTITION (...)] REBUILD For the PARTITION clause syntax, see LanguageManual DDL#Add_Partitions.\nIf WITH DEFERRED REBUILD is specified on CREATE INDEX, then the newly created index is initially empty (regardless of whether the table contains any data). The ALTER INDEX \u0026hellip; REBUILD command can be used to build the index structure for all partitions or a single partition.\nIf data in the base table changes, then the REBUILD command must be used to bring the index up to date. This is an atomic operation, so if the table was previously indexed, and a rebuild fails, then the stale index remains untouched.\nDROP INDEX DROP INDEX index_name ON table_name An index can be dropped at any time with DROP INDEX. This will also cascade to the index table (if one exists).\nAttempting to drop an index table directly with DROP TABLE will fail.\nWhen an indexed base table is dropped, the DROP implicitly cascades to all indexes (and their corresponding index tables if any).\nWhen an indexed base table has one of its partitions dropped, this implicitly cascades to drop corresponding partitions from all indexes.\nQuestion: what do we do if the index partitioning granularity is not the same as the table partitioning granularity? Probably just ignore the drop, and let the user clean up manually with a new ALTER INDEX DROP PARTITION statement.\nPlugin Interface An index handler has these main responsibilities:\n During CREATE INDEX, validating the format of the base table and then generating the structure of the index table (if any) and filling any additional information into the index\u0026rsquo;s storage descriptor During REBUILD, producing a plan for reading the base table\u0026rsquo;s data and writing to the index storage and/or index table During DROP, deleting any index-specific storage (index tables are dropped automatically by Hive) During queries, participating in optimization in order to convert operators such as filters into index access plans (this part is out of scope for the moment)  The corresponding Java inerface is defined below, together with a companion abstract base class which handlers should extend.\npackage org.apache.hadoop.hive.ql.metadata; import java.util.List; import org.apache.hadoop.conf.Configurable; import org.apache.hadoop.hive.ql.plan.api.Task; /** * HiveIndexHandler defines a pluggable interface for adding new * index handlers to Hive. */ public interface HiveIndexHandler extends Configurable { /** * Determines whether this handler implements indexes by creating * an index table. * * @return true if index creation implies creation of an index table in Hive; * false if the index representation is not stored in a Hive table */ boolean usesIndexTable(); /** * Requests that the handler validate an index definition and * fill in additional information about its stored representation. * * @param baseTable the definition of the table being indexed * * @param index the definition of the index being created * * @param indexTable a partial definition of the index table to be used for * storing the index representation, or null if usesIndexTable() returns * false; the handler can augment the index's storage descriptor * (e.g. with information about input/output format) * and/or the index table's definition (typically with additional * columns containing the index representation, e.g. pointers into HDFS) * * @throw HiveException if the index definition is invalid with * respect to either the base table or the supplied index table definition */ void analyzeIndexDefinition( org.apache.hadoop.hive.metastore.api.Table baseTable, org.apache.hadoop.hive.metastore.api.Index index, org.apache.hadoop.hive.metastore.api.Table indexTable) throws HiveException; /** * Requests that the handler generate a plan for building the index; * the plan should read the base table and write out the index representation. * * @param baseTable the definition of the table being indexed * * @param index the definition of the index * * @param partitions a list of specific partitions of the base * table for which the index should be built, or null if * an index for the entire table should be rebuilt * * @param indexTable the definition of the index table, or * null if usesIndexTable() returns null * * @return list of tasks to be executed in parallel for building * the index * * @throw HiveException if plan generation fails */ List\u0026lt;Task\u0026lt;?\u0026gt;\u0026gt; generateIndexBuildTaskList( org.apache.hadoop.hive.metastore.api.Table baseTable, org.apache.hadoop.hive.metastore.api.Index index, List\u0026lt;org.apache.hadoop.hive.metastore.api.Partition\u0026gt; partitions, org.apache.hadoop.hive.metastore.api.Table indexTable) throws HiveException; } /** * Abstract base class for index handlers. This is provided as insulation * so that as HiveIndexHandler evolves, default implementations of new * methods can be added here in order to avoid breaking existing * plugin implementations. */ public abstract class AbstractIndexHandler implements HiveIndexHandler { } For CREATE INDEX, Hive first calls usesIndexTable() on the handler to determine whether an index table will be created. If this returns false, the statement fails immediately if the user specified any table storage options for the index. However, if usesIndexTable() returns true, then Hive creates a partial table definition for the index table based on the index definition (such as the covered columns) combined with any table storage options supplied by the user. Next, Hive calls analyzeIndexDefinition (passing either null or the partial index table definition for the indexTable parameter). The handler responds by validating the definitions (throwing an exception if any unsupported combination is detected) and then filling in additional information on the index and indexTable parameters as output. Hive then stores these results in the metastore.\nTBD: we will be adding methods for calling the handler when an index is dropped (e.g. to give a cleanup opportunity to a handler which stores the index representation in an external system such as HBase)\nReference Implementation The reference implementation creates what is referred to as a \u0026ldquo;compact\u0026rdquo; index. This means that rather than storing the HDFS location of each occurrence of a particular value, it only stores the addresses of HDFS blocks containing that value. This is optimized for point-lookups in the case where a value typically occurs more than once in nearby rows; the index size is kept small since there are many fewer blocks than rows. The tradeoff is that extra work is required during queries in order to filter out the other rows from the indexed blocks.\nThe compact index is stored in an index table. The index table columns consist of the indexed columns from the base table followed by a _bucketname string column (indicating the name of the file containing the indexed block) followed by an _offsets arraycolumn (indicating the block offsets within the corresponding file). The index table is stored as sorted on the indexed columns (but not on the generated columns).\nThe reference implementation can be plugged in with\nADD JAR /path/to/hive_idx-compact.jar; CREATE INDEX ... AS 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler'; TBD: algorithm for building the index\nTBD: mechanism for searching the index\nTBD: validation on base table (can be any managed table?)\nTBD: validation on index table format (can be any managed table format?)\nTBD  specs for SHOW/DESCRIBE INDEX (HIVE-1497) ALTER INDEX DROP PARTITION? ALTER INDEX SET IDXPROPERTIES, change tableformat, etc what happens when the structure of a table or partition changes after it has already been indexed automatic indexing as part of INSERT when WITH DEFERRED REBUILD is not specified prevent creation of an index on an index table? metastore upgrade script stats collection for index tables intersection with new Hive concurrency control feature (what locks do we take for various index operations?)  Current Status (JIRA)    Type Key Summary Assignee Reporter Priority Status Resolution Created Updated Due     New Feature HIVE-21792 Hive Indexes\u0026hellip; Again Unassigned David Mollitor Major Open Unresolved May 24, 2019 Feb 27, 2024    Improvement HIVE-18448 Drop Support For Indexes From Apache Hive Zoltan Haindrich David Mollitor Minor Closed Fixed Jan 12, 2018 May 28, 2022    Bug HIVE-18035 NullPointerException on querying a table with a compact index Unassigned Brecht Machiels Major Open Unresolved Nov 09, 2017 Nov 13, 2017    Bug HIVE-15282 Different modification times are used when an index is built and when its staleness is checked Marta Kuczora Marta Kuczora Major Resolved Fixed Nov 24, 2016 Jul 21, 2017    Bug HIVE-13844 Invalid index handler in org.apache.hadoop.hive.ql.index.HiveIndex class Svetozar Ivanov Svetozar Ivanov Minor Closed Fixed May 25, 2016 Feb 27, 2024    Bug HIVE-13377 Lost rows when using compact index on parquet table Unassigned Gabriel C Balan Minor Open Unresolved Mar 29, 2016 Mar 29, 2016    Bug HIVE-12877 Hive use index for queries will lose some data if the Query file is compressed. Unassigned yangfang Major Patch Available Unresolved Jan 15, 2016 Feb 01, 2016 Jan 15, 2016   Bug HIVE-11227 Kryo exception during table creation in Hive Unassigned Akamai Major Open Unresolved Jul 10, 2015 Oct 21, 2022 Jul 12, 2015   Bug HIVE-11154 Indexing not activated with left outer join and where clause Bennie Can Bennie Can Major Open Unresolved Jun 30, 2015 Jun 30, 2015 Jul 11, 2015   Bug HIVE-10021 \u0026ldquo;Alter index rebuild\u0026rdquo; statements submitted through HiveServer2 fail when Sentry is enabled Aihua Xu Richard Williams Major Closed Fixed Mar 19, 2015 Feb 16, 2016    Bug HIVE-9656 Create Index Failed without WITH DEFERRED REBUILD Chaoyu Tang Will Du Major Open Unresolved Feb 11, 2015 Sep 28, 2015    Bug HIVE-9639 Create Index failed in Multiple version of Hive running Unassigned Will Du Major Open Unresolved Feb 10, 2015 Mar 14, 2015    Bug HIVE-8475 add test case for use of index from not-current database Thejas Nair Thejas Nair Major Closed Fixed Oct 15, 2014 Nov 13, 2014    Bug HIVE-7692 when table is dropped associated indexes also should be dropped Thejas Nair Thejas Nair Major Resolved Not A Problem Aug 12, 2014 Nov 04, 2014    Bug HIVE-7239 Fix bug in HiveIndexedInputFormat implementation that causes incorrect query result when input backed by Sequence/RC files Illya Yalovyy Sumit Kumar Major Closed Fixed Jun 16, 2014 Jul 26, 2017    Bug HIVE-6996 FS based stats broken with indexed tables Ashutosh Chauhan Ashutosh Chauhan Major Closed Fixed Apr 30, 2014 Jun 09, 2014    Bug HIVE-6921 index creation fails with sql std auth turned on Ashutosh Chauhan Ashutosh Chauhan Major Closed Fixed Apr 16, 2014 Jun 09, 2014    Bug HIVE-5902 Cannot create INDEX on TABLE in HIVE 0.12 Unassigned Juraj Volentier Major Open Unresolved Nov 27, 2013 Mar 14, 2015    Bug HIVE-5664 Drop cascade database fails when the db has any tables with indexes Venki Korukanti Venki Korukanti Major Closed Fixed Oct 28, 2013 Feb 19, 2015    Bug HIVE-5631 Index creation on a skew table fails Venki Korukanti Venki Korukanti Major Closed Fixed Oct 23, 2013 Feb 19, 2015     Authenticate to retrieve your issues\nShowing 20 out of 57 issues\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/indexdev_27362104/","tags":null,"title":"Apache Hive : IndexDev"},{"categories":null,"contents":"Apache Hive : IndexDev Bitmap = Bitmap Indexing =\n Introduction Approach Proposal  First implementation Second iteration   Example  Introduction This document explains the proposed design for adding a bitmap index handler (https://issues.apache.org/jira/browse/HIVE-1803).\nBitmap indexing (http://en.wikipedia.org/wiki/Bitmap_index) is a standard technique for indexing columns with few distinct\nvalues, such as gender.\nApproach We want to develop a bitmap index that can reuse as much of the existing Compact Index code as possible.\nProposal First implementation This implementation confers some of the benefits of bitmap indexing and should be easy to implement given the already existing compact index, but it does few of the optimizations such as compression that a really good bitmap index should do.\nLike the complex index, this implementation uses an index table. The index table on a column \u0026ldquo;key\u0026rdquo; has four or more columns: first, the columns that are being indexed, then _bucketname, _offset, and _bitmaps. _bucketname is a string pointing to the hadoop file that is storing this block in the table, _offset is the block offset of a block, and _bitmaps is an uncompressed bitmap encoding (an Array of bytes) of the bitmap for this column value, bucketname, and row offset. Each bit in the bitmap corresponds to one row in the block. The bit is 1 if that row has the value of the values in the columns being indexed, and a 0 if not. If a key value does not appear in a block at all, the value is not stored in the map.\nWhen querying this index, if there are boolean AND or OR operations done on the predicates with bitmap indexes, we can use bitwise operations to try to eliminate blocks as well. We can then eliminate blocks that do not contain the value combinations we are interested in. We can use this data to generate the filename, array of block offsets format that the compact index handler uses and reuse that in the bitmap index query.\nSecond iteration The basic implementation\u0026rsquo;s only compression is eliminating blocks where all rows are 0s. This is unlikely to happen for larger blocks, so we need a better compression format. What we can do is do byte-aligned bitmap compression, where the bitmap is an array of bytes, and a byte of all 1s or all 0s implies one or more bytes where every value is 0 or 1. Then, we would just need to add another column in the bitmap index table that is an array of Ints that describe how long the gaps are and logic to expand the compression.\nExample Suppose we have a bitmap index on a key where, on the first block, value \u0026ldquo;a\u0026rdquo; appears in rows 5, 12, and 64, and value \u0026ldquo;b\u0026rdquo; appears in rows 7, 8, and 9. Then, for the preliminary implementation, the first entry in the index table will be:\n\u0026lt;https://issues.apache.org/jira/secure/attachment/12460083/bitmap_index_1.png\u0026gt;\nThe values in the array represent the bitmap for each block, where each 32-bit BigInt value stores 32 rows.\nFor the second iteration, the first entry will be:\n\u0026lt;https://issues.apache.org/jira/secure/attachment/12460124/bitmap_index_2.png\u0026gt;\nThis one uses 1-byte array entries, so each value in the array stores 8 rows. If an entry is 0x00 or 0xFF, it represents 1 or more consecutive bytes of zeros, (in this case 5 and 4, respectively)\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/indexdev-bitmap_27362028/","tags":null,"title":"Apache Hive : IndexDev Bitmap"},{"categories":null,"contents":"Apache Hive : Introduction to Apache Hive The Apache Hive™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage and queried using SQL syntax.\nBuilt on top of Apache Hadoop™, Hive provides the following features:\n Tools to enable easy access to data via SQL, thus enabling data warehousing tasks such as extract/transform/load (ETL), reporting, and data analysis. A mechanism to impose structure on a variety of data formats Access to files stored either directly in Apache HDFS™ or in other data storage systems such as Apache HBase™ Query execution via Apache Tez™ or MapReduce Procedural language with HPL-SQL Sub-second query retrieval via Hive LLAP, Apache YARN and Apache Slider.  Hive provides standard SQL functionality, including many of the later SQL:2003, SQL:2011, and SQL:2016 features for analytics.\nHive\u0026rsquo;s SQL can also be extended with user code via user defined functions (UDFs), user defined aggregates (UDAFs), and user defined table functions (UDTFs).\nThere is not a single \u0026ldquo;Hive format\u0026rdquo; in which data must be stored. Hive comes with built in connectors for comma and tab-separated values (CSV/TSV) text files, Apache Parquet™, Apache ORC™, and other formats. Users can extend Hive with connectors for other formats. Please see File Formats and Hive SerDe in the Developer Guide for details.\nHive is not designed for online transaction processing (OLTP) workloads. It is best used for traditional data warehousing tasks.\nHive is designed to maximize scalability (scale out with more machines added dynamically to the Hadoop cluster), performance, extensibility, fault-tolerance, and loose-coupling with its input formats.\nComponents of Hive include HCatalog and WebHCat.\n HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — including Pig and MapReduce — to more easily read and write data on the grid. WebHCat provides a service that you can use to run Hadoop MapReduce (or YARN), Pig, Hive jobs. You can also perform Hive metadata operations using an HTTP (REST style) interface.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/introduction-to-apache-hive_283118337/","tags":null,"title":"Apache Hive : Introduction to Apache Hive"},{"categories":null,"contents":"Apache Hive : JDBC Storage Handler    Syntax  Table Properties Supported Data Type Column/Type Mapping   Auto Shipping Securing Password Partitioning Computation Pushdown Using a Non-default Schema  MariaDB MS SQL Oracle PostgreSQL    Syntax JdbcStorageHandler supports reading from jdbc data source in Hive. Currently writing to a jdbc data source is not supported. To use JdbcStorageHandler, you need to create an external table using JdbcStorageHandler. Here is a simple example:\nCREATE EXTERNAL TABLE student_jdbc ( name string, age int, gpa double ) STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' TBLPROPERTIES ( \u0026quot;hive.sql.database.type\u0026quot; = \u0026quot;MYSQL\u0026quot;, \u0026quot;hive.sql.jdbc.driver\u0026quot; = \u0026quot;com.mysql.jdbc.Driver\u0026quot;, \u0026quot;hive.sql.jdbc.url\u0026quot; = \u0026quot;jdbc:mysql://localhost/sample\u0026quot;, \u0026quot;hive.sql.dbcp.username\u0026quot; = \u0026quot;hive\u0026quot;, \u0026quot;hive.sql.dbcp.password\u0026quot; = \u0026quot;hive\u0026quot;, \u0026quot;hive.sql.table\u0026quot; = \u0026quot;STUDENT\u0026quot;, \u0026quot;hive.sql.dbcp.maxActive\u0026quot; = \u0026quot;1\u0026quot; ); You can also alter table properties of the jdbc external table using alter table statement, just like other non-native Hive table:\nALTER TABLE student_jdbc SET TBLPROPERTIES (\u0026quot;hive.sql.dbcp.password\u0026quot; = \u0026quot;passwd\u0026quot;); Table Properties In the create table statement, you are required to specify the following table properties:\n hive.sql.database.type: MYSQL, POSTGRES, ORACLE, DERBY, DB2 hive.sql.jdbc.url: jdbc connection string hive.sql.jdbc.driver: jdbc driver class hive.sql.dbcp.username: jdbc user name hive.sql.dbcp.password: jdbc password in clear text, this parameter is strongly discouraged. The recommended way is to store it in a keystore. See the section “securing password” for detail hive.sql.table / hive.sql.query: You will need to specify either “hive.sql.table” or “hive.sql.query” to tell how to get data from jdbc database. “hive.sql.table” denotes a single table, and “hive.sql.query” denotes an arbitrary sql query.  Besides the above required properties, you can also specify optional parameters to tune the connection details and performance:\n hive.sql.catalog: jdbc catalog name (only valid if “hive.sql.table“ is specified) hive.sql.schema: jdbc schema name (only valid if “hive.sql.table“ is specified) hive.sql.jdbc.fetch.size: number of rows to fetch in a batch hive.sql.dbcp.xxx: all dbcp parameters will pass to commons-dbcp. See https://commons.apache.org/proper/commons-dbcp/configuration.html for definition of the parameters. For example, if you specify hive.sql.dbcp.maxActive=1 in table property, Hive will pass maxActive=1 to commons-dbcp  Supported Data Type The column data type for a Hive JdbcStorageHandler table can be:\n Numeric data type: byte, short, int, long, float, double Decimal with scale and precision String date type: string, char, varchar Date Timestamp  Note complex data type: struct, map, array are not supported\nColumn/Type Mapping hive.sql.table / hive.sql.query defines a tabular data with a schema. The schema definition has to be the same as the table schema definition. For example, the following create table statement will fail:\nCREATE EXTERNAL TABLE student_jdbc ( name string, age int, gpa double ) STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' TBLPROPERTIES ( . . . . . . \u0026quot;hive.sql.query\u0026quot; = \u0026quot;SELECT name, age, gpa, gender FROM STUDENT\u0026quot;, ); However, column name and column type of hive.sql.table / hive.sql.query schema may be different than the table schema. In this case, database column maps to hive column by position. If data type is different, Hive will try to convert it according to Hive table schema. For example:\nCREATE EXTERNAL TABLE student_jdbc ( sname string, age int, effective_gpa decimal(4,3) ) STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' TBLPROPERTIES ( . . . . . . \u0026quot;hive.sql.query\u0026quot; = \u0026quot;SELECT name, age, gpa FROM STUDENT\u0026quot;, ); Hive will try to convert the double “gpa” of underlining table STUDENT to decimal(4,3) as the effective_gpa field of the student_jdbc table. In case the conversion is not possible, Hive will produce null for the field.\nAuto Shipping JdbcStorageHandler will ship required jars to MR/Tez/LLAP backend automatically if JdbcStorageHandler is used in the query. User don’t need to add jar manually. JdbcStorageHandler will also ship required jdbc driver jar to the backend if it detects any jdbc driver jar in classpath (include mysql, postgres, oracle and mssql). However, user are still required to copy jdbc driver jar to hive classpath (usually, lib directory in hive).\nSecuring Password In most cases, we don’t want to store jdbc password in clear text in table property \u0026ldquo;hive.sql.dbcp.password\u0026rdquo;. Instead, user can store password in a Java keystore file on HDFS using the following command:\nhadoop credential create host1.password -provider jceks://hdfs/user/foo/test.jceks -v passwd1 hadoop credential create host2.password -provider jceks://hdfs/user/foo/test.jceks -v passwd2 This will create a keystore file located on hdfs://user/foo/test.jceks which contains two keys: host1.password and host2.password. When creating table in Hive, you will need to specify “hive.sql.dbcp.password.keystore” and “hive.sql.dbcp.password.key” instead of “hive.sql.dbcp.password” in create table statement:\nCREATE EXTERNAL TABLE student_jdbc ( name string, age int, gpa double ) STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' TBLPROPERTIES ( . . . . . . \u0026quot;hive.sql.dbcp.password.keystore\u0026quot; = \u0026quot;jceks://hdfs/user/foo/test.jceks\u0026quot;, \u0026quot;hive.sql.dbcp.password.key\u0026quot; = \u0026quot;host1.password\u0026quot;, . . . . . . ); You will need to protect the keystore file by only authorize targeted user to read this file using authorizer (such as ranger). Hive will check the permission of the keystore file to make sure user has read permission of it when creating/altering table.\nPartitioning Hive is able to split the jdbc data source and process each split in parallel. User can use the following table property to decide whether or not to split and how many splits to split into:\n hive.sql.numPartitions: how many split to generate for the data source, 1 if no split hive.sql.partitionColumn: which column to split on. If this is specified, Hive will split the column into hive.sql.numPartitions equal intervals from hive.sql.lowerBound to hive.sql.upperBound. If partitionColumn is not defined but numPartitions \u0026gt; 1, Hive will split the data source using offset. However, offset is not always reliable for some databases. It is highly recommended to define a partitionColumn if you want to split the data source. The partitionColumn must exist in the schema “hive.sql.table”/”hive.sql.query” produces. hive.sql.lowerBound / hive.sql.upperBound: lower/upper bound of the partitionColumn used to calculate the intervals. Both properties are optional. If undefined, Hive will do a MIN/MAX query against the data source to get the lower/upper bound. Note both hive.sql.lowerBound and hive.sql.upperBound cannot be null. The first and last split are open ended. And all null value for the column will go to the first split.  For example:\nTBLPROPERTIES ( . . . . . . \u0026quot;hive.sql.table\u0026quot; = \u0026quot;DEMO\u0026quot;, \u0026quot;hive.sql.partitionColumn\u0026quot; = \u0026quot;num\u0026quot;, \u0026quot;hive.sql.numPartitions\u0026quot; = \u0026quot;3\u0026quot;, \u0026quot;hive.sql.lowerBound\u0026quot; = \u0026quot;1\u0026quot;, \u0026quot;hive.sql.upperBound\u0026quot; = \u0026quot;10\u0026quot;, . . . . . . ); This table will create 3 splits: num\u0026lt;4 or num is null, 4\u0026lt;=num\u0026lt;7, num\u0026gt;=7\nTBLPROPERTIES ( . . . . . . \u0026quot;hive.sql.query\u0026quot; = \u0026quot;SELECT name, age, gpa/5.0*100 AS percentage FROM STUDENT\u0026quot;, \u0026quot;hive.sql.partitionColumn\u0026quot; = \u0026quot;percentage\u0026quot;, \u0026quot;hive.sql.numPartitions\u0026quot; = \u0026quot;4\u0026quot;, . . . . . . ); Hive will do a jdbc query to get the MIN/MAX of the percentage column of the query, which is 60, 100. Then table will create 4 splits: (,70),[70,80),[80,90),[90,). The first split also include null value.\nTo see the splits generated by JdbcStorageHandler, looking for the following messages in hiveserver2 log or Tez AM log:\njdbc.JdbcInputFormat: Num input splits created 4 jdbc.JdbcInputFormat: split:interval:ikey[,70) jdbc.JdbcInputFormat: split:interval:ikey[70,80) jdbc.JdbcInputFormat: split:interval:ikey[80,90) jdbc.JdbcInputFormat: split:interval:ikey[90,) Computation Pushdown Hive will pushdown computation to jdbc table aggressively, so we can make best usage of the native capacity of jdbc data source.\nFor example, if we have another table voter_jdbc:\nEATE EXTERNAL TABLE voter_jdbc ( name string, age int, registration string, contribution decimal(10,2) ) STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' TBLPROPERTIES ( \u0026quot;hive.sql.database.type\u0026quot; = \u0026quot;MYSQL\u0026quot;, \u0026quot;hive.sql.jdbc.driver\u0026quot; = \u0026quot;com.mysql.jdbc.Driver\u0026quot;, \u0026quot;hive.sql.jdbc.url\u0026quot; = \u0026quot;jdbc:mysql://localhost/sample\u0026quot;, \u0026quot;hive.sql.dbcp.username\u0026quot; = \u0026quot;hive\u0026quot;, \u0026quot;hive.sql.dbcp.password\u0026quot; = \u0026quot;hive\u0026quot;, \u0026quot;hive.sql.table\u0026quot; = \u0026quot;VOTER\u0026quot; ); Then the following join operation will push down to mysql:\nselect * from student_jdbc join voter_jdbc on student_jdbc.name=voter_jdbc.name; This can be manifest by explain:\nexplain select * from student_jdbc join voter_jdbc on student_jdbc.name=voter_jdbc.name; . . . . . . TableScan alias: student_jdbc properties: hive.sql.query SELECT `t`.`name`, `t`.`age`, `t`.`gpa`, `t0`.`name` AS `name0`, `t0`.`age` AS `age0`, `t0`.`registration`, `t0`.`contribution` FROM (SELECT * FROM `STUDENT` WHERE `name` IS NOT NULL) AS `t` INNER JOIN (SELECT * FROM `VOTER` WHERE `name` IS NOT NULL) AS `t0` ON `t`.`name` = `t0`.`name` . . . . . . Computation pushdown will only happen when the jdbc table is defined by “hive.sql.table”. Hive will rewrite the data source with a “hive.sql.query” property with more computation on top of the table. In the above example, mysql will run the query and retrieve the join result, rather than fetch both tables and do the join in Hive.\nThe operators can be pushed down include filter, transform, join, union, aggregation and sort.\nThe derived mysql query can be very complex and in many cases we don’t want to split the data source thus run the complex query multiple times on each split. So if the computation is more then just filter and transform, Hive will not split the query result even if “hive.sql.numPartitions” is more than 1.\nUsing a Non-default Schema The notion of schema differs from DBMS to DBMS, such as Oracle, MSSQL, MySQL, and PostgreSQL. Correct usage of the hive.sql.schema table property can prevent problems with client connections to external JDBC tables. For more information, see Hive-25591. To create external tables based on a user-defined schema in a JDBC-compliant database, follow the examples below for respective databases.\nMariaDB CREATE SCHEMA bob; CREATE TABLE bob.country ( id int, name varchar(20) ); insert into bob.country values (1, 'India'); insert into bob.country values (2, 'Russia'); insert into bob.country values (3, 'USA'); CREATE SCHEMA alice; CREATE TABLE alice.country ( id int, name varchar(20) ); insert into alice.country values (4, 'Italy'); insert into alice.country values (5, 'Greece'); insert into alice.country values (6, 'China'); insert into alice.country values (7, 'Japan'); MS SQL CREATE DATABASE world; USE world; CREATE SCHEMA bob; CREATE TABLE bob.country ( id int, name varchar(20) ); insert into bob.country values (1, 'India'); insert into bob.country values (2, 'Russia'); insert into bob.country values (3, 'USA'); CREATE SCHEMA alice; CREATE TABLE alice.country ( id int, name varchar(20) ); insert into alice.country values (4, 'Italy'); insert into alice.country values (5, 'Greece'); insert into alice.country values (6, 'China'); insert into alice.country values (7, 'Japan'); Create a user and associate them with a default schema. For example:\nCREATE LOGIN greg WITH PASSWORD = 'GregPass123!$'; CREATE USER greg FOR LOGIN greg WITH DEFAULT_SCHEMA=bob; Allow the user to connect to the database and run queries. For example:\nGRANT CONNECT, SELECT TO greg; Oracle In Oracle, dividing the tables into different namespaces/schemas is achieved through different users. The CREATE SCHEMA statement exists in Oracle, but has different semantics from those defined by SQL Standard and those adopted in other DBMS.\nTo create \u0026ldquo;local\u0026rdquo; users in Oracle you need to be connected to the Pluggable Database (PDB), not to the Container Database (CDB). The following example was tested in Oracle XE edition, using only PDB XEPDB1.\nALTER SESSION SET CONTAINER = XEPDB1; Create the bob schema/user and give appropriate connections to be able to connect to the database. For example:\nCREATE USER bob IDENTIFIED BY bobpass; ALTER USER bob QUOTA UNLIMITED ON users; GRANT CREATE SESSION TO bob; CREATE TABLE bob.country ( id int, name varchar(20) ); insert into bob.country values (1, 'India'); insert into bob.country values (2, 'Russia'); insert into bob.country values (3, 'USA'); Create the alice schema/user and give appropriate connections to be able to connect to the database. For example:\nCREATE USER alice IDENTIFIED BY alicepass; ALTER USER alice QUOTA UNLIMITED ON users; GRANT CREATE SESSION TO alice; CREATE TABLE alice.country ( id int, name varchar(20) ); insert into alice.country values (4, 'Italy'); insert into alice.country values (5, 'Greece'); insert into alice.country values (6, 'China'); insert into alice.country values (7, 'Japan'); Without the SELECT ANY privilege, a user cannot see the tables/views of another user. When a user connects to the database using a specific user and schema it is not possible to refer to tables in another user/schema\n\u0026ndash; namespace. You need to grant the SELECT ANY privilege. For example:\nGRANT SELECT ANY TABLE TO bob; GRANT SELECT ANY TABLE TO alice; Allow the users to perform inserts on any table/view in the database, not only those present on their own schema. For example:\nGRANT INSERT ANY TABLE TO bob; GRANT INSERT ANY TABLE TO alice; PostgreSQL CREATE SCHEMA bob; CREATE TABLE bob.country ( id int, name varchar(20) ); insert into bob.country values (1, 'India'); insert into bob.country values (2, 'Russia'); insert into bob.country values (3, 'USA'); CREATE SCHEMA alice; CREATE TABLE alice.country ( id int, name varchar(20) ); insert into alice.country values (4, 'Italy'); insert into alice.country values (5, 'Greece'); insert into alice.country values (6, 'China'); insert into alice.country values (7, 'Japan'); Create a user and associate them with a default schema \u0026lt;=\u0026gt; search_path. For example:\nCREATE ROLE greg WITH LOGIN PASSWORD 'GregPass123!$'; ALTER ROLE greg SET search_path TO bob; Grant the necessary permissions to access the schema. For example:\nGRANT USAGE ON SCHEMA bob TO greg; GRANT SELECT ON ALL TABLES IN SCHEMA bob TO greg; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/jdbc-storage-handler_95651916/","tags":null,"title":"Apache Hive : JDBC Storage Handler"},{"categories":null,"contents":"Apache Hive : Kudu Integration Hive Kudu Integration  Hive Kudu Integration  Overview Implementation Hive Configuration Table Creation Impala Tables Data Ingest  Examples      Overview Apache Kudu is a an Open Source data storage engine that makes fast analytics on fast and changing data easy. Implementation The initial implementation was added to Hive 4.0 in HIVE-12971 and is designed to work with Kudu 1.2+.\nThere are two main components which make up the implementation: the KuduStorageHandler and the KuduPredicateHandler. The KuduStorageHandler is a Hive StorageHandler implementation. The primary roles of this class are to manage the mapping of a Hive table to a Kudu table and configures Hive queries. The KuduPredicateHandler is used push down filter operations to Kudu for more efficient IO.\nNOTE: The initial implementation is considered ***experimental*** as there are remaining sub-jiras open to make the implementation more configurable and performant. Currently only external tables pointing at existing Kudu tables are supported. Support for creating and altering underlying Kudu tables in tracked via HIVE-22021. Additionally full support for UPDATE, UPSERT, and DELETE statement support is tracked by HIVE-22027.\nHive Configuration To issue queries against Kudu using Hive, one optional parameter can be provided by the Hive configuration:\n   Hive Configuration      hive.kudu.master.addresses.default Comma-separated list of all of the Kudu master addresses.This value is only used for a given table if the kudu.master_addresses table property is not set.    For those familiar with Kudu, the master addresses configuration is the normal configuration value necessary to connect to Kudu. The easiest way to provide this value is by using the -hiveconf option to the hive command. hive -hiveconf hive.kudu.master.addresses.default=localhost:7051 Table Creation To access Kudu tables, a Hive table must be created using the CREATE command with the STORED BY clause. Until HIVE-22021 is completed, the EXTERNAL keyword is required and will create a Hive table that references an existing Kudu table. Dropping the external Hive table will not remove the underlying Kudu table.\nCREATE EXTERNAL TABLE kudu_table (foo INT, bar STRING, baz DOUBLE) STORED BY 'org.apache.hadoop.hive.kudu.KuduStorageHandler' TBLPROPERTIES ( \u0026quot;kudu.table_name\u0026quot;=\u0026quot;default.kudu_table\u0026quot;, \u0026quot;kudu.master_addresses\u0026quot;=\u0026quot;localhost:7051\u0026quot; ); In the above statement, normal Hive column name and type pairs are provided as is the case with normal create table statements. The full KuduStorageHandler class name is provided to inform Hive that Kudu will back this Hive table. A number of TBLPROPERTIES can be provided to configure the KuduStorageHandler. The most important property is kudu.table_name which tells hive which Kudu table it should reference. The other common property is kudu.master_addresses which configures the Kudu master addresses for this table. If the kudu.master_addresses property is not provided, the hive.kudu.master.addresses.default configuration will be used. Impala Tables Because Impala creates tables with the same storage handler metadata in the HiveMetastore, tables created or altered via Impala DDL can be accessed from Hive. This is especially useful until HIVE-22021 is complete and full DDL support is available through Hive. See the Kudu documentation and the Impala documentation for more details.\nData Ingest Though it is a common practice to ingest the data into Kudu tables via tools like Apache NiFi or Apache Spark and query the data via Hive, data can also be inserted to the Kudu tables via Hive INSERT statements. It is important to note that when data is inserted a Kudu UPSERT operation is actually used to avoid primary key constraint issues. Making this more flexible is tracked via HIVE-22024. Additionally UPDATE and DELETE operations are not supported. Enabling that functionality is tracked via HIVE-22027.\nExamples INSERT INTO kudu_table SELECT * FROM other_table; INSERT INTO TABLE kudu_table VALUES (1, 'test 1', 1.1), (2, 'test 2', 2.2); ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/kudu-integration_133631955/","tags":null,"title":"Apache Hive : Kudu Integration"},{"categories":null,"contents":"Apache Hive : LanguageManual This is the Hive Language Manual. For other Hive documentation, see the Hive wiki\u0026rsquo;s Home page.\n  Commands and CLIs\n Commands Hive CLI (old) Beeline CLI (new) Variable Substitution HCatalog CLI    File Formats\n Avro Files ORC Files Parquet Compressed Data Storage LZO Compression    Data Types\n  Data Definition Statements\n DDL Statements  Bucketed Tables   Statistics (Analyze and Describe) Indexes Archiving    Data Manipulation Statements\n DML: Load, Insert, Update, Delete Import/Export    Data Retrieval: Queries\n Select  Group By Sort/Distribute/Cluster/Order By Transform and Map-Reduce Scripts Operators and User-Defined Functions (UDFs) XPath-specific Functions Joins Join Optimization Union Lateral View   Sub Queries Sampling Virtual Columns Windowing and Analytics Functions Enhanced Aggregation, Cube, Grouping and Rollup Procedural Language: Hive HPL/SQL Explain Execution Plan    Locks\n  Authorization\n Storage Based Authorization SQL Standard Based Authorization Hive deprecated authorization mode / Legacy Mode    Configuration Properties\n  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual_27362030/","tags":null,"title":"Apache Hive : LanguageManual"},{"categories":null,"contents":"Apache Hive : LanguageManual Archiving Archiving for File Count Reduction Note: Archiving should be considered an advanced command due to the caveats involved.\n Archiving for File Count Reduction  Overview Settings Usage  Archive Unarchive   Cautions and Limitations Under the Hood    Overview Due to the design of HDFS, the number of files in the filesystem directly affects the memory consumption in the namenode. While normally not a problem for small clusters, memory usage may hit the limits of accessible memory on a single machine when there are \u0026gt;50-100 million files. In such situations, it is advantageous to have as few files as possible.\nThe use of Hadoop Archives is one approach to reducing the number of files in partitions. Hive has built-in support to convert files in existing partitions to a Hadoop Archive (HAR) so that a partition that may once have consisted of 100\u0026rsquo;s of files can occupy just ~3 files (depending on settings). However, the trade-off is that queries may be slower due to the additional overhead in reading from the HAR.\nNote that archiving does NOT compress the files – HAR is analogous to the Unix tar command.\nSettings There are 3 settings that should be configured before archiving is used. (Example values are shown.)\nhive\u0026gt; set hive.archive.enabled=true; hive\u0026gt; set hive.archive.har.parentdir.settable=true; hive\u0026gt; set har.partfile.size=1099511627776; hive.archive.enabled controls whether archiving operations are enabled.\nhive.archive.har.parentdir.settable informs Hive whether the parent directory can be set while creating the archive. In recent versions of Hadoop the -p option can specify the root directory of the archive. For example, if /dir1/dir2/file is archived with /dir1 as the parent directory, then the resulting archive file will contain the directory structure dir2/file. In older versions of Hadoop (prior to 2011), this option was not available and therefore Hive must be configured to accommodate this limitation.\nhar.partfile.size controls the size of the files that make up the archive. The archive will contain *size_of_partition*``/``har.partfile.size files, rounded up. Higher values mean fewer files, but will result in longer archiving times due to the reduced number of mappers.\nUsage Archive Once the configuration values are set, a partition can be archived with the command:\nALTER TABLE table_name ARCHIVE PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...) For example:\nALTER TABLE srcpart ARCHIVE PARTITION(ds='2008-04-08', hr='12') Once the command is issued, a mapreduce job will perform the archiving. Unlike Hive queries, there is no output on the CLI to indicate process.\nUnarchive The partition can be reverted back to its original files with the unarchive command:\nALTER TABLE srcpart UNARCHIVE PARTITION(ds='2008-04-08', hr='12') Cautions and Limitations  In some older versions of Hadoop, HAR had a few bugs that could cause data loss or other errors. Be sure that these patches are integrated into your version of Hadoop:  https://issues.apache.org/jira/browse/HADOOP-6591 (fixed in Hadoop 0.21.0)\nhttps://issues.apache.org/jira/browse/MAPREDUCE-1548 (fixed in Hadoop 0.22.0)\nhttps://issues.apache.org/jira/browse/MAPREDUCE-2143 (fixed in Hadoop 0.22.0)\nhttps://issues.apache.org/jira/browse/MAPREDUCE-1752 (fixed in Hadoop 0.23.0)\n The HarFileSystem class still has a bug that has yet to be fixed:  https://issues.apache.org/jira/browse/MAPREDUCE-1877 (moved to https://issues.apache.org/jira/browse/HADOOP-10906 in 2014)\nHive comes with the HiveHarFileSystem class that addresses some of these issues, and is by default the value for fs.har.impl. Keep this in mind if you\u0026rsquo;re rolling your own version of HarFileSystem:\n The default HiveHarFileSystem.getFileBlockLocations() has no locality. That means it may introduce higher network loads or reduced performance. Archived partitions cannot be overwritten with INSERT OVERWRITE. The partition must be unarchived first. If two processes attempt to archive the same partition at the same time, bad things could happen. (Need to implement concurrency support.)  Under the Hood Internally, when a partition is archived, a HAR is created using the files from the partition\u0026rsquo;s original location (such as /warehouse/table/ds=1). The parent directory of the partition is specified to be the same as the original location and the resulting archive is named \u0026lsquo;data.har\u0026rsquo;. The archive is moved under the original directory (such as /warehouse/table/ds=1/data.har), and the partition\u0026rsquo;s location is changed to point to the archive.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-archiving_27362031/","tags":null,"title":"Apache Hive : LanguageManual Archiving"},{"categories":null,"contents":"Apache Hive : LanguageManual Authorization Hive Authorization\n Introduction Hive Authorization Options  Use Cases Overview of Authorization Modes  1 Storage Based Authorization in the Metastore Server  Fall Back Authorizer   2 SQL Standards Based Authorization in HiveServer2 3 Authorization using Apache Ranger \u0026amp; Sentry 4 Old default Hive Authorization (Legacy Mode)   Addressing Authorization Needs of Multiple Use Cases   Explain Authorization More Information  Introduction Note that this documentation is referring to Authorization which is verifying if a user has permission to perform a certain action, and not about Authentication (verifying the identity of the user). Strong authentication for tools like the Hive command line is provided through the use of Kerberos. There are additional authentication options for users of HiveServer2.\nHive Authorization Options Three modes of Hive authorization are available to satisfy different use cases.\nUse Cases It is useful to think of authorization in terms of two primary use cases of Hive.  Hive as a table storage layer. This is the use case for Hive\u0026rsquo;s HCatalog API users such as Apache Pig, MapReduce and some Massively Parallel Processing databases (Cloudera Impala, Facebook Presto, Spark SQL etc). In this case, Hive provides a table abstraction and metadata for files on storage (typically HDFS). These users have direct access to HDFS and the metastore server (which provides an API for metadata access). HDFS access is authorized through the use of HDFS permissions. Metadata access needs to be authorized using Hive configuration. Hive as a SQL query engine. This is one of the most common use cases of Hive. This is the \u0026lsquo;Hive view\u0026rsquo; of SQL users and BI tools. This use case has the following two subcategories:  Hive command line users. These users have direct access to HDFS and the Hive metastore, which makes this use case similar to use case 1. Note, that usage of Hive CLI will be officially deprecated soon in favor of Beeline. ODBC/JDBC and other HiveServer2 API users (Beeline CLI is an example). These users have all data/metadata access happening through HiveServer2. They don\u0026rsquo;t have direct access to HDFS or the metastore.    Overview of Authorization Modes 1 Storage Based Authorization in the Metastore Server In use cases 1 and 2a, the users have direct access to the data. Hive configurations don\u0026rsquo;t control the data access. The HDFS permissions act as one source of truth for the table storage access. By enabling Storage Based Authorization in the Metastore Server, you can use this single source for truth and have a consistent data and metadata authorization policy. To control metadata access on the metadata objects such as Databases, Tables and Partitions, it checks if you have permission on corresponding directories on the file system. You can also protect access through HiveServer2 (use case 2b above) by ensuring that the queries run as the end user (hive.server2.enable.doAs option should be \u0026ldquo;true\u0026rdquo; in HiveServer2 configuration – this is a default value).\nNote, that through the use of HDFS ACL (available in Hadoop 2.4 onwards) you have a lot of flexibility in controlling access to the file system, which in turn provides more flexibility with Storage Based Authorization. This functionality is available as of Hive 0.14 (HIVE-7583).\nWhile relying on Storage based authorization for restricting access, you still need to enable one of the security options 2 or 3 listed below or use FallbackHiveAuthorizer to protect actions within the HiveServer2 instance.\nFall Back Authorizer You need to use Hive 2.3.4 or 3.1.1 or later to use Fall Back Authorizer.\nAdmin needs to specify the following entries in hiveserver2-site.xml: \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.enabled\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.authorization.manager\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hive.ql.security.authorization.plugin.fallback.FallbackHiveAuthorizerFactory\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; FallbackHiveAuthorizerFactory will do the following to mitigate above mentioned threat:\n Disallow local file location in sql statements except for admin Allow \u0026ldquo;set\u0026rdquo; only selected whitelist parameters Disallow dfs commands except for admin Disallow \u0026ldquo;ADD JAR\u0026rdquo; statement Disallow \u0026ldquo;COMPILE\u0026rdquo; statement Disallow \u0026ldquo;TRANSFORM\u0026rdquo; statement  2 SQL Standards Based Authorization in HiveServer2 Although Storage Based Authorization can provide access control at the level of Databases, Tables and Partitions, it can not control authorization at finer levels such as columns and views because the access control provided by the file system is at the level of directory and files. A prerequisite for fine grained access control is a data server that is able to provide just the columns and rows that a user needs (or has) access to. In the case of file system access, the whole file is served to the user. HiveServer2 satisfies this condition, as it has an API that understands rows and columns (through the use of SQL), and is able to serve just the columns and rows that your SQL query asked for.\nSQL Standards Based Authorization (introduced in Hive 0.13.0, HIVE-5837) can be used to enable fine grained access control. It is based on the SQL standard for authorization, and uses the familiar grant/revoke statements to control access. It needs to be enabled through HiveServer2 configuration. Note that for use case 2a (Hive command line) SQL Standards Based Authorization is disabled. This is because secure access control is not possible for the Hive command line using an access control policy in Hive, because users have direct access to HDFS and so they can easily bypass the SQL standards based authorization checks or even disable it altogether. Disabling this avoids giving a false sense of security to users.\n3 Authorization using Apache Ranger \u0026amp; Sentry Apache Ranger and Apache Sentry are apache projects that use plugins provided by hive to do authorization.\nThe policies are maintained under repositories under those projects.\nYou also get many advanced features using them. For example, with Ranger you can view and manage policies through web interface, view auditing information, have dynamic row and column level access control (including column masking) based on runtime attributes.\n4 Old default Hive Authorization (Legacy Mode) Hive Old Default Authorization (was default before Hive 2.0.0) is the authorization mode that has been available in earlier versions of Hive. However, this mode does not have a complete access control model, leaving many security gaps unaddressed. For example, the permissions needed to grant privileges for a user are not defined, and any user can grant themselves access to a table or database.\nThis model is similar to the SQL standards based authorization mode, in that it provides grant/revoke statement-based access control. However, the access control policy is different from SQL standards based authorization, and they are not compatible. Use of this mode is also supported for Hive command line users. However, for reasons mentioned under the discussion of SQL standards based authorization (above), it is not a secure mode of authorization for the Hive command line.\nAddressing Authorization Needs of Multiple Use Cases Storage based authorization provides a simple way to address all the use cases described above. However, if you need finer grained access control for SQL users, you can also enable SQL standards based authorization mode in HiveServer2.\nThat is, you can have storage based authorization enabled for metastore API calls (in the Hive metastore) and have SQL standards based authorization enabled in HiveServer2 at the same time.\nExplain Authorization Version 0.14 — EXPLAIN AUTHORIZATION\nStarting in Hive 0.14.0, the HiveQL command EXPLAIN AUTHORIZATION shows all entities that need to be authorized to execute a query, as well as any authorization failures.\nMore Information For detailed information about the Hive authorization modes, see:\n Storage Based Authorization in the Metastore Server  also see HCatalog Authorization   SQL Standard Based Hive Authorization Hive deprecated authorization mode / Legacy Mode  also see the design document and Security    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-authorization_27362032/","tags":null,"title":"Apache Hive : LanguageManual Authorization"},{"categories":null,"contents":"Apache Hive : LanguageManual Cli Hive CLI  Hive CLI Deprecation in favor of Beeline CLI  Hive Command Line Options  Examples The hiverc File Logging Tool to Clear Dangling Scratch Directories   Hive Batch Mode Commands Hive Interactive Shell Commands  Hive Resources     HCatalog CLI  $HIVE_HOME/bin/hive is a shell utility which can be used to run Hive queries in either interactive or batch mode.\nDeprecation in favor of Beeline CLI HiveServer2 (introduced in Hive 0.11) has its own CLI called Beeline, which is a JDBC client based on SQLLine. Due to new development being focused on HiveServer2, Hive CLI will soon be deprecated in favor of Beeline (HIVE-10511).\nSee Replacing the Implementation of Hive CLI Using Beeline and Beeline – New Command Line Shell in the HiveServer2 documentation.\nHive Command Line Options To get help, run \u0026ldquo;hive -H\u0026rdquo; or \u0026ldquo;hive --help\u0026rdquo;.\nUsage (as it is in Hive 0.9.0):\nusage: hive -d,--define \u0026lt;key=value\u0026gt; Variable substitution to apply to Hive commands. e.g. -d A=B or --define A=B -e \u0026lt;quoted-query-string\u0026gt; SQL from command line -f \u0026lt;filename\u0026gt; SQL from files -H,--help Print help information -h \u0026lt;hostname\u0026gt; Connecting to Hive Server on remote host --hiveconf \u0026lt;property=value\u0026gt; Use value for given property --hivevar \u0026lt;key=value\u0026gt; Variable substitution to apply to hive commands. e.g. --hivevar A=B -i \u0026lt;filename\u0026gt; Initialization SQL file -p \u0026lt;port\u0026gt; Connecting to Hive Server on port number -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) Version information\nAs of Hive 0.10.0 there is one additional command line option:\n--database \u0026lt;dbname\u0026gt; Specify the database to use Note: The variant \u0026ldquo;-hiveconf\u0026rdquo; is supported as well as \u0026ldquo;--hiveconf\u0026rdquo;.\nExamples See Variable Substitution for examples of using the hiveconf option.\n Example of running a query from the command line  $HIVE_HOME/bin/hive -e 'select a.col from tab1 a'  Example of setting Hive configuration variables  $HIVE_HOME/bin/hive -e 'select a.col from tab1 a' --hiveconf hive.exec.scratchdir=/home/my/hive_scratch --hiveconf mapred.reduce.tasks=32  Example of dumping data out from a query into a file using silent mode  $HIVE_HOME/bin/hive -S -e 'select a.col from tab1 a' \u0026gt; a.txt  Example of running a script non-interactively from local disk  $HIVE_HOME/bin/hive -f /home/my/hive-script.sql  Example of running a script non-interactively from a Hadoop supported filesystem (starting in Hive 0.14)  $HIVE_HOME/bin/hive -f hdfs://\u0026lt;namenode\u0026gt;:\u0026lt;port\u0026gt;/hive-script.sql $HIVE_HOME/bin/hive -f s3://mys3bucket/s3-script.sql  Example of running an initialization script before entering interactive mode  $HIVE_HOME/bin/hive -i /home/my/hive-init.sql The hiverc File The CLI when invoked without the -i option will attempt to load $HIVE_HOME/bin/.hiverc and $HOME/.hiverc as initialization files.\nLogging Hive uses log4j for logging. These logs are not emitted to the standard output by default but are instead captured to a log file specified by Hive\u0026rsquo;s log4j properties file. By default Hive will use hive-log4j.default in the conf/ directory of the Hive installation which writes out logs to /tmp/\u0026lt;userid\u0026gt;/hive.log and uses the WARN level.\nIt is often desirable to emit the logs to the standard output and/or change the logging level for debugging purposes. These can be done from the command line as follows:\n $HIVE_HOME/bin/hive --hiveconf hive.root.logger=INFO,console hive.root.logger specifies the logging level as well as the log destination. Specifying console as the target sends the logs to the standard error (instead of the log file).\nSee Hive Logging in Getting Started for more information.\nTool to Clear Dangling Scratch Directories See Scratch Directory Management in Setting Up HiveServer2 for information about scratch directories and a command-line tool for removing dangling scratch directories that can be used in the Hive CLI as well as HiveServer2.\nHive Batch Mode Commands When $HIVE_HOME/bin/hive is run with the -e or -f option, it executes SQL commands in batch mode.\n hive -e '\u0026lt;query-string\u0026gt;' executes the query string. hive -f \u0026lt;filepath\u0026gt; executes one or more SQL queries from a file.  Version 0.14\nAs of Hive 0.14, can be from one of the Hadoop supported filesystems (HDFS, S3, etc.) as well.\n$HIVE_HOME/bin/hive -f hdfs://\u0026lt;namenode\u0026gt;:\u0026lt;port\u0026gt;/hive-script.sql``$HIVE_HOME/bin/hive -f s3://mys3bucket/s3-script.sql\nSee HIVE-7136 for more details.\nHive Interactive Shell Commands When $HIVE_HOME/bin/hive is run without either the -e or -f option, it enters interactive shell mode.\nUse \u0026ldquo;;\u0026rdquo; (semicolon) to terminate commands. Comments in scripts can be specified using the \u0026ldquo;\u0026ndash;\u0026rdquo; prefix.\n   Command Description     quit exit Use quit or exit to leave the interactive shell.   reset Resets the configuration to the default values (as of Hive 0.10: see HIVE-3202).   set = Sets the value of a particular configuration variable (key). Note: If you misspell the variable name, the CLI will not show an error.   set Prints a list of configuration variables that are overridden by the user or Hive.   set -v Prints all Hadoop and Hive configuration variables.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * Adds one or more files, jars, or archives to the list of resources in the distributed cache. See Hive Resources below for more information.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * As of Hive 1.2.0, adds one or more files, jars or archives to the list of resources in the distributed cache using an Ivy URL of the form ivy://group:module:version?query_string. See Hive Resources below for more information.   list FILE[S] list JAR[S] list ARCHIVE[S] Lists the resources already added to the distributed cache. See Hive Resources below for more information.   list FILE[S] * list JAR[S] * list ARCHIVE[S] * Checks whether the given resources are already added to the distributed cache or not. See Hive Resources below for more information.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * Removes the resource(s) from the distributed cache.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * As of Hive 1.2.0, removes the resource(s) which were added using the from the distributed cache. See Hive Resources below for more information.   !  Executes a shell command from the Hive shell.   dfs  Executes a dfs command from the Hive shell.    Executes a Hive query and prints results to standard output.   source  Executes a script file inside the CLI.    Sample Usage:\n hive\u0026gt; set mapred.reduce.tasks=32; hive\u0026gt; set; hive\u0026gt; select a.* from tab1; hive\u0026gt; !ls; hive\u0026gt; dfs -ls; Hive Resources Hive can manage the addition of resources to a session where those resources need to be made available at query execution time. The resources can be files, jars, or archives. Any locally accessible file can be added to the session.\nOnce a resource is added to a session, Hive queries can refer to it by its name (in map/reduce/transform clauses) and the resource is available locally at execution time on the entire Hadoop cluster. Hive uses Hadoop\u0026rsquo;s Distributed Cache to distribute the added resources to all the machines in the cluster at query execution time.\nUsage:\n ADD { FILE[S] | JAR[S] | ARCHIVE[S] } \u0026lt;filepath1\u0026gt; [\u0026lt;filepath2\u0026gt;]* LIST { FILE[S] | JAR[S] | ARCHIVE[S] } [\u0026lt;filepath1\u0026gt; \u0026lt;filepath2\u0026gt; ..] DELETE { FILE[S] | JAR[S] | ARCHIVE[S] } [\u0026lt;filepath1\u0026gt; \u0026lt;filepath2\u0026gt; ..]  FILE resources are just added to the distributed cache. Typically, this might be something like a transform script to be executed. JAR resources are also added to the Java classpath. This is required in order to reference objects they contain such as UDFs. See Hive Plugins for more information about custom UDFs. ARCHIVE resources are automatically unarchived as part of distributing them.  Example:\n hive\u0026gt; add FILE /tmp/tt.py; hive\u0026gt; list FILES; /tmp/tt.py hive\u0026gt; select from networks a MAP a.networkid USING 'python tt.py' as nn where a.ds = '2009-01-04' limit 10;   Version 1.2.0\nAs of Hive 1.2.0, resources can be added and deleted using Ivy URLs of the form ivy://group:module:version?query_string.\n group – Which module group the module comes from. Translates directly to a Maven groupId or an Ivy Organization. module – The name of the module to load. Translates directly to a Maven artifactId or an Ivy artifact. version – The version of the module to use. Any version or * (for latest) or an Ivy Range can be used.  Various parameters can be passed in the query_string to configure how and which jars are added to the artifactory. The parameters are in the form of key value pairs separated by \u0026lsquo;\u0026amp;\u0026rsquo;.\nUsage:\nADD { FILE[S] | JAR[S] | ARCHIVE[S] } \u0026lt;ivy://org:module:version?key=value\u0026amp;key=value\u0026amp;...\u0026gt; \u0026lt;ivy://org:module:version?key=value\u0026amp;key1=value1\u0026amp;...\u0026gt;* DELETE { FILE[S] | JAR[S] | ARCHIVE[S] } \u0026lt;ivy://org:module:version\u0026gt; \u0026lt;ivy://org:module:version\u0026gt;* Also, we can mix and in the same ADD and DELETE commands.\nADD { FILE[S] | JAR[S] | ARCHIVE[S] } { \u0026lt;ivyurl\u0026gt; | \u0026lt;filepath\u0026gt; } \u0026lt;ivyurl\u0026gt;* \u0026lt;filepath\u0026gt;* DELETE { FILE[S] | JAR[S] | ARCHIVE[S] } { \u0026lt;ivyurl\u0026gt; | \u0026lt;filepath\u0026gt; } \u0026lt;ivyurl\u0026gt;* \u0026lt;filepath\u0026gt;* The different parameters that can be passed are:\n exclude: Takes a comma separated value of the form org:module. transitive: Takes values true or false. Defaults to true. When transitive = true, all the transitive dependencies are downloaded and added to the classpath. ext: The extension of the file to add. \u0026lsquo;jar\u0026rsquo; by default. classifier: The maven classifier to resolve by.  Examples:\nhive\u0026gt;ADD JAR ivy://org.apache.pig:pig:0.10.0?exclude=org.apache.hadoop:avro; hive\u0026gt;ADD JAR ivy://org.apache.pig:pig:0.10.0?exclude=org.apache.hadoop:avro\u0026amp;transitive=false; The DELETE command will delete the resource and all its transitive dependencies unless some dependencies are shared by other resources. If two resources share a set of transitive dependencies and one of the resources is deleted using the DELETE syntax, then all the transitive dependencies will be deleted for the resource except the ones which are shared.\nExamples:\nhive\u0026gt;ADD JAR ivy://org.apache.pig:pig:0.10.0 hive\u0026gt;ADD JAR ivy://org.apache.pig:pig:0.11.1.15 hive\u0026gt;DELETE JAR ivy://org.apache.pig:pig:0.10.0 If A is the set containing the transitive dependencies of pig-0.10.0 and B is the set containing the transitive dependencies of pig-0.11.1.15, then after executing the above commands, A-(A intersection B) will be deleted.\nSee HIVE-9664 for more details.\nIt is not neccessary to add files to the session if the files used in a transform script are already available on all machines in the Hadoop cluster using the same path name. For example:\n ... MAP a.networkid USING 'wc -l' ...\nHere wc is an executable available on all machines. ... MAP a.networkid USING '/home/nfsserv1/hadoopscripts/tt.py' ...\nHere tt.py may be accessible via an NFS mount point that\u0026rsquo;s configured identically on all the cluster nodes.  Note that Hive configuration parameters can also specify jars, files, and archives. See Configuration Variables for more information.\nHCatalog CLI Version\nHCatalog is installed with Hive, starting with Hive release 0.11.0.\nMany (but not all) hcat commands can be issued as hive commands, and vice versa. See the HCatalog Command Line Interface document in the HCatalog manual for more information.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-cli_27362033/","tags":null,"title":"Apache Hive : LanguageManual Cli"},{"categories":null,"contents":"Apache Hive : LanguageManual Commands Commands are non-SQL statements such as setting a property or adding a resource. They can be used in HiveQL scripts or directly in the CLI or Beeline.\n   Command Description     quit exit Use quit or exit to leave the interactive shell.   reset Resets the configuration to the default values (as of Hive 0.10: see HIVE-3202). Any configuration parameters that were set using the set command or -hiveconf parameter in hive commandline will get reset to default value.Note that this does not apply to configuration parameters that were set in set command using the \u0026ldquo;hiveconf:\u0026rdquo; prefix for the key name (for historic reasons).   set = Sets the value of a particular configuration variable (key). Note: If you misspell the variable name, the CLI will not show an error.   set Prints a list of configuration variables that are overridden by the user or Hive.   set -v Prints all Hadoop and Hive configuration variables.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * Adds one or more files, jars, or archives to the list of resources in the distributed cache. See Hive Resources for more information.   add FILE[S] * add JAR[S] * add ARCHIVE[S]* As of Hive 1.2.0, adds one or more files, jars or archives to the list of resources in the distributed cache using an Ivy URL of the form ivy://group:module:version?query_string. See Hive Resources for more information.   list FILE[S] list JAR[S] list ARCHIVE[S] Lists the resources already added to the distributed cache. See Hive Resources for more information.   list FILE[S] * list JAR[S] * list ARCHIVE[S] * Checks whether the given resources are already added to the distributed cache or not. See Hive Resources for more information.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * Removes the resource(s) from the distributed cache.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * As of Hive 1.2.0, removes the resource(s) which were added using the from the distributed cache. See Hive Resources for more information.   !  Executes a shell command from the Hive shell.   dfs  Executes a dfs command from the Hive shell.    Executes a Hive query and prints results to standard output.   source FILE  Executes a script file inside the CLI.   compile \u0026lt;groovy string\u0026gt; AS GROOVY NAMED  This allows inline Groovy code to be compiled and be used as a UDF (as of Hive 0.13.0). For a usage example, see Nov. 2013 Hive Contributors Meetup Presentations – Using Dynamic Compilation with Hive.   show processlist Displays information about the operations currently running on HiveServer2. It helps to troubleshoot issues such as long running queries, connection starvation, etc. The command was introduced in HIVE-27829.    Sample Usage:\n hive\u0026gt; set mapred.reduce.tasks=32; hive\u0026gt; set; hive\u0026gt; select a.* from tab1; hive\u0026gt; !ls; hive\u0026gt; dfs -ls; 0: jdbc:hive2://localhost:10000\u0026gt; show processlist; +------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+ | User Name | Ip Addr | Execution Engine | Session Id | Session Active Time (s) | Session Idle Time (s) | Query ID | State | Opened Timestamp | Elapsed Time (s) | Runtime (s) | +------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+ | hive | 127.0.0.1 | mr | 66df357a-90bf-43cb-847f-279aa6df1c24 | 113 | 7 | rtrivedi_20240709124106_d0f00d7a-6fab-4fcd-9f41-d53bb296275d | RUNNING | 1720546866774 | 16 | Not finished | | hive | 127.0.0.1 | mr | 7daa873e-bb46-462e-bc38-94cb8d3e7c17 | 83 | 29 | rtrivedi_20240709124106_2dc53c4c-e522-4aed-a969-3d48ac01ba81 | RUNNING | 1720546866774 | 17 | Not finished | +------------+------------+-------------------+---------------------------------------+--------------------------+------------------------+----------------------------------------------------+----------+-------------------+-------------------+---------------+ ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-commands_34838882/","tags":null,"title":"Apache Hive : LanguageManual Commands"},{"categories":null,"contents":"Apache Hive : LanguageManual DDL Hive Data Definition Language\n Overview Keywords, Non-reserved Keywords and Reserved Keywords Create/Drop/Alter/Use Database Create/Drop/Alter Connector Create/Drop/Truncate Table Alter Table/Partition/Column Create/Drop/Alter View Create/Drop/Alter Materialized View Create/Drop/Alter Index Create/Drop Macro Create/Drop/Reload Function Create/Drop/Grant/Revoke Roles and Privileges Show Describe Abort    Scheduled queries\n  Datasketches integration\n  HCatalog and WebHCat DDL\n  Overview HiveQL DDL statements are documented here, including:\n CREATE DATABASE/SCHEMA, TABLE, VIEW, FUNCTION, INDEX DROP DATABASE/SCHEMA, TABLE, VIEW, INDEX TRUNCATE TABLE ALTER DATABASE/SCHEMA, TABLE, VIEW MSCK REPAIR TABLE (or ALTER TABLE RECOVER PARTITIONS) SHOW DATABASES/SCHEMAS, TABLES, TBLPROPERTIES, VIEWS, PARTITIONS, FUNCTIONS, INDEX[ES], COLUMNS, CREATE TABLE DESCRIBE DATABASE/SCHEMA, table_name, view_name, materialized_view_name  PARTITION statements are usually options of TABLE statements, except for SHOW PARTITIONS.\nKeywords, Non-reserved Keywords and Reserved Keywords     All Keywords     Version Non-reserved Keywords   Hive 1.2.0 ADD, ADMIN, AFTER, ANALYZE, ARCHIVE, ASC, BEFORE, BUCKET, BUCKETS, CASCADE, CHANGE, CLUSTER, CLUSTERED, CLUSTERSTATUS, COLLECTION, COLUMNS, COMMENT, COMPACT, COMPACTIONS, COMPUTE, CONCATENATE, CONTINUE, DATA, DATABASES, DATETIME, DAY, DBPROPERTIES, DEFERRED, DEFINED, DELIMITED, DEPENDENCY, DESC, DIRECTORIES, DIRECTORY, DISABLE, DISTRIBUTE, ENABLE, ESCAPED, EXCLUSIVE, EXPLAIN, EXPORT, FIELDS, FILE, FILEFORMAT, FIRST, FORMAT, FORMATTED, FUNCTIONS, HOLD_DDLTIME, HOUR, IDXPROPERTIES, IGNORE, INDEX, INDEXES, INPATH, INPUTDRIVER, INPUTFORMAT, ITEMS, JAR, KEYS, LIMIT, LINES, LOAD, LOCATION, LOCK, LOCKS, LOGICAL, LONG, MAPJOIN, MATERIALIZED, METADATA, MINUS, MINUTE, MONTH, MSCK, NOSCAN, NO_DROP, OFFLINE, OPTION, OUTPUTDRIVER, OUTPUTFORMAT, OVERWRITE, OWNER, PARTITIONED, PARTITIONS, PLUS, PRETTY, PRINCIPALS, PROTECTION, PURGE, READ, READONLY, REBUILD, RECORDREADER, RECORDWRITER, REGEXP, RELOAD, RENAME, REPAIR, REPLACE, REPLICATION, RESTRICT, REWRITE, RLIKE, ROLE, ROLES, SCHEMA, SCHEMAS, SECOND, SEMI, SERDE, SERDEPROPERTIES, SERVER, SETS, SHARED, SHOW, SHOW_DATABASE, SKEWED, SORT, SORTED, SSL, STATISTICS, STORED, STREAMTABLE, STRING, STRUCT, TABLES, TBLPROPERTIES, TEMPORARY, TERMINATED, TINYINT, TOUCH, TRANSACTIONS, UNARCHIVE, UNDO, UNIONTYPE, UNLOCK, UNSET, UNSIGNED, URI, USE, UTC, VIEW, WHILE, YEAR   Hive 2.0.0 removed: HOLD_DDLTIME, IGNORE, NO_DROP, OFFLINE, PROTECTION, READONLY, REGEXP, RLIKEadded: AUTOCOMMIT, ISOLATION, LEVEL, OFFSET, SNAPSHOT, TRANSACTION, WORK, WRITE   Hive 2.1.0 added: ABORT, KEY, LAST, NORELY, NOVALIDATE, NULLS, RELY, VALIDATE   Hive 2.2.0 removed: MINUSadded: CACHE, DAYS, DAYOFWEEK, DUMP, HOURS, MATCHED, MERGE, MINUTES, MONTHS, QUARTER, REPL, SECONDS, STATUS, VIEWS, WEEK, WEEKS, YEARS   Hive 2.3.0 removed: MERGEadded: DETAIL, EXPRESSION, OPERATOR, SUMMARY, VECTORIZATION, WAIT   Hive 3.0.0 removed: PRETTYadded: ACTIVATE, ACTIVE, ALLOC_FRACTION, CHECK, DEFAULT, DO, ENFORCED, KILL, MANAGEMENT, MAPPING, MOVE, PATH, PLAN, PLANS, POOL, QUERY, QUERY_PARALLELISM, REOPTIMIZATION, RESOURCE, SCHEDULING_POLICY, UNMANAGED, WORKLOAD, ZONE   Hive 3.1.0 N/A   Hive 4.0.0 added: AST, AT, BRANCH, CBO, COST, CRON, DCPROPERTIES, DEBUG, DISABLED, DISTRIBUTED, ENABLED, EVERY, EXECUTE, EXECUTED, EXPIRE_SNAPSHOTS, IGNORE, JOINCOST, MANAGED, MANAGEDLOCATION, OPTIMIZE, REMOTE, RESPECT, RETAIN, RETENTION, SCHEDULED, SET_CURRENT_SNAPSHOT, SNAPSHOTS, SPEC, SYSTEM_TIME, SYSTEM_VERSION, TAG, TRANSACTIONAL, TRIM, TYPE, UNKNOWN, URL, WITHIN    Version information\nREGEXP and RLIKE are non-reserved keywords prior to Hive 2.0.0 and reserved keywords starting in Hive 2.0.0 (HIVE-11703).\nReserved keywords are permitted as identifiers if you quote them as described in Supporting Quoted Identifiers in Column Names (version 0.13.0 and later, see HIVE-6013). Most of the keywords are reserved through HIVE-6617 in order to reduce the ambiguity in grammar (version 1.2.0 and later). There are two ways if the user still would like to use those reserved keywords as identifiers: (1) use quoted identifiers, (2) set hive.support.sql11.reserved.keywords=false. (version 2.1.0 and earlier) Create/Drop/Alter/Use Database Create Database CREATE [REMOTE] (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [MANAGEDLOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)]; The uses of SCHEMA and DATABASE are interchangeable – they mean the same thing. CREATE DATABASE was added in Hive 0.6 (HIVE-675). The WITH DBPROPERTIES clause was added in Hive 0.7 (HIVE-1836).\nMANAGEDLOCATION was added to database in Hive 4.0.0 (HIVE-22995). LOCATION now refers to the default directory for external tables and MANAGEDLOCATION refers to the default directory for managed tables. Its recommended that MANAGEDLOCATION be within metastore.warehouse.dir so all managed tables have a common root where common governance policies. It can be used with metastore.warehouse.tenant.colocation to have it point to a directory outside the warehouse root directory to have a tenant based common root where quotas and other policies can be set. REMOTE databases were added in Hive 4.0.0 (HIVE-24396) for support for Data connectors. See documentation for Data connectors. Drop Database DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; The uses of SCHEMA and DATABASE are interchangeable – they mean the same thing. DROP DATABASE was added in Hive 0.6 (HIVE-675). The default behavior is RESTRICT, where DROP DATABASE will fail if the database is not empty. To drop the tables in the database as well, use DROP DATABASE \u0026hellip; CASCADE. Support for RESTRICT and CASCADE was added in Hive 0.8 (HIVE-2090).\nAlter Database ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...); -- (Note: SCHEMA added in Hive 0.14.0) ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0) ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later) ALTER (DATABASE|SCHEMA) database_name SET MANAGEDLOCATION hdfs_path; -- (Note: Hive 4.0.0 and later) The uses of SCHEMA and DATABASE are interchangeable – they mean the same thing. ALTER SCHEMA was added in Hive 0.14 (HIVE-6601).\nThe ALTER DATABASE \u0026hellip; SET LOCATION statement does not move the contents of the database\u0026rsquo;s current directory to the newly specified location. It does not change the locations associated with any tables/partitions under the specified database. It only changes the default parent-directory where new tables will be added for this database. This behaviour is analogous to how changing a table-directory does not move existing partitions to a different location.\nThe ALTER DATABASE \u0026hellip; SET MANAGEDLOCATION statement does not move the contents of the database\u0026rsquo;s managed tables directories to the newly specified location. It does not change the locations associated with any tables/partitions under the specified database. It only changes the default parent-directory where new tables will be added for this database. This behaviour is analogous to how changing a table-directory does not move existing partitions to a different location.\nNo other metadata about a database can be changed. Use Database USE database_name; USE DEFAULT; USE sets the current database for all subsequent HiveQL statements. To revert to the default database, use the keyword \u0026ldquo;default\u0026rdquo; instead of a database name. To check which database is currently being used: SELECT [current_database()](#current_database--) (as of Hive 0.13.0).\nUSE database_name was added in Hive 0.6 (HIVE-675).\nCreate/Drop/Alter Connector Create Connector CREATE CONNECTOR [IF NOT EXISTS] connector_name [TYPE datasource_type] [URL datasource_url] [COMMENT connector_comment] [WITH DCPROPERTIES (property_name=property_value, ...)]; Since Hive 4.0.0 via HIVE-24396 Support for Data connectors was added in hive 4.0.0. Initial commit includes connector implementations for JDBC based datasource like MYSQL, POSTGRES, DERBY. Additional connector implementations will be added via followup commits. TYPE - Type of the remote datasource this connector connects to. for example MYSQL. The type determines the Driver class and any other params specific to this datasource.\nURL - URL of the remote datasource. In case of JDBC datasource, it would be the JDBC connection URL. For hive types, it would be the thrift URL.\nCOMMENT - A short description for this connector.\nDCPROPERTIES: Contains a set of name/value pairs that are set for the connector. The credentials for the remote datasource are specified as part of the DCPROPERTIES as documented in the JDBC Storage Handler docs. All properties that start with a prefix of \u0026ldquo;hive.sql\u0026rdquo; are added to the tables mapped by this connector.\nDrop Connector DROP CONNECTOR [IF EXISTS] connector_name; Since Hive 4.0.0 via HIVE-24396. If there are databases that are mapped by this connector, drop still succeeds. Users will see errors when running DDLs like \u0026ldquo;show tables\u0026rdquo; in the mapped databases.\nAlter Connector ALTER CONNECTOR connector_name SET DCPROPERTIES (property_name=property_value, ...); ALTER CONNECTOR connector_name SET URL new_url; ALTER CONNECTOR connector_name SET OWNER [USER|ROLE] user_or_role; Since Hive 4.0.0 via HIVE-24396\nThe ALTER CONNECTOR \u0026hellip; SET DCPROPERTIES replaces the existing properties with the new set of properties specified in the ALTER DDL.\nThe ALTER CONNECTOR \u0026hellip; SET URL replaces the existing URL with a new URL for the remote datasource. Any REMOTE databases that were created using the connector will continue to work as they are associated by name.\nThe ALTER CONNECTOR \u0026hellip; SET OWNER changes the ownership of the connector object in hive.\nCreate/Drop/Truncate Table  Create Table  Managed and External Tables Storage Formats Row Formats \u0026amp; SerDe Partitioned Tables External Tables Create Table As Select (CTAS) Create Table Like Bucketed Sorted Tables Skewed Tables Temporary Tables Transactional Tables Constraints   Drop Table Truncate Table  Create Table CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; data_type : primitive_type | array_type | map_type | struct_type | union_type -- (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later) array_type : ARRAY \u0026lt; data_type \u0026gt; map_type : MAP \u0026lt; primitive_type, data_type \u0026gt; struct_type : STRUCT \u0026lt; col_name : data_type [COMMENT col_comment], ...\u0026gt; union_type : UNIONTYPE \u0026lt; data_type, data_type, ... \u0026gt; -- (Note: Available in Hive 0.7.0 and later) row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | JSONFILE -- (Note: Available in Hive 4.0.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname column_constraint_specification: : [ PRIMARY KEY|UNIQUE|NOT NULL|DEFAULT [default_value]|CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ] default_value: : [ LITERAL|CURRENT_USER()|CURRENT_DATE()|CURRENT_TIMESTAMP()|NULL ] constraint_specification: : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ] [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE [, CONSTRAINT constraint_name UNIQUE (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ] [, CONSTRAINT constraint_name CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ] CREATE TABLE creates a table with the given name. An error is thrown if a table or view with the same name already exists. You can use IF NOT EXISTS to skip the error.\n Table names and column names are case insensitive but SerDe and property names are case sensitive.  In Hive 0.12 and earlier, only alphanumeric and underscore characters are allowed in table and column names. In Hive 0.13 and later, column names can contain any Unicode character (see HIVE-6013), however, dot (.) and colon (:) yield errors on querying, so they are disallowed in Hive 1.2.0 (see HIVE-10120). Any column name that is specified within backticks (```) is treated literally. Within a backtick string, use double backticks (````) to represent a backtick character. Backtick quotation also enables the use of reserved keywords for table and column identifiers. To revert to pre-0.13.0 behavior and restrict column names to alphanumeric and underscore characters, set the configuration property [hive.support.quoted.identifiers](#hive-support-quoted-identifiers) to none. In this configuration, backticked names are interpreted as regular expressions. For details, see Supporting Quoted Identifiers in Column Names.   Table and column comments are string literals (single-quoted). A table created without the EXTERNAL clause is called a managed table because Hive manages its data. To find out if a table is managed or external, look for tableType in the output of DESCRIBE EXTENDED table_name. The TBLPROPERTIES clause allows you to tag the table definition with your own metadata key/value pairs. Some predefined table properties also exist, such as last_modified_user and last_modified_time which are automatically added and managed by Hive. Other predefined table properties include:  TBLPROPERTIES (\u0026ldquo;comment\u0026rdquo;=\u0026quot;table_comment\u0026quot;) TBLPROPERTIES (\u0026ldquo;hbase.table.name\u0026rdquo;=\u0026quot;table_name\u0026quot;) – see HBase Integration. TBLPROPERTIES (\u0026ldquo;immutable\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) or (\u0026ldquo;immutable\u0026rdquo;=\u0026ldquo;false\u0026rdquo;) in release 0.13.0+ (HIVE-6406) – see Inserting Data into Hive Tables from Queries. TBLPROPERTIES (\u0026ldquo;orc.compress\u0026rdquo;=\u0026ldquo;ZLIB\u0026rdquo;) or (\u0026ldquo;orc.compress\u0026rdquo;=\u0026ldquo;SNAPPY\u0026rdquo;) or (\u0026ldquo;orc.compress\u0026rdquo;=\u0026ldquo;NONE\u0026rdquo;) and other ORC properties – see ORC Files. TBLPROPERTIES (\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) or (\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;false\u0026rdquo;) in release 0.14.0+, the default is \u0026ldquo;false\u0026rdquo; – see Hive Transactions. TBLPROPERTIES (\u0026ldquo;NO_AUTO_COMPACTION\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) or (\u0026ldquo;NO_AUTO_COMPACTION\u0026rdquo;=\u0026ldquo;false\u0026rdquo;), the default is \u0026ldquo;false\u0026rdquo; – see Hive Transactions. TBLPROPERTIES (\u0026ldquo;compactor.mapreduce.map.memory.mb\u0026rdquo;=\u0026quot;mapper_memory\u0026quot;) – see Hive Transactions. TBLPROPERTIES (\u0026ldquo;compactorthreshold.hive.compactor.delta.num.threshold\u0026rdquo;=\u0026quot;threshold_num\u0026quot;) – see Hive Transactions. TBLPROPERTIES (\u0026ldquo;compactorthreshold.hive.compactor.delta.pct.threshold\u0026rdquo;=\u0026quot;threshold_pct\u0026quot;) – see Hive Transactions. TBLPROPERTIES (\u0026ldquo;auto.purge\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) or (\u0026ldquo;auto.purge\u0026rdquo;=\u0026ldquo;false\u0026rdquo;) in release 1.2.0+ (HIVE-9118) – see Drop Table, Drop Partitions, Truncate Table, and Insert Overwrite. TBLPROPERTIES (\u0026ldquo;EXTERNAL\u0026rdquo;=\u0026ldquo;TRUE\u0026rdquo;) in release 0.6.0+ (HIVE-1329) – Change a managed table to an external table and vice versa for \u0026ldquo;FALSE\u0026rdquo;.  As of Hive 2.4.0 (HIVE-16324) the value of the property \u0026lsquo;EXTERNAL\u0026rsquo; is parsed as a boolean (case insensitive true or false) instead of a case sensitive string comparison.   TBLPROPERTIES (\u0026ldquo;external.table.purge\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) in release 4.0.0+ (HIVE-19981) when set on external table would delete the data as well.   To specify a database for the table, either issue the USE database_name statement prior to the CREATE TABLE statement (in Hive 0.6 and later) or qualify the table name with a database name (\u0026quot;database_name.table.name\u0026quot; in Hive 0.7 and later).\nThe keyword \u0026ldquo;default\u0026rdquo; can be used for the default database.  See Alter Table below for more information about table comments, table properties, and SerDe properties.\nSee Type System and Hive Data Types for details about the primitive and complex data types.\nManaged and External Tables By default Hive creates managed tables, where files, metadata and statistics are managed by internal Hive processes. For details on the differences between managed and external table see Managed vs. External Tables.\nStorage Formats Hive supports built-in and custom-developed file formats. See CompressedStorage for details on compressed table storage.\nThe following are some of the formats built-in to Hive:\n   Storage Format Description     STORED AS TEXTFILE Stored as plain text files. TEXTFILE is the default file format, unless the configuration parameter hive.default.fileformat has a different setting.Use the DELIMITED clause to read delimited files.Enable escaping for the delimiter characters by using the \u0026lsquo;ESCAPED BY\u0026rsquo; clause (such as ESCAPED BY \u0026lsquo;') Escaping is needed if you want to work with data that can contain these delimiter characters. A custom NULL format can also be specified using the \u0026lsquo;NULL DEFINED AS\u0026rsquo; clause (default is \u0026lsquo;\\N\u0026rsquo;).(Hive 4.0) All BINARY columns in the table are assumed to be base64 encoded. To read the data as raw bytes:TBLPROPERTIES (\u0026ldquo;hive.serialization.decode.binary.as.base64\u0026rdquo;=\u0026ldquo;false\u0026rdquo;)   STORED AS SEQUENCEFILE Stored as compressed Sequence File.   STORED AS ORC Stored as ORC file format. Supports ACID Transactions \u0026amp; Cost-based Optimizer (CBO). Stores column-level metadata.   STORED AS PARQUET Stored as Parquet format for the Parquet columnar storage format in Hive 0.13.0 and later; Use ROW FORMAT SERDE \u0026hellip; STORED AS INPUTFORMAT \u0026hellip; OUTPUTFORMAT syntax \u0026hellip; in Hive 0.10, 0.11, or 0.12.   STORED AS AVRO Stored as Avro format in Hive 0.14.0 and later (see Avro SerDe).   STORED AS RCFILE Stored as Record Columnar File format.   STORED AS JSONFILE Stored as Json file format in Hive 4.0.0 and later.   STORED BY Stored by a non-native table format. To create or link to a non-native table, for example a table backed by HBase or Druid or Accumulo. See StorageHandlers for more information on this option.   INPUTFORMAT and OUTPUTFORMAT in the file_format to specify the name of a corresponding InputFormat and OutputFormat class as a string literal.For example, \u0026lsquo;org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat\u0026rsquo;. For LZO compression, the values to use are \u0026lsquo;INPUTFORMAT \u0026ldquo;com.hadoop.mapred.DeprecatedLzoTextInputFormat\u0026rdquo; OUTPUTFORMAT \u0026ldquo;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026rdquo;\u0026rsquo; (see LZO Compression).    Row Formats \u0026amp; SerDe You can create tables with a custom SerDe or using a native SerDe. A native SerDe is used if ROW FORMAT is not specified or ROW FORMAT DELIMITED is specified.\nUse the SERDE clause to create a table with a custom SerDe. For more information on SerDes see:\n Hive SerDe SerDe HCatalog Storage Formats  You must specify a list of columns for tables that use a native SerDe. Refer to the Types part of the User Guide for the allowable column types.\nA list of columns for tables that use a custom SerDe may be specified but Hive will query the SerDe to determine the actual list of columns for this table.\nFor general information about SerDes, see Hive SerDe in the Developer Guide. Also see SerDe for details about input and output processing.\nTo change a table\u0026rsquo;s SerDe or SERDEPROPERTIES, use the ALTER TABLE statement as described below in Add SerDe Properties.\n   Row Format Description     RegExROW FORMAT SERDE\u0026rsquo;org.apache.hadoop.hive.serde2.RegexSerDe\u0026rsquo;WITH SERDEPROPERTIES (\u0026ldquo;input.regex\u0026rdquo; = \u0026ldquo;\u0026quot;)STORED AS TEXTFILE; Stored as plain text file, translated by Regular Expression.The following example defines a table in the default Apache Weblog format.CREATE TABLE apachelog (``host STRING,``identity STRING,``user STRING,``time STRING,``request STRING,``status STRING,``size STRING,``referer STRING,``agent STRING)``ROW FORMAT SERDE``'org.apache.hadoop.hive.serde2.RegexSerDe'``WITH SERDEPROPERTIES (``\u0026quot;input.regex\u0026quot; `=``\u0026quot;([^]) ([^]) ([^]*) (-   JSON ROW FORMAT SERDE \u0026lsquo;org.apache.hive.hcatalog.data.JsonSerDe\u0026rsquo; STORED AS TEXTFILE Stored as plain text file in JSON format.The JsonSerDe for JSON files is available in Hive 0.12 and later.In some distributions, a reference to hive-hcatalog-core.jar is required.ADD JAR /usr/lib/hive-hcatalog/lib/hive-hcatalog-core.jar;CREATE TABLE my_table(a string, b``bigint``, ...)``ROW FORMAT SERDE``'org.apache.hive.hcatalog.data.JsonSerDe'``STORED``AS TEXTFILE;The JsonSerDe was moved to Hive from HCatalog and before it was in hive-contrib project. It was added to the Hive distribution by HIVE-4895.An Amazon SerDe is available at s3://elasticmapreduce/samples/hive-ads/libs/jsonserde.jar for releases prior to 0.12.0.The JsonSerDe for JSON files is available in Hive 0.12 and later.Starting in Hive 3.0.0, JsonSerDe is added to Hive Serde as \u0026ldquo;org.apache.hadoop.hive.serde2.JsonSerDe\u0026rdquo; (HIVE-19211).CREATE TABLE my_table(a string, b``bigint``, ...)``ROW FORMAT SERDE``'org.apache.hadoop.hive.serde2.JsonSerDe'``STORED``AS TEXTFILE;Or STORED AS JSONFILE is supported starting in Hive 4.0.0 (HIVE-19899), so you can create table as follows:CREATE TABLE my_table(a string, b``bigint``, ...) STORED AS JSONFILE;   CSV/TSVROW FORMAT SERDE \u0026lsquo;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026rsquo; STORED AS TEXTFILE Stored as plain text file in CSV / TSV format. The CSVSerde is available in Hive 0.14 and greater.The following example creates a TSV (Tab-separated) file.CREATE` `TABLE` `my_table(a string, b string, ...)`ROW FORMAT SERDE\u0026lsquo;org.apache.hadoop.hive.serde2.OpenCSVSerde\u0026rsquo;WITH` `SERDEPROPERTIES (\u0026ldquo;separatorChar\u0026rdquo; =\u0026quot;\\t\u0026quot;,\u0026quot;quoteChar\u0026quot;` `=\u0026quot;'\u0026quot;,\u0026ldquo;escapeChar\u0026rdquo; =\u0026quot;\\\\\u0026quot;)STOREDAS TEXTFILE;Default properties for SerDe is Comma-Separated (CSV) file DEFAULT_ESCAPE_CHARACTER `DEFAULT_QUOTE_CHARACTER \u0026quot;``DEFAULT_SEPARATOR ,This SerDe works for most CSV data, but does not handle embedded newlines. To use the SerDe, specify the fully qualified class name org.apache.hadoop.hive.serde2.OpenCSVSerde. Documentation is based on original documentation at https://github.com/ogrodnek/csv-serde.LimitationsThis SerDe treats all columns to be of type String. Even if you create a table with non-string column types using this SerDe, the DESCRIBE TABLE output would show string column type. The type information is retrieved from the SerDe. To convert columns to the desired type in a table, you can create a view over the table that does the CAST to the desired type.The CSV SerDe is based on https://github.com/ogrodnek/csv-serde, and was added to the Hive distribution in HIVE-7777.The CSVSerde has been built and tested against Hive 0.14 and later, and uses Open-CSV 2.3 which is bundled with the Hive distribution.For general information about SerDes, see Hive SerDe in the Developer Guide. Also see SerDe for details about input and output processing.    Partitioned Tables Partitioned tables can be created using the PARTITIONED BY clause. A table can have one or more partition columns and a separate data directory is created for each distinct value combination in the partition columns. Further, tables or partitions can be bucketed using CLUSTERED BY columns, and data can be sorted within that bucket via SORT BY columns. This can improve performance on certain kinds of queries.\nIf, when creating a partitioned table, you get this error: \u0026ldquo;FAILED: Error in semantic analysis: Column repeated in partitioning columns,\u0026rdquo; it means you are trying to include the partitioned column in the data of the table itself. You probably really do have the column defined. However, the partition you create makes a pseudocolumn on which you can query, so you must rename your table column to something else (that users should not query on!).\nFor example, suppose your original unpartitioned table had three columns: id, date, and name.\nExample:\nid int, date date, name varchar Now you want to partition on date. Your Hive definition could use \u0026ldquo;dtDontQuery\u0026rdquo; as a column name so that \u0026ldquo;date\u0026rdquo; can be used for partitioning (and querying).\nExample:\ncreate table table_name ( id int, dtDontQuery string, name string ) partitioned by (date string) Now your users will still query on \u0026ldquo;where date = '...'\u0026rdquo; but the second column dtDontQuery will hold the original values.\nHere\u0026rsquo;s an example statement to create a partitioned table:\nExample:\nCREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) STORED AS SEQUENCEFILE; The statement above creates the page_view table with viewTime, userid, page_url, referrer_url, and ip columns (including comments). The table is also partitioned and data is stored in sequence files. The data format in the files is assumed to be field-delimited by ctrl-A and row-delimited by newline.\nExample:\nCREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\001' STORED AS SEQUENCEFILE; The above statement lets you create the same table as the previous table.\nIn the previous examples the data is stored in \u0026lt;hive.metastore.warehouse.dir\u0026gt;/page_view. Specify a value for the key [hive.metastore.warehouse.dir](#hive-metastore-warehouse-dir) in the Hive config file hive-site.xml.\nExternal Tables The EXTERNAL keyword lets you create a table and provide a LOCATION so that Hive does not use a default location for this table. This comes in handy if you already have data generated. When dropping an EXTERNAL table, data in the table is NOT deleted from the file system. Starting Hive 4.0.0 (\nHIVE-19981 Managed tables converted to external tables by the HiveStrictManagedMigration utility should be set to delete data when the table is dropped Closed\n ) setting table property external.table.purge=true, will also delete the data.\nAn EXTERNAL table points to any HDFS location for its storage, rather than being stored in a folder specified by the configuration property [hive.metastore.warehouse.dir](#hive-metastore-warehouse-dir).\nExample:\nCREATE EXTERNAL TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User', country STRING COMMENT 'country of origination') COMMENT 'This is the staging page view table' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\054' STORED AS TEXTFILE LOCATION '\u0026lt;hdfs_location\u0026gt;'; You can use the above statement to create a page_view table which points to any HDFS location for its storage. But you still have to make sure that the data is delimited as specified in the CREATE statement above.\nFor another example of creating an external table, see Loading Data in the Tutorial.\nCreate Table As Select (CTAS) Tables can also be created and populated by the results of a query in one create-table-as-select (CTAS) statement. The table created by CTAS is atomic, meaning that the table is not seen by other users until all the query results are populated. So other users will either see the table with the complete results of the query or will not see the table at all.\nThere are two parts in CTAS, the SELECT part can be any SELECT statement supported by HiveQL. The CREATE part of the CTAS takes the resulting schema from the SELECT part and creates the target table with other table properties such as the SerDe and storage format.\nStarting with Hive 3.2.0, CTAS statements can define a partitioning specification for the target table (HIVE-20241).\nCTAS has these restrictions:\n The target table cannot be an external table. The target table cannot be a list bucketing table.  Example:\nCREATE TABLE new_key_value_store ROW FORMAT SERDE \u0026quot;org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe\u0026quot; STORED AS RCFile AS SELECT (key % 1024) new_key, concat(key, value) key_value_pair FROM key_value_store SORT BY new_key, key_value_pair; The above CTAS statement creates the target table new_key_value_store with the schema (new_key DOUBLE, key_value_pair STRING) derived from the results of the SELECT statement. If the SELECT statement does not specify column aliases, the column names will be automatically assigned to _col0, _col1, and _col2 etc. In addition, the new target table is created using a specific SerDe and a storage format independent of the source tables in the SELECT statement.\nStarting with Hive 0.13.0, the SELECT statement can include one or more common table expressions (CTEs), as shown in the SELECT syntax. For an example, see Common Table Expression.\nBeing able to select data from one table to another is one of the most powerful features of Hive. Hive handles the conversion of the data from the source format to the destination format as the query is being executed.\nCreate Table Like The LIKE form of CREATE TABLE allows you to copy an existing table definition exactly (without copying its data). In contrast to CTAS, the statement below creates a new empty_key_value_store table whose definition exactly matches the existing key_value_store in all particulars other than table name. The new table contains no rows.\nCREATE TABLE empty_key_value_store LIKE key_value_store [TBLPROPERTIES (property_name=property_value, ...)]; Before Hive 0.8.0, CREATE TABLE LIKE view_name would make a copy of the view. In Hive 0.8.0 and later releases, CREATE TABLE LIKE view_name creates a table by adopting the schema of view_name (fields and partition columns) using defaults for SerDe and file formats.\nBucketed Sorted Tables Example:\nCREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\001' COLLECTION ITEMS TERMINATED BY '\\002' MAP KEYS TERMINATED BY '\\003' STORED AS SEQUENCEFILE; In the example above, the page_view table is bucketed (clustered by) userid and within each bucket the data is sorted in increasing order of viewTime. Such an organization allows the user to do efficient sampling on the clustered column - in this case userid. The sorting property allows internal operators to take advantage of the better-known data structure while evaluating queries, also increasing efficiency. MAP KEYS and COLLECTION ITEMS keywords can be used if any of the columns are lists or maps.\nThe CLUSTERED BY and SORTED BY creation commands do not affect how data is inserted into a table – only how it is read. This means that users must be careful to insert data correctly by specifying the number of reducers to be equal to the number of buckets, and using CLUSTER BY and SORT BY commands in their query.\nThere is also an example of creating and populating bucketed tables.\nSkewed Tables Version information\nAs of Hive 0.10.0 (HIVE-3072 and HIVE-3649). See HIVE-3026 for additional JIRA tickets that implemented list bucketing in Hive 0.10.0 and 0.11.0.\nDesign documents\nRead the Skewed Join Optimization and List Bucketing design documents for more information.\nThis feature can be used to improve performance for tables where one or more columns have skewed values. By specifying the values that appear very often (heavy skew) Hive will split those out into separate files (or directories in case of list bucketing) automatically and take this fact into account during queries so that it can skip or include the whole file (or directory in case of list bucketing) if possible.\nThis can be specified on a per-table level during table creation.\nThe following example shows one column with three skewed values, optionally with the STORED AS DIRECTORIES clause which specifies list bucketing.\nExample:\nCREATE TABLE list_bucket_single (key STRING, value STRING) SKEWED BY (key) ON (1,5,6) [STORED AS DIRECTORIES]; And here is an example of a table with two skewed columns.\nExample:\nCREATE TABLE list_bucket_multiple (col1 STRING, col2 int, col3 STRING) SKEWED BY (col1, col2) ON (('s1',1), ('s3',3), ('s13',13), ('s78',78)) [STORED AS DIRECTORIES]; For corresponding ALTER TABLE statements, see Alter Table Skewed or Stored as Directories below.\nTemporary Tables Version information\nAs of Hive 0.14.0 (HIVE-7090).\nA table that has been created as a temporary table will only be visible to the current session. Data will be stored in the user\u0026rsquo;s scratch directory, and deleted at the end of the session.\nIf a temporary table is created with a database/table name of a permanent table which already exists in the database, then within that session any references to that table will resolve to the temporary table, rather than to the permanent table. The user will not be able to access the original table within that session without either dropping the temporary table, or renaming it to a non-conflicting name.\nTemporary tables have the following limitations:\n Partition columns are not supported. No support for creation of indexes.  Starting in Hive 1.1.0 the storage policy for temporary tables can be set to memory, ssd, or default with the hive.exec.temporary.table.storage configuration parameter (see HDFS Storage Types and Storage Policies).\nExample:\nCREATE TEMPORARY TABLE list_bucket_multiple (col1 STRING, col2 int, col3 STRING); Transactional Tables Version information\nAs of Hive 4.0 (HIVE-18453).\nA table that supports operations with ACID semantics. See this for more details about transactional tables.\nExample:\nCREATE TRANSACTIONAL TABLE transactional_table_test(key string, value string) PARTITIONED BY(ds string) STORED AS ORC; Constraints Version information\nAs of Hive 2.1.0 (HIVE-13290).\nHive includes support for non-validated primary and foreign key constraints. Some SQL tools generate more efficient queries when constraints are present. Since these constraints are not validated, an upstream system needs to ensure data integrity before it is loaded into Hive.\nExample:\ncreate table pk(id1 integer, id2 integer, primary key(id1, id2) disable novalidate); create table fk(id1 integer, id2 integer, constraint c1 foreign key(id1, id2) references pk(id2, id1) disable novalidate); Version information\nAs of Hive 3.0.0 (HIVE-16575, HIVE-18726, HIVE-18953).\nHive includes support for UNIQUE, NOT NULL, DEFAULT and CHECK constraints. Beside UNIQUE all three type of constraints are enforced.\nExample:\ncreate table constraints1(id1 integer UNIQUE disable novalidate, id2 integer NOT NULL, usr string DEFAULT current_user(), price double CHECK (price \u0026gt; 0 AND price \u0026lt;= 1000)); create table constraints2(id1 integer, id2 integer, constraint c1_unique UNIQUE(id1) disable novalidate); create table constraints3(id1 integer, id2 integer, constraint c1_check CHECK(id1 + id2 \u0026gt; 0)); DEFAULT on complex data types such as map, struct, array is not supported.\nDrop Table DROP TABLE [IF EXISTS] table_name [PURGE]; -- (Note: PURGE available in Hive 0.14.0 and later) DROP TABLE removes metadata and data for this table. The data is actually moved to the .Trash/Current directory if Trash is configured (and PURGE is not specified). The metadata is completely lost.\nWhen dropping an EXTERNAL table, data in the table will NOT be deleted from the file system. Starting Hive 4.0.0 (\nHIVE-19981 Managed tables converted to external tables by the HiveStrictManagedMigration utility should be set to delete data when the table is dropped Closed\n ) setting table property external.table.purge=true, will also delete the data.\nWhen dropping a table referenced by views, no warning is given (the views are left dangling as invalid and must be dropped or recreated by the user).\nOtherwise, the table information is removed from the metastore and the raw data is removed as if by \u0026lsquo;hadoop dfs -rm\u0026rsquo;. In many cases, this results in the table data being moved into the user\u0026rsquo;s .Trash folder in their home directory; users who mistakenly DROP TABLEs may thus be able to recover their lost data by recreating a table with the same schema, recreating any necessary partitions, and then moving the data back into place manually using Hadoop. This solution is subject to change over time or across installations as it relies on the underlying implementation; users are strongly encouraged not to drop tables capriciously.\nVersion information: PURGE\nThe PURGE option is added in version 0.14.0 by HIVE-7100.\nIf PURGE is specified, the table data does not go to the .Trash/Current directory and so cannot be retrieved in the event of a mistaken DROP. The purge option can also be specified with the table property auto.purge (see TBLPROPERTIES above).\nIn Hive 0.7.0 or later, DROP returns an error if the table doesn\u0026rsquo;t exist, unless IF EXISTS is specified or the configuration variable hive.exec.drop.ignorenonexistent is set to true.\nSee the Alter Partition section below for how to drop partitions.\nTruncate Table Version information\nAs of Hive 0.11.0 (HIVE-446).\nTRUNCATE [TABLE] table_name [PARTITION partition_spec]; partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...) Removes all rows from a table or partition(s). The rows will be trashed if the filesystem Trash is enabled, otherwise they are deleted (as of Hive 2.2.0 with HIVE-14626). Currently the target table should be native/managed table or an exception will be thrown. User can specify partial partition_spec for truncating multiple partitions at once and omitting partition_spec will truncate all partitions in the table.\nStarting with HIVE 2.3.0 (HIVE-15880) if the table property \u0026ldquo;auto.purge\u0026rdquo; (see TBLPROPERTIES above) is set to \u0026ldquo;true\u0026rdquo; the data of the table is not moved to Trash when a TRUNCATE TABLE command is issued against it and cannot be retrieved in the event of a mistaken TRUNCATE. This is applicable only for managed tables (see managed tables). This behavior can be turned off if the \u0026ldquo;auto.purge\u0026rdquo; property is unset or set to false for a managed table.\nStarting with Hive 4.0 (HIVE-23183) the TABLE token is optional, previous versions required it.\nAlter Table/Partition/Column  Alter Table  Rename Table Alter Table Properties  Alter Table Comment   Add SerDe Properties Remove SerDe Properties Alter Table Storage Properties Alter Table Skewed or Stored as Directories  Alter Table Skewed Alter Table Not Skewed Alter Table Not Stored as Directories Alter Table Set Skewed Location   Alter Table Constraints Additional Alter Table Statements   Alter Partition  Add Partitions  Dynamic Partitions   Rename Partition Exchange Partition Discover Partitions Partition Retention Recover Partitions (MSCK REPAIR TABLE) Drop Partitions (Un)Archive Partition   Alter Either Table or Partition  Alter Table/Partition File Format Alter Table/Partition Location Alter Table/Partition Touch Alter Table/Partition Protections Alter Table/Partition Compact Alter Table/Partition Concatenate Alter Table/Partition Update columns   Alter Column  Rules for Column Names Change Column Name/Type/Position/Comment Add/Replace Columns Partial Partition Specification    Alter table statements enable you to change the structure of an existing table. You can add columns/partitions, change SerDe, add table and SerDe properties, or rename the table itself. Similarly, alter table partition statements allow you change the properties of a specific partition in the named table.\nAlter Table Rename Table ALTER TABLE table_name RENAME TO new_table_name; This statement lets you change the name of a table to a different name.\nAs of version 0.6, a rename on a managed table moves its HDFS location. Rename has been changed as of version 2.2.0 (HIVE-14909) so that a managed table\u0026rsquo;s HDFS location is moved only if the table is created without a LOCATION clause and under its database directory. Hive versions prior to 0.6 just renamed the table in the metastore without moving the HDFS location.\nAlter Table Properties ALTER TABLE table_name SET TBLPROPERTIES table_properties; table_properties: : (property_name = property_value, property_name = property_value, ... ) You can use this statement to add your own metadata to the tables. Currently last_modified_user, last_modified_time properties are automatically added and managed by Hive. Users can add their own properties to this list. You can do DESCRIBE EXTENDED TABLE to get this information.\nFor more information, see the TBLPROPERTIES clause in Create Table above.\nAlter Table Comment To change the comment of a table you have to change the comment property of the TBLPROPERTIES:\nALTER TABLE table_name SET TBLPROPERTIES ('comment' = new_comment); Add SerDe Properties ALTER TABLE table_name [PARTITION partition_spec] SET SERDE serde_class_name [WITH SERDEPROPERTIES serde_properties]; ALTER TABLE table_name [PARTITION partition_spec] SET SERDEPROPERTIES serde_properties; serde_properties: : (property_name = property_value, property_name = property_value, ... ) These statements enable you to change a table\u0026rsquo;s SerDe or add user-defined metadata to the table\u0026rsquo;s SerDe object.\nThe SerDe properties are passed to the table\u0026rsquo;s SerDe when it is being initialized by Hive to serialize and deserialize data. So users can store any information required for their custom SerDe here. Refer to the SerDe documentation and Hive SerDe in the Developer Guide for more information, and see Row Format, Storage Format, and SerDe above for details about setting a table\u0026rsquo;s SerDe and SERDEPROPERTIES in a CREATE TABLE statement.\nNote that both property_name and property_value must be quoted.\nExample:\nALTER TABLE table_name SET SERDEPROPERTIES ('field.delim' = ','); Remove SerDe Properties Version information\nRemove SerDe Properties is supported as of Hive 4.0.0 (HIVE-21952).\nALTER TABLE table_name [PARTITION partition_spec] UNSET SERDEPROPERTIES (property_name, ... ); These statements enable you to remove user-defined metadata to the table\u0026rsquo;s SerDe object.\nNote that property_name must be quoted.\nExample:\nALTER TABLE table_name UNSET SERDEPROPERTIES ('field.delim'); Alter Table Storage Properties ALTER TABLE table_name CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name, ...)] INTO num_buckets BUCKETS; These statements change the table\u0026rsquo;s physical storage properties.\nNOTE: These commands will only modify Hive\u0026rsquo;s metadata, and will NOT reorganize or reformat existing data. Users should make sure the actual data layout conforms with the metadata definition.\nAlter Table Skewed or Stored as Directories Version information\nAs of Hive 0.10.0 (HIVE-3072 and HIVE-3649). See HIVE-3026 for additional JIRA tickets that implemented list bucketing in Hive 0.10.0 and 0.11.0.\nA table\u0026rsquo;s SKEWED and STORED AS DIRECTORIES options can be changed with ALTER TABLE statements. See Skewed Tables above for the corresponding CREATE TABLE syntax.\nAlter Table Skewed ALTER TABLE table_name SKEWED BY (col_name1, col_name2, ...) ON ([(col_name1_value, col_name2_value, ...) [, (col_name1_value, col_name2_value), ...] [STORED AS DIRECTORIES]; The STORED AS DIRECTORIES option determines whether a skewed table uses the list bucketing feature, which creates subdirectories for skewed values.\nAlter Table Not Skewed ALTER TABLE table_name NOT SKEWED; The NOT SKEWED option makes the table non-skewed and turns off the list bucketing feature (since a list-bucketing table is always skewed). This affects partitions created after the ALTER statement, but has no effect on partitions created before the ALTER statement.\nAlter Table Not Stored as Directories ALTER TABLE table_name NOT STORED AS DIRECTORIES; This turns off the list bucketing feature, although the table remains skewed.\nAlter Table Set Skewed Location ALTER TABLE table_name SET SKEWED LOCATION (col_name1=\u0026quot;location1\u0026quot; [, col_name2=\u0026quot;location2\u0026quot;, ...] ); This changes the location map for list bucketing.\nAlter Table Constraints Version information\nAs of Hive release 2.1.0.\n Table constraints can be added or removed via ALTER TABLE statements.\nALTER TABLE table_name ADD CONSTRAINT constraint_name PRIMARY KEY (column, ...) DISABLE NOVALIDATE; ALTER TABLE table_name ADD CONSTRAINT constraint_name FOREIGN KEY (column, ...) REFERENCES table_name(column, ...) DISABLE NOVALIDATE RELY; ALTER TABLE table_name ADD CONSTRAINT constraint_name UNIQUE (column, ...) DISABLE NOVALIDATE; ALTER TABLE table_name CHANGE COLUMN column_name column_name data_type CONSTRAINT constraint_name NOT NULL ENABLE; ALTER TABLE table_name CHANGE COLUMN column_name column_name data_type CONSTRAINT constraint_name DEFAULT default_value ENABLE; ALTER TABLE table_name CHANGE COLUMN column_name column_name data_type CONSTRAINT constraint_name CHECK check_expression ENABLE; ALTER TABLE table_name DROP CONSTRAINT constraint_name; Additional Alter Table Statements See Alter Either Table or Partition below for more DDL statements that alter tables.\nAlter Partition Partitions can be added, renamed, exchanged (moved), dropped, or (un)archived by using the PARTITION clause in an ALTER TABLE statement, as described below. To make the metastore aware of partitions that were added directly to HDFS, you can use the metastore check command (MSCK) or on Amazon EMR you can use the RECOVER PARTITIONS option of ALTER TABLE. See Alter Either Table or Partition below for more ways to alter partitions.\nVersion 1.2+\nAs of Hive 1.2 (HIVE-10307), the partition values specified in partition specification are type checked, converted, and normalized to conform to their column types if the property hive.typecheck.on.insert is set to true (default). The values can be number literals.\nAdd Partitions ALTER TABLE table_name ADD [IF NOT EXISTS] PARTITION partition_spec [LOCATION 'location'][, PARTITION partition_spec [LOCATION 'location'], ...]; partition_spec: : (partition_column = partition_col_value, partition_column = partition_col_value, ...) You can use ALTER TABLE ADD PARTITION to add partitions to a table. Partition values should be quoted only if they are strings. The location must be a directory inside of which data files reside. (ADD PARTITION changes the table metadata, but does not load data. If the data does not exist in the partition\u0026rsquo;s location, queries will not return any results.) An error is thrown if the partition_spec for the table already exists. You can use IF NOT EXISTS to skip the error.\nVersion 0.7\nAlthough it is proper syntax to have multiple partition_spec in a single ALTER TABLE, if you do this in version 0.7 your partitioning scheme will fail. That is, every query specifying a partition will always use only the first partition.\nSpecifically, the following example will FAIL silently and without error in Hive 0.7, and all queries will go only to dt=\u0026lsquo;2008-08-08\u0026rsquo; partition, no matter which partition you specify.\nExample:\nALTER TABLE page_view ADD PARTITION (dt='2008-08-08', country='us') location '/path/to/us/part080808' PARTITION (dt='2008-08-09', country='us') location '/path/to/us/part080809'; In Hive 0.8 and later, you can add multiple partitions in a single ALTER TABLE statement as shown in the previous example.\nIn Hive 0.7, if you want to add many partitions you should use the following form:\nALTER TABLE table_name ADD PARTITION (partCol = 'value1') location 'loc1'; ALTER TABLE table_name ADD PARTITION (partCol = 'value2') location 'loc2'; ... ALTER TABLE table_name ADD PARTITION (partCol = 'valueN') location 'locN'; Dynamic Partitions Partitions can be added to a table dynamically, using a Hive INSERT statement (or a Pig STORE statement). See these documents for details and examples:\n Design Document for Dynamic Partitions Tutorial: Dynamic-Partition Insert Hive DML: Dynamic Partition Inserts HCatalog Dynamic Partitioning  Usage with Pig Usage from MapReduce    Rename Partition Version information\nAs of Hive 0.9.\nALTER TABLE table_name PARTITION partition_spec RENAME TO PARTITION partition_spec; This statement lets you change the value of a partition column. One of use cases is that you can use this statement to normalize your legacy partition column value to conform to its type. In this case, the type conversion and normalization are not enabled for the column values in old partition_spec even with property hive.typecheck.on.insert set to true (default) which allows you to specify any legacy data in form of string in the old partition_spec.\nExchange Partition Partitions can be exchanged (moved) between tables.\nVersion information\nAs of Hive 0.12 (HIVE-4095). Multiple partitions supported in Hive versions 1.2.2, 1.3.0, and 2.0.0+.\n-- Move partition from table_name_1 to table_name_2 ALTER TABLE table_name_2 EXCHANGE PARTITION (partition_spec) WITH TABLE table_name_1; -- multiple partitions ALTER TABLE table_name_2 EXCHANGE PARTITION (partition_spec, partition_spec2, ...) WITH TABLE table_name_1; This statement lets you move the data in a partition from a table to another table that has the same schema and does not already have that partition.\nFor further details on this feature, see Exchange Partition and HIVE-4095.\nDiscover Partitions Table property \u0026ldquo;discover.partitions\u0026rdquo; can now be specified to control automatic discovery and synchronization of partition metadata in Hive Metastore. When Hive Metastore Service (HMS) is started in remote service mode, a background thread (PartitionManagementTask) gets scheduled periodically every 300s (configurable via metastore.partition.management.task.frequency config) that looks for tables with \u0026ldquo;discover.partitions\u0026rdquo; table property set to true and performs MSCK REPAIR in sync mode. If the table is a transactional table, then Exclusive Lock is obtained for that table before performing MSCK REPAIR. With this table property, \u0026ldquo;MSCK REPAIR TABLE table_name SYNC PARTITIONS\u0026rdquo; is no longer required to be run manually. Version information\nAs of Hive 4.0.0 (HIVE-20707). Partition Retention Table property \u0026ldquo;partition.retention.period\u0026rdquo; can now be specified for partitioned tables with a retention interval. When a retention interval is specified, the background thread running in HMS (refer Discover Partitions section), will check the age (creation time) of the partition and if the partition\u0026rsquo;s age is older than the retention period, it will be dropped. Dropping partitions after retention period will also delete the data in that partition. For example, if an external partitioned table with \u0026lsquo;date\u0026rsquo; partition is created with table properties \u0026ldquo;discover.partitions\u0026rdquo;=\u0026ldquo;true\u0026rdquo; and \u0026ldquo;partition.retention.period\u0026rdquo;=\u0026ldquo;7d\u0026rdquo; then only the partitions created in last 7 days are retained.\nVersion information\nAs of Hive 4.0.0 (HIVE-20707). Recover Partitions (MSCK REPAIR TABLE) Hive stores a list of partitions for each table in its metastore. If, however, new partitions are directly added to HDFS (say by using hadoop fs -put command) or removed from HDFS, the metastore (and hence Hive) will not be aware of these changes to partition information unless the user runs ALTER TABLE table_name ADD/DROP PARTITION commands on each of the newly added or removed partitions, respectively.\nHowever, users can run a metastore check command with the repair table option:\nMSCK [REPAIR] TABLE table_name [ADD/DROP/SYNC PARTITIONS]; which will update metadata about partitions to the Hive metastore for partitions for which such metadata doesn\u0026rsquo;t already exist. The default option for MSC command is ADD PARTITIONS. With this option, it will add any partitions that exist on HDFS but not in metastore to the metastore. The DROP PARTITIONS option will remove the partition information from metastore, that is already removed from HDFS. The SYNC PARTITIONS option is equivalent to calling both ADD and DROP PARTITIONS. See HIVE-874 and HIVE-17824 for more details. When there is a large number of untracked partitions, there is a provision to run MSCK REPAIR TABLE batch wise to avoid OOME (Out of Memory Error). By giving the configured batch size for the property hive.msck.repair.batch.size it can run in the batches internally. The default value of the property is zero, it means it will execute all the partitions at once. MSCK command without the REPAIR option can be used to find details about metadata mismatch metastore.\nThe equivalent command on Amazon Elastic MapReduce (EMR)\u0026rsquo;s version of Hive is:\nALTER TABLE table_name RECOVER PARTITIONS; Starting with Hive 1.3, MSCK will throw exceptions if directories with disallowed characters in partition values are found on HDFS. Use hive.msck.path.validation setting on the client to alter this behavior; \u0026ldquo;skip\u0026rdquo; will simply skip the directories. \u0026ldquo;ignore\u0026rdquo; will try to create partitions anyway (old behavior). This may or may not work.\nDrop Partitions ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...] [IGNORE PROTECTION] [PURGE]; -- (Note: PURGE available in Hive 1.2.0 and later, IGNORE PROTECTION not available 2.0.0 and later) You can use ALTER TABLE DROP PARTITION to drop a partition for a table. This removes the data and metadata for this partition. The data is actually moved to the .Trash/Current directory if Trash is configured, unless PURGE is specified, but the metadata is completely lost (see Drop Table above).\nVersion Information: PROTECTION\nIGNORE PROTECTION is no longer available in versions 2.0.0 and later. This functionality is replaced by using one of the several security options available with Hive (see SQL Standard Based Hive Authorization). See HIVE-11145 for details.\nFor tables that are protected by NO_DROP CASCADE, you can use the predicate IGNORE PROTECTION to drop a specified partition or set of partitions (for example, when splitting a table between two Hadoop clusters):\nALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec IGNORE PROTECTION; The above command will drop that partition regardless of protection stats.\nVersion information: PURGE\nThe PURGE option is added to ALTER TABLE in version 1.2.1 by HIVE-10934.\nIf PURGE is specified, the partition data does not go to the .Trash/Current directory and so cannot be retrieved in the event of a mistaken DROP:\nALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec PURGE; -- (Note: Hive 1.2.0 and later) The purge option can also be specified with the table property auto.purge (see TBLPROPERTIES above).\nIn Hive 0.7.0 or later, DROP returns an error if the partition doesn\u0026rsquo;t exist, unless IF EXISTS is specified or the configuration variable hive.exec.drop.ignorenonexistent is set to true.\nALTER TABLE page_view DROP PARTITION (dt='2008-08-08', country='us'); (Un)Archive Partition ALTER TABLE table_name ARCHIVE PARTITION partition_spec; ALTER TABLE table_name UNARCHIVE PARTITION partition_spec; Archiving is a feature to moves a partition\u0026rsquo;s files into a Hadoop Archive (HAR). Note that only the file count will be reduced; HAR does not provide any compression. See LanguageManual Archiving for more information\nAlter Either Table or Partition Alter Table/Partition File Format ALTER TABLE table_name [PARTITION partition_spec] SET FILEFORMAT file_format; This statement changes the table\u0026rsquo;s (or partition\u0026rsquo;s) file format. For available file_format options, see the section above on CREATE TABLE. The operation only changes the table metadata. Any conversion of existing data must be done outside of Hive.\nAlter Table/Partition Location ALTER TABLE table_name [PARTITION partition_spec] SET LOCATION \u0026quot;new location\u0026quot;; Alter Table/Partition Touch ALTER TABLE table_name TOUCH [PARTITION partition_spec]; TOUCH reads the metadata, and writes it back. This has the effect of causing the pre/post execute hooks to fire. An example use case is if you have a hook that logs all the tables/partitions that were modified, along with an external script that alters the files on HDFS directly. Since the script modifies files outside of hive, the modification wouldn\u0026rsquo;t be logged by the hook. The external script could call TOUCH to fire the hook and mark the said table or partition as modified.\nAlso, it may be useful later if we incorporate reliable last modified times. Then touch would update that time as well.\nNote that TOUCH doesn\u0026rsquo;t create a table or partition if it doesn\u0026rsquo;t already exist. (See Create Table.)\nAlter Table/Partition Protections Version information\nAs of Hive 0.7.0 (HIVE-1413). The CASCADE clause for NO_DROP was added in HIVE 0.8.0 (HIVE-2605).\nThis functionality was removed in Hive 2.0.0. This functionality is replaced by using one of the several security options available with Hive (see SQL Standard Based Hive Authorization). See HIVE-11145 for details.\nALTER TABLE table_name [PARTITION partition_spec] ENABLE|DISABLE NO_DROP [CASCADE]; ALTER TABLE table_name [PARTITION partition_spec] ENABLE|DISABLE OFFLINE; Protection on data can be set at either the table or partition level. Enabling NO_DROP prevents a table from being dropped. Enabling OFFLINE prevents the data in a table or partition from being queried, but the metadata can still be accessed.\nIf any partition in a table has NO_DROP enabled, the table cannot be dropped either. Conversely, if a table has NO_DROP enabled then partitions may be dropped, but with NO_DROP CASCADE partitions cannot be dropped either unless the drop partition command specifies IGNORE PROTECTION.\nAlter Table/Partition Compact Version information\nIn Hive release 0.13.0 and later when transactions are being used, the ALTER TABLE statement can request compaction of a table or partition.\nAs of Hive release 1.3.0 and 2.1.0 when transactions are being used, the ALTER TABLE \u0026hellip; COMPACT statement can include a TBLPROPERTIES clause that is either to change compaction MapReduce job properties or to overwrite any other Hive table properties. More details can be found here.\nAs of Hive release 4.0.0-alpha-2 compaction pooling is available.\nAs of Hive release 4.0.0 rebalance compaction is available.\nALTER TABLE table_name [PARTITION (partition_key = 'partition_value' [, ...])] COMPACT 'compaction_type'[AND WAIT] [CLUSTERED INTO n BUCKETS] [ORDER BY col_list] [POOL 'pool_name'] [WITH OVERWRITE TBLPROPERTIES (\u0026quot;property\u0026quot;=\u0026quot;value\u0026quot; [, ...])]; In general you do not need to request compactions when Hive transactions are being used, because the system will detect the need for them and initiate the compaction. However, if compaction is turned off for a table or you want to compact the table at a time the system would not choose to, ALTER TABLE can initiate the compaction. By default the statement will enqueue a request for compaction and return. To watch the progress of the compaction, use SHOW COMPACTIONS. As of Hive 2.2.0 \u0026ldquo;AND WAIT\u0026rdquo; may be specified to have the operation block until compaction completes.\nThe compaction_type can be MAJOR, MINOR or REBALANCE. See the Basic Design section in Hive Transactions for more information.\nMore in formation on compaction pooling can be found here: Compaction pooling\nMore in formation on rebalance compaction pooling can be found here: Rebalance Compaction\nThe [CLUSTERED INTO n BUCKETS] and [ORDER BY col_list] clauses are only supported for REBALANCE compaction.\nAlter Table/Partition Concatenate Version information\nIn Hive release 0.8.0 RCFile added support for fast block level merging of small RCFiles using concatenate command. In Hive release 0.14.0 ORC files added support fast stripe level merging of small ORC files using concatenate command.\nALTER TABLE table_name [PARTITION (partition_key = 'partition_value' [, ...])] CONCATENATE; If the table or partition contains many small RCFiles or ORC files, then the above command will merge them into larger files. In case of RCFile the merge happens at block level whereas for ORC files the merge happens at stripe level thereby avoiding the overhead of decompressing and decoding the data.\nAlter Table/Partition Update columns Version information\nIn Hive release 3.0.0 this command was added to let the user sync serde stored schema information to metastore.\nALTER TABLE table_name [PARTITION (partition_key = 'partition_value' [, ...])] UPDATE COLUMNS; Tables that have serdes which self-describe the table schema may have different schemas in reality and the ones stored in Hive Metastore. For example when a user creates an Avro stored table using a schema url or schema literal, the schema will be inserted into HMS and then will never be changed in HMS regardless of url or literal changes within the serde. This can lead to problems especially when integrating with other Apache components.\nThe update columns feature provides a way for the user to let any schema changes made in the serde to be synced into HMS. It works on both the table and the partitions levels, and obviously only for tables whose schema is not tracked by HMS (see metastore.serdes.using.metastore.for.schema). Using the command on these latter serde types will result in error.\nAlter Column Rules for Column Names Column names are case insensitive.\nVersion information\nIn Hive release 0.12.0 and earlier, column names can only contain alphanumeric and underscore characters.\nIn Hive release 0.13.0 and later, by default column names can be specified within backticks (```) and contain any Unicode character (HIVE-6013), however, dot (.) and colon (:) yield errors on querying. Within a string delimited by backticks, all characters are treated literally except that double backticks (````) represent one backtick character. The pre-0.13.0 behavior can be used by setting [hive.support.quoted.identifiers](#hive-support-quoted-identifiers) to none, in which case backticked names are interpreted as regular expressions. See Supporting Quoted Identifiers in Column Names for details.\nBacktick quotation enables the use of reserved keywords for column names, as well as table names.\nChange Column Name/Type/Position/Comment ALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT]; This command will allow users to change a column\u0026rsquo;s name, data type, comment, or position, or an arbitrary combination of them. The PARTITION clause is available in Hive 0.14.0 and later; see Upgrading Pre-Hive 0.13.0 Decimal Columns for usage. A patch for Hive 0.13 is also available (see HIVE-7971).\nThe CASCADE|RESTRICT clause is available in Hive 1.1.0. ALTER TABLE CHANGE COLUMN with CASCADE command changes the columns of a table\u0026rsquo;s metadata, and cascades the same change to all the partition metadata. RESTRICT is the default, limiting column change only to table metadata.\nALTER TABLE CHANGE COLUMN CASCADE clause will override the table partition\u0026rsquo;s column metadata regardless of the table or partition\u0026rsquo;s protection mode. Use with discretion.\nThe column change command will only modify Hive\u0026rsquo;s metadata, and will not modify data. Users should make sure the actual data layout of the table/partition conforms with the metadata definition.\nExample:\nCREATE TABLE test_change (a int, b int, c int); // First change column a's name to a1. ALTER TABLE test_change CHANGE a a1 INT; // Next change column a1's name to a2, its data type to string, and put it after column b. ALTER TABLE test_change CHANGE a1 a2 STRING AFTER b; // The new table's structure is: b int, a2 string, c int. // Then change column c's name to c1, and put it as the first column. ALTER TABLE test_change CHANGE c c1 INT FIRST; // The new table's structure is: c1 int, b int, a2 string. // Add a comment to column a1 ALTER TABLE test_change CHANGE a1 a1 INT COMMENT 'this is column a1'; Add/Replace Columns ALTER TABLE table_name [PARTITION partition_spec] -- (Note: Hive 0.14.0 and later) ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) [CASCADE|RESTRICT] -- (Note: Hive 1.1.0 and later) ADD COLUMNS lets you add new columns to the end of the existing columns but before the partition columns. This is supported for Avro backed tables as well, for Hive 0.14 and later.\nREPLACE COLUMNS removes all existing columns and adds the new set of columns. This can be done only for tables with a native SerDe (DynamicSerDe, MetadataTypedColumnsetSerDe, LazySimpleSerDe and ColumnarSerDe). Refer to Hive SerDe for more information. REPLACE COLUMNS can also be used to drop columns. For example, \u0026ldquo;ALTER TABLE test_change REPLACE COLUMNS (a int, b int);\u0026rdquo; will remove column \u0026lsquo;c\u0026rsquo; from test_change\u0026rsquo;s schema.\nThe PARTITION clause is available in Hive 0.14.0 and later; see Upgrading Pre-Hive 0.13.0 Decimal Columns for usage.\nThe CASCADE|RESTRICT clause is available in Hive 1.1.0. ALTER TABLE ADD|REPLACE COLUMNS with CASCADE command changes the columns of a table\u0026rsquo;s metadata, and cascades the same change to all the partition metadata. RESTRICT is the default, limiting column changes only to table metadata.\nALTER TABLE ADD or REPLACE COLUMNS CASCADE will override the table partition\u0026rsquo;s column metadata regardless of the table or partition\u0026rsquo;s protection mode. Use with discretion.The column change command will only modify Hive\u0026rsquo;s metadata, and will not modify data. Users should make sure the actual data layout of the table/partition conforms with the metadata definition.\nPartial Partition Specification As of Hive 0.14 (HIVE-8411), users are able to provide a partial partition spec for certain above alter column statements, similar to dynamic partitioning. So rather than having to issue an alter column statement for each partition that needs to be changed:\nALTER TABLE foo PARTITION (ds='2008-04-08', hr=11) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); ALTER TABLE foo PARTITION (ds='2008-04-08', hr=12) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); ... \u0026hellip; you can change many existing partitions at once using a single ALTER statement with a partial partition specification:\n// hive.exec.dynamic.partition needs to be set to true to enable dynamic partitioning with ALTER PARTITION SET hive.exec.dynamic.partition = true; // This will alter all existing partitions in the table with ds='2008-04-08' -- be sure you know what you are doing! ALTER TABLE foo PARTITION (ds='2008-04-08', hr) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); // This will alter all existing partitions in the table -- be sure you know what you are doing! ALTER TABLE foo PARTITION (ds, hr) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); Similar to dynamic partitioning, hive.exec.dynamic.partition must be set to true to enable use of partial partition specs during ALTER PARTITION. This is supported for the following operations:\n Change column Add column Replace column File Format Serde Properties  Create/Drop/Alter View  Create View Drop View Alter View Properties Alter View As Select  Version information\nView support is only available in Hive 0.6 and later.\nCreate View CREATE VIEW [IF NOT EXISTS] [db_name.]view_name [(column_name [COMMENT column_comment], ...) ] [COMMENT view_comment] [TBLPROPERTIES (property_name = property_value, ...)] AS SELECT ...; CREATE VIEW creates a view with the given name. An error is thrown if a table or view with the same name already exists. You can use IF NOT EXISTS to skip the error.\nIf no column names are supplied, the names of the view\u0026rsquo;s columns will be derived automatically from the defining SELECT expression. (If the SELECT contains unaliased scalar expressions such as x+y, the resulting view column names will be generated in the form _C0, _C1, etc.) When renaming columns, column comments can also optionally be supplied. (Comments are not automatically inherited from underlying columns.)\nA CREATE VIEW statement will fail if the view\u0026rsquo;s defining SELECT expression is invalid.\nNote that a view is a purely logical object with no associated storage. When a query references a view, the view\u0026rsquo;s definition is evaluated in order to produce a set of rows for further processing by the query. (This is a conceptual description; in fact, as part of query optimization, Hive may combine the view\u0026rsquo;s definition with the query\u0026rsquo;s, e.g. pushing filters from the query down into the view.)\nA view\u0026rsquo;s schema is frozen at the time the view is created; subsequent changes to underlying tables (e.g. adding a column) will not be reflected in the view\u0026rsquo;s schema. If an underlying table is dropped or changed in an incompatible fashion, subsequent attempts to query the invalid view will fail.\nViews are read-only and may not be used as the target of LOAD/INSERT/ALTER. For changing metadata, see ALTER VIEW.\nA view may contain ORDER BY and LIMIT clauses. If a referencing query also contains these clauses, the query-level clauses are evaluated after the view clauses (and after any other operations in the query). For example, if a view specifies LIMIT 5, and a referencing query is executed as (select * from v LIMIT 10), then at most 5 rows will be returned.\nStarting with Hive 0.13.0, the view\u0026rsquo;s select statement can include one or more common table expressions (CTEs) as shown in the SELECT syntax. For examples of CTEs in CREATE VIEW statements, see Common Table Expression.\nExample:\nCREATE VIEW onion_referrers(url COMMENT 'URL of Referring page') COMMENT 'Referrers to The Onion website' AS SELECT DISTINCT referrer_url FROM page_view WHERE page_url='http://www.theonion.com'; Use SHOW CREATE TABLE to display the CREATE VIEW statement that created a view. As of Hive 2.2.0, SHOW VIEWS displays a list of views in a database.\nVersion Information\nOriginally, the file format for views was hard coded as SequenceFile. Hive 2.1.0 (HIVE-13736) made views follow the same defaults as tables and indexes using the hive.default.fileformatand hive.default.fileformat.managed properties.\nDrop View DROP VIEW [IF EXISTS] [db_name.]view_name; DROP VIEW removes metadata for the specified view. (It is illegal to use DROP TABLE on a view.)\nWhen dropping a view referenced by other views, no warning is given (the dependent views are left dangling as invalid and must be dropped or recreated by the user).\nIn Hive 0.7.0 or later, DROP returns an error if the view doesn\u0026rsquo;t exist, unless IF EXISTS is specified or the configuration variable hive.exec.drop.ignorenonexistent is set to true.\nExample:\nDROP VIEW onion_referrers; Alter View Properties ALTER VIEW [db_name.]view_name SET TBLPROPERTIES table_properties; table_properties: : (property_name = property_value, property_name = property_value, ...) As with ALTER TABLE, you can use this statement to add your own metadata to a view.\nAlter View As Select Version information\nAs of Hive 0.11.\nALTER VIEW [db_name.]view_name AS select_statement; Alter View As Select changes the definition of a view, which must exist. The syntax is similar to that for CREATE VIEW and the effect is the same as for CREATE OR REPLACE VIEW.\nNote: The view must already exist, and if the view has partitions, it could not be replaced by Alter View As Select.\nCreate/Drop/Alter Materialized View  Create Materialized View Drop Materialized View Alter Materialized View  Version information\nMaterialized view support is only available in Hive 3.0 and later.\nThis section provides an introduction to Hive materialized views syntax. More information about materialized view support and usage in Hive can be found here.\nCreate Materialized View CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [DISABLE REWRITE] [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] [CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] AS SELECT ...; CREATE MATERIALIZED VIEW creates a view with the given name. An error is thrown if a table, view or materialized view with the same name already exists. You can use IF NOT EXISTS to skip the error.\nThe names of the materialized view\u0026rsquo;s columns will be derived automatically from the defining SELECT expression.\nA CREATE MATERIALIZED VIEW statement will fail if the view\u0026rsquo;s defining SELECT expression is invalid.\nBy default, materialized views are enabled to be used by the query optimizer for automatic rewriting when they are created.\nVersion information\nPARTITIONED ON is supported as of Hive 3.2.0 (HIVE-14493).\nVersion information\nCLUSTERED/DISTRIBUTED/SORTED ON is supported as of Hive 4.0.0 (HIVE-18842).\nDrop Materialized View DROP MATERIALIZED VIEW [db_name.]materialized_view_name; DROP MATERIALIZED VIEW removes metadata and data for this materialized view.\nAlter Materialized View Once a materialized view has been created, the optimizer will be able to exploit its definition semantics to automatically rewrite incoming queries using materialized views, and hence, accelerate query execution. Users can selectively enable/disable materialized views for rewriting. Recall that, by default, materialized views are enabled for rewriting at creation time. To alter that behavior, the following statement can be used:\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE; Create/Drop/Alter Index Version information\nAs of Hive 0.7.\nIndexing Is Removed since 3.0! See Indexes design document\nThis section provides a brief introduction to Hive indexes, which are documented more fully here:\n Overview of Hive Indexes Indexes design document  In Hive 0.12.0 and earlier releases, the index name is case-sensitive for CREATE INDEX and DROP INDEX statements. However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752). This bug is fixed in Hive 0.13.0 by making index names case-insensitive for all HiveQL statements. For releases prior to 0.13.0, the best practice is to use lowercase letters for all index names.\nCreate Index CREATE INDEX index_name ON TABLE base_table_name (col_name, ...) AS index_type [WITH DEFERRED REBUILD] [IDXPROPERTIES (property_name=property_value, ...)] [IN TABLE index_table_name] [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] [LOCATION hdfs_path] [TBLPROPERTIES (...)] [COMMENT \u0026quot;index comment\u0026quot;]; CREATE INDEX creates an index on a table using the given list of columns as keys. See CREATE INDEX in the Indexes design document.\nDrop Index DROP INDEX [IF EXISTS] index_name ON table_name; DROP INDEX drops the index, as well as deleting the index table.\nIn Hive 0.7.0 or later, DROP returns an error if the index doesn\u0026rsquo;t exist, unless IF EXISTS is specified or the configuration variable hive.exec.drop.ignorenonexistent is set to true.\nAlter Index ALTER INDEX index_name ON table_name [PARTITION partition_spec] REBUILD; ALTER INDEX \u0026hellip; REBUILD builds an index that was created using the WITH DEFERRED REBUILD clause, or rebuilds a previously built index. If PARTITION is specified, only that partition is rebuilt.\nCreate/Drop Macro Version information\nAs of Hive 0.12.0.\nBug fixes:\n Prior to Hive 1.3.0 and 2.0.0 when a HiveQL macro was used more than once while processing the same row, Hive returned the same result for all invocations even though the arguments were different. (See HIVE-11432.) Prior to Hive 1.3.0 and 2.0.0 when multiple macros were used while processing the same row, an ORDER BY clause could give wrong results. (See HIVE-12277.) Prior to Hive 2.1.0 when multiple macros were used while processing the same row, results of the later macros were overwritten by that of the first. (See HIVE-13372.) Hive 0.12.0 introduced macros to HiveQL, prior to which they could only be created in Java.  Create Temporary Macro CREATE TEMPORARY MACRO macro_name([col_name col_type, ...]) expression; CREATE TEMPORARY MACRO creates a macro using the given optional list of columns as inputs to the expression. Macros exist for the duration of the current session.\nExamples:\nCREATE TEMPORARY MACRO fixed_number() 42; CREATE TEMPORARY MACRO string_len_plus_two(x string) length(x) + 2; CREATE TEMPORARY MACRO simple_add (x int, y int) x + y; Drop Temporary Macro DROP TEMPORARY MACRO [IF EXISTS] macro_name; DROP TEMPORARY MACRO returns an error if the function doesn\u0026rsquo;t exist, unless IF EXISTS is specified.\nCreate/Drop/Reload Function Temporary Functions Create Temporary Function CREATE TEMPORARY FUNCTION function_name AS class_name; This statement lets you create a function that is implemented by the class_name. You can use this function in Hive queries as long as the session lasts. You can use any class that is in the class path of Hive. You can add jars to class path by executing \u0026lsquo;ADD JAR\u0026rsquo; statements. Please refer to the CLI section Hive Interactive Shell Commands, including Hive Resources, for more information on how to add/delete files from the Hive classpath. Using this, you can register User Defined Functions (UDF\u0026rsquo;s).\nAlso see Hive Plugins for general information about creating custom UDFs.\nDrop Temporary Function You can unregister a UDF as follows:\nDROP TEMPORARY FUNCTION [IF EXISTS] function_name; In Hive 0.7.0 or later, DROP returns an error if the function doesn\u0026rsquo;t exist, unless IF EXISTS is specified or the configuration variable hive.exec.drop.ignorenonexistent is set to true.\nPermanent Functions In Hive 0.13 or later, functions can be registered to the metastore, so they can be referenced in a query without having to create a temporary function each session.\nCreate Function Version information\nAs of Hive 0.13.0 (HIVE-6047).\nCREATE FUNCTION [db_name.]function_name AS class_name [USING JAR|FILE|ARCHIVE 'file_uri' [, JAR|FILE|ARCHIVE 'file_uri'] ]; This statement lets you create a function that is implemented by the class_name. Jars, files, or archives which need to be added to the environment can be specified with the USING clause; when the function is referenced for the first time by a Hive session, these resources will be added to the environment as if ADD JAR/FILE had been issued. If Hive is not in local mode, then the resource location must be a non-local URI such as an HDFS location.\nThe function will be added to the database specified, or to the current database at the time that the function was created. The function can be referenced by fully qualifying the function name (db_name.function_name), or can be referenced without qualification if the function is in the current database.\nDrop Function Version information\nAs of Hive 0.13.0 (HIVE-6047).\nDROP FUNCTION [IF EXISTS] function_name; DROP returns an error if the function doesn\u0026rsquo;t exist, unless IF EXISTS is specified or the configuration variable hive.exec.drop.ignorenonexistent is set to true.\nReload Function Version information\nAs of Hive 1.2.0 (HIVE-2573).\nRELOAD (FUNCTIONS|FUNCTION); As of HIVE-2573, creating permanent functions in one Hive CLI session may not be reflected in HiveServer2 or other Hive CLI sessions, if they were started before the function was created. Issuing RELOAD FUNCTIONS within a HiveServer2 or HiveCLI session will allow it to pick up any changes to the permanent functions that may have been done by a different HiveCLI session. Due to backward compatibility reasons RELOAD FUNCTION; is also accepted.\nCreate/Drop/Grant/Revoke Roles and Privileges Hive deprecated authorization mode / Legacy Mode has information about these DDL statements:\n CREATE ROLE GRANT ROLE REVOKE ROLE GRANT privilege_type REVOKE privilege_type DROP ROLE SHOW ROLE GRANT SHOW GRANT  For SQL standard based authorization in Hive 0.13.0 and later releases, see these DDL statements:\n Role Management Commands  CREATE ROLE GRANT ROLE REVOKE ROLE DROP ROLE SHOW ROLES SHOW ROLE GRANT SHOW CURRENT ROLES SET ROLE SHOW PRINCIPALS   Object Privilege Commands  GRANT privilege_type REVOKE privilege_type SHOW GRANT    Show  Show Databases Show Connectors Show Tables/Views/Materialized Views/Partitions/Indexes  Show Tables Show Views Show Materialized Views Show Partitions Show Table/Partition Extended Show Table Properties Show Create Table Show Indexes   Show Columns Show Functions Show Granted Roles and Privileges Show Locks Show Conf Show Transactions Show Compactions  These statements provide a way to query the Hive metastore for existing data and metadata accessible to this Hive system.\nShow Databases SHOW (DATABASES|SCHEMAS) [LIKE 'identifier_with_wildcards']; SHOW DATABASES or SHOW SCHEMAS lists all of the databases defined in the metastore. The uses of SCHEMAS and DATABASES are interchangeable – they mean the same thing.\nThe optional LIKE clause allows the list of databases to be filtered using a regular expression. Wildcards in the regular expression can only be \u0026lsquo;\u0026rsquo; for any character(s) or \u0026lsquo;|\u0026rsquo; for a choice. Examples are \u0026lsquo;employees\u0026rsquo;, \u0026lsquo;emp\u0026rsquo;, \u0026lsquo;emp*|*ees\u0026rsquo;, all of which will match the database named \u0026lsquo;employees\u0026rsquo;.\nVersion information: SHOW DATABASES\nStarting from 4.0.0 we accept only SQL type like expressions containing \u0026lsquo;%\u0026rsquo; for any character(s), and \u0026lsquo;_\u0026rsquo; for a single character. Examples are \u0026lsquo;employees\u0026rsquo;, \u0026lsquo;emp%\u0026rsquo;, \u0026lsquo;emplo_ees\u0026rsquo;, all of which will match the database named \u0026lsquo;employees\u0026rsquo;.\nShow Connectors SHOW CONNECTORS; Since Hive 4.0.0 via HIVE-24396\nSHOW CONNECTORS lists all of the connectors defined in the metastore (depending on the user\u0026rsquo;s access).\nShow Tables/Views/Materialized Views/Partitions/Indexes Show Tables SHOW TABLES [IN database_name] ['identifier_with_wildcards']; SHOW TABLES lists all the base tables and views in the current database (or the one explicitly named using the IN clause) with names matching the optional regular expression. Wildcards in the regular expression can only be \u0026lsquo;\u0026rsquo; for any character(s) or \u0026lsquo;|\u0026rsquo; for a choice. Examples are \u0026lsquo;page_view\u0026rsquo;, \u0026lsquo;page_v\u0026rsquo;, \u0026lsquo;view|page\u0026rsquo;, all which will match the \u0026lsquo;page_view\u0026rsquo; table. Matching tables are listed in alphabetical order. It is not an error if there are no matching tables found in metastore. If no regular expression is given then all tables in the selected database are listed.\nShow Views Version information\nIntroduced in Hive 2.2.0 via HIVE-14558.\nSHOW VIEWS [IN/FROM database_name] [LIKE 'pattern_with_wildcards']; SHOW VIEWS lists all the views in the current database (or the one explicitly named using the IN or FROM clause) with names matching the optional regular expression. Wildcards in the regular expression can only be \u0026lsquo;\u0026rsquo; for any character(s) or \u0026lsquo;|\u0026rsquo; for a choice. Examples are \u0026lsquo;page_view\u0026rsquo;, \u0026lsquo;page_v\u0026rsquo;, \u0026lsquo;view|page\u0026rsquo;, all which will match the \u0026lsquo;page_view\u0026rsquo; view. Matching views are listed in alphabetical order. It is not an error if no matching views are found in metastore. If no regular expression is given then all views in the selected database are listed.\nExamples\nSHOW VIEWS; -- show all views in the current database SHOW VIEWS 'test_*'; -- show all views that start with \u0026quot;test_\u0026quot; SHOW VIEWS '*view2'; -- show all views that end in \u0026quot;view2\u0026quot; SHOW VIEWS LIKE 'test_view1|test_view2'; -- show views named either \u0026quot;test_view1\u0026quot; or \u0026quot;test_view2\u0026quot; SHOW VIEWS FROM test1; -- show views from database test1 SHOW VIEWS IN test1; -- show views from database test1 (FROM and IN are same) SHOW VIEWS IN test1 \u0026quot;test_*\u0026quot;; -- show views from database test2 that start with \u0026quot;test_\u0026quot; Show Materialized Views SHOW MATERIALIZED VIEWS [IN/FROM database_name] [LIKE 'pattern_with_wildcards’]; SHOW MATERIALIZED VIEWS lists all the views in the current database (or the one explicitly named using the IN or FROM clause) with names matching the optional regular expression. It also shows additional information about the materialized view, e.g., whether rewriting is enabled, and the refresh mode for the materialized view. Wildcards in the regular expression can only be \u0026lsquo;*\u0026rsquo; for any character(s) or \u0026lsquo;|\u0026rsquo; for a choice. If no regular expression is given then all materialized views in the selected database are listed.\nShow Partitions SHOW PARTITIONS table_name; SHOW PARTITIONS lists all the existing partitions for a given base table. Partitions are listed in alphabetical order.\nVersion information\nAs of Hive 0.6, SHOW PARTITIONS can filter the list of partitions as shown below.\nIt is also possible to specify parts of a partition specification to filter the resulting list.\nExamples:\nSHOW PARTITIONS table_name PARTITION(ds='2010-03-03'); -- (Note: Hive 0.6 and later) SHOW PARTITIONS table_name PARTITION(hr='12'); -- (Note: Hive 0.6 and later) SHOW PARTITIONS table_name PARTITION(ds='2010-03-03', hr='12'); -- (Note: Hive 0.6 and later) Version information\nStarting with Hive 0.13.0, SHOW PARTITIONS can specify a database (HIVE-5912).\nSHOW PARTITIONS [db_name.]table_name [PARTITION(partition_spec)]; -- (Note: Hive 0.13.0 and later) Example:\nSHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03', hr='12'); -- (Note: Hive 0.13.0 and later) Version information\nStarting with Hive 4.0.0, SHOW PARTITIONS can optionally use the WHERE/ORDER BY/LIMIT clause to filter/order/limit the resulting list (HIVE-22458). These clauses work in a similar way as they do in a SELECT statement.\nSHOW PARTITIONS [db_name.]table_name [PARTITION(partition_spec)] [WHERE where_condition] [ORDER BY col_list] [LIMIT rows]; -- (Note: Hive 4.0.0 and later) Example:\nSHOW PARTITIONS databaseFoo.tableBar LIMIT 10; -- (Note: Hive 4.0.0 and later) SHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03') LIMIT 10; -- (Note: Hive 4.0.0 and later) SHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03') ORDER BY hr DESC LIMIT 10; -- (Note: Hive 4.0.0 and later) SHOW PARTITIONS databaseFoo.tableBar PARTITION(ds='2010-03-03') WHERE hr \u0026gt;= 10 ORDER BY hr DESC LIMIT 10; -- (Note: Hive 4.0.0 and later) SHOW PARTITIONS databaseFoo.tableBar WHERE hr \u0026gt;= 10 AND ds='2010-03-03' ORDER BY hr DESC LIMIT 10; -- (Note: Hive 4.0.0 and later) Note: Please use hr \u0026gt;= 10 instead of hr - 10 \u0026gt;= 0 to filter the results, as Metastore would not push the latter predicate down into the underlying storage.\nShow Table/Partition Extended SHOW TABLE EXTENDED [IN|FROM database_name] LIKE 'identifier_with_wildcards' [PARTITION(partition_spec)]; SHOW TABLE EXTENDED will list information for all tables matching the given regular expression. Users cannot use regular expression for table name if a partition specification is present. This command\u0026rsquo;s output includes basic table information and file system information like totalNumberFiles, totalFileSize, maxFileSize, minFileSize,lastAccessTime, and lastUpdateTime. If partition is present, it will output the given partition\u0026rsquo;s file system information instead of table\u0026rsquo;s file system information.\nExample\nhive\u0026gt; show table extended like part_table; OK tableName:part_table owner:thejas location:file:/tmp/warehouse/part_table inputformat:org.apache.hadoop.mapred.TextInputFormat outputformat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat columns:struct columns { i32 i} partitioned:true partitionColumns:struct partition_columns { string d} totalNumberFiles:1 totalFileSize:2 maxFileSize:2 minFileSize:2 lastAccessTime:0 lastUpdateTime:1459382233000 Show Table Properties Version information\nAs of Hive 0.10.0.\nSHOW TBLPROPERTIES tblname; SHOW TBLPROPERTIES tblname(\u0026quot;foo\u0026quot;); The first form lists all of the table properties for the table in question, one per row separated by tabs. The second form of the command prints only the value for the property that\u0026rsquo;s being asked for.\nFor more information, see the TBLPROPERTIES clause in Create Table above.\nShow Create Table Version information\nAs of Hive 0.10.\nSHOW CREATE TABLE ([db_name.]table_name|view_name); SHOW CREATE TABLE shows the CREATE TABLE statement that creates a given table, or the CREATE VIEW statement that creates a given view.\nShow Indexes Version information\nAs of Hive 0.7.\nIndexing Is Removed since 3.0! See Indexes design document\nSHOW [FORMATTED] (INDEX|INDEXES) ON table_with_index [(FROM|IN) db_name]; SHOW INDEXES shows all of the indexes on a certain column, as well as information about them: index name, table name, names of the columns used as keys, index table name, index type, and comment. If the FORMATTED keyword is used, then column titles are printed for each column.\nShow Columns Version information\nAs of Hive 0.10.\nSHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name]; SHOW COLUMNS shows all the columns in a table including partition columns.\nVersion information\nSHOW COLUMNS (FROM|IN) table_name [(FROM|IN) db_name] [ LIKE 'pattern_with_wildcards']; Added in Hive 3.0 by HIVE-18373.\nSHOW COLUMNS lists all the columns in the table with names matching the optional regular expression. Wildcards in the regular expression can only be \u0026lsquo;\u0026rsquo; for any character(s) or \u0026lsquo;|\u0026rsquo; for a choice. Examples are \u0026lsquo;cola\u0026rsquo;, \u0026lsquo;col\u0026rsquo;, \u0026lsquo;a|col\u0026rsquo;, all which will match the \u0026lsquo;cola\u0026rsquo; column. Matching columns are listed in alphabetical order. It is not an error if no matching columns are found in table. If no regular expression is given then all columns in the selected table are listed.\nExamples\n-- SHOW COLUMNS CREATE DATABASE test_db; USE test_db; CREATE TABLE foo(col1 INT, col2 INT, col3 INT, cola INT, colb INT, colc INT, a INT, b INT, c INT); -- SHOW COLUMNS basic syntax SHOW COLUMNS FROM foo; -- show all column in foo SHOW COLUMNS FROM foo \u0026quot;*\u0026quot;; -- show all column in foo SHOW COLUMNS IN foo \u0026quot;col*\u0026quot;; -- show columns in foo starting with \u0026quot;col\u0026quot; OUTPUT col1,col2,col3,cola,colb,colc SHOW COLUMNS FROM foo '*c'; -- show columns in foo ending with \u0026quot;c\u0026quot; OUTPUT c,colc SHOW COLUMNS FROM foo LIKE \u0026quot;col1|cola\u0026quot;; -- show columns in foo either col1 or cola OUTPUT col1,cola SHOW COLUMNS FROM foo FROM test_db LIKE 'col*'; -- show columns in foo starting with \u0026quot;col\u0026quot; OUTPUT col1,col2,col3,cola,colb,colc SHOW COLUMNS IN foo IN test_db LIKE 'col*'; -- show columns in foo starting with \u0026quot;col\u0026quot; (FROM/IN same) OUTPUT col1,col2,col3,cola,colb,colc -- Non existing column pattern resulting in no match SHOW COLUMNS IN foo \u0026quot;nomatch*\u0026quot;; SHOW COLUMNS IN foo \u0026quot;col+\u0026quot;; -- + wildcard not supported SHOW COLUMNS IN foo \u0026quot;nomatch\u0026quot;; Show Functions SHOW FUNCTIONS [LIKE \u0026quot;\u0026lt;pattern\u0026gt;\u0026quot;]; SHOW FUNCTIONS lists all the user defined and builtin functions, filtered by the the regular expression if specified with LIKE.\nShow Granted Roles and Privileges Hive deprecated authorization mode / Legacy Mode has information about these SHOW statements:\n SHOW ROLE GRANT SHOW GRANT  In Hive 0.13.0 and later releases, SQL standard based authorization has these SHOW statements:\n SHOW ROLE GRANT SHOW GRANT SHOW CURRENT ROLES SHOW ROLES SHOW PRINCIPALS  Show Locks SHOW LOCKS \u0026lt;table_name\u0026gt;; SHOW LOCKS \u0026lt;table_name\u0026gt; EXTENDED; SHOW LOCKS \u0026lt;table_name\u0026gt; PARTITION (\u0026lt;partition_spec\u0026gt;); SHOW LOCKS \u0026lt;table_name\u0026gt; PARTITION (\u0026lt;partition_spec\u0026gt;) EXTENDED; SHOW LOCKS (DATABASE|SCHEMA) database_name; -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0) SHOW LOCKS displays the locks on a table or partition. See Hive Concurrency Model for information about locks.\nSHOW LOCKS (DATABASE|SCHEMA) is supported from Hive 0.13 for DATABASE (see HIVE-2093) and Hive 0.14 for SCHEMA (see HIVE-6601). SCHEMA and DATABASE are interchangeable – they mean the same thing.\nWhen Hive transactions are being used, SHOW LOCKS returns this information (see HIVE-6460):\n database name table name partition name (if the table is partitioned) the state the lock is in, which can be:  \u0026ldquo;acquired\u0026rdquo; – the requestor holds the lock \u0026ldquo;waiting\u0026rdquo; – the requestor is waiting for the lock \u0026ldquo;aborted\u0026rdquo; – the lock has timed out but has not yet been cleaned up   Id of the lock blocking this one, if this lock is in \u0026ldquo;waiting\u0026rdquo; state the type of lock, which can be:  \u0026ldquo;exclusive\u0026rdquo; – no one else can hold the lock at the same time (obtained mostly by DDL operations such as drop table) \u0026ldquo;shared_read\u0026rdquo; – any number of other shared_read locks can lock the same resource at the same time (obtained by reads; confusingly, an insert operation also obtains a shared_read lock) \u0026ldquo;shared_write\u0026rdquo; – any number of shared_read locks can lock the same resource at the same time, but no other shared_write locks are allowed (obtained by update and delete)   ID of the transaction this lock is associated with, if there is one last time the holder of this lock sent a heartbeat indicating it was still alive the time the lock was acquired, if it has been acquired Hive user who requested the lock host the user is running on agent info – a string that helps identify the entity that issued the lock request. For a SQL client this is the query ID, for streaming client it may be Storm bolt ID for example.  Show Conf Version information\nAs of Hive 0.14.0.\nSHOW CONF \u0026lt;configuration_name\u0026gt;; SHOW CONF returns a description of the specified configuration property.\n default value required type description  Note that SHOW CONF does not show the current value of a configuration property. For current property settings, use the \u0026ldquo;set\u0026rdquo; command in the CLI or a HiveQL script (see Commands) or in Beeline (see Beeline Hive Commands).\nShow Transactions Version information\nAs of Hive 0.13.0 (see Hive Transactions).\nSHOW TRANSACTIONS; SHOW TRANSACTIONS is for use by administrators when Hive transactions are being used. It returns a list of all currently open and aborted transactions in the system, including this information:\n transaction ID transaction state user who started the transaction machine where the transaction was started timestamp when the transaction was started (as of Hive 2.2.0) timestamp for last heartbeat (as of Hive 2.2.0 )  Show Compactions Version information\nAs of Hive 0.13.0 (see Hive Transactions).\nSHOW COMPACTIONS [DATABASE.][TABLE] [PARTITION (\u0026lt;partition_spec\u0026gt;)] [POOL_NAME] [TYPE] [STATE] [ORDER BY `start` DESC] [LIMIT 10]; SHOW COMPACTIONS returns a list of all compaction requests currently being processed or scheduled, including this information:\n \u0026ldquo;CompactionId\u0026rdquo; - unique internal id (As of Hive 3.0) \u0026ldquo;Database\u0026rdquo; - Hive database name \u0026ldquo;Table\u0026rdquo; - table name \u0026ldquo;Partition\u0026rdquo; - partition name (if the table is partitioned) \u0026ldquo;Type\u0026rdquo; - whether it is a major or minor compaction \u0026ldquo;State\u0026rdquo; - the state the compaction is in, which can be:  \u0026ldquo;initiated\u0026rdquo; – waiting in the queue to be compacted \u0026ldquo;working\u0026rdquo; – being compacted \u0026ldquo;ready for cleaning\u0026rdquo; – the compaction has been done and the old files are scheduled to be cleaned \u0026ldquo;failed\u0026rdquo; – the job failed. The metastore log will have more detail. \u0026ldquo;succeeded\u0026rdquo; – A-ok \u0026ldquo;attempted\u0026rdquo; – initiator attempted to schedule a compaction but failed. The metastore log will have more information.   \u0026ldquo;Worker\u0026rdquo; - thread ID of the worker thread doing the compaction (only if in working state) \u0026ldquo;Start Time\u0026rdquo; - the time at which the compaction started (only if in working or ready for cleaning state) \u0026ldquo;Duration(ms)\u0026rdquo; - time this compaction took (As of Hive 2.2 ) \u0026ldquo;HadoopJobId\u0026rdquo; - Id of the submitted Hadoop job (As of Hive 2.2) “Enqueue Time” - Time spent by compaction before start “Initiator host”- Host ID which started compaction “TxnId” - A transaction Id associated with this compaction “Commit Time” - Total time taken by compaction “Highest WriteId” Highest writeId that compactor includes “Pool name”- A pool associated with given compaction or default if not associated “Error message”- error message if any  Examples:\nExamples SHOW COMPACTIONS. — show all compactions of all tables and partitions currently being compacted or scheduled for compaction SHOW COMPACTIONS DATABASE db1 — show all compactions of all tables from given database which are currently being compacted or scheduled for compaction SHOW COMPACTIONS SCHEMA db1 — show all compactions of all tables from given database which are currently being compacted or scheduled for compaction SHOW COMPACTIONS tbl0 — show all compactions from given table which are currently being compacted or scheduled for compaction SHOW COMPACTIONS compactionid =1 — show all compactions with given compaction ID SHOW COMPACTIONS db1.tbl0 PARTITION (p=101,day='Monday') POOL 'pool0' TYPE 'minor' STATUS 'ready for clean' ORDER BY cq_table DESC, cq_state LIMIT 42 — show all compactions from specific database/table filtered based on pool name/type.state/status and ordered with given clause Compactions are initiated automatically, but can also be initiated manually with an ALTER TABLE COMPACT statement.\nDescribe  Describe Database Describe Dataconnector Describe Table/View/Materialized View/Column  Display Column Statistics   Describe Partition Hive 2.0+: Syntax Change  Describe Database Version information\nAs of Hive 0.7.\nDESCRIBE DATABASE [EXTENDED] db_name; DESCRIBE SCHEMA [EXTENDED] db_name; -- (Note: Hive 1.1.0 and later) DESCRIBE DATABASE shows the name of the database, its comment (if one has been set), and its root location on the filesystem. The uses of SCHEMA and DATABASE are interchangeable – they mean the same thing. DESCRIBE SCHEMA is added in Hive 1.1.0 (HIVE-8803).\nEXTENDED also shows the database properties.\nDescribe Dataconnector DESCRIBE CONNECTOR [EXTENDED] connector_name; Since Hive 4.0.0 via HIVE-24396\nDESCRIBE CONNECTOR shows the name of the connector, its comment (if one has been set), and its datasource URL and datasource type. EXTENDED also shows the dataconnector\u0026rsquo;s properties. Any clear-text passwords set will be shown in clear text as well.\nDescribe Table/View/Materialized View/Column There are two formats for the describe table/view/materialized view/column syntax, depending on whether or not the database is specified.\nIf the database is not specified, the optional column information is provided after a dot:\nDESCRIBE [EXTENDED|FORMATTED] table_name[.col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ]; -- (Note: Hive 1.x.x and 0.x.x only. See \u0026quot;Hive 2.0+: New Syntax\u0026quot; below) If the database is specified, the optional column information is provided after a space:\nDESCRIBE [EXTENDED|FORMATTED] [db_name.]table_name[ col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ]; -- (Note: Hive 1.x.x and 0.x.x only. See \u0026quot;Hive 2.0+: New Syntax\u0026quot; below) DESCRIBE shows the list of columns including partition columns for the given table. If the EXTENDED keyword is specified then it will show all the metadata for the table in Thrift serialized form. This is generally only useful for debugging and not for general use. If the FORMATTED keyword is specified, then it will show the metadata in a tabular format.\nNote: DESCRIBE EXTENDED shows the number of rows only if statistics were gathered when the data was loaded (see Newly Created Tables), and if the Hive CLI is used instead of a Thrift client or Beeline. HIVE-6285 will address this issue. Although ANALYZE TABLE gathers statistics after the data has been loaded (see Existing Tables), it does not currently provide information about the number of rows.\nIf a table has a complex column then you can examine the attributes of this column by specifying table_name.complex_col_name (and field_name for an element of a struct, \u0026lsquo;$elem$\u0026rsquo; for array element, \u0026lsquo;$key$\u0026rsquo; for map key, and \u0026lsquo;$value$\u0026rsquo; for map value). You can specify this recursively to explore the complex column type.\nFor a view, DESCRIBE EXTENDED or FORMATTED can be used to retrieve the view\u0026rsquo;s definition. Two relevant attributes are provided: both the original view definition as specified by the user, and an expanded definition used internally by Hive.\nFor materialized views, DESCRIBE EXTENDED or FORMATTED provides additional information on whether rewriting is enabled and whether the given materialized view is considered to be up-to-date for automatic rewriting with respect to the data in the source tables that it uses.\nVersion information — partition \u0026amp; non-partition columns\nIn Hive 0.10.0 and earlier, no distinction is made between partition columns and non-partition columns while displaying columns for DESCRIBE TABLE. From Hive 0.12.0 onwards, they are displayed separately.\nIn Hive 0.13.0 and later, the configuration parameter hive.display.partition.cols.separately lets you use the old behavior, if desired (HIVE-6689). For an example, see the test case in the patch for HIVE-6689.\nBug fixed in Hive 0.10.0 — database qualifiers\nDatabase qualifiers for table names were introduced in Hive 0.7.0, but they were broken for DESCRIBE until a bug fix in Hive 0.10.0 (HIVE-1977).\nBug fixed in Hive 0.13.0 — quoted identifiers\nPrior to Hive 0.13.0 DESCRIBE did not accept backticks () surrounding table identifiers, so DESCRIBE could not be used for tables with names that matched reserved keywords ([HIVE-2949](https://issues.apache.org/jira/browse/HIVE-2949) and [HIVE-6187](https://issues.apache.org/jira/browse/HIVE-6187)). As of 0.13.0, all identifiers specified within backticks are treated literally when the configuration parameter [hive.support.quoted.identifiers](#hive-support-quoted-identifiers) has its default value of \u0026quot;column`\u0026rdquo; (HIVE-6013). The only exception is that double backticks (``) represent a single backtick character.\nDisplay Column Statistics Version information\nAs of Hive 0.14.0; see HIVE-7050 and HIVE-7051. (The FOR COLUMNS option of ANALYZE TABLE is available as of Hive 0.10.0.)\nANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS will compute column statistics for all columns in the specified table (and for all partitions if the table is partitioned). To view the gathered column statistics, the following statements can be used:\nDESCRIBE FORMATTED [db_name.]table_name column_name; -- (Note: Hive 0.14.0 and later) DESCRIBE FORMATTED [db_name.]table_name column_name PARTITION (partition_spec); -- (Note: Hive 0.14.0 to 1.x.x) -- (see \u0026quot;Hive 2.0+: New Syntax\u0026quot; below) See Statistics in Hive: Existing Tables for more information about the ANALYZE TABLE command.\nDescribe Partition There are two formats for the describe partition syntax, depending on whether or not the database is specified.\nIf the database is not specified, the optional column information is provided after a dot:\nDESCRIBE [EXTENDED|FORMATTED] table_name[.column_name] PARTITION partition_spec; -- (Note: Hive 1.x.x and 0.x.x only. See \u0026quot;Hive 2.0+: New Syntax\u0026quot; below) If the database is specified, the optional column information is provided after a space:\nDESCRIBE [EXTENDED|FORMATTED] [db_name.]table_name [column_name] PARTITION partition_spec; -- (Note: Hive 1.x.x and 0.x.x only. See \u0026quot;Hive 2.0+: New Syntax\u0026quot; below) This statement lists metadata for a given partition. The output is similar to that of DESCRIBE table_name. Presently, the column information associated with a particular partition is not used while preparing plans. As of Hive 1.2 (HIVE-10307), the partition column values specified in partition_spec are type validated, converted and normalized to their column types when hive.typecheck.on.insert is set to true (default). These values can be number literals.\nExample:\nhive\u0026gt; show partitions part_table; OK d=abc hive\u0026gt; DESCRIBE extended part_table partition (d='abc'); OK i int d string # Partition Information # col_name data_type comment d string Detailed Partition Information Partition(values:[abc], dbName:default, tableName:part_table, createTime:1459382234, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:i, type:int, comment:null), FieldSchema(name:d, type:string, comment:null)], location:file:/tmp/warehouse/part_table/d=abc, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=1}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:{numFiles=1, COLUMN_STATS_ACCURATE=true, transient_lastDdlTime=1459382234, numRows=1, totalSize=2, rawDataSize=1}) Time taken: 0.325 seconds, Fetched: 9 row(s) hive\u0026gt; DESCRIBE formatted part_table partition (d='abc'); OK # col_name data_type comment i int # Partition Information # col_name data_type comment d string # Detailed Partition Information Partition Value: [abc] Database: default Table: part_table CreateTime: Wed Mar 30 16:57:14 PDT 2016 LastAccessTime: UNKNOWN Protect Mode: None Location: file:/tmp/warehouse/part_table/d=abc Partition Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 1 rawDataSize 1 totalSize 2 transient_lastDdlTime 1459382234 # Storage Information SerDe Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe InputFormat: org.apache.hadoop.mapred.TextInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: serialization.format 1 Time taken: 0.334 seconds, Fetched: 35 row(s) Hive 2.0+: Syntax Change Hive 2.0+: New syntax\nIn Hive 2.0 release onward, the describe table command has a syntax change which is backward incompatible. See HIVE-12184 for details.\nDESCRIBE [EXTENDED | FORMATTED] [db_name.]table_name [PARTITION partition_spec] [col_name ( [.field_name] | [.'$elem$'] | [.'$key$'] | [.'$value$'] )* ]; Warning: The new syntax could break current scripts.\n It no longer accepts DOT separated table_name and column_name. They would have to be SPACE-separated. DB and TABLENAME are DOT-separated. column_name can still contain DOTs for complex datatypes. Optional partition_spec has to appear after the table_name but prior to the optional column_name. In the previous syntax, column_name appears in between table_name and partition_spec.  Examples:\nDESCRIBE FORMATTED default.src_table PARTITION (part_col = 100) columnA; DESCRIBE default.src_thrift lintString.$elem$.myint; Abort  Abort Transactions  Abort Transactions Version information\nAs of Hive 1.3.0 and 2.1.0 (see Hive Transactions).\nABORT TRANSACTIONS transactionID [ transactionID ...]; ABORT TRANSACTIONS cleans up the specified transaction IDs from the Hive metastore so that users do not need to interact with the metastore directly in order to remove dangling or failed transactions. ABORT TRANSACTIONS is added in Hive 1.3.0 and 2.1.0 (HIVE-12634).\nExample:\nABORT TRANSACTIONS 0000007 0000008 0000010 0000015; This command can be used together with SHOW TRANSACTIONS. The latter can help figure out the candidate transaction IDs to be cleaned up.\nScheduled queries Documentation is available on the Scheduled Queries page.\nDatasketches integration Documentation is available on the Datasketches Integration page\nHCatalog and WebHCat DDL For information about DDL in HCatalog and WebHCat, see:\n HCatalog DDL in the HCatalog manual WebHCat DDL Resources in the WebHCat manual  Save\nSave\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-ddl_27362034/","tags":null,"title":"Apache Hive : LanguageManual DDL"},{"categories":null,"contents":"Apache Hive : LanguageManual DDL BucketedTables This is a brief example on creating and populating bucketed tables. (For another example, see Bucketed Sorted Tables.)\nBucketed tables are fantastic in that they allow much more efficient sampling than do non-bucketed tables, and they may later allow for time saving operations such as mapside joins. However, the bucketing specified at table creation is not enforced when the table is written to, and so it is possible for the table\u0026rsquo;s metadata to advertise properties which are not upheld by the table\u0026rsquo;s actual layout. This should obviously be avoided. Here\u0026rsquo;s how to do it right.\nFirst, table creation:\nCREATE TABLE user_info_bucketed(user_id BIGINT, firstname STRING, lastname STRING) COMMENT 'A bucketed copy of user_info' PARTITIONED BY(ds STRING) CLUSTERED BY(user_id) INTO 256 BUCKETS; Note that we specify a column (user_id) to base the bucketing.\nThen we populate the table\nset hive.enforce.bucketing = true; -- (Note: Not needed in Hive 2.x onward) FROM user_id INSERT OVERWRITE TABLE user_info_bucketed PARTITION (ds='2009-02-25') SELECT userid, firstname, lastname WHERE ds='2009-02-25'; Version 0.x and 1.x only\nThe command set hive.enforce.bucketing = true; allows the correct number of reducers and the cluster by column to be automatically selected based on the table. Otherwise, you would need to set the number of reducers to be the same as the number of buckets as in set mapred.reduce.tasks = 256; and have a CLUSTER BY ... clause in the select.\nHow does Hive distribute the rows across the buckets? In general, the bucket number is determined by the expression hash_function(bucketing_column) mod num_buckets. (There\u0026rsquo;s a \u0026lsquo;0x7FFFFFFF in there too, but that\u0026rsquo;s not that important). The hash_function depends on the type of the bucketing column. For an int, it\u0026rsquo;s easy, hash_int(i) == i. For example, if user_id were an int, and there were 10 buckets, we would expect all user_id\u0026rsquo;s that end in 0 to be in bucket 1, all user_id\u0026rsquo;s that end in a 1 to be in bucket 2, etc. For other datatypes, it\u0026rsquo;s a little tricky. In particular, the hash of a BIGINT is not the same as the BIGINT. And the hash of a string or a complex datatype will be some number that\u0026rsquo;s derived from the value, but not anything humanly-recognizable. For example, if user_id were a STRING, then the user_id\u0026rsquo;s in bucket 1 would probably not end in 0. In general, distributing rows based on the hash will give you a even distribution in the buckets.\nSo, what can go wrong? As long as you use the syntax above and set hive.enforce.bucketing = true (for Hive 0.x and 1.x), the tables should be populated properly. Things can go wrong if the bucketing column type is different during the insert and on read, or if you manually cluster by a value that\u0026rsquo;s different from the table definition.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-ddl-bucketedtables_27362035/","tags":null,"title":"Apache Hive : LanguageManual DDL BucketedTables"},{"categories":null,"contents":"Apache Hive : LanguageManual DML Hive Data Manipulation Language  Hive Data Manipulation Language  Loading files into tables  Syntax Synopsis Notes   Inserting data into Hive Tables from queries  Syntax Synopsis Notes Dynamic Partition Inserts  Example Additional Documentation     Writing data into the filesystem from queries  Syntax Synopsis Notes   Inserting values into tables from SQL  Syntax Synopsis Examples   Update  Syntax Synopsis Notes   Delete  Syntax Synopsis Notes   Merge  Syntax Synopsis Performance Note Notes Examples      There are multiple ways to modify data in Hive:\n LOAD INSERT  into Hive tables from queries into directories from queries into Hive tables from SQL   UPDATE DELETE MERGE  EXPORT and IMPORT commands are also available (as of Hive 0.8).\nLoading files into tables Hive does not do any transformation while loading data into tables. Load operations are currently pure copy/move operations that move datafiles into locations corresponding to Hive tables.\nSyntax LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] [INPUTFORMAT 'inputformat' SERDE 'serde'] (3.0 or later) Synopsis Load operations prior to Hive 3.0 are pure copy/move operations that move datafiles into locations corresponding to Hive tables.\n filepath can be:  a relative path, such as project/data1 an absolute path, such as /user/hive/project/data1 a full URI with scheme and (optionally) an authority, such as hdfs://namenode:9000/user/hive/project/data1   The target being loaded to can be a table or a partition. If the table is partitioned, then one must specify a specific partition of the table by specifying values for all of the partitioning columns. filepath can refer to a file (in which case Hive will move the file into the table) or it can be a directory (in which case Hive will move all the files within that directory into the table). In either case, filepath addresses a set of files. If the keyword LOCAL is specified, then:  the load command will look for filepath in the local file system. If a relative path is specified, it will be interpreted relative to the user\u0026rsquo;s current working directory. The user can specify a full URI for local files as well - for example: \u0026lt;file:///user/hive/project/data1\u0026gt; the load command will try to copy all the files addressed by filepath to the target filesystem. The target file system is inferred by looking at the location attribute of the table. The copied data files will then be moved to the table. Note: If you run this command against a HiveServer2 instance then the local path refers to a path on the HiveServer2 instance. HiveServer2 must have the proper permissions to access that file.   If the keyword LOCAL is not specified, then Hive will either use the full URI of filepath, if one is specified, or will apply the following rules:  If scheme or authority are not specified, Hive will use the scheme and authority from the hadoop configuration variable fs.default.name that specifies the Namenode URI. If the path is not absolute, then Hive will interpret it relative to /user/\u0026lt;username\u0026gt; Hive will move the files addressed by filepath into the table (or partition)   If the OVERWRITE keyword is used then the contents of the target table (or partition) will be deleted and replaced by the files referred to by filepath; otherwise the files referred by filepath will be added to the table.  Additional load operations are supported by Hive 3.0 onwards as Hive internally rewrites the load into an INSERT AS SELECT.\n If table has partitions, however, the load command does not have them, the load would be converted into INSERT AS SELECT and assume that the last set of columns are partition columns. It will throw an error if the file does not conform to the expected schema. If table is bucketed then the following rules apply:  In strict mode : launches an INSERT AS SELECT job. In non-strict mode : if the file names conform to the naming convention (if the file belongs to bucket 0, it should be named 000000_0 or 000000_0_copy_1, or if it belongs to bucket 2 the names should be like 000002_0 or 000002_0_copy_3, etc.) then it will be a pure copy/move operation, else it will launch an INSERT AS SELECT job.   filepath can contain subdirectories, provided each file conforms to the schema. inputformat can be any Hive input format such as text, ORC, etc. serde can be the associated Hive SERDE. Both inputformat and serde are case sensitive.  Example of such a schema:\nCREATE TABLE tab1 (col1 int, col2 int) PARTITIONED BY (col3 int) STORED AS ORC; LOAD DATA LOCAL INPATH 'filepath' INTO TABLE tab1; Here, partition information is missing which would otherwise give an error, however, if the file(s) located at filepath conform to the table schema such that each row ends with partition column(s) then the load will rewrite into an INSERT AS SELECT job.\nThe uncompressed data should look like this:\n(1,2,3), (2,3,4), (4,5,3) etc.\nNotes  filepath cannot contain subdirectories (except for Hive 3.0 or later, as described above). If the keyword LOCAL is not given, filepath must refer to files within the same filesystem as the table\u0026rsquo;s (or partition\u0026rsquo;s) location. Hive does some minimal checks to make sure that the files being loaded match the target table. Currently it checks that if the table is stored in sequencefile format, the files being loaded are also sequencefiles, and vice versa. A bug that prevented loading a file when its name includes the \u0026ldquo;+\u0026rdquo; character is fixed in release 0.13.0 (HIVE-6048). Please read CompressedStorage if your datafile is compressed.  Inserting data into Hive Tables from queries Query Results can be inserted into tables by using the insert clause.\nSyntax Standard syntax: INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement; INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; Hive extension (multiple inserts): FROM from_statement INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 [INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] [INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...; FROM from_statement INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 [INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] [INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2] ...; Hive extension (dynamic partition inserts): INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; Synopsis  INSERT OVERWRITE will overwrite any existing data in the table or partition  unless IF NOT EXISTS is provided for a partition (as of Hive 0.9.0). As of Hive 2.3.0 (HIVE-15880), if the table has TBLPROPERTIES (\u0026quot;auto.purge\u0026quot;=\u0026quot;true\u0026quot;) the previous data of the table is not moved to Trash when INSERT OVERWRITE query is run against the table. This functionality is applicable only for managed tables (see managed tables) and is turned off when \u0026ldquo;auto.purge\u0026rdquo; property is unset or set to false.   INSERT INTO will append to the table or partition, keeping the existing data intact. (Note: INSERT INTO syntax is only available starting in version 0.8.)  As of Hive 0.13.0, a table can be made immutable by creating it with TBLPROPERTIES (\u0026quot;immutable\u0026quot;=\u0026quot;true\u0026quot;). The default is \u0026ldquo;immutable\u0026rdquo;=\u0026ldquo;false\u0026rdquo;.\nINSERT INTO behavior into an immutable table is disallowed if any data is already present, although INSERT INTO still works if the immutable table is empty. The behavior of INSERT OVERWRITE is not affected by the \u0026ldquo;immutable\u0026rdquo; table property.\nAn immutable table is protected against accidental updates due to a script loading data into it being run multiple times by mistake. The first insert into an immutable table succeeds and successive inserts fail, resulting in only one set of data in the table, instead of silently succeeding with multiple copies of the data in the table.   Inserts can be done to a table or a partition. If the table is partitioned, then one must specify a specific partition of the table by specifying values for all of the partitioning columns. If hive.typecheck.on.insert is set to true, these values are validated, converted and normalized to conform to their column types (Hive 0.12.0 onward). Multiple insert clauses (also known as Multi Table Insert) can be specified in the same query. The output of each of the select statements is written to the chosen table (or partition). Currently the OVERWRITE keyword is mandatory and implies that the contents of the chosen table or partition are replaced with the output of corresponding select statement. The output format and serialization class is determined by the table\u0026rsquo;s metadata (as specified via DDL commands on the table). As of Hive 0.14, if a table has an OutputFormat that implements AcidOutputFormat and the system is configured to use a transaction manager that implements ACID, then INSERT OVERWRITE will be disabled for that table. This is to avoid users unintentionally overwriting transaction history. The same functionality can be achieved by using TRUNCATE TABLE (for non-partitioned tables) or DROP PARTITION followed by INSERT INTO. As of Hive 1.1.0 the TABLE keyword is optional. As of Hive 1.2.0 each INSERT INTO T can take a column list like INSERT INTO T (z, x, c1). See Description of HIVE-9481 for examples. As of Hive 3.1.0 INSERT OVERWRITE from a source with UNION ALL on full CRUD ACID tables is not allowed.  Notes  Multi Table Inserts minimize the number of data scans required. Hive can insert data into multiple tables by scanning the input data just once (and applying different query operators) to the input data. Starting with Hive 0.13.0, the select statement can include one or more common table expressions (CTEs) as shown in the SELECT syntax. For an example, see Common Table Expression.  Dynamic Partition Inserts Version information\nThis information reflects the situation in Hive 0.12; dynamic partition inserts were added in Hive 0.6.\nIn the dynamic partition inserts, users can give partial partition specifications, which means just specifying the list of partition column names in the PARTITION clause. The column values are optional. If a partition column value is given, we call this a static partition, otherwise it is a dynamic partition. Each dynamic partition column has a corresponding input column from the select statement. This means that the dynamic partition creation is determined by the value of the input column. The dynamic partition columns must be specified last among the columns in the SELECT statement and in the same order in which they appear in the PARTITION() clause. As of Hive 3.0.0 (HIVE-19083) there is no need to specify dynamic partition columns. Hive will automatically generate partition specification if it is not specified.\nDynamic partition inserts are disabled by default prior to Hive 0.9.0 and enabled by default in Hive 0.9.0 and later. These are the relevant configuration properties for dynamic partition inserts:\n   Configuration property Default Note     hive.exec.dynamic.partition true Needs to be set to true to enable dynamic partition inserts   hive.exec.dynamic.partition.mode strict In strict mode, the user must specify at least one static partition in case the user accidentally overwrites all partitions, in nonstrict mode all partitions are allowed to be dynamic   hive.exec.max.dynamic.partitions.pernode 100 Maximum number of dynamic partitions allowed to be created in each mapper/reducer node   hive.exec.max.dynamic.partitions 1000 Maximum number of dynamic partitions allowed to be created in total   hive.exec.max.created.files 100000 Maximum number of HDFS files created by all mappers/reducers in a MapReduce job   hive.error.on.empty.partition false Whether to throw an exception if dynamic partition insert generates empty results    Example FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.cnt Here the country partition will be dynamically created by the last column from the SELECT clause (i.e. pvs.cnt). Note that the name is not used. In nonstrict mode the dt partition could also be dynamically created.\nAdditional Documentation  Design Document  Original design doc HIVE-936   Tutorial: Dynamic-Partition Insert HCatalog Dynamic Partitioning  Usage with Pig Usage from MapReduce    Writing data into the filesystem from queries Query results can be inserted into filesystem directories by using a slight variation of the syntax above:\nSyntax Standard syntax: INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] (Note: Only available starting with Hive 0.11.0) SELECT ... FROM ... Hive extension (multiple inserts): FROM from_statement INSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1 [INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] (Note: Only available starting with Hive 0.13) Synopsis  Directory can be a full URI. If scheme or authority are not specified, Hive will use the scheme and authority from the hadoop configuration variable fs.default.name that specifies the Namenode URI. If LOCAL keyword is used, Hive will write data to the directory on the local file system. Data written to the filesystem is serialized as text with columns separated by ^A and rows separated by newlines. If any of the columns are not of primitive type, then those columns are serialized to JSON format.  Notes  INSERT OVERWRITE statements to directories, local directories, and tables (or partitions) can all be used together within the same query. INSERT OVERWRITE statements to HDFS filesystem directories are the best way to extract large amounts of data from Hive. Hive can write to HDFS directories in parallel from within a map-reduce job. The directory is, as you would expect, OVERWRITten; in other words, if the specified path exists, it is clobbered and replaced with the output. As of Hive 0.11.0 the separator used can be specified; in earlier versions it was always the ^A character (\\001). However, custom separators are only supported for LOCAL writes in Hive versions 0.11.0 to 1.1.0 – this bug is fixed in version 1.2.0 (see HIVE-5672). In Hive 0.14, inserts into ACID compliant tables will deactivate vectorization for the duration of the select and insert. This will be done automatically. ACID tables that have data inserted into them can still be queried using vectorization.  Inserting values into tables from SQL The INSERT\u0026hellip;VALUES statement can be used to insert data into tables directly from SQL.\nVersion Information\nINSERT\u0026hellip;VALUES is available starting in Hive 0.14.\nSyntax Standard Syntax: INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES values_row [, values_row ...] Where values_row is: ( value [, value ...] ) where a value is either null or any valid SQL literal Synopsis  Each row listed in the VALUES clause is inserted into table tablename. Values must be provided for every column in the table. The standard SQL syntax that allows the user to insert values into only some columns is not yet supported. To mimic the standard SQL, nulls can be provided for columns the user does not wish to assign a value to. Dynamic partitioning is supported in the same way as for INSERT\u0026hellip;SELECT. If the table being inserted into supports ACID and a transaction manager that supports ACID is in use, this operation will be auto-committed upon successful completion. Hive does not support literals for complex types (array, map, struct, union), so it is not possible to use them in INSERT INTO\u0026hellip;VALUES clauses. This means that the user cannot insert data into a complex datatype column using the INSERT INTO\u0026hellip;VALUES clause.  Examples CREATE TABLE students (name VARCHAR(64), age INT, gpa DECIMAL(3, 2)) CLUSTERED BY (age) INTO 2 BUCKETS STORED AS ORC; INSERT INTO TABLE students VALUES ('fred flintstone', 35, 1.28), ('barney rubble', 32, 2.32); CREATE TABLE pageviews (userid VARCHAR(64), link STRING, came_from STRING) PARTITIONED BY (datestamp STRING) CLUSTERED BY (userid) INTO 256 BUCKETS STORED AS ORC; INSERT INTO TABLE pageviews PARTITION (datestamp = '2014-09-23') VALUES ('jsmith', 'mail.com', 'sports.com'), ('jdoe', 'mail.com', null); INSERT INTO TABLE pageviews PARTITION (datestamp) VALUES ('tjohnson', 'sports.com', 'finance.com', '2014-09-23'), ('tlee', 'finance.com', null, '2014-09-21'); INSERT INTO TABLE pageviews VALUES ('tjohnson', 'sports.com', 'finance.com', '2014-09-23'), ('tlee', 'finance.com', null, '2014-09-21'); Update Version Information\nUPDATE is available starting in Hive 0.14.\nUpdates can only be performed on tables that support ACID. See Hive Transactions for details.\nSyntax Standard Syntax: UPDATE tablename SET column = value [, column = value ...] [WHERE expression] Synopsis  The referenced column must be a column of the table being updated. The value assigned must be an expression that Hive supports in the select clause. Thus arithmetic operators, UDFs, casts, literals, etc. are supported. Subqueries are not supported. Only rows that match the WHERE clause will be updated. Partitioning columns cannot be updated. Bucketing columns cannot be updated. In Hive 0.14, upon successful completion of this operation the changes will be auto-committed.  Notes  Vectorization will be turned off for update operations. This is automatic and requires no action on the part of the user. Non-update operations are not affected. Updated tables can still be queried using vectorization. In version 0.14 it is recommended that you set hive.optimize.sort.dynamic.partition=false when doing updates, as this produces more efficient execution plans.  Delete Version Information\nDELETE is available starting in Hive 0.14.\nDeletes can only be performed on tables that support ACID. See Hive Transactions for details.\nSyntax Standard Syntax: DELETE FROM tablename [WHERE expression] Synopsis  Only rows that match the WHERE clause will be deleted. In Hive 0.14, upon successful completion of this operation the changes will be auto-committed.  Notes  Vectorization will be turned off for delete operations. This is automatic and requires no action on the part of the user. Non-delete operations are not affected. Tables with deleted data can still be queried using vectorization. In version 0.14 it is recommended that you set hive.optimize.sort.dynamic.partition=false when doing deletes, as this produces more efficient execution plans.  Merge Version Information\nMERGE is available starting in Hive 2.2.\nMerge can only be performed on tables that support ACID. See Hive Transactions for details.\nSyntax Standard Syntax: MERGE INTO \u0026lt;target table\u0026gt; AS T USING \u0026lt;source expression/table\u0026gt; AS S ON \u0026lt;boolean expression1\u0026gt; WHEN MATCHED [AND \u0026lt;boolean expression2\u0026gt;] THEN UPDATE SET \u0026lt;set clause list\u0026gt; WHEN MATCHED [AND \u0026lt;boolean expression3\u0026gt;] THEN DELETE WHEN NOT MATCHED [AND \u0026lt;boolean expression4\u0026gt;] THEN INSERT VALUES\u0026lt;value list\u0026gt; Synopsis  Merge allows actions to be performed on a target table based on the results of a join with a source table. In Hive 2.2, upon successful completion of this operation the changes will be auto-committed.  Performance Note SQL Standard requires that an error is raised if the ON clause is such that more than 1 row in source matches a row in target. This check is computationally expensive and may affect the overall runtime of a MERGE statement significantly. hive.merge.cardinality.check=false may be used to disable the check at your own risk. If the check is disabled, but the statement has such a cross join effect, it may lead to data corruption.\nNotes  1, 2, or 3 WHEN clauses may be present; at most 1 of each type: UPDATE/DELETE/INSERT. WHEN NOT MATCHED must be the last WHEN clause. If both UPDATE and DELETE clauses are present, the first one in the statement must include [AND ]. Vectorization will be turned off for merge operations. This is automatic and requires no action on the part of the user. Non-delete operations are not affected. Tables with deleted data can still be queried using vectorization.  Examples  See here.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-dml_27362036/","tags":null,"title":"Apache Hive : LanguageManual DML"},{"categories":null,"contents":"Apache Hive : LanguageManual Explain  EXPLAIN Syntax  Example The CBO Clause The AST Clause The DEPENDENCY Clause The AUTHORIZATION Clause The LOCKS Clause The VECTORIZATION Clause The ANALYZE Clause User-level Explain Output    EXPLAIN Syntax Hive provides an EXPLAIN command that shows the execution plan for a query. The syntax for this statement is as follows:\nEXPLAIN [EXTENDED|CBO|AST|DEPENDENCY|AUTHORIZATION|LOCKS|VECTORIZATION|ANALYZE] query AUTHORIZATION is supported from HIVE 0.14.0 via HIVE-5961. VECTORIZATION is supported from Hive 2.3.0 via HIVE-11394. LOCKS is supported from Hive 3.2.0 via HIVE-17683.\nAST was removed from EXPLAIN EXTENDED in HIVE-13533 and reinstated as a separate command in HIVE-15932.\nThe use of EXTENDED in the EXPLAIN statement produces extra information about the operators in the plan. This is typically physical information like file names.\nA Hive query gets converted into a sequence (it is more a Directed Acyclic Graph) of stages. These stages may be map/reduce stages or they may even be stages that do metastore or file system operations like move and rename. The explain output has three parts:\n The Abstract Syntax Tree for the query The dependencies between the different stages of the plan The description of each of the stages  The description of the stages itself shows a sequence of operators with the metadata associated with the operators. The metadata may comprise things like filter expressions for the FilterOperator or the select expressions for the SelectOperator or the output file names for the FileSinkOperator.\nExample As an example, consider the following EXPLAIN query:\nEXPLAIN FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key; The output of this statement contains the following parts:\n The Dependency Graph  STAGE DEPENDENCIES: Stage-1 is a root stage Stage-2 depends on stages: Stage-1 Stage-0 depends on stages: Stage-2 This shows that Stage-1 is the root stage, Stage-2 is executed after Stage-1 is done and Stage-0 is executed after Stage-2 is done.\n The plans of each Stage  STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: src Reduce Output Operator key expressions: expr: key type: string sort order: + Map-reduce partition columns: expr: rand() type: double tag: -1 value expressions: expr: substr(value, 4) type: string Reduce Operator Tree: Group By Operator aggregations: expr: sum(UDFToDouble(VALUE.0)) keys: expr: KEY.0 type: string mode: partial1 File Output Operator compressed: false table: input format: org.apache.hadoop.mapred.SequenceFileInputFormat output format: org.apache.hadoop.mapred.SequenceFileOutputFormat name: binary_table Stage: Stage-2 Map Reduce Alias -\u0026gt; Map Operator Tree: /tmp/hive-zshao/67494501/106593589.10001 Reduce Output Operator key expressions: expr: 0 type: string sort order: + Map-reduce partition columns: expr: 0 type: string tag: -1 value expressions: expr: 1 type: double Reduce Operator Tree: Group By Operator aggregations: expr: sum(VALUE.0) keys: expr: KEY.0 type: string mode: final Select Operator expressions: expr: 0 type: string expr: 1 type: double Select Operator expressions: expr: UDFToInteger(0) type: int expr: 1 type: double File Output Operator compressed: false table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe name: dest_g1 Stage: Stage-0 Move Operator tables: replace: true table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe name: dest_g1 In this example there are 2 map/reduce stages (Stage-1 and Stage-2) and 1 File System related stage (Stage-0). Stage-0 basically moves the results from a temporary directory to the directory corresponding to the table dest_g1.\nSort order indicates the number of columns in key expressions that are used for sorting. Each \u0026ldquo;+\u0026rdquo; represents one column sorted in ascending order, and each \u0026ldquo;-\u0026rdquo; represents a column sorted in descending order.\nA map/reduce stage itself has 2 parts:\n A mapping from table alias to Map Operator Tree – This mapping tells the mappers which operator tree to call in order to process the rows from a particular table or result of a previous map/reduce stage. In Stage-1 in the above example, the rows from src table are processed by the operator tree rooted at a Reduce Output Operator. Similarly, in Stage-2 the rows of the results of Stage-1 are processed by another operator tree rooted at another Reduce Output Operator. Each of these Reduce Output Operators partitions the data to the reducers according to the criteria shown in the metadata. A Reduce Operator Tree – This is the operator tree which processes all the rows on the reducer of the map/reduce job. In Stage-1 for example, the Reducer Operator Tree is carrying out a partial aggregation whereas the Reducer Operator Tree in Stage-2 computes the final aggregation from the partial aggregates computed in Stage-1.  The CBO Clause The CBO clause outputs the plan generated by Calcite optimizer. It can optionally include information about the cost of the plan using Calcite default cost model and cost model used for join reordering. Since Hive release 4.0.0 (HIVE-17503 / HIVE-21184).\nSyntax: EXPLAIN [FORMATTED] CBO [COST|JOINCOST]\n COST option prints the plan and cost calculated using Calcite default cost model. JOINCOST option prints the plan and cost calculated using the cost model used for join reordering.  For example, we can execute the following statement:\nEXPLAIN CBO WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year =2000 GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total_return ctr1, store, customer WHERE ctr1.ctr_total_return \u0026gt; (SELECT AVG(ctr_total_return)*1.2 FROM customer_total_return ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk) AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'NM' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100 The query will be optimized and Hive produces the following output:\nCBO PLAN: HiveSortLimit(sort0=[$0], dir0=[ASC], fetch=[100]) HiveProject(c_customer_id=[$1]) HiveJoin(condition=[AND(=($3, $7), \u0026gt;($4, $6))], joinType=[inner], algorithm=[none], cost=[not available]) HiveJoin(condition=[=($2, $0)], joinType=[inner], algorithm=[none], cost=[not available]) HiveProject(c_customer_sk=[$0], c_customer_id=[$1]) HiveFilter(condition=[IS NOT NULL($0)]) HiveTableScan(table=[[default, customer]], table:alias=[customer]) HiveJoin(condition=[=($3, $1)], joinType=[inner], algorithm=[none], cost=[not available]) HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2]) HiveAggregate(group=[{1, 2}], agg#0=[sum($3)]) HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available]) HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14]) HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7), IS NOT NULL($3))]) HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns]) HiveProject(d_date_sk=[$0]) HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))]) HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim]) HiveProject(s_store_sk=[$0]) HiveFilter(condition=[AND(=($24, _UTF-16LE'NM'), IS NOT NULL($0))]) HiveTableScan(table=[[default, store]], table:alias=[store]) HiveProject(_o__c0=[*(/($1, $2), 1.2)], ctr_store_sk=[$0]) HiveAggregate(group=[{1}], agg#0=[sum($2)], agg#1=[count($2)]) HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2]) HiveAggregate(group=[{1, 2}], agg#0=[sum($3)]) HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available]) HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14]) HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7))]) HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns]) HiveProject(d_date_sk=[$0]) HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))]) HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim]) In turn, we can execute the following command:\nEXPLAIN CBO COST WITH customer_total_return AS (SELECT sr_customer_sk AS ctr_customer_sk, sr_store_sk AS ctr_store_sk, SUM(SR_FEE) AS ctr_total_return FROM store_returns, date_dim WHERE sr_returned_date_sk = d_date_sk AND d_year =2000 GROUP BY sr_customer_sk, sr_store_sk) SELECT c_customer_id FROM customer_total_return ctr1, store, customer WHERE ctr1.ctr_total_return \u0026gt; (SELECT AVG(ctr_total_return)*1.2 FROM customer_total_return ctr2 WHERE ctr1.ctr_store_sk = ctr2.ctr_store_sk) AND s_store_sk = ctr1.ctr_store_sk AND s_state = 'NM' AND ctr1.ctr_customer_sk = c_customer_sk ORDER BY c_customer_id LIMIT 100 It will produce a similar plan, but the cost for each operator will be embedded next to the operator descriptors:\nCBO PLAN: HiveSortLimit(sort0=[$0], dir0=[ASC], fetch=[100]): rowcount = 100.0, cumulative cost = {2.395588892021712E26 rows, 1.197794434438787E26 cpu, 0.0 io}, id = 1683 HiveProject(c_customer_id=[$1]): rowcount = 1.1977944344387866E26, cumulative cost = {2.395588892021712E26 rows, 1.197794434438787E26 cpu, 0.0 io}, id = 1681 HiveJoin(condition=[AND(=($3, $7), \u0026gt;($4, $6))], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 1.1977944344387866E26, cumulative cost = {1.1977944575829254E26 rows, 4.160211553874922E10 cpu, 0.0 io}, id = 1679 HiveJoin(condition=[=($2, $0)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 2.3144135067474273E18, cumulative cost = {2.3144137967122499E18 rows, 1.921860676139634E10 cpu, 0.0 io}, id = 1663 HiveProject(c_customer_sk=[$0], c_customer_id=[$1]): rowcount = 7.2E7, cumulative cost = {2.24E8 rows, 3.04000001E8 cpu, 0.0 io}, id = 1640 HiveFilter(condition=[IS NOT NULL($0)]): rowcount = 7.2E7, cumulative cost = {1.52E8 rows, 1.60000001E8 cpu, 0.0 io}, id = 1638 HiveTableScan(table=[[default, customer]], table:alias=[customer]): rowcount = 8.0E7, cumulative cost = {8.0E7 rows, 8.0000001E7 cpu, 0.0 io}, id = 1055 HiveJoin(condition=[=($3, $1)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 2.1429754692105807E11, cumulative cost = {2.897408225471977E11 rows, 1.891460676039634E10 cpu, 0.0 io}, id = 1661 HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2]): rowcount = 6.210443022113779E9, cumulative cost = {7.544327346205959E10 rows, 1.891460312135634E10 cpu, 0.0 io}, id = 1685 HiveAggregate(group=[{1, 2}], agg#0=[sum($3)]): rowcount = 6.210443022113779E9, cumulative cost = {6.92328304399458E10 rows, 2.8327405501500005E8 cpu, 0.0 io}, id = 1654 HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 6.2104430221137794E10, cumulative cost = {6.2246082040067795E10 rows, 2.8327405501500005E8 cpu, 0.0 io}, id = 1652 HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14]): rowcount = 4.198394835000001E7, cumulative cost = {1.4155904670000002E8 rows, 2.8311809440000004E8 cpu, 0.0 io}, id = 1645 HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7), IS NOT NULL($3))]): rowcount = 4.198394835000001E7, cumulative cost = {9.957509835000001E7 rows, 1.15182301E8 cpu, 0.0 io}, id = 1643 HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns]): rowcount = 5.759115E7, cumulative cost = {5.759115E7 rows, 5.7591151E7 cpu, 0.0 io}, id = 1040 HiveProject(d_date_sk=[$0]): rowcount = 9861.615, cumulative cost = {92772.23000000001 rows, 155960.615 cpu, 0.0 io}, id = 1650 HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))]): rowcount = 9861.615, cumulative cost = {82910.615 rows, 146099.0 cpu, 0.0 io}, id = 1648 HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim]): rowcount = 73049.0, cumulative cost = {73049.0 rows, 73050.0 cpu, 0.0 io}, id = 1043 HiveProject(s_store_sk=[$0]): rowcount = 230.04000000000002, cumulative cost = {2164.08 rows, 3639.04 cpu, 0.0 io}, id = 1659 HiveFilter(condition=[AND(=($24, _UTF-16LE'NM'), IS NOT NULL($0))]): rowcount = 230.04000000000002, cumulative cost = {1934.04 rows, 3409.0 cpu, 0.0 io}, id = 1657 HiveTableScan(table=[[default, store]], table:alias=[store]): rowcount = 1704.0, cumulative cost = {1704.0 rows, 1705.0 cpu, 0.0 io}, id = 1050 HiveProject(_o__c0=[*(/($1, $2), 1.2)], ctr_store_sk=[$0]): rowcount = 6.900492246793088E8, cumulative cost = {8.537206083312463E10 rows, 2.2383508777352882E10 cpu, 0.0 io}, id = 1677 HiveAggregate(group=[{1}], agg#0=[sum($2)], agg#1=[count($2)]): rowcount = 6.900492246793088E8, cumulative cost = {8.468201160844533E10 rows, 2.1003410327994267E10 cpu, 0.0 io}, id = 1675 HiveProject(sr_customer_sk=[$0], sr_store_sk=[$1], $f2=[$2]): rowcount = 6.900492246793088E9, cumulative cost = {8.381945007759619E10 rows, 2.1003410327994267E10 cpu, 0.0 io}, id = 1686 HiveAggregate(group=[{1, 2}], agg#0=[sum($3)]): rowcount = 6.900492246793088E9, cumulative cost = {7.69189578308031E10 rows, 3.01933587615E8 cpu, 0.0 io}, id = 1673 HiveJoin(condition=[=($0, $4)], joinType=[inner], algorithm=[none], cost=[not available]): rowcount = 6.900492246793088E10, cumulative cost = {6.915590405316087E10 rows, 3.01933587615E8 cpu, 0.0 io}, id = 1671 HiveProject(sr_returned_date_sk=[$0], sr_customer_sk=[$3], sr_store_sk=[$7], sr_fee=[$14]): rowcount = 4.66488315E7, cumulative cost = {1.50888813E8 rows, 3.01777627E8 cpu, 0.0 io}, id = 1667 HiveFilter(condition=[AND(IS NOT NULL($0), IS NOT NULL($7))]): rowcount = 4.66488315E7, cumulative cost = {1.042399815E8 rows, 1.15182301E8 cpu, 0.0 io}, id = 1665 HiveTableScan(table=[[default, store_returns]], table:alias=[store_returns]): rowcount = 5.759115E7, cumulative cost = {5.759115E7 rows, 5.7591151E7 cpu, 0.0 io}, id = 1040 HiveProject(d_date_sk=[$0]): rowcount = 9861.615, cumulative cost = {92772.23000000001 rows, 155960.615 cpu, 0.0 io}, id = 1650 HiveFilter(condition=[AND(=($6, 2000), IS NOT NULL($0))]): rowcount = 9861.615, cumulative cost = {82910.615 rows, 146099.0 cpu, 0.0 io}, id = 1648 HiveTableScan(table=[[default, date_dim]], table:alias=[date_dim]): rowcount = 73049.0, cumulative cost = {73049.0 rows, 73050.0 cpu, 0.0 io}, id = 1043 The AST Clause Outputs the query\u0026rsquo;s Abstract Syntax Tree.\nExample:\nEXPLAIN AST FROM src INSERT OVERWRITE TABLE dest_g1 SELECT src.key, sum(substr(src.value,4)) GROUP BY src.key; Outputs:\nABSTRACT SYNTAX TREE: (TOK_QUERY (TOK_FROM (TOK_TABREF src)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB dest_g1)) (TOK_SELECT (TOK_SELEXPR (TOK_COLREF src key)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_FUNCTION substr (TOK_COLREF src value) 4)))) (TOK_GROUPBY (TOK_COLREF src key)))) The DEPENDENCY Clause The use of DEPENDENCY in the EXPLAIN statement produces extra information about the inputs in the plan. It shows various attributes for the inputs. For example, for a query like:\nEXPLAIN DEPENDENCY SELECT key, count(1) FROM srcpart WHERE ds IS NOT NULL GROUP BY key the following output is produced:\n{\u0026quot;input_partitions\u0026quot;:[{\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-08/hr=11\u0026quot;},{\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-08/hr=12\u0026quot;},{\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-09/hr=11\u0026quot;},{\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-09/hr=12\u0026quot;}],\u0026quot;input_tables\u0026quot;:[{\u0026quot;tablename\u0026quot;:\u0026quot;default@srcpart\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;MANAGED_TABLE\u0026quot;}]} The inputs contain both the tables and the partitions. Note that the table is present even if none of the partitions is accessed in the query.\nThe dependencies show the parents in case a table is accessed via a view. Consider the following queries:\nCREATE VIEW V1 AS SELECT key, value from src; EXPLAIN DEPENDENCY SELECT * FROM V1; The following output is produced:\n{\u0026quot;input_partitions\u0026quot;:[],\u0026quot;input_tables\u0026quot;:[{\u0026quot;tablename\u0026quot;:\u0026quot;default@v1\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;VIRTUAL_VIEW\u0026quot;},{\u0026quot;tablename\u0026quot;:\u0026quot;default@src\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;MANAGED_TABLE\u0026quot;,\u0026quot;tableParents\u0026quot;:\u0026quot;[default@v1]\u0026quot;}]} As above, the inputs contain the view V1 and the table \u0026lsquo;src\u0026rsquo; that the view V1 refers to.\nAll the outputs are shown if a table is being accessed via multiple parents.\nCREATE VIEW V2 AS SELECT ds, key, value FROM srcpart WHERE ds IS NOT NULL; CREATE VIEW V4 AS SELECT src1.key, src2.value as value1, src3.value as value2 FROM V1 src1 JOIN V2 src2 on src1.key = src2.key JOIN src src3 ON src2.key = src3.key; EXPLAIN DEPENDENCY SELECT * FROM V4; The following output is produced.\n{\u0026quot;input_partitions\u0026quot;:[{\u0026quot;partitionParents\u0026quot;:\u0026quot;[default@v2]\u0026quot;,\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-08/hr=11\u0026quot;},{\u0026quot;partitionParents\u0026quot;:\u0026quot;[default@v2]\u0026quot;,\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-08/hr=12\u0026quot;},{\u0026quot;partitionParents\u0026quot;:\u0026quot;[default@v2]\u0026quot;,\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-09/hr=11\u0026quot;},{\u0026quot;partitionParents\u0026quot;:\u0026quot;[default@v2]\u0026quot;,\u0026quot;partitionName\u0026quot;:\u0026quot;default\u0026lt;at:var at:name=\u0026quot;srcpart\u0026quot; /\u0026gt;ds=2008-04-09/hr=12\u0026quot;}],\u0026quot;input_tables\u0026quot;:[{\u0026quot;tablename\u0026quot;:\u0026quot;default@v4\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;VIRTUAL_VIEW\u0026quot;},{\u0026quot;tablename\u0026quot;:\u0026quot;default@v2\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;VIRTUAL_VIEW\u0026quot;,\u0026quot;tableParents\u0026quot;:\u0026quot;[default@v4]\u0026quot;},{\u0026quot;tablename\u0026quot;:\u0026quot;default@v1\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;VIRTUAL_VIEW\u0026quot;,\u0026quot;tableParents\u0026quot;:\u0026quot;[default@v4]\u0026quot;},{\u0026quot;tablename\u0026quot;:\u0026quot;default@src\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;MANAGED_TABLE\u0026quot;,\u0026quot;tableParents\u0026quot;:\u0026quot;[default@v4, default@v1]\u0026quot;},{\u0026quot;tablename\u0026quot;:\u0026quot;default@srcpart\u0026quot;,\u0026quot;tabletype\u0026quot;:\u0026quot;MANAGED_TABLE\u0026quot;,\u0026quot;tableParents\u0026quot;:\u0026quot;[default@v2]\u0026quot;}]} As can be seen, src is being accessed via parents v1 and v4.\nThe AUTHORIZATION Clause The use of AUTHORIZATION in the EXPLAIN statement shows all entities needed to be authorized to execute the query and authorization failures if any exist. For example, for a query like:\nEXPLAIN AUTHORIZATION SELECT * FROM src JOIN srcpart; the following output is produced:\nINPUTS: default@srcpart default@src default@srcpart@ds=2008-04-08/hr=11 default@srcpart@ds=2008-04-08/hr=12 default@srcpart@ds=2008-04-09/hr=11 default@srcpart@ds=2008-04-09/hr=12 OUTPUTS: hdfs://localhost:9000/tmp/.../-mr-10000 CURRENT_USER: navis OPERATION: QUERY AUTHORIZATION_FAILURES: Permission denied: Principal [name=navis, type=USER] does not have following privileges for operation QUERY [[SELECT] on Object [type=TABLE_OR_VIEW, name=default.src], [SELECT] on Object [type=TABLE_OR_VIEW, name=default.srcpart]] With the FORMATTED keyword, it will be returned in JSON format.\n \u0026quot;OUTPUTS\u0026quot;:[\u0026quot;hdfs://localhost:9000/tmp/.../-mr-10000\u0026quot;],\u0026quot;INPUTS\u0026quot;:[\u0026quot;default@srcpart\u0026quot;,\u0026quot;default@src\u0026quot;,\u0026quot;default@srcpart@ds=2008-04-08/hr=11\u0026quot;,\u0026quot;default@srcpart@ds=2008-04-08/hr=12\u0026quot;,\u0026quot;default@srcpart@ds=2008-04-09/hr=11\u0026quot;,\u0026quot;default@srcpart@ds=2008-04-09/hr=12\u0026quot;],\u0026quot;OPERATION\u0026quot;:\u0026quot;QUERY\u0026quot;,\u0026quot;CURRENT_USER\u0026quot;:\u0026quot;navis\u0026quot;,\u0026quot;AUTHORIZATION_FAILURES\u0026quot;:[\u0026quot;Permission denied: Principal [name=navis, type=USER] does not have following privileges for operation QUERY [[SELECT] on Object [type=TABLE_OR_VIEW, name=default.src], [SELECT] on Object [type=TABLE_OR_VIEW, name=default.srcpart]]\u0026quot;]} The LOCKS Clause This is useful to understand what locks the system will acquire to run the specified query. Since Hive release 3.2.0 (HIVE-17683).\nFor example\nEXPLAIN LOCKS UPDATE target SET b = 1 WHERE p IN (SELECT t.q1 FROM source t WHERE t.a1=5) Will produce output like this.\nLOCK INFORMATION: default.source -\u0026gt; SHARED_READ default.target.p=1/q=2 -\u0026gt; SHARED_READ default.target.p=1/q=3 -\u0026gt; SHARED_READ default.target.p=2/q=2 -\u0026gt; SHARED_READ default.target.p=2/q=2 -\u0026gt; SHARED_WRITE default.target.p=1/q=3 -\u0026gt; SHARED_WRITE default.target.p=1/q=2 -\u0026gt; SHARED_WRITE  EXPLAIN FORMATTED LOCKS \u0026lt;sql\u0026gt; is also supported which will produce JSON encoded output.\nThe VECTORIZATION Clause Adds detail to the EXPLAIN output showing why Map and Reduce work is not vectorized. Since Hive release 2.3.0 (HIVE-11394).\nSyntax: EXPLAIN VECTORIZATION [ONLY] [SUMMARY|OPERATOR|EXPRESSION|DETAIL]\n ONLY option suppresses most non-vectorization elements. SUMMARY (default) shows vectorization information for the PLAN (is vectorization enabled) and a summary of Map and Reduce work. OPERATOR shows vectorization information for operators. E.g. Filter Vectorization. Includes all information of SUMMARY. EXPRESSION shows vectorization information for expressions. E.g. predicateExpression. Includes all information of SUMMARY and OPERATOR. DETAIL shows detail-level vectorization information. It includes all information of SUMMARY, OPERATOR, and EXPRESSION.  The optional clause defaults are not ONLY and SUMMARY.\nSee HIVE-11394 for more details and examples.\nThe ANALYZE Clause Annotates the plan with actual row counts. Since in Hive 2.2.0 (HIVE-14362)\nFormat is: (estimated row count) / (actual row count)\nExample:\nFor the below tablescan; the estimation was 500 rows; but actually the scan only yielded 13 rows.\n[...] TableScan [TS_13] (rows=500/13 width=178) Output:[\u0026quot;key\u0026quot;,\u0026quot;value\u0026quot;] [...] User-level Explain Output Since HIVE-8600 in Hive 1.1.0, we support a user-level explain extended output for any query at the log4j INFO level after set hive.log.explain.output****=true (default is false).\nSince HIVE-18469 in Hive 3.1.0, the user-level explain extended output for any query will be shown in the WebUI / Drilldown / Query Plan after set hive.server2.webui.explain.output=true (default is false).\nSince HIVE-9780 in Hive 1.2.0, we support a user-level explain for Hive on Tez users. After set hive.explain.user=true (default is false) if the following query is sent, the user can see a much more clearly readable tree of operations.\nSince HIVE-11133 in Hive 3.0.0, we support a user-level explain for Hive on Spark users. A separate configuration is used for Hive-on-Spark, hive.spark.explain.user which is set to false by default.\nEXPLAIN select sum(hash(key)), sum(hash(value)) from src_orc_merge_test_part where ds='2012-01-03' and ts='2012-01-03+14:46:31' Plan optimized by CBO. Vertex dependency in root stage Reducer 2 \u0026lt;- Map 1 (SIMPLE_EDGE) Stage-0 Fetch Operator limit:-1 Stage-1 Reducer 2 File Output Operator [FS_8] compressed:false Statistics:Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE table:{\u0026quot;serde:\u0026quot;:\u0026quot;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026quot;,\u0026quot;input format:\u0026quot;:\u0026quot;org.apache.hadoop.mapred.TextInputFormat\u0026quot;,\u0026quot;output format:\u0026quot;:\u0026quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026quot;} Group By Operator [GBY_6] | aggregations:[\u0026quot;sum(VALUE._col0)\u0026quot;,\u0026quot;sum(VALUE._col1)\u0026quot;] | outputColumnNames:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;] | Statistics:Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE |\u0026lt;-Map 1 [SIMPLE_EDGE] Reduce Output Operator [RS_5] sort order: Statistics:Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE value expressions:_col0 (type: bigint), _col1 (type: bigint) Group By Operator [GBY_4] aggregations:[\u0026quot;sum(_col0)\u0026quot;,\u0026quot;sum(_col1)\u0026quot;] outputColumnNames:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;] Statistics:Num rows: 1 Data size: 16 Basic stats: COMPLETE Column stats: NONE Select Operator [SEL_2] outputColumnNames:[\u0026quot;_col0\u0026quot;,\u0026quot;_col1\u0026quot;] Statistics:Num rows: 500 Data size: 47000 Basic stats: COMPLETE Column stats: NONE TableScan [TS_0] alias:src_orc_merge_test_part Statistics:Num rows: 500 Data size: 47000 Basic stats: COMPLETE Column stats: NONE ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-explain_27362037/","tags":null,"title":"Apache Hive : LanguageManual Explain"},{"categories":null,"contents":"Apache Hive : LanguageManual GroupBy  Group By Syntax  Simple Examples Select statement and group by clause   Advanced Features  Multi-Group-By Inserts Map-side Aggregation for Group By Grouping Sets, Cubes, Rollups, and the GROUPING__ID Function    Group By Syntax groupByClause: GROUP BY groupByExpression (, groupByExpression)* groupByExpression: expression groupByQuery: SELECT expression (, expression)* FROM src groupByClause? In groupByExpression columns are specified by name, not by position number. However in Hive 0.11.0 and later, columns can be specified by position when configured as follows:\n For Hive 0.11.0 through 2.1.x, set hive.groupby.orderby.position.alias to true (the default is false). For Hive 2.2.0 and later, set hive.groupby.position.alias to true (the default is false).  Simple Examples In order to count the number of rows in a table:\n SELECT COUNT(*) FROM table2; Note that for versions of Hive which don\u0026rsquo;t include HIVE-287, you\u0026rsquo;ll need to use COUNT(1) in place of COUNT(*).\nIn order to count the number of distinct users by gender one could write the following query:\n INSERT OVERWRITE TABLE pv_gender_sum SELECT pv_users.gender, count (DISTINCT pv_users.userid) FROM pv_users GROUP BY pv_users.gender; Multiple aggregations can be done at the same time, however, no two aggregations can have different DISTINCT columns. For example, the following is possible because count(DISTINCT) and sum(DISTINCT) specify the same column:\n INSERT OVERWRITE TABLE pv_gender_agg SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid) FROM pv_users GROUP BY pv_users.gender; Note that for versions of Hive which don\u0026rsquo;t include HIVE-287, you\u0026rsquo;ll need to use COUNT(1) in place of COUNT(*).\nHowever, the following query is not allowed. We don\u0026rsquo;t allow multiple DISTINCT expressions in the same query.\n INSERT OVERWRITE TABLE pv_gender_agg SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip) FROM pv_users GROUP BY pv_users.gender; Select statement and group by clause When using group by clause, the select statement can only include columns included in the group by clause. Of course, you can have as many aggregation functions (e.g. count) in the select statement as well.\nLet\u0026rsquo;s take a simple example\nCREATE TABLE t1(a INTEGER, b INTGER); A group by query on the above table could look like:\nSELECT a, sum(b) FROM t1 GROUP BY a; The above query works because the select clause contains a (the group by key) and an aggregation function (sum(b)).\nHowever, the query below DOES NOT work:\nSELECT a, b FROM t1 GROUP BY a; This is because the select clause has an additional column (b) that is not included in the group by clause (and it\u0026rsquo;s not an aggregation function either). This is because, if the table t1 looked like:\na b ------ 100 1 100 2 100 3 Since the grouping is only done on a, what value of b should Hive display for the group a=100? One can argue that it should be the first value or the lowest value but we all agree that there are multiple possible options. Hive does away with this guessing by making it invalid SQL (HQL, to be precise) to have a column in the select clause that is not included in the group by clause.\nAdvanced Features Multi-Group-By Inserts The output of the aggregations or simple selects can be further sent into multiple tables or even to hadoop dfs files (which can then be manipulated using hdfs utilitites). e.g. if along with the gender breakdown, one needed to find the breakdown of unique page views by age, one could accomplish that with the following query:\n FROM pv_users INSERT OVERWRITE TABLE pv_gender_sum SELECT pv_users.gender, count(DISTINCT pv_users.userid) GROUP BY pv_users.gender INSERT OVERWRITE DIRECTORY '/user/facebook/tmp/pv_age_sum' SELECT pv_users.age, count(DISTINCT pv_users.userid) GROUP BY pv_users.age; Map-side Aggregation for Group By hive.map.aggr controls how we do aggregations. The default is false. If it is set to true, Hive will do the first-level aggregation directly in the map task.\nThis usually provides better efficiency, but may require more memory to run successfully.\n set hive.map.aggr=true; SELECT COUNT(*) FROM table2; Note that for versions of Hive which don\u0026rsquo;t include HIVE-287, you\u0026rsquo;ll need to use COUNT(1) in place of COUNT(*).\nGrouping Sets, Cubes, Rollups, and the GROUPING__ID Function Version\nGrouping sets, CUBE and ROLLUP operators, and the GROUPING__ID function were added in Hive release 0.10.0.\nSee Enhanced Aggregation, Cube, Grouping and Rollup for information about these aggregation operators.\nAlso see the JIRAs:\n HIVE-2397 Support with rollup option for group by HIVE-3433 Implement CUBE and ROLLUP operators in Hive HIVE-3471 Implement grouping sets in Hive HIVE-3613 Implement grouping_id function  New in Hive release 0.11.0:\n HIVE-3552 HIVE-3552 performant manner for performing cubes/rollups/grouping sets for a high number of grouping set keys  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-groupby_27362038/","tags":null,"title":"Apache Hive : LanguageManual GroupBy"},{"categories":null,"contents":"Apache Hive : LanguageManual ImportExport Import/Export  Import/Export  Overview Export Syntax Import Syntax Replication usage Examples    Version\nThe EXPORT and IMPORT commands were added in Hive 0.8.0 (see HIVE-1918).\nReplication extensions to the EXPORT and IMPORT commands were added in Hive 1.2.0 (see HIVE-7973 and Hive Replication Development).\nOverview The EXPORT command exports the data of a table or partition, along with the metadata, into a specified output location. This output location can then be moved over to a different Hadoop or Hive instance and imported from there with the IMPORT command.\nWhen exporting a partitioned table, the original data may be located in different HDFS locations. The ability to export/import a subset of the partition is also supported.\nExported metadata is stored in the target directory, and data files are stored in subdirectories.\nThe EXPORT and IMPORT commands work independently of the source and target metastore DBMS used; for example, they can be used between Derby and MySQL databases.\nIMPORT will create target table/partition if it does not exist. All the table properties/parameters will be that of table that was used in EXPORT to generate the archive. If target exists, checks are performed that it has appropriate schema, Input/OutputFormat, etc. If target table exists and is not partitioned, it must be empty. If target table exists and is partitioned, partitions being imported must not exist in the table. Export Syntax EXPORT TABLE tablename [PARTITION (part_column=\u0026quot;value\u0026quot;[, ...])] TO 'export_target_path' [ FOR replication('eventid') ] Import Syntax IMPORT [[EXTERNAL] TABLE new_or_original_tablename [PARTITION (part_column=\u0026quot;value\u0026quot;[, ...])]] FROM 'source_path' [LOCATION 'import_target_path'] Replication usage The EXPORT and IMPORT commands behave slightly differently when used in the context of replication, and are intended to be used by tools that perform replication between hive warehouses. In most cases, end users will not need to use this additional tag, except when doing a manual bootstrap of a replication-destination warehouse so that an incremental replication tool can take over from that point.\nThey make use of a special table property called \u0026ldquo;repl.last.id\u0026rdquo; in a table or partition (depending on what object is being replicated) object to make sure that a replication export/import will only update objects if the update is newer than the object it affects. On the export end, it tags the replication export dump with an id that is monotonically increasing on the source warehouse (incrementing each time there is a source warehouse metastore modification). In addition, an export that is tagged as being for replication will not result in an error if it attempts to export an object which does not currently exist. (This is because in the general flow of replication, it is quite possible that by the time an event is acted upon for replication by an external tool, it is possible that the object has been removed, and thus, should not halt the replication pipeline.)\nOn the import side, there is no syntax change, but import is run on an export dump that was generated with the FOR REPLICATION tag, it will check the object it is replicating into if it exists. If that object already exists, it checks the repl.last.id property of that object to determine if what is being imported is newer than the current state of the object in the destination warehouse. If the update is newer, then it replaces the object with the newer information. If the update is older than the object already in place, the update is ignored, and causes no error.\nFor those using EXPORT for the first-time manual bootstrapping usecase, users are recommended to use a \u0026quot; FOR replication(\u0026lsquo;bootstrapping\u0026rsquo;) \u0026quot; tag. (Advanced users note : The choice of \u0026ldquo;bootstrapping\u0026rdquo; here is arbitrary-ish, and could just as well have been \u0026ldquo;foo\u0026rdquo;. The real goal is to have a value such that all further incremental replication ids will be greater than this original id. Thus, the integral value of this initial id should be 0, and thus, any string which does not contain numbers is acceptable. Having an initial tag be \u0026ldquo;123456\u0026rdquo;, however, would be bad, as it could cause further updates which have a repl.last.id \u0026lt; 123456 to not be applied.)\nExamples Simple export and import:\nexport table department to 'hdfs_exports_location/department'; import from 'hdfs_exports_location/department'; Rename table on import:\nexport table department to 'hdfs_exports_location/department'; import table imported_dept from 'hdfs_exports_location/department'; Export partition and import:\nexport table employee partition (emp_country=\u0026quot;in\u0026quot;, emp_state=\u0026quot;ka\u0026quot;) to 'hdfs_exports_location/employee'; import from 'hdfs_exports_location/employee'; Export table and import partition:\nexport table employee to 'hdfs_exports_location/employee'; import table employee partition (emp_country=\u0026quot;us\u0026quot;, emp_state=\u0026quot;tn\u0026quot;) from 'hdfs_exports_location/employee'; Specify the import location:\nexport table department to 'hdfs_exports_location/department'; import table department from 'hdfs_exports_location/department' location 'import_target_location/department'; Import as an external table:\nexport table department to 'hdfs_exports_location/department'; import external table department from 'hdfs_exports_location/department'; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-importexport_27837968/","tags":null,"title":"Apache Hive : LanguageManual ImportExport"},{"categories":null,"contents":"Apache Hive : LanguageManual Indexing  Indexing Is Removed since 3.0 Overview of Hive Indexes Indexing Resources  Configuration Parameters for Hive Indexes   Simple Examples  Indexing Is Removed since 3.0 There are alternate options which might work similarily to indexing:\n Materialized views with automatic rewriting can result in very similar results. Hive 2.3.0 adds support for materialzed views. Using columnar file formats (Parquet, ORC) – they can do selective scanning; they may even skip entire files/blocks.  Indexing has been removed in version 3.0 (HIVE-18448).\nOverview of Hive Indexes The goal of Hive indexing is to improve the speed of query lookup on certain columns of a table. Without an index, queries with predicates like \u0026lsquo;WHERE tab1.col1 = 10\u0026rsquo; load the entire table or partition and process all the rows. But if an index exists for col1, then only a portion of the file needs to be loaded and processed.\nThe improvement in query speed that an index can provide comes at the cost of additional processing to create the index and disk space to store the index.\nVersions\nHive indexing was added in version 0.7.0, and bitmap indexing was added in version 0.8.0.\nIndexing Resources Documentation and examples of how to use Hive indexes can be found here:\n Indexes – design document (lists indexing JIRAs with current status, starting with HIVE-417) Create/Drop/Alter Index – HiveQL Language Manual DDL Show Indexes – HiveQL Language Manual DDL Bitmap indexes – added in Hive version 0.8.0 (HIVE-1803) Indexed Hive – overview and examples by Prafulla Tekawade and Nikhil Deshpande, October 2010 Tutorial: SQL-like join and index with MapReduce using Hadoop and Hive – blog by Ashish Garg, April 2012  Configuration Parameters for Hive Indexes The Configuration Properties document describes parameters that configure Hive indexes.\nSimple Examples This section gives some indexing examples adapted from the Hive test suite.\nCase sensitivity\nIn Hive 0.12.0 and earlier releases, the index name is case-sensitive for CREATE INDEX and DROP INDEX statements. However, ALTER INDEX requires an index name that was created with lowercase letters (see HIVE-2752). This bug is fixed in Hive 0.13.0 by making index names case-insensitive for all HiveQL statements. For releases prior to 0.13.0, the best practice is to use lowercase letters for all index names.\n Create/build, show, and drop index:\nCREATE INDEX table01_index ON TABLE table01 (column2) AS 'COMPACT'; SHOW INDEX ON table01; DROP INDEX table01_index ON table01; Create then build, show formatted (with column names), and drop index:\nCREATE INDEX table02_index ON TABLE table02 (column3) AS 'COMPACT' WITH DEFERRED REBUILD; ALTER INDEX table02_index ON table2 REBUILD; SHOW FORMATTED INDEX ON table02; DROP INDEX table02_index ON table02; Create bitmap index, build, show, and drop:\nCREATE INDEX table03_index ON TABLE table03 (column4) AS 'BITMAP' WITH DEFERRED REBUILD; ALTER INDEX table03_index ON table03 REBUILD; SHOW FORMATTED INDEX ON table03; DROP INDEX table03_index ON table03; Create index in a new table:\nCREATE INDEX table04_index ON TABLE table04 (column5) AS 'COMPACT' WITH DEFERRED REBUILD IN TABLE table04_index_table; Create index stored as RCFile:\nCREATE INDEX table05_index ON TABLE table05 (column6) AS 'COMPACT' STORED AS RCFILE; Create index stored as text file:\nCREATE INDEX table06_index ON TABLE table06 (column7) AS 'COMPACT' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE; Create index with index properties:\nCREATE INDEX table07_index ON TABLE table07 (column8) AS 'COMPACT' IDXPROPERTIES (\u0026quot;prop1\u0026quot;=\u0026quot;value1\u0026quot;, \u0026quot;prop2\u0026quot;=\u0026quot;value2\u0026quot;); Create index with table properties:\nCREATE INDEX table08_index ON TABLE table08 (column9) AS 'COMPACT' TBLPROPERTIES (\u0026quot;prop3\u0026quot;=\u0026quot;value3\u0026quot;, \u0026quot;prop4\u0026quot;=\u0026quot;value4\u0026quot;); Drop index if exists:\nDROP INDEX IF EXISTS table09_index ON table09; Rebuild index on a partition:\nALTER INDEX table10_index ON table10 PARTITION (columnX='valueQ', columnY='valueR') REBUILD; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-indexing_31822176/","tags":null,"title":"Apache Hive : LanguageManual Indexing"},{"categories":null,"contents":"Apache Hive : LanguageManual JoinOptimization Join Optimization  Join Optimization  Improvements to the Hive Optimizer Star Join Optimization  Star Schema Example Prior Support for MAPJOIN  Limitations of Prior Implementation   Enhancements for Star Joins   Optimize Chains of Map Joins\n Current and Future Optimizations    Optimize Auto Join Conversion\n - [Current Optimization](#current-optimization)+ [Auto Conversion to SMB Map Join](#auto-conversion-to-smb-map-join) - [SMB Join across Tables with Different Keys](#smb-join-across-tables-with-different-keys)    Generate Hash Tables on the Task Side\n Pros and Cons of Client-Side Hash Tables Task-Side Generation of Hash Tables  Further Options for Optimization            For a general discussion of Hive joins including syntax, examples, and restrictions, see the Joins wiki doc.\nImprovements to the Hive Optimizer Version\nThe join optimizations described here were added in Hive version 0.11.0. See HIVE-3784 and related JIRAs.\nThis document describes optimizations of Hive\u0026rsquo;s query execution planning to improve the efficiency of joins and reduce the need for user hints.\nHive automatically recognizes various use cases and optimizes for them. Hive 0.11 improves the optimizer for these cases:\n Joins where one side fits in memory. In the new optimization:  that side is loaded into memory as a hash table only the larger table needs to be scanned fact tables have a smaller footprint in memory   Star-schema joins Hints are no longer needed for many cases. Map joins are automatically picked up by the optimizer.  Star Join Optimization A simple schema for decision support systems or data warehouses is the star schema, where events are collected in large fact tables, while smaller supporting tables (dimensions) are used to describe the data.\nThe TPC DS is an example of such a schema. It models a typical retail warehouse where the events are sales and typical dimensions are date of sale, time of sale, or demographic of the purchasing party. Typical queries aggregate and filter fact tables along properties in the dimension tables.\nStar Schema Example Select count(*) cnt From store_sales ss join household_demographics hd on (ss.ss_hdemo_sk = hd.hd_demo_sk) join time_dim t on (ss.ss_sold_time_sk = t.t_time_sk) join store s on (s.s_store_sk = ss.ss_store_sk) Where t.t_hour = 8 t.t_minute \u0026gt;= 30 hd.hd_dep_count = 2 order by cnt; Prior Support for MAPJOIN Hive supports MAPJOINs, which are well suited for this scenario – at least for dimensions small enough to fit in memory. Before release 0.11, a MAPJOIN could be invoked either through an optimizer hint:\nselect /*+ MAPJOIN(time_dim) */ count(*) from store_sales join time_dim on (ss_sold_time_sk = t_time_sk) or via auto join conversion:\nset hive.auto.convert.join=true; select count(*) from store_sales join time_dim on (ss_sold_time_sk = t_time_sk) The default value for hive.auto.convert.join was false in Hive 0.10.0. Hive 0.11.0 changed the default to true (HIVE-3297). Note that hive-default.xml.template incorrectly gives the default as false in Hive 0.11.0 through 0.13.1.\nMAPJOINs are processed by loading the smaller table into an in-memory hash map and matching keys with the larger table as they are streamed through. The prior implementation has this division of labor:\n Local work:  read records via standard table scan (including filters and projections) from source on local machine build hashtable in memory write hashtable to local disk upload hashtable to dfs add hashtable to distributed cache   Map task  read hashtable from local disk (distributed cache) into memory match records' keys against hashtable combine matches and write to output   No reduce task  Limitations of Prior Implementation The MAPJOIN implementation prior to Hive 0.11 has these limitations:\n The mapjoin operator can only handle one key at a time; that is, it can perform a multi-table join, but only if all the tables are joined on the same key. (Typical star schema joins do not fall into this category.) Hints are cumbersome for users to apply correctly and auto conversion doesn\u0026rsquo;t have enough logic to consistently predict if a MAPJOIN will fit into memory or not. A chain of MAPJOINs is not coalesced into a single map-only job, unless the query is written as a cascading sequence of mapjoin(table, subquery(mapjoin(table, subquery....). Auto conversion never produces a single map-only job. The hashtable for the mapjoin operator has to be generated for each run of the query, which involves downloading all the data to the Hive client machine as well as uploading the generated hashtable files.  Enhancements for Star Joins The optimizer enhancements in Hive 0.11 focus on efficient processing of the joins needed in star schema configurations. The initial work was limited to star schema joins where all dimension tables after filtering and projecting fit into memory at the same time. Scenarios where only some of the dimension tables fit into memory are now implemented as well (HIVE-3996).\nThe join optimizations can be grouped into three parts:\n Execute chains of mapjoins in the operator tree in a single map-only job, when maphints are used. Extend optimization to the auto-conversion case (generating an appropriate backup plan when optimizing). Generate in-memory hashtable completely on the task side. (Future work.)  The following sections describe each of these optimizer enhancements.\nOptimize Chains of Map Joins The following query will produce two separate map-only jobs when executed:\nselect /*+ MAPJOIN(time_dim, date_dim) */ count(*) from store_sales join time_dim on (ss_sold_time_sk = t_time_sk) join date_dim on (ss_sold_date_sk = d_date_sk) where t_hour = 8 and d_year = 2002 It is likely, though, that for small dimension tables the parts of both tables needed would fit into memory at the same time. This reduces the time needed to execute this query dramatically, as the fact table is only read once instead of reading it twice and writing it to HDFS to communicate between the jobs.\nCurrent and Future Optimizations  Merge M*-MR patterns into a single MR. Merge MJ-\u0026gt;MJ into a single MJ when possible. Merge MJ* patterns into a single Map stage as a chain of MJ operators. (Not yet implemented.)  If [hive.auto.convert.join](#hive-auto-convert-join) is set to true the optimizer not only converts joins to mapjoins but also merges MJ* patterns as much as possible.\nOptimize Auto Join Conversion When auto join is enabled, there is no longer a need to provide the map-join hints in the query. The auto join option can be enabled with two configuration parameters:\nset hive.auto.convert.join.noconditionaltask = true; set hive.auto.convert.join.noconditionaltask.size = 10000000; The default for [hive.auto.convert.join.noconditionaltask](#hive-auto-convert-join-noconditionaltask) is true which means auto conversion is enabled. (Originally the default was false – see HIVE-3784 – but it was changed to true by HIVE-4146 before Hive 0.11.0 was released.)\nThe size configuration enables the user to control what size table can fit in memory. This value represents the sum of the sizes of tables that can be converted to hashmaps that fit in memory. Currently, n-1 tables of the join have to fit in memory for the map-join optimization to take effect. There is no check to see if the table is a compressed one or not and what the potential size of the table can be. The effect of this assumption on the results is discussed in the next section.\nFor example, the previous query just becomes:\nselect count(*) from store_sales join time_dim on (ss_sold_time_sk = t_time_sk) join date_dim on (ss_sold_date_sk = d_date_sk) where t_hour = 8 and d_year = 2002 If time_dim and date_dim fit in the size configuration provided, the respective joins are converted to map-joins. If the sum of the sizes of the tables can fit in the configured size, then the two map-joins are combined resulting in a single map-join. This reduces the number of MR-jobs required and significantly boosts the speed of execution of this query. This example can be easily extended for multi-way joins as well and will work as expected.\nOuter joins offer more challenges. Since a map-join operator can only stream one table, the streamed table needs to be the one from which all of the rows are required. For the left outer join, this is the table on the left side of the join; for the right outer join, the table on the right side, etc. This means that even though an inner join can be converted to a map-join, an outer join cannot be converted. An outer join can only be converted if the table(s) apart from the one that needs to be streamed can be fit in the size configuration. A full outer join cannot be converted to a map-join at all since both tables need to be streamed.\nAuto join conversion also affects the sort-merge-bucket joins.\nVersion 0.13.0 and later\nHive 0.13.0 introduced [hive.auto.convert.join.use.nonstaged](#hive-auto-convert-join-use-nonstaged) with a default of false (HIVE-6144).\nFor conditional joins, if the input stream from a small alias can be directly applied to the join operator without filtering or projection, then it does not need to be pre-staged in the distributed cache via a MapReduce local task. Setting hive.auto.convert.join.use.nonstaged to true avoids pre-staging in those cases.\nCurrent Optimization  Group as many MJ operators as possible into one MJ.  As Hive goes through the conversion to map-joins for join operators based on the configuration flags, an effort is made at the end of these conversions to group as many together as possible. Going through in a sequence, if the sum of the sizes of the tables participating in the individual map-join operators is within the limit configured by the noConditionalTask.size flag, these MJ operators are combined together. This ensures more speedup with regard to these queries.\nAuto Conversion to SMB Map Join Sort-Merge-Bucket (SMB) joins can be converted to SMB map joins as well. SMB joins are used wherever the tables are sorted and bucketed. The join boils down to just merging the already sorted tables, allowing this operation to be faster than an ordinary map-join. However, if the tables are partitioned, there could be a slow down as each mapper would need to get a very small chunk of a partition which has a single key.\nThe following configuration settings enable the conversion of an SMB to a map-join SMB:\nset hive.auto.convert.sortmerge.join=true; set hive.optimize.bucketmapjoin = true; set hive.optimize.bucketmapjoin.sortedmerge = true; There is an option to set the big table selection policy using the following configuration:\nset hive.auto.convert.sortmerge.join.bigtable.selection.policy = org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ; By default, the selection policy is average partition size. The big table selection policy helps determine which table to choose for only streaming, as compared to hashing and streaming.\nThe available selection policies are:\norg.apache.hadoop.hive.ql.optimizer.AvgPartitionSizeBasedBigTableSelectorForAutoSMJ (default) org.apache.hadoop.hive.ql.optimizer.LeftmostBigTableSelectorForAutoSMJ org.apache.hadoop.hive.ql.optimizer.TableSizeBasedBigTableSelectorForAutoSMJ The names describe their uses. This is especially useful for the fact-fact join (query 82 in the TPC DS benchmark).\nSMB Join across Tables with Different Keys If the tables have differing number of keys, for example Table A has 2 SORT columns and Table B has 1 SORT column, then you might get an index out of bounds exception.\nThe following query results in an index out of bounds exception because emp_person let us say for example has 1 sort column while emp_pay_history has 2 sort columns.\nError Hive 0.11\nSELECT p.*, py.* FROM emp_person p INNER JOIN emp_pay_history py ON p.empid = py.empid This works fine.\nWorking query Hive 0.11\nSELECT p.*, py.* FROM emp_pay_history py INNER JOIN emp_person p ON p.empid = py.empid  Generate Hash Tables on the Task Side Future work will make it possible to generate in-memory hashtables completely on the task side.\nPros and Cons of Client-Side Hash Tables Generating the hashtable (or multiple hashtables for multitable joins) on the client machine has drawbacks. (The client machine is the host that is used to run the Hive client and submit jobs.)\n Data locality: The client machine typically is not a data node. All the data accessed is remote and has to be read via the network. Specs: For the same reason, it is not clear what the specifications of the machine running this processing will be. It might have limitations in memory, hard drive, or CPU that the task nodes do not have. HDFS upload: The data has to be brought back to the cluster and replicated via the distributed cache to be used by task nodes.  Pre-processing the hashtables on the client machine also has some benefits:\n What is stored in the distributed cache is likely to be smaller than the original table (filter and projection). In contrast, loading hashtables directly on the task nodes using the distributed cache means larger objects in the cache, potentially reducing opportunities for using MAPJOIN.  Task-Side Generation of Hash Tables When the hashtables are generated completely on the task side, all task nodes have to access the original data source to generate the hashtable. Since in the normal case this will happen in parallel it will not affect latency, but Hive has a concept of storage handlers and having many tasks access the same external data source (HBase, database, etc.) might overwhelm or slow down the source.\nFurther Options for Optimization  Increase the replication factor on dimension tables. Use the distributed cache to hold dimension tables.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-joinoptimization_33293167/","tags":null,"title":"Apache Hive : LanguageManual JoinOptimization"},{"categories":null,"contents":"Apache Hive : LanguageManual Joins Hive Joins  Hive Joins  Join Syntax Examples MapJoin Restrictions Join Optimization  Predicate Pushdown in Outer Joins Enhancements in Hive Version 0.11      Join Syntax Hive supports the following syntax for joining tables:\njoin_table: table_reference [INNER] JOIN table_factor [join_condition] | table_reference {LEFT|RIGHT|FULL} [OUTER] JOIN table_reference join_condition | table_reference LEFT SEMI JOIN table_reference join_condition | table_reference CROSS JOIN table_reference [join_condition] (as of Hive 0.10) table_reference: table_factor | join_table table_factor: tbl_name [alias] | table_subquery alias | ( table_references ) join_condition: ON expression See Select Syntax for the context of this join syntax.\nVersion 0.13.0+: Implicit join notation\nImplicit join notation is supported starting with Hive 0.13.0 (see HIVE-5558). This allows the FROM clause to join a comma-separated list of tables, omitting the JOIN keyword. For example:\nSELECT * FROM table1 t1, table2 t2, table3 t3 WHERE t1.id = t2.id AND t2.id = t3.id AND t1.zipcode = '02535';\nVersion 0.13.0+: Unqualified column references\nUnqualified column references are supported in join conditions, starting with Hive 0.13.0 (see HIVE-6393). Hive attempts to resolve these against the inputs to a Join. If an unqualified column reference resolves to more than one table, Hive will flag it as an ambiguous reference.\nFor example:\nCREATE TABLE a (k1 string, v1 string);\nCREATE TABLE b (k2 string, v2 string);\nSELECT k1, v1, k2, v2\nFROM a JOIN b ON k1 = k2;\nVersion 2.2.0+: Complex expressions in ON clause\nComplex expressions in ON clause are supported, starting with Hive 2.2.0 (see HIVE-15211, HIVE-15251). Prior to that, Hive did not support join conditions that are not equality conditions.\nIn particular, syntax for join conditions was restricted as follows:\njoin_condition: ON equality_expression ( AND equality_expression )* equality_expression: expression = expression Examples Some salient points to consider when writing join queries are as follows:\n Complex join expressions are allowed e.g.   SELECT a.* FROM a JOIN b ON (a.id = b.id)  SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department)  SELECT a.* FROM a LEFT OUTER JOIN b ON (a.id \u0026lt;\u0026gt; b.id) are valid joins.\n More than 2 tables can be joined in the same query e.g.   SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) is a valid join.\n Hive converts joins over multiple tables into a single map/reduce job if for every table the same column is used in the join clauses e.g.   SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) is converted into a single map/reduce job as only key1 column for b is involved in the join. On the other hand\n SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) is converted into two map/reduce jobs because key1 column from b is used in the first join condition and key2 column from b is used in the second one. The first map/reduce job joins a with b and the results are then joined with c in the second map/reduce job.\n In every map/reduce stage of the join, the last table in the sequence is streamed through the reducers where as the others are buffered. Therefore, it helps to reduce the memory needed in the reducer for buffering the rows for a particular value of the join key by organizing the tables such that the largest tables appear last in the sequence. e.g. in   SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) all the three tables are joined in a single map/reduce job and the values for a particular value of the key for tables a and b are buffered in the memory in the reducers. Then for each row retrieved from c, the join is computed with the buffered rows. Similarly for\n SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2) there are two map/reduce jobs involved in computing the join. The first of these joins a with b and buffers the values of a while streaming the values of b in the reducers. The second of one of these jobs buffers the results of the first join while streaming the values of c through the reducers.\n In every map/reduce stage of the join, the table to be streamed can be specified via a hint. e.g. in   SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) all the three tables are joined in a single map/reduce job and the values for a particular value of the key for tables b and c are buffered in the memory in the reducers. Then for each row retrieved from a, the join is computed with the buffered rows. If the STREAMTABLE hint is omitted, Hive streams the rightmost table in the join.\n LEFT, RIGHT, and FULL OUTER joins exist in order to provide more control over ON clauses for which there is no match. For example, this query:   SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) will return a row for every row in a. This output row will be a.val,b.val when there is a b.key that equals a.key, and the output row will be a.val,NULL when there is no corresponding b.key. Rows from b which have no corresponding a.key will be dropped. The syntax \u0026ldquo;FROM a LEFT OUTER JOIN b\u0026rdquo; must be written on one line in order to understand how it works\u0026ndash;a is to the LEFT of b in this query, and so all rows from a are kept; a RIGHT OUTER JOIN will keep all rows from b, and a FULL OUTER JOIN will keep all rows from a and all rows from b. OUTER JOIN semantics should conform to standard SQL specs.\n Joins occur BEFORE WHERE CLAUSES. So, if you want to restrict the OUTPUT of a join, a requirement should be in the WHERE clause, otherwise it should be in the JOIN clause. A big point of confusion for this issue is partitioned tables:   SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key) WHERE a.ds='2009-07-07' AND b.ds='2009-07-07' will join a on b, producing a list of a.val and b.val. The WHERE clause, however, can also reference other columns of a and b that are in the output of the join, and then filter them out. However, whenever a row from the JOIN has found a key for a and no key for b, all of the columns of b will be NULL, including the ds column. This is to say, you will filter out all rows of join output for which there was no valid b.key, and thus you have outsmarted your LEFT OUTER requirement. In other words, the LEFT OUTER part of the join is irrelevant if you reference any column of b in the WHERE clause. Instead, when OUTER JOINing, use this syntax:\n SELECT a.val, b.val FROM a LEFT OUTER JOIN b ON (a.key=b.key AND b.ds='2009-07-07' AND a.ds='2009-07-07') ..the result is that the output of the join is pre-filtered, and you won\u0026rsquo;t get post-filtering trouble for rows that have a valid a.key but no matching b.key. The same logic applies to RIGHT and FULL joins.\n Joins are NOT commutative! Joins are left-associative regardless of whether they are LEFT or RIGHT joins.   SELECT a.val1, a.val2, b.val, c.val FROM a JOIN b ON (a.key = b.key) LEFT OUTER JOIN c ON (a.key = c.key) \u0026hellip;first joins a on b, throwing away everything in a or b that does not have a corresponding key in the other table. The reduced table is then joined on c. This provides unintuitive results if there is a key that exists in both a and c but not b: The whole row (including a.val1, a.val2, and a.key) is dropped in the \u0026ldquo;a JOIN b\u0026rdquo; step because it is not in b. The result does not have a.key in it, so when it is LEFT OUTER JOINed with c, c.val does not make it in because there is no c.key that matches an a.key (because that row from a was removed). Similarly, if this were a RIGHT OUTER JOIN (instead of LEFT), we would end up with an even weirder effect: NULL, NULL, NULL, c.val, because even though we specified a.key=c.key as the join key, we dropped all rows of a that did not match the first JOIN.\nTo achieve the more intuitive effect, we should instead do FROM c LEFT OUTER JOIN a ON (c.key = a.key) LEFT OUTER JOIN b ON (c.key = b.key).\n LEFT SEMI JOIN implements the uncorrelated IN/EXISTS subquery semantics in an efficient way. As of Hive 0.13 the IN/NOT IN/EXISTS/NOT EXISTS operators are supported using subqueries so most of these JOINs don\u0026rsquo;t have to be performed manually anymore. The restrictions of using LEFT SEMI JOIN are that the right-hand-side table should only be referenced in the join condition (ON-clause), but not in WHERE- or SELECT-clauses etc.   SELECT a.key, a.value FROM a WHERE a.key in (SELECT b.key FROM B); can be rewritten to:\n SELECT a.key, a.val FROM a LEFT SEMI JOIN b ON (a.key = b.key)  If all but one of the tables being joined are small, the join can be performed as a map only job. The query   SELECT /*+ MAPJOIN(b) */ a.key, a.value FROM a JOIN b ON a.key = b.key does not need a reducer. For every mapper of A, B is read completely. The restriction is that a FULL/RIGHT OUTER JOIN b cannot be performed.\n If the tables being joined are bucketized on the join columns, and the number of buckets in one table is a multiple of the number of buckets in the other table, the buckets can be joined with each other. If table A has 4 buckets and table B has 4 buckets, the following join   SELECT /*+ MAPJOIN(b) */ a.key, a.value FROM a JOIN b ON a.key = b.key can be done on the mapper only. Instead of fetching B completely for each mapper of A, only the required buckets are fetched. For the query above, the mapper processing bucket 1 for A will only fetch bucket 1 of B. It is not the default behavior, and is governed by the following parameter\n set hive.optimize.bucketmapjoin = true  If the tables being joined are sorted and bucketized on the join columns, and they have the same number of buckets, a sort-merge join can be performed. The corresponding buckets are joined with each other at the mapper. If both A and B have 4 buckets,   SELECT /*+ MAPJOIN(b) */ a.key, a.value FROM A a JOIN B b ON a.key = b.key can be done on the mapper only. The mapper for the bucket for A will traverse the corresponding bucket for B. This is not the default behavior, and the following parameters need to be set:\n set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat; set hive.optimize.bucketmapjoin = true; set hive.optimize.bucketmapjoin.sortedmerge = true; MapJoin Restrictions  If all but one of the tables being joined are small, the join can be performed as a map only job. The query   SELECT /*+ MAPJOIN(b) */ a.key, a.value FROM a JOIN b ON a.key = b.key does not need a reducer. For every mapper of A, B is read completely.\n The following is not supported.  Union Followed by a MapJoin Lateral View Followed by a MapJoin Reduce Sink (Group By/Join/Sort By/Cluster By/Distribute By) Followed by MapJoin MapJoin Followed by Union MapJoin Followed by Join MapJoin Followed by MapJoin   The configuration variable hive.auto.convert.join (if set to true) automatically converts the joins to mapjoins at runtime if possible, and it should be used instead of the mapjoin hint. The mapjoin hint should only be used for the following query.  If all the inputs are bucketed or sorted, and the join should be converted to a bucketized map-side join or bucketized sort-merge join.   Consider the possibility of multiple mapjoins on different keys:  select /*+MAPJOIN(smallTableTwo)*/ idOne, idTwo, value FROM ( select /*+MAPJOIN(smallTableOne)*/ idOne, idTwo, value FROM bigTable JOIN smallTableOne on (bigTable.idOne = smallTableOne.idOne) ) firstjoin JOIN smallTableTwo ON (firstjoin.idTwo = smallTableTwo.idTwo) The above query is not supported. Without the mapjoin hint, the above query would be executed as 2 map-only jobs. If the user knows in advance that the inputs are small enough to fit in memory, the following configurable parameters can be used to make sure that the query executes in a single map-reduce job.\n+ hive.auto.convert.join.noconditionaltask - Whether Hive enable the optimization about converting common join into mapjoin based on the input file size. If this paramater is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the specified size, the join is directly converted to a mapjoin (there is no conditional task). + hive.auto.convert.join.noconditionaltask.size - If hive.auto.convert.join.noconditionaltask is off, this parameter does not take affect. However, if it is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than this size, the join is directly converted to a mapjoin(there is no conditional task). The default is 10MB.  Join Optimization Predicate Pushdown in Outer Joins See Hive Outer Join Behavior for information about predicate pushdown in outer joins.\nEnhancements in Hive Version 0.11 See Join Optimization for information about enhancements to join optimization introduced in Hive version 0.11.0. The use of hints is de-emphasized in the enhanced optimizations (HIVE-3784 and related JIRAs).\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-joins_27362039/","tags":null,"title":"Apache Hive : LanguageManual Joins"},{"categories":null,"contents":"Apache Hive : LanguageManual LateralView  Lateral View Syntax Description Example Multiple Lateral Views Outer Lateral Views  Lateral View Syntax lateralView: LATERAL VIEW udtf(expression) tableAlias AS columnAlias (',' columnAlias)* fromClause: FROM baseTable (lateralView)* Description Lateral view is used in conjunction with user-defined table generating functions such as explode(). As mentioned in Built-in Table-Generating Functions, a UDTF generates zero or more output rows for each input row. A lateral view first applies the UDTF to each row of base table and then joins resulting output rows to the input rows to form a virtual table having the supplied table alias.\nVersion\nPrior to Hive 0.6.0, lateral view did not support the predicate push-down optimization. In Hive 0.5.0 and earlier, if you used a WHERE clause your query may not have compiled. A workaround was to add set hive.optimize.ppd=false; before your query. The fix was made in Hive 0.6.0; see https://issues.apache.org/jira/browse/HIVE-1056: Predicate push down does not work with UDTF\u0026rsquo;s.\nVersion\nFrom Hive 0.12.0, column aliases can be omitted. In this case, aliases are inherited from field names of StructObjectInspector which is returned from UTDF.\nExample Consider the following base table named pageAds. It has two columns: pageid (name of the page) and adid_list (an array of ads appearing on the page):\n   Column name Column type     pageid STRING   adid_list Array    An example table with two rows:\n   pageid adid_list     front_page [1, 2, 3]   contact_page [3, 4, 5]    and the user would like to count the total number of times an ad appears across all pages.\nA lateral view with explode() can be used to convert adid_list into separate rows using the query:\nSELECT pageid, adid FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid; The resulting output will be\n   pageid (string) adid (int)     \u0026ldquo;front_page\u0026rdquo; 1   \u0026ldquo;front_page\u0026rdquo; 2   \u0026ldquo;front_page\u0026rdquo; 3   \u0026ldquo;contact_page\u0026rdquo; 3   \u0026ldquo;contact_page\u0026rdquo; 4   \u0026ldquo;contact_page\u0026rdquo; 5    Then in order to count the number of times a particular ad appears, count/group by can be used:\nSELECT adid, count(1) FROM pageAds LATERAL VIEW explode(adid_list) adTable AS adid GROUP BY adid;          int adid count(1)   1 1   2 1   3 2   4 1   5 1    Multiple Lateral Views A FROM clause can have multiple LATERAL VIEW clauses. Subsequent LATERAL VIEWS can reference columns from any of the tables appearing to the left of the LATERAL VIEW.\nFor example, the following could be a valid query:\nSELECT * FROM exampleTable LATERAL VIEW explode(col1) myTable1 AS myCol1 LATERAL VIEW explode(myCol1) myTable2 AS myCol2; LATERAL VIEW clauses are applied in the order that they appear. For example with the following base table:\n         Array\u0026lt;int\u0026gt; col1 Array\u0026lt;string\u0026gt; col2   [1, 2] [a\u0026quot;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;]   [3, 4] [d\u0026quot;, \u0026ldquo;e\u0026rdquo;, \u0026ldquo;f\u0026rdquo;]    The query:\nSELECT myCol1, col2 FROM baseTable LATERAL VIEW explode(col1) myTable1 AS myCol1; Will produce:\n         int mycol1 Array\u0026lt;string\u0026gt; col2   1 [a\u0026quot;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;]   2 [a\u0026quot;, \u0026ldquo;b\u0026rdquo;, \u0026ldquo;c\u0026rdquo;]   3 [d\u0026quot;, \u0026ldquo;e\u0026rdquo;, \u0026ldquo;f\u0026rdquo;]   4 [d\u0026quot;, \u0026ldquo;e\u0026rdquo;, \u0026ldquo;f\u0026rdquo;]    A query that adds an additional LATERAL VIEW:\nSELECT myCol1, myCol2 FROM baseTable LATERAL VIEW explode(col1) myTable1 AS myCol1 LATERAL VIEW explode(col2) myTable2 AS myCol2; Will produce:\n         int myCol1 string myCol2   1 \u0026ldquo;a\u0026rdquo;   1 \u0026ldquo;b\u0026rdquo;   1 \u0026ldquo;c\u0026rdquo;   2 \u0026ldquo;a\u0026rdquo;   2 \u0026ldquo;b\u0026rdquo;   2 \u0026ldquo;c\u0026rdquo;   3 \u0026ldquo;d\u0026rdquo;   3 \u0026ldquo;e\u0026rdquo;   3 \u0026ldquo;f\u0026rdquo;   4 \u0026ldquo;d\u0026rdquo;   4 \u0026ldquo;e\u0026rdquo;   4 \u0026ldquo;f\u0026rdquo;    Outer Lateral Views Version\nIntroduced in Hive version 0.12.0\nThe user can specify the optional OUTER keyword to generate rows even when a LATERAL VIEW usually would not generate a row. This happens when the UDTF used does not generate any rows which happens easily with explode when the column to explode is empty. In this case the source row would never appear in the results. OUTER can be used to prevent that and rows will be generated with NULL values in the columns coming from the UDTF.\nFor example, the following query returns an empty result:\nSELEC * FROM src LATERAL VIEW explode(array()) C AS a limit 10; But with the OUTER keyword\nSELECT * FROM src LATERAL VIEW OUTER explode(array()) C AS a limit 10; it will produce:\n238 val_238 NULL\n86 val_86 NULL\n311 val_311 NULL\n27 val_27 NULL\n165 val_165 NULL\n409 val_409 NULL\n255 val_255 NULL\n278 val_278 NULL\n98 val_98 NULL\n\u0026hellip;\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-lateralview_27362040/","tags":null,"title":"Apache Hive : LanguageManual LateralView"},{"categories":null,"contents":"Apache Hive : LanguageManual LZO LZO Compression  LZO Compression  General LZO Concepts Prerequisites  Lzo/Lzop Installations core-site.xml Table Definition   Hive Queries  Option 1: Directly Create LZO Files Option 2: Write Custom Java to Create LZO Files      General LZO Concepts LZO is a lossless data compression library that favors speed over compression ratio. See http://www.oberhumer.com/opensource/lzo and http://www.lzop.org for general information about LZO and see Compressed Data Storage for information about compression in Hive.\nImagine a simple data file that has three columns\n id first name last name  Let\u0026rsquo;s populate a data file containing 4 records:\n19630001 john lennon 19630002 paul mccartney 19630003 george harrison 19630004 ringo starr Let\u0026rsquo;s call the data file /path/to/dir/names.txt.\nIn order to make it into an LZO file, we can use the lzop utility and it will create a names.txt.lzo file.\nNow copy the file names.txt.lzo to HDFS.\nPrerequisites Lzo/Lzop Installations lzo and lzop need to be installed on every node in the Hadoop cluster. The details of these installations are beyond the scope of this document.\ncore-site.xml Add the following to your core-site.xml:\n com.hadoop.compression.lzo.LzoCodec com.hadoop.compression.lzo.LzopCodec  For example:\n\u0026lt;property\u0026gt;\n\u0026lt;name\u0026gt;io.compression.codecs\u0026lt;/name\u0026gt;\n\u0026lt;value\u0026gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec\u0026lt;/value\u0026gt;\n\u0026lt;/property\u0026gt;\n\u0026lt;property\u0026gt;\n\u0026lt;name\u0026gt;io.compression.codec.lzo.class\u0026lt;/name\u0026gt;\n\u0026lt;value\u0026gt;com.hadoop.compression.lzo.LzoCodec\u0026lt;/value\u0026gt;\n\u0026lt;/property\u0026gt;\nNext we run the command to create an LZO index file:\nhadoop jar /path/to/jar/hadoop-lzo-cdh4-0.4.15-gplextras.jar com.hadoop.compression.lzo.LzoIndexer /path/to/HDFS/dir/containing/lzo/files This creates names.txt.lzo on HDFS.\nTable Definition The following hive -e command creates an LZO-compressed external table:\nhive -e \u0026quot;CREATE EXTERNAL TABLE IF NOT EXISTS hive_table_name (column_1 datatype_1......column_N datatype_N) PARTITIONED BY (partition_col_1 datatype_1 ....col_P datatype_P) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS INPUTFORMAT \\\u0026quot;com.hadoop.mapred.DeprecatedLzoTextInputFormat\\\u0026quot; OUTPUTFORMAT \\\u0026quot;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\\\u0026quot;; Note: The double quotes have to be escaped so that the \u0026lsquo;hive -e\u0026rsquo; command works correctly.\nSee CREATE TABLE and Hive CLI for information about command syntax.\nHive Queries Option 1: Directly Create LZO Files  Directly create LZO files as the output of the Hive query. Use lzop command utility or your custom Java to generate .lzo.index for the .lzo files.  Hive Query Parameters\nSET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec SET hive.exec.compress.output=true SET mapreduce.output.fileoutputformat.compress=true For example:\nhive -e \u0026quot;SET mapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec; SET hive.exec.compress.output=true;SET mapreduce.output.fileoutputformat.compress=true; \u0026lt;query-string\u0026gt;\u0026quot; Note: If the data sets are large or number of output files are large , then this option does not work.\nOption 2: Write Custom Java to Create LZO Files  Create text files as the output of the Hive query. Write custom Java code to  convert Hive query generated text files to .lzo files generate .lzo.index files for the .lzo files generated above    Hive Query Parameters\nPrefix the query string with these parameters:\nSET hive.exec.compress.output=false SET mapreduce.output.fileoutputformat.compress=false For example:\nhive -e \u0026quot;SET hive.exec.compress.output=false;SET mapreduce.output.fileoutputformat.compress=false;\u0026lt;query-string\u0026gt;\u0026quot; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-lzo_33298193/","tags":null,"title":"Apache Hive : LanguageManual LZO"},{"categories":null,"contents":"Apache Hive : LanguageManual ORC ORC Files  ORC Files  ORC File Format  File Structure Stripe Structure   HiveQL Syntax Serialization and Compression  Integer Column Serialization String Column Serialization Compression   ORC File Dump Utility ORC Configuration Parameters   ORC Format Specification  ORC File Format Version\nIntroduced in Hive version 0.11.0.\nThe Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. It was designed to overcome limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data.\nCompared with RCFile format, for example, ORC file format has many advantages such as:\n a single file as the output of each task, which reduces the NameNode\u0026rsquo;s load Hive type support including datetime, decimal, and the complex types (struct, list, map, and union) light-weight indexes stored within the file  skip row groups that don\u0026rsquo;t pass predicate filtering seek to a given row   block-mode compression based on data type  run-length encoding for integer columns dictionary encoding for string columns   concurrent reads of the same file using separate RecordReaders ability to split files without scanning for markers bound the amount of memory needed for reading or writing metadata stored using Protocol Buffers, which allows addition and removal of fields  File Structure An ORC file contains groups of row data called stripes, along with auxiliary information in a file footer. At the end of the file a postscript holds compression parameters and the size of the compressed footer.\nThe default stripe size is 250 MB. Large stripe sizes enable large, efficient reads from HDFS.\nThe file footer contains a list of stripes in the file, the number of rows per stripe, and each column\u0026rsquo;s data type. It also contains column-level aggregates count, min, max, and sum.\nThis diagram illustrates the ORC file structure:\nStripe Structure As shown in the diagram, each stripe in an ORC file holds index data, row data, and a stripe footer.\nThe stripe footer contains a directory of stream locations. Row data is used in table scans.\nIndex data includes min and max values for each column and the row positions within each column. (A bit field or bloom filter could also be included.) Row index entries provide offsets that enable seeking to the right compression block and byte within a decompressed block. Note that ORC indexes are used only for the selection of stripes and row groups and not for answering queries.\nHaving relatively frequent row index entries enables row-skipping within a stripe for rapid reads, despite large stripe sizes. By default every 10,000 rows can be skipped.\nWith the ability to skip large sets of rows based on filter predicates, you can sort a table on its secondary keys to achieve a big reduction in execution time. For example, if the primary partition is transaction date, the table can be sorted on state, zip code, and last name. Then looking for records in one state will skip the records of all other states.\nA complete specification of the format is given in the ORC specification.\nHiveQL Syntax File formats are specified at the table (or partition) level. You can specify the ORC file format with HiveQL statements such as these:\n CREATE TABLE ... STORED AS ORC ALTER TABLE ... [PARTITION partition_spec] SET FILEFORMAT ORC SET hive.default.fileformat=Orc  The parameters are all placed in the TBLPROPERTIES (see Create Table). They are:\n   Key Default Notes     orc.compress ZLIB high level compression (one of NONE, ZLIB, SNAPPY)   orc.compress.size 262,144 number of bytes in each compression chunk   orc.stripe.size 67,108,864 number of bytes in each stripe   orc.row.index.stride 10,000 number of rows between index entries (must be \u0026gt;= 1000)   orc.create.index true whether to create row indexes   orc.bloom.filter.columns \u0026quot;\u0026quot; comma separated list of column names for which bloom filter should be created   orc.bloom.filter.fpp 0.05 false positive probability for bloom filter (must \u0026gt;0.0 and \u0026lt;1.0)    For example, creating an ORC stored table without compression:\ncreate table Addresses ( name string, street string, city string, state string, zip int ) stored as orc tblproperties (\u0026quot;orc.compress\u0026quot;=\u0026quot;NONE\u0026quot;); Version 0.14.0+: CONCATENATE\n[ALTER TABLE table_name [PARTITION partition_spec] CONCATENATE](#alter-table-table_name-[partition-partition_spec]-concatenate) can be used to merge small ORC files into a larger file, starting in Hive 0.14.0. The merge happens at the stripe level, which avoids decompressing and decoding the data.\nSerialization and Compression The serialization of column data in an ORC file depends on whether the data type is integer or string.\nInteger Column Serialization Integer columns are serialized in two streams.\n present bit stream: is the value non-null? data stream: a stream of integers  Integer data is serialized in a way that takes advantage of the common distribution of numbers:\n Integers are encoded using a variable-width encoding that has fewer bytes for small integers. Repeated values are run-length encoded. Values that differ by a constant in the range (-128 to 127) are run-length encoded.  The variable-width encoding is based on Google\u0026rsquo;s protocol buffers and uses the high bit to represent whether this byte is not the last and the lower 7 bits to encode data. To encode negative numbers, a zigzag encoding is used where 0, -1, 1, -2, and 2 map into 0, 1, 2, 3, 4, and 5 respectively.\nEach set of numbers is encoded this way:\n If the first byte (b0) is negative:  -b0 variable-length integers follow.   If the first byte (b0) is positive:  it represents b0 + 3 repeated integers the second byte (-128 to +127) is added between each repetition 1 variable-length integer.    In run-length encoding, the first byte specifies run length and whether the values are literals or duplicates. Duplicates can step by -128 to +128. Run-length encoding uses protobuf style variable-length integers.\nString Column Serialization Serialization of string columns uses a dictionary to form unique column values. The dictionary is sorted to speed up predicate filtering and improve compression ratios.\nString columns are serialized in four streams.\n present bit stream: is the value non-null? dictionary data: the bytes for the strings dictionary length: the length of each entry row data: the row values  Both the dictionary length and the row values are run-length encoded streams of integers.\nCompression Streams are compressed using a codec, which is specified as a table property for all streams in that table. To optimize memory use, compression is done incrementally as each block is produced. Compressed blocks can be jumped over without first having to be decompressed for scanning. Positions in the stream are represented by a block start location and an offset into the block.\nThe codec can be Snappy, Zlib, or none.\nORC File Dump Utility The ORC file dump utility analyzes ORC files. To invoke it, use this command:\n// Hive version 0.11 through 0.14: hive --orcfiledump \u0026lt;location-of-orc-file\u0026gt; // Hive version 1.1.0 and later: hive --orcfiledump [-d] [--rowindex \u0026lt;col_ids\u0026gt;] \u0026lt;location-of-orc-file\u0026gt; // Hive version 1.2.0 and later: hive --orcfiledump [-d] [-t] [--rowindex \u0026lt;col_ids\u0026gt;] \u0026lt;location-of-orc-file\u0026gt; // Hive version 1.3.0 and later: hive --orcfiledump [-j] [-p] [-d] [-t] [--rowindex \u0026lt;col_ids\u0026gt;] [--recover] [--skip-dump] [--backup-path \u0026lt;new-path\u0026gt;] \u0026lt;location-of-orc-file-or-directory\u0026gt; Specifying -d in the command will cause it to dump the ORC file data rather than the metadata (Hive 1.1.0 and later).\nSpecifying --rowindex with a comma separated list of column ids will cause it to print row indexes for the specified columns, where 0 is the top level struct containing all of the columns and 1 is the first column id (Hive 1.1.0 and later).\nSpecifying -t in the command will print the timezone id of the writer.\nSpecifying -j in the command will print the ORC file metadata in JSON format. To pretty print the JSON metadata, add -p to the command.\nSpecifying --recover in the command will recover a corrupted ORC file generated by Hive streaming.\nSpecifying --skip-dump along with --recover will perform recovery without dumping metadata.\nSpecifying --backup-path with a new-path will let the recovery tool move corrupted files to the specified backup path (default: /tmp).\n is the URI of the ORC file.\n is the URI of the ORC file or directory. From Hive 1.3.0 onward, this URI can be a directory containing ORC files.\nORC Configuration Parameters The ORC configuration parameters are described in Hive Configuration Properties – ORC File Format.\nORC Format Specification The ORC specification has moved to ORC project.\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-orc_31818911/","tags":null,"title":"Apache Hive : LanguageManual ORC"},{"categories":null,"contents":"Apache Hive : LanguageManual Sampling  Sampling Syntax  Sampling Bucketized Table Block Sampling    Sampling Syntax Sampling Bucketized Table  table_sample: TABLESAMPLE (BUCKET x OUT OF y [ON colname]) The TABLESAMPLE clause allows the users to write queries for samples of the data instead of the whole table. The TABLESAMPLE clause can be added to any table in the FROM clause. The buckets are numbered starting from 1. colname indicates the column on which to sample each row in the table. colname can be one of the non-partition columns in the table or rand() indicating sampling on the entire row instead of an individual column. The rows of the table are \u0026lsquo;bucketed\u0026rsquo; on the colname randomly into y buckets numbered 1 through y. Rows which belong to bucket x are returned.\nIn the following example the 3rd bucket out of the 32 buckets of the table source. \u0026rsquo;s' is the table alias.\n SELECT * FROM source TABLESAMPLE(BUCKET 3 OUT OF 32 ON rand()) s; Input pruning: Typically, TABLESAMPLE will scan the entire table and fetch the sample. But, that is not very efficient. Instead, the table can be created with a CLUSTERED BY clause which indicates the set of columns on which the table is hash-partitioned/clustered on. If the columns specified in the TABLESAMPLE clause match the columns in the CLUSTERED BY clause, TABLESAMPLE scans only the required hash-partitions of the table.\nExample:\nSo in the above example, if table \u0026lsquo;source\u0026rsquo; was created with \u0026lsquo;CLUSTERED BY id INTO 32 BUCKETS\u0026rsquo;\n TABLESAMPLE(BUCKET 3 OUT OF 16 ON id) would pick out the 3rd and 19th clusters as each bucket would be composed of (32/16)=2 clusters.\nOn the other hand the tablesample clause\n TABLESAMPLE(BUCKET 3 OUT OF 64 ON id) would pick out half of the 3rd cluster as each bucket would be composed of (32/64)=1/2 of a cluster.\nFor information about creating bucketed tables with the CLUSTERED BY clause, see Create Table (especially Bucketed Sorted Tables) and Bucketed Tables.\nBlock Sampling Block sampling is available starting with Hive 0.8. Addressed under JIRA - https://issues.apache.org/jira/browse/HIVE-2121\n block_sample: TABLESAMPLE (n PERCENT) This will allow Hive to pick up at least n% data size (notice it doesn\u0026rsquo;t necessarily mean number of rows) as inputs. Only CombineHiveInputFormat is supported and some special compression formats are not handled. If we fail to sample it, the input of MapReduce job will be the whole table/partition. We do it in HDFS block level so that the sampling granularity is block size. For example, if block size is 256MB, even if n% of input size is only 100MB, you get 256MB of data.\nIn the following example the input size 0.1% or more will be used for the query.\n SELECT * FROM source TABLESAMPLE(0.1 PERCENT) s; Sometimes you want to sample the same data with different blocks, you can change this seed number:\n set hive.sample.seednumber=\u0026lt;INTEGER\u0026gt;; Or user can specify total length to be read, but it has same limitation with PERCENT sampling. (As of Hive 0.10.0 - https://issues.apache.org/jira/browse/HIVE-3401)\n block_sample: TABLESAMPLE (ByteLengthLiteral) ByteLengthLiteral : (Digit)+ ('b' | 'B' | 'k' | 'K' | 'm' | 'M' | 'g' | 'G') In the following example the input size 100M or more will be used for the query.\n SELECT * FROM source TABLESAMPLE(100M) s; Hive also supports limiting input by row count basis, but it acts differently with above two. First, it does not need CombineHiveInputFormat which means this can be used with non-native tables. Second, the row count given by user is applied to each split. So total row count can be vary by number of input splits. (As of Hive 0.10.0 - https://issues.apache.org/jira/browse/HIVE-3401)\n block_sample: TABLESAMPLE (n ROWS) For example, the following query will take the first 10 rows from each input split.\n SELECT * FROM source TABLESAMPLE(10 ROWS); ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-sampling_27362042/","tags":null,"title":"Apache Hive : LanguageManual Sampling"},{"categories":null,"contents":"Apache Hive : LanguageManual Select  Select Syntax  WHERE Clause ALL and DISTINCT Clauses Partition Based Queries HAVING Clause LIMIT Clause REGEX Column Specification More Select Syntax    GROUP BY; SORT/ORDER/CLUSTER/DISTRIBUTE BY; JOIN (Hive Joins, Join Optimization, Outer Join Behavior); UNION; TABLESAMPLE; Subqueries; Virtual Columns; Operators and UDFs; LATERAL VIEW; Windowing, OVER, and Analytics; Common Table Expressions\nSelect Syntax [WITH CommonTableExpression (, CommonTableExpression)*] (Note: Only available starting with Hive 0.13.0) SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT [offset,] rows]  A SELECT statement can be part of a union query or a subquery of another query. table_reference indicates the input to the query. It can be a regular table, a view, a join construct or a subquery. Table names and column names are case insensitive.  In Hive 0.12 and earlier, only alphanumeric and underscore characters are allowed in table and column names. In Hive 0.13 and later, column names can contain any Unicode character (see HIVE-6013). Any column name that is specified within backticks (```) is treated literally. Within a backtick string, use double backticks (````) to represent a backtick character. To revert to pre-0.13.0 behavior and restrict column names to alphanumeric and underscore characters, set the configuration property [hive.support.quoted.identifiers](#hive-support-quoted-identifiers) to none. In this configuration, backticked names are interpreted as regular expressions. For details, see Supporting Quoted Identifiers in Column Names (attached to HIVE-6013). Also see REGEX Column Specification below.   Simple query. For example, the following query retrieves all columns and all rows from table t1.  SELECT * FROM t1 Note\nAs of Hive 0.13.0, FROM is optional (for example, SELECT 1+1).\n To get the current database (as of Hive 0.13.0), use the current_database() function:  SELECT current_database()  To specify a database, either qualify the table names with database names (\u0026quot;db_name.table_name\u0026quot; starting in Hive 0.7) or issue the USE statement before the query statement (starting in Hive 0.6).  \u0026ldquo;db_name.table_name\u0026rdquo; allows a query to access tables in different databases.\nUSE sets the database for all subsequent HiveQL statements. Reissue it with the keyword \u0026ldquo;default\u0026rdquo; to reset to the default database.\nUSE database_name; SELECT query_specifications; USE default; WHERE Clause The WHERE condition is a boolean expression. For example, the following query returns only those sales records which have an amount greater than 10 from the US region. Hive supports a number of operators and UDFs in the WHERE clause:\nSELECT * FROM sales WHERE amount \u0026gt; 10 AND region = \u0026quot;US\u0026quot; As of Hive 0.13 some types of subqueries are supported in the WHERE clause.\nALL and DISTINCT Clauses The ALL and DISTINCT options specify whether duplicate rows should be returned. If none of these options are given, the default is ALL (all matching rows are returned). DISTINCT specifies removal of duplicate rows from the result set. Note, Hive supports SELECT DISTINCT * starting in release 1.1.0 (HIVE-9194).\nhive\u0026gt; SELECT col1, col2 FROM t1 1 3 1 3 1 4 2 5 hive\u0026gt; SELECT DISTINCT col1, col2 FROM t1 1 3 1 4 2 5 hive\u0026gt; SELECT DISTINCT col1 FROM t1 1 2 ALL and DISTINCT can also be used in a UNION clause – see Union Syntax for more information.\nPartition Based Queries In general, a SELECT query scans the entire table (other than for sampling). If a table created using the PARTITIONED BY clause, a query can do partition pruning and scan only a fraction of the table relevant to the partitions specified by the query. Hive currently does partition pruning if the partition predicates are specified in the WHERE clause or the ON clause in a JOIN. For example, if table page_views is partitioned on column date, the following query retrieves rows for just days between 2008-03-01 and 2008-03-31.\n SELECT page_views.* FROM page_views WHERE page_views.date \u0026gt;= '2008-03-01' AND page_views.date \u0026lt;= '2008-03-31' If a table page_views is joined with another table dim_users, you can specify a range of partitions in the ON clause as follows:\n SELECT page_views.* FROM page_views JOIN dim_users ON (page_views.user_id = dim_users.id AND page_views.date \u0026gt;= '2008-03-01' AND page_views.date \u0026lt;= '2008-03-31')  See also Partition Filter Syntax. See also Group By. See also Sort By / Cluster By / Distribute By / Order By.  HAVING Clause Hive added support for the HAVING clause in version 0.7.0. In older versions of Hive it is possible to achieve the same effect by using a subquery, e.g:\nSELECT col1 FROM t1 GROUP BY col1 HAVING SUM(col2) \u0026gt; 10 can also be expressed as\nSELECT col1 FROM (SELECT col1, SUM(col2) AS col2sum FROM t1 GROUP BY col1) t2 WHERE t2.col2sum \u0026gt; 10 LIMIT Clause The LIMIT clause can be used to constrain the number of rows returned by the SELECT statement.\nLIMIT takes one or two numeric arguments, which must both be non-negative integer constants.\nThe first argument specifies the offset of the first row to return (as of Hive 2.0.0) and the second specifies the maximum number of rows to return.\nWhen a single argument is given, it stands for the maximum number of rows and the offset defaults to 0.\n The following query returns 5 arbitrary customers\nSELECT * FROM customers LIMIT 5 The following query returns the first 5 customers to be created\nSELECT * FROM customers ORDER BY create_date LIMIT 5  The following query returns the 3rd to the 7th customers to be created\nSELECT * FROM customers ORDER BY create_date LIMIT 2,5 REGEX Column Specification A SELECT statement can take regex-based column specification in Hive releases prior to 0.13.0, or in 0.13.0 and later releases if the configuration property [hive.support.quoted.identifiers](#hive-support-quoted-identifiers) is set to none.  We use Java regex syntax. Try http://www.fileformat.info/tool/regex.htm for testing purposes. The following query selects all columns except ds and hr.  SELECT `(ds|hr)?+.+` FROM sales More Select Syntax See the following documents for additional syntax and features of SELECT statements:\n GROUP BY SORT/ORDER/CLUSTER/DISTRIBUTE BY JOIN  Hive Joins Join Optimization Outer Join Behavior   UNION TABLESAMPLE Subqueries Virtual Columns Operators and UDFs LATERAL VIEW Windowing, OVER, and Analytics Common Table Expressions     ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-select_27362043/","tags":null,"title":"Apache Hive : LanguageManual Select"},{"categories":null,"contents":"Apache Hive : LanguageManual SortBy  Order, Sort, Cluster, and Distribute By  Syntax of Order By Syntax of Sort By  Difference between Sort By and Order By Setting Types for Sort By   Syntax of Cluster By and Distribute By    Order, Sort, Cluster, and Distribute By This describes the syntax of SELECT clauses ORDER BY, SORT BY, CLUSTER BY, and DISTRIBUTE BY. See Select Syntax for general information.\nSyntax of Order By The ORDER BY syntax in Hive QL is similar to the syntax of ORDER BY in SQL language.\ncolOrder: ( ASC | DESC ) colNullOrder: (NULLS FIRST | NULLS LAST) -- (Note: Available in Hive 2.1.0 and later) orderBy: ORDER BY colName colOrder? colNullOrder? (',' colName colOrder? colNullOrder?)* query: SELECT expression (',' expression)* FROM src orderBy There are some limitations in the \u0026ldquo;order by\u0026rdquo; clause. In the strict mode (i.e., hive.mapred.mode=strict), the order by clause has to be followed by a \u0026ldquo;limit\u0026rdquo; clause. The limit clause is not necessary if you set hive.mapred.mode to nonstrict. The reason is that in order to impose total order of all results, there has to be one reducer to sort the final output. If the number of rows in the output is too large, the single reducer could take a very long time to finish.\nNote that columns are specified by name, not by position number. However in Hive 0.11.0 and later, columns can be specified by position when configured as follows:\n For Hive 0.11.0 through 2.1.x, set hive.groupby.orderby.position.alias to true (the default is false). For Hive 2.2.0 and later, hive.orderby.position.alias is true by default.  The default sorting order is ascending (ASC).\nIn Hive 2.1.0 and later, specifying the null sorting order for each of the columns in the \u0026ldquo;order by\u0026rdquo; clause is supported. The default null sorting order for ASC order is NULLS FIRST, while the default null sorting order for DESC order is NULLS LAST.\nIn Hive 3.0.0 and later, order by without limit in subqueries and views will be removed by the optimizer. To disable it, set hive.remove.orderby.in.subquery to false.\nSyntax of Sort By The SORT BY syntax is similar to the syntax of ORDER BY in SQL language.\ncolOrder: ( ASC | DESC ) sortBy: SORT BY colName colOrder? (',' colName colOrder?)* query: SELECT expression (',' expression)* FROM src sortBy Hive uses the columns in SORT BY to sort the rows before feeding the rows to a reducer. The sort order will be dependent on the column types. If the column is of numeric type, then the sort order is also in numeric order. If the column is of string type, then the sort order will be lexicographical order.\nIn Hive 3.0.0 and later, sort by without limit in subqueries and views will be removed by the optimizer. To disable it, set hive.remove.orderby.in.subquery to false.\nDifference between Sort By and Order By Hive supports SORT BY which sorts the data per reducer. The difference between \u0026ldquo;order by\u0026rdquo; and \u0026ldquo;sort by\u0026rdquo; is that the former guarantees total order in the output while the latter only guarantees ordering of the rows within a reducer. If there are more than one reducer, \u0026ldquo;sort by\u0026rdquo; may give partially ordered final results.\nNote: It may be confusing as to the difference between SORT BY alone of a single column and CLUSTER BY. The difference is that CLUSTER BY partitions by the field and SORT BY if there are multiple reducers partitions randomly in order to distribute data (and load) uniformly across the reducers.\nBasically, the data in each reducer will be sorted according to the order that the user specified. The following example shows\nSELECT key, value FROM src SORT BY key ASC, value DESC The query had 2 reducers, and the output of each is:\n0 5 0 3 3 6 9 1 0 4 0 3 1 1 2 5 Setting Types for Sort By After a transform, variable types are generally considered to be strings, meaning that numeric data will be sorted lexicographically. To overcome this, a second SELECT statement with casts can be used before using SORT BY.\nFROM (FROM (FROM src SELECT TRANSFORM(value) USING 'mapper' AS value, count) mapped SELECT cast(value as double) AS value, cast(count as int) AS count SORT BY value, count) sorted SELECT TRANSFORM(value, count) USING 'reducer' AS whatever Syntax of Cluster By and Distribute By Cluster By and Distribute By are used mainly with the Transform/Map-Reduce Scripts. But, it is sometimes useful in SELECT statements if there is a need to partition and sort the output of a query for subsequent queries.\nCluster By is a short-cut for both Distribute By and Sort By.\nHive uses the columns in Distribute By to distribute the rows among reducers. All rows with the same Distribute By columns will go to the same reducer. However, Distribute By does not guarantee clustering or sorting properties on the distributed keys.\nFor example, we are Distributing By x on the following 5 rows to 2 reducer:\nx1 x2 x4 x3 x1 Reducer 1 got\nx1 x2 x1 Reducer 2 got\nx4 x3 Note that all rows with the same key x1 is guaranteed to be distributed to the same reducer (reducer 1 in this case), but they are not guaranteed to be clustered in adjacent positions.\nIn contrast, if we use Cluster By x, the two reducers will further sort rows on x:\nReducer 1 got\nx1 x1 x2 Reducer 2 got\nx3 x4 Instead of specifying Cluster By, the user can specify Distribute By and Sort By, so the partition columns and sort columns can be different. The usual case is that the partition columns are a prefix of sort columns, but that is not required.\nSELECT col1, col2 FROM t1 CLUSTER BY col1 SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SELECT col1, col2 FROM t1 DISTRIBUTE BY col1 SORT BY col1 ASC, col2 DESC  FROM ( FROM pv_users MAP ( pv_users.userid, pv_users.date ) USING 'map_script' AS c1, c2, c3 DISTRIBUTE BY c2 SORT BY c2, c1) map_output INSERT OVERWRITE TABLE pv_users_reduced REDUCE ( map_output.c1, map_output.c2, map_output.c3 ) USING 'reduce_script' AS date, count; Note that columns are specified by name, not by position number. However in HIVE-28572 and later, columns can be specified by position when configured as follows:\n set hive.orderby.position.alias=true; set hive.cbo.enable=true;  When any of the above conditions are not met, no distribution is performed.\nIn the following example we distribute by the 3rd and the 1st column (birthdate, age):\nset hive.orderby.position.alias=true; set hive.cbo.enable=true; SELECT age, name, birthdate FROM author DISTRIBUTE BY 3, 1; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-sortby_27362045/","tags":null,"title":"Apache Hive : LanguageManual SortBy"},{"categories":null,"contents":"Apache Hive : LanguageManual SubQueries  Subqueries in the FROM Clause Subqueries in the WHERE Clause  Subqueries in the FROM Clause SELECT ... FROM (subquery) name ... SELECT ... FROM (subquery) AS name ... (Note: Only valid starting with Hive 0.13.0) Hive supports subqueries only in the FROM clause (through Hive 0.12). The subquery has to be given a name because every table in a FROM clause must have a name. Columns in the subquery select list must have unique names. The columns in the subquery select list are available in the outer query just like columns of a table. The subquery can also be a query expression with UNION. Hive supports arbitrary levels of subqueries.\nThe optional keyword \u0026ldquo;AS\u0026rdquo; can be included before the subquery name in Hive 0.13.0 and later versions (HIVE-6519).\nExample with simple subquery:\nSELECT col FROM ( SELECT a+b AS col FROM t1 ) t2 Example with subquery containing a UNION ALL:\nSELECT t3.col FROM ( SELECT a+b AS col FROM t1 UNION ALL SELECT c+d AS col FROM t2 ) t3 Subqueries in the WHERE Clause As of Hive 0.13 some types of subqueries are supported in the WHERE clause. Those are queries where the result of the query can be treated as a constant for IN and NOT IN statements (called uncorrelated subqueries because the subquery does not reference columns from the parent query):\nSELECT * FROM A WHERE A.a IN (SELECT foo FROM B); The other supported types are EXISTS and NOT EXISTS subqueries:\nSELECT A FROM T1 WHERE EXISTS (SELECT B FROM T2 WHERE T1.X = T2.Y) There are a few limitations:\n These subqueries are only supported on the right-hand side of an expression. IN/NOT IN subqueries may only select a single column. EXISTS/NOT EXISTS must have one or more correlated predicates. References to the parent query are only supported in the WHERE clause of the subquery.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-subqueries_27362044/","tags":null,"title":"Apache Hive : LanguageManual SubQueries"},{"categories":null,"contents":"Apache Hive : LanguageManual Transform  Transform/Map-Reduce Syntax  SQL Standard Based Authorization Disallows TRANSFORM TRANSFORM Examples   Schema-less Map-reduce Scripts Typing the output of TRANSFORM  Transform/Map-Reduce Syntax Users can also plug in their own custom mappers and reducers in the data stream by using features natively supported in the Hive language. e.g. in order to run a custom mapper script - map_script - and a custom reducer script - reduce_script - the user can issue the following command which uses the TRANSFORM clause to embed the mapper and the reducer scripts.\nBy default, columns will be transformed to STRING and delimited by TAB before feeding to the user script; similarly, all NULL values will be converted to the literal string \\N in order to differentiate NULL values from empty strings. The standard output of the user script will be treated as TAB-separated STRING columns, any cell containing only \\N will be re-interpreted as a NULL, and then the resulting STRING column will be cast to the data type specified in the table declaration in the usual way. User scripts can output debug information to standard error which will be shown on the task detail page on hadoop. These defaults can be overridden with ROW FORMAT \u0026hellip;.\nIn windows, use \u0026ldquo;cmd /c your_script\u0026rdquo; instead of just \u0026ldquo;your_script\u0026rdquo;\nWarning\nIt is your responsibility to sanitize any STRING columns prior to transformation. If your STRING column contains tabs, an identity transformer will not give you back what you started with! To help with this, see REGEXP_REPLACE and replace the tabs with some other character on their way into the TRANSFORM() call.\nWarning\nFormally, MAP \u0026hellip; and REDUCE \u0026hellip; are syntactic transformations of SELECT TRANSFORM ( \u0026hellip; ). In other words, they serve as comments or notes to the reader of the query. BEWARE: Use of these keywords may be dangerous as (e.g.) typing \u0026ldquo;REDUCE\u0026rdquo; does not force a reduce phase to occur and typing \u0026ldquo;MAP\u0026rdquo; does not force a new map phase!\nPlease also see Sort By / Cluster By / Distribute By and Larry Ogrodnek\u0026rsquo;s blog post.\nclusterBy: CLUSTER BY colName (',' colName)* distributeBy: DISTRIBUTE BY colName (',' colName)* sortBy: SORT BY colName (ASC | DESC)? (',' colName (ASC | DESC)?)* rowFormat : ROW FORMAT (DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [ESCAPED BY char] [LINES SEPARATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES property_name=property_value, property_name=property_value, ...]) outRowFormat : rowFormat inRowFormat : rowFormat outRecordReader : RECORDREADER className query: FROM ( FROM src MAP expression (',' expression)* (inRowFormat)? USING 'my_map_script' ( AS colName (',' colName)* )? (outRowFormat)? (outRecordReader)? ( clusterBy? | distributeBy? sortBy? ) src_alias ) REDUCE expression (',' expression)* (inRowFormat)? USING 'my_reduce_script' ( AS colName (',' colName)* )? (outRowFormat)? (outRecordReader)? FROM ( FROM src SELECT TRANSFORM '(' expression (',' expression)* ')' (inRowFormat)? USING 'my_map_script' ( AS colName (',' colName)* )? (outRowFormat)? (outRecordReader)? ( clusterBy? | distributeBy? sortBy? ) src_alias ) SELECT TRANSFORM '(' expression (',' expression)* ')' (inRowFormat)? USING 'my_reduce_script' ( AS colName (',' colName)* )? (outRowFormat)? (outRecordReader)? SQL Standard Based Authorization Disallows TRANSFORM The TRANSFORM clause is disallowed when SQL standard based authorization is configured in Hive 0.13.0 and later releases (HIVE-6415).\nTRANSFORM Examples Example #1:\n FROM ( FROM pv_users MAP pv_users.userid, pv_users.date USING 'map_script' AS dt, uid CLUSTER BY dt) map_output INSERT OVERWRITE TABLE pv_users_reduced REDUCE map_output.dt, map_output.uid USING 'reduce_script' AS date, count; FROM ( FROM pv_users SELECT TRANSFORM(pv_users.userid, pv_users.date) USING 'map_script' AS dt, uid CLUSTER BY dt) map_output INSERT OVERWRITE TABLE pv_users_reduced SELECT TRANSFORM(map_output.dt, map_output.uid) USING 'reduce_script' AS date, count; Example #2\n FROM ( FROM src SELECT TRANSFORM(src.key, src.value) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe' USING '/bin/cat' AS (tkey, tvalue) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe' RECORDREADER 'org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordReader' ) tmap INSERT OVERWRITE TABLE dest1 SELECT tkey, tvalue Schema-less Map-reduce Scripts If there is no AS clause after USING my_script, Hive assumes that the output of the script contains 2 parts: key which is before the first tab, and value which is the rest after the first tab. Note that this is different from specifying AS key, value because in that case, value will only contain the portion between the first tab and the second tab if there are multiple tabs.\nNote that we can directly do CLUSTER BY key without specifying the output schema of the scripts.\n FROM ( FROM pv_users MAP pv_users.userid, pv_users.date USING 'map_script' CLUSTER BY key) map_output INSERT OVERWRITE TABLE pv_users_reduced REDUCE map_output.key, map_output.value USING 'reduce_script' AS date, count; Typing the output of TRANSFORM The output fields from a script are typed as strings by default; for example in\n SELECT TRANSFORM(stuff) USING 'script' AS thing1, thing2 They can be immediately casted with the syntax:\n SELECT TRANSFORM(stuff) USING 'script' AS (thing1 INT, thing2 INT) ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-transform_27362047/","tags":null,"title":"Apache Hive : LanguageManual Transform"},{"categories":null,"contents":"Apache Hive : LanguageManual Types Hive Data Types  Hive Data Types  Overview  Numeric Types Date/Time Types String Types Misc Types Complex Types   Column Types  Integral Types (TINYINT, SMALLINT, INT/INTEGER, BIGINT)  Strings Varchar Char Timestamps  Casting Dates   Intervals Decimals  Decimal Literals Decimal Type Incompatibilities between Hive 0.12.0 and 0.13.0  Upgrading Pre-Hive 0.13.0 Decimal Columns     Union Types   Literals  Floating Point Types  Decimal Types  Using Decimal Types Mathematical UDFs Casting Decimal Values Testing Decimal Types       Handling of NULL Values Change Types Allowed Implicit Conversions    Overview This lists all supported data types in Hive. See Type System in the Tutorial for additional information.\nFor data types supported by HCatalog, see:\n HCatLoader Data Types HCatStorer Data Types HCatRecord Data Types  Numeric Types  TINYINT (1-byte signed integer, from -128 to 127) SMALLINT (2-byte signed integer, from -32,768 to 32,767)    INT/INTEGER (4-byte signed integer, from -2,147,483,648 to 2,147,483,647)\n* [`BIGINT`](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=82706456#LanguageManualTypes-bigint) (8-byte signed integer, from `-9,223,372,036,854,775,808` to `9,223,372,036,854,775,807`) * `FLOAT` (4-byte single precision floating point number) * `DOUBLE` (8-byte double precision floating point number) * ``` DOUBLE PRECISION (alias for DOUBLE, only available starting with Hive [2.2.0](https://issues.apache.org/jira/browse/HIVE-13556))  DECIMAL  Introduced in Hive 0.11.0 with a precision of 38 digits Hive 0.13.0 introduced user-definable precision and scale   NUMERIC (same as DECIMAL, starting with Hive 3.0.0)  Date/Time Types  TIMESTAMP (Note: Only available starting with Hive 0.8.0) DATE (Note: Only available starting with Hive 0.12.0) INTERVAL (Note: Only available starting with Hive 1.2.0)  String Types  STRING VARCHAR (Note: Only available starting with Hive 0.12.0) CHAR (Note: Only available starting with Hive 0.13.0)  Misc Types  BOOLEAN BINARY (Note: Only available starting with Hive 0.8.0)  Complex Types  arrays: ARRAY\u0026lt;data_type\u0026gt; (Note: negative values and non-constant expressions are allowed as of Hive 0.14.) maps: MAP\u0026lt;primitive_type, data_type\u0026gt; (Note: negative values and non-constant expressions are allowed as of Hive 0.14.) structs: STRUCT\u0026lt;col_name : data_type [COMMENT col_comment], ...\u0026gt; union: UNIONTYPE\u0026lt;data_type, data_type, ...\u0026gt; (Note: Only available starting with Hive 0.7.0.)  Column Types Integral Types (TINYINT, SMALLINT, INT/INTEGER, BIGINT)  Integral literals are assumed to be INT by default, unless the number exceeds the range of INT in which case it is interpreted as a BIGINT, or if one of the following postfixes is present on the number.\n   Type Postfix Example     TINYINT Y 100Y   SMALLINT S 100S   BIGINT L 100L     Version\nINTEGER is introduced as a synonym for INT in Hive 2.2.0 (HIVE-14950).\nStrings String literals can be expressed with either single quotes (') or double quotes (\u0026quot;). Hive uses C-style escaping within the strings.\nVarchar Varchar types are created with a length specifier (between 1 and 65535), which defines the maximum number of characters allowed in the character string. If a string value being converted/assigned to a varchar value exceeds the length specifier, the string is silently truncated. Character length is determined by the number of code points contained by the character string.\nLike string, trailing whitespace is significant in varchar and will affect comparison results.\nLimitations\nNon-generic UDFs cannot directly use varchar type as input arguments or return values. String UDFs can be created instead, and the varchar values will be converted to strings and passed to the UDF. To use varchar arguments directly or to return varchar values, create a GenericUDF.\nThere may be other contexts which do not support varchar, if they rely on reflection-based methods for retrieving type information. This includes some SerDe implementations.\nVersion\nVarchar datatype was introduced in Hive 0.12.0 (HIVE-4844).\nChar Char types are similar to Varchar but they are fixed-length meaning that values shorter than the specified length value are padded with spaces but trailing spaces are not important during comparisons. The maximum length is fixed at 255.\nCREATE TABLE foo (bar CHAR(10)) Version\nChar datatype was introduced in Hive 0.13.0 (HIVE-5191).\nTimestamps Supports traditional UNIX timestamp with optional nanosecond precision.\nSupported conversions:\n Integer numeric types: Interpreted as UNIX timestamp in seconds Floating point numeric types: Interpreted as UNIX timestamp in seconds with decimal precision Strings: JDBC compliant java.sql.Timestamp format \u0026ldquo;YYYY-MM-DD HH:MM:SS.fffffffff\u0026rdquo; (9 decimal place precision)  Timestamps are interpreted to be timezoneless and stored as an offset from the UNIX epoch. Convenience UDFs for conversion to and from timezones are provided (to_utc_timestamp, from_utc_timestamp).\nAll existing datetime UDFs (month, day, year, hour, etc.) work with the TIMESTAMP data type.\nTimestamps in text files have to use the format yyyy-mm-dd hh:mm:ss[.f...]. If they are in another format, declare them as the appropriate type (INT, FLOAT, STRING, etc.) and use a UDF to convert them to timestamps.\nTimestamps in Parquet files may be stored as int64 (as opposed to int96) by setting hive.parquet.write.int64.timestamp=true and hive.parquet.timestamp.time.unit to a default storage time unit. (\u0026quot;nanos\u0026quot;, \u0026quot;micros\u0026quot;, \u0026quot;millis\u0026quot;; default: \u0026quot;micros\u0026quot;). Note that because only 64 bits are stored, int64 timestamps stored as \u0026quot;nanos\u0026quot; will be stored as NULL if outside the range of 1677-09-21T00:12:43.15 and 2262-04-11T23:47:16.8.\nOn the table level, alternative timestamp formats can be supported by providing the format to the SerDe property \u0026ldquo;timestamp.formats\u0026rdquo; (as of release 1.2.0 with HIVE-9298). For example, yyyy-MM-dd'T'HH:mm:ss.SSS,yyyy-MM-dd'T'HH:mm:ss.\nVersion\nTimestamps were introduced in Hive 0.8.0 (HIVE-2272).\nDates\nDATE values describe a particular year/month/day, in the form YYYY-­MM-­DD. For example, DATE \u0026lsquo;2013-­01-­01\u0026rsquo;. Date types do not have a time of day component. The range of values supported for the Date type is 0000-­01-­01 to 9999-­12-­31, dependent on support by the primitive Java Date type.\nVersion\nDates were introduced in Hive 0.12.0 (HIVE-4055).\nCasting Dates Date types can only be converted to/from Date, Timestamp, or String types. Casting with user-specified formats is documented here.\n   Valid casts to/from Date type Result     cast(date as date) Same date value   cast(timestamp as date) The year/month/day of the timestamp is determined, based on the local timezone, and returned as a date value.   cast(string as date) If the string is in the form \u0026lsquo;YYYY-MM-DD\u0026rsquo;, then a date value corresponding to that year/month/day is returned. If the string value does not match this formate, then NULL is returned.   cast(date as timestamp) A timestamp value is generated corresponding to midnight of the year/month/day of the date value, based on the local timezone.   cast(date as string) The year/month/day represented by the Date is formatted as a string in the form \u0026lsquo;YYYY-MM-DD\u0026rsquo;.    Intervals    Supported Interval Description Example Meaning Since     Intervals of time units:SECOND / MINUTE / DAY / MONTH / YEAR INTERVAL \u0026lsquo;1\u0026rsquo; DAY an interval of 1 day(s) Hive 1.2.0 (HIVE-9792).   Year to month intervals, format: SY-MS: optional sign (+/-)Y: year countM: month count INTERVAL \u0026lsquo;1-2\u0026rsquo; YEAR TO MONTH shorthand for:INTERVAL \u0026lsquo;1\u0026rsquo; YEAR +INTERVAL \u0026lsquo;2\u0026rsquo; MONTH Hive 1.2.0 (HIVE-9792).   Day to second intervals, format: SD H:M:S.nnnnnnS: optional sign (+/-)D: day countH: hours M: minutesS: secondsnnnnnn: optional nanotime INTERVAL \u0026lsquo;1 2:3:4.000005\u0026rsquo; DAY shorthand for:INTERVAL \u0026lsquo;1\u0026rsquo; DAY+INTERVAL \u0026lsquo;2\u0026rsquo; HOUR +INTERVAL \u0026lsquo;3\u0026rsquo; MINUTE +INTERVAL \u0026lsquo;4\u0026rsquo; SECOND +INTERVAL \u0026lsquo;5\u0026rsquo; NANO Hive 1.2.0 (HIVE-9792).   Support for intervals with constant numbers INTERVAL 1 DAY aids query readability / portability  Hive 2.2.0 (HIVE-13557).   Support for intervals with expressions:this may involve other functions/columns.The expression must return with a number (which is not floating-point) or with a string. INTERVAL (1+dt) DAY enables dynamic intervals Hive 2.2.0 (HIVE-13557).   Optional usage of interval keywordthe usage of the INTERVAL keyword is mandatoryfor intervals with expressions (ex: INTERVAL (1+dt) SECOND) 1 DAY'1-2' YEAR TO MONTH INTERVAL 1 DAYINTERVAL \u0026lsquo;1-2\u0026rsquo; YEARS TO MONTH Hive 2.2.0 (HIVE-13557).   Add timeunit aliases to aid portability / readability: SECONDS / MINUTES / HOURS / DAYS / WEEKS / MONTHS / YEARS 2 SECONDS 2 SECOND Hive 2.2.0 (HIVE-13557).    Decimals Version\nDecimal datatype was introduced in Hive 0.11.0 (HIVE-2693) and revised in Hive 0.13.0 (HIVE-3976).\nNUMERIC is the same as DECIMAL as of Hive 3.0.0 (HIVE-16764).\nThe DECIMAL type in Hive is based on Java\u0026rsquo;s BigDecimal which is used for representing immutable arbitrary precision decimal numbers in Java. All regular number operations (e.g. +, -, *, /) and relevant UDFs (e.g. Floor, Ceil, Round, and many more) handle decimal types. You can cast to/from decimal types like you would do with other numeric types. The persistence format of the decimal type supports both scientific and non-scientific notation. Therefore, regardless of whether your dataset contains data like 4.004E+3 (scientific notation) or 4004 (non-scientific notation) or a combination of both, DECIMAL can be used for it.\n Hive 0.11 and 0.12 have the precision of the DECIMAL type fixed and limited to 38 digits. As of Hive 0.13 users can specify scale and precision when creating tables with the DECIMAL datatype using a DECIMAL(precision, scale) syntax. If scale is not specified, it defaults to 0 (no fractional digits). If no precision is specified, it defaults to 10.  CREATE TABLE foo ( a DECIMAL, -- Defaults to decimal(10,0) b DECIMAL(9, 7) ) For usage, see LanguageManual Types#Floating Point Types in the Literals section below.\nDecimal Literals Integral literals larger than BIGINT must be handled with Decimal(38,0). The Postfix BD is required. Example:\nselect CAST(18446744073709001000BD AS DECIMAL(38,0)) from my_table limit 1; Decimal Type Incompatibilities between Hive 0.12.0 and 0.13.0 With the changes in the Decimal data type in Hive 0.13.0, the pre-Hive 0.13.0 columns (of type \u0026ldquo;decimal\u0026rdquo;) will be treated as being of type decimal(10,0). What this means is that existing data being read from these tables will be treated as 10-digit integer values, and data being written to these tables will be converted to 10-digit integer values before being written. To avoid these issues, Hive users on 0.12 or earlier with tables containing Decimal columns will be required to migrate their tables, after upgrading to Hive 0.13.0 or later.\nUpgrading Pre-Hive 0.13.0 Decimal Columns If the user was on Hive 0.12.0 or earlier and created tables with decimal columns, they should perform the following steps on these tables after upgrading to Hive 0.13.0 or later.\n Determine what precision/scale you would like to set for the decimal column in the table. For each decimal column in the table, update the column definition to the desired precision/scale using the ALTER TABLE command:  ALTER TABLE foo CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); If the table is not a partitioned table, then you are done. If the table has partitions, then go on to step 3. 3. If the table is a partitioned table, then find the list of partitions for the table:\nSHOW PARTITIONS foo; ds=2008-04-08/hr=11 ds=2008-04-08/hr=12 ... Each existing partition in the table must also have its DECIMAL column changed to add the desired precision/scale.  This can be done with a single ALTER TABLE CHANGE COLUMN by using dynamic partitioning (available for ALTER TABLE CHANGE COLUMN in Hive 0.14 or later, with HIVE-8411):\nSET hive.exec.dynamic.partition = true; -- hive.exec.dynamic.partition needs to be set to true to enable dynamic partitioning with ALTER PARTITION -- This will alter all existing partitions of the table - be sure you know what you are doing! ALTER TABLE foo PARTITION (ds, hr) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); Alternatively, this can be done one partition at a time using ALTER TABLE CHANGE COLUMN, by specifying one partition per statement (This is available in Hive 0.14 or later, with HIVE-7971.):\nALTER TABLE foo PARTITION (ds='2008-04-08', hr=11) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); ALTER TABLE foo PARTITION (ds='2008-04-08', hr=12) CHANGE COLUMN dec_column_name dec_column_name DECIMAL(38,18); ... The Decimal datatype is discussed further in Floating Point Types below.\nUnion Types UNIONTYPE support is incomplete\nThe UNIONTYPE datatype was introduced in Hive 0.7.0 (HIVE-537), but full support for this type in Hive remains incomplete. Queries that reference UNIONTYPE fields in JOIN (HIVE-2508), WHERE, and GROUP BY clauses will fail, and Hive does not define syntax to extract the tag or value fields of a UNIONTYPE. This means that UNIONTYPEs are effectively pass-through-only.\nUnion types can at any one point hold exactly one of their specified data types. You can create an instance of this type using the create_union UDF:\nCREATE TABLE union_test(foo UNIONTYPE\u0026lt;int, double, array\u0026lt;string\u0026gt;, struct\u0026lt;a:int,b:string\u0026gt;\u0026gt;); SELECT foo FROM union_test; {0:1} {1:2.0} {2:[\u0026quot;three\u0026quot;,\u0026quot;four\u0026quot;]} {3:{\u0026quot;a\u0026quot;:5,\u0026quot;b\u0026quot;:\u0026quot;five\u0026quot;}} {2:[\u0026quot;six\u0026quot;,\u0026quot;seven\u0026quot;]} {3:{\u0026quot;a\u0026quot;:8,\u0026quot;b\u0026quot;:\u0026quot;eight\u0026quot;}} {0:9} {1:10.0} The first part in the deserialized union is the tag which lets us know which part of the union is being used. In this example 0 means the first data_type from the definition which is an int and so on.\nTo create a union you have to provide this tag to the create_union UDF:\nSELECT create_union(0, key), create_union(if(key\u0026lt;100, 0, 1), 2.0, value), create_union(1, \u0026quot;a\u0026quot;, struct(2, \u0026quot;b\u0026quot;)) FROM src LIMIT 2; {0:\u0026quot;238\u0026quot;}\t{1:\u0026quot;val_238\u0026quot;}\t{1:{\u0026quot;col1\u0026quot;:2,\u0026quot;col2\u0026quot;:\u0026quot;b\u0026quot;}} {0:\u0026quot;86\u0026quot;}\t{0:2.0}\t{1:{\u0026quot;col1\u0026quot;:2,\u0026quot;col2\u0026quot;:\u0026quot;b\u0026quot;}} Literals Floating Point Types Floating point literals are assumed to be DOUBLE. Scientific notation is not yet supported.\nDecimal Types Version\nDecimal datatype was introduced in Hive 0.11.0 (HIVE-2693). See Decimal Datatype above.\nNUMERIC is the same as DECIMAL as of Hive 3.0.0 (HIVE-16764).\nDecimal literals provide precise values and greater range for floating point numbers than the DOUBLE type. Decimal data types store exact representations of numeric values, while DOUBLE data types store very close approximations of numeric values.\nDecimal types are needed for use cases in which the (very close) approximation of a DOUBLE is insufficient, such as financial applications, equality and inequality checks, and rounding operations. They are also needed for use cases that deal with numbers outside the DOUBLE range (approximately -10308 to 10308) or very close to zero (-10-308 to 10-308). For a general discussion of the limits of the DOUBLE type, see the Wikipedia article Double-precision floating-point format.\nThe precision of a Decimal type is limited to 38 digits in Hive. See HIVE-4271 and HIVE-4320 for comments about the reasons for choosing this limit.\nUsing Decimal Types You can create a table in Hive that uses the Decimal type with the following syntax:\ncreate table decimal_1 (t decimal); The table decimal_1 is a table having one field of type decimal which is basically a Decimal value.\nYou can read and write values in such a table using either the LazySimpleSerDe or the LazyBinarySerDe. For example:\nalter table decimal_1 set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'; or:\nalter table decimal_1 set serde 'org.apache.hadoop.hive.serde2.lazy.LazyBinarySerDe'; You can use a cast to convert a Decimal value to any other primitive type such as a BOOLEAN. For example:\nselect cast(t as boolean) from decimal_2; Mathematical UDFs Decimal also supports many arithmetic operators, mathematical UDFs and UDAFs with the same syntax as used in the case of DOUBLE.\nBasic mathematical operations that can use decimal types include:\n Positive Negative Addition Subtraction Multiplication Division Average (avg) Sum Count Modulus (pmod) Sign – Hive 0.13.0 and later Exp – Hive 0.13.0 and later Ln – Hive 0.13.0 and later Log2 – Hive 0.13.0 and later Log10 – Hive 0.13.0 and later Log(base) – Hive 0.13.0 and later Sqrt – Hive 0.13.0 and later Sin – Hive 0.13.0 and later Asin – Hive 0.13.0 and later Cos – Hive 0.13.0 and later Acos – Hive 0.13.0 and later Tan – Hive 0.13.0 and later Atan – Hive 0.13.0 and later Radians – Hive 0.13.0 and later Degrees – Hive 0.13.0 and later  These rounding functions can also take decimal types:\n Floor Ceiling Round  Power(decimal, n) only supports positive integer values for the exponent n.\nCasting Decimal Values Casting is supported between decimal values and any other primitive type such as integer, double, boolean, and so on.\nTesting Decimal Types Two new tests have been added as part of the TestCliDriver framework within Hive. They are decimal_1.q and decimal_2.q. Other tests such as udf7.q cover the gamut of UDFs mentioned above.\nMore tests need to be added that demonstrate failure or when certain types of casts are prevented (for example, casting to date). There is some ambiguity in the round function because the rounding of Decimal does not work exactly as the SQL standard, and therefore it has been omitted in the current work.\nFor general information about running Hive tests, see How to Contribute to Apache Hive and Hive Developer FAQ.\nHandling of NULL Values Missing values are represented by the special value NULL. To import data with NULL fields, check documentation of the SerDe used by the table. (The default Text Format uses LazySimpleSerDe which interprets the string \\N as NULL when importing.)\nChange Types When hive.metastore.disallow.incompatible.col.type.changes is set to false, the types of columns in Metastore can be changed from any type to any other type. After such a type change, if the data can be shown correctly with the new type, the data will be displayed. Otherwise, the data will be displayed as NULL.\nAllowed Implicit Conversions     void boolean tinyint smallint int bigint float double decimal string varchar timestamp date binary     void to true true true true true true true true true true true true true true   boolean to false true false false false false false false false false false false false false   tinyint to false false true true true true true true true true true false false false   smallint to false false false true true true true true true true true false false false   int to false false false false true true true true true true true false false false   bigint to false false false false false true true true true true true false false false   float to false false false false false false true true true true true false false false   double to false false false false false false false true true true true false false false   decimal to false false false false false false false false true true true false false false   string to false false false false false false false true true true true false false false   varchar to false false false false false false false true true true true false false false   timestamp to false false false false false false false false false true true true false false   date to false false false false false false false false false true true false true false   binary to false false false false false false false false false false false false false true    Save\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-types_27838462/","tags":null,"title":"Apache Hive : LanguageManual Types"},{"categories":null,"contents":"Apache Hive : LanguageManual UDF Hive Operators and User-Defined Functions (UDFs)  Hive Operators and User-Defined Functions (UDFs)  Built-in Operators  Operators Precedences Relational Operators Arithmetic Operators Logical Operators String Operators Complex Type Constructors Operators on Complex Types   Built-in Functions  Mathematical Functions  Mathematical Functions and Operators for Decimal Datatypes   Collection Functions Type Conversion Functions Date Functions Conditional Functions String Functions Data Masking Functions Misc. Functions  xpath get_json_object     Built-in Aggregate Functions (UDAF) Built-in Table-Generating Functions (UDTF)  Usage Examples  explode (array) explode (map) posexplode (array) inline (array of structs) stack (values)   explode posexplode json_tuple parse_url_tuple   GROUPing and SORTing on f(column) Utility Functions UDF internals Creating Custom UDFs    Case-insensitive\nAll Hive keywords are case-insensitive, including the names of Hive operators and functions.\nIn Beeline or the CLI, use the commands below to show the latest documentation:\nSHOW FUNCTIONS; DESCRIBE FUNCTION \u0026lt;function_name\u0026gt;; DESCRIBE FUNCTION EXTENDED \u0026lt;function_name\u0026gt;; Bug for expression caching when UDF nested in UDF or function\nWhen hive.cache.expr.evaluation is set to true (which is the default) a UDF can give incorrect results if it is nested in another UDF or a Hive function. This bug affects releases 0.12.0, 0.13.0, and 0.13.1. Release 0.14.0 fixed the bug (HIVE-7314).\nThe problem relates to the UDF\u0026rsquo;s implementation of the getDisplayString method, as discussed in the Hive user mailing list.\nBuilt-in Operators Operators Precedences    Example Operators Description     A[B] , A.identifier bracket_op([]), dot(.) element selector, dot   -A unary(+), unary(-), unary(~) unary prefix operators   A IS [NOT] (NULL TRUE FALSE)   A ^ B bitwise xor(^) bitwise xor   A * B star(*), divide(/), mod(%), div(DIV) multiplicative operators   A + B plus(+), minus(-) additive operators   A  B   A \u0026amp; B bitwise and(\u0026amp;) bitwise and   A B bitwise or(    Relational Operators The following operators compare the passed operands and generate a TRUE or FALSE value depending on whether the comparison between the operands holds.\n   Operator Operand types Description     A = B All primitive types TRUE if expression A is equal to expression B otherwise FALSE.   A == B All primitive types Synonym for the = operator.   A \u0026lt;=\u0026gt; B All primitive types Returns same result with EQUAL(=) operator for non-null operands, but returns TRUE if both are NULL, FALSE if one of the them is NULL. (As of version 0.9.0.)   A \u0026lt;\u0026gt; B All primitive types NULL if A or B is NULL, TRUE if expression A is NOT equal to expression B, otherwise FALSE.   A != B All primitive types Synonym for the \u0026lt;\u0026gt; operator.   A \u0026lt; B All primitive types NULL if A or B is NULL, TRUE if expression A is less than expression B, otherwise FALSE.   A \u0026lt;= B All primitive types NULL if A or B is NULL, TRUE if expression A is less than or equal to expression B, otherwise FALSE.   A \u0026gt; B All primitive types NULL if A or B is NULL, TRUE if expression A is greater than expression B, otherwise FALSE.   A \u0026gt;= B All primitive types NULL if A or B is NULL, TRUE if expression A is greater than or equal to expression B, otherwise FALSE.   A [NOT] BETWEEN B AND C All primitive types NULL if A, B or C is NULL, TRUE if A is greater than or equal to B AND A less than or equal to C, otherwise FALSE. This can be inverted by using the NOT keyword. (As of version 0.9.0.)   A IS NULL All types TRUE if expression A evaluates to NULL, otherwise FALSE.   A IS NOT NULL All types FALSE if expression A evaluates to NULL, otherwise TRUE.   A IS [NOT] (TRUE FALSE) Boolean types   A [NOT] LIKE B strings NULL if A or B is NULL, TRUE if string A matches the SQL simple regular expression B, otherwise FALSE. The comparison is done character by character. The _ character in B matches any character in A (similar to . in posix regular expressions) while the % character in B matches an arbitrary number of characters in A (similar to .* in posix regular expressions). For example, \u0026lsquo;foobar\u0026rsquo; like \u0026lsquo;foo\u0026rsquo; evaluates to FALSE whereas \u0026lsquo;foobar\u0026rsquo; like \u0026lsquo;foo_ _ _\u0026rsquo; evaluates to TRUE and so does \u0026lsquo;foobar\u0026rsquo; like \u0026lsquo;foo%\u0026rsquo;.   A RLIKE B strings NULL if A or B is NULL, TRUE if any (possibly empty) substring of A matches the Java regular expression B, otherwise FALSE. For example, \u0026lsquo;foobar\u0026rsquo; RLIKE \u0026lsquo;foo\u0026rsquo; evaluates to TRUE and so does \u0026lsquo;foobar\u0026rsquo; RLIKE \u0026lsquo;^f.*r$\u0026rsquo;.   A REGEXP B strings Same as RLIKE.    Arithmetic Operators The following operators support various common arithmetic operations on the operands. All return number types; if any of the operands are NULL, then the result is also NULL.\n   Operator Operand types Description     A + B All number types Gives the result of adding A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands. For example since every integer is a float, therefore float is a containing type of integer so the + operator on a float and an int will result in a float.   A - B All number types Gives the result of subtracting B from A. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A * B All number types Gives the result of multiplying A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands. Note that if the multiplication causing overflow, you will have to cast one of the operators to a type higher in the type hierarchy.   A / B All number types Gives the result of dividing A by B. The result is a double type in most cases. When A and B are both integers, the result is a double type except when the hive.compat configuration parameter is set to \u0026ldquo;0.13\u0026rdquo; or \u0026ldquo;latest\u0026rdquo; in which case the result is a decimal type.   A DIV B Integer types Gives the integer part resulting from dividing A by B. E.g 17 div 3 results in 5.   A % B All number types Gives the reminder resulting from dividing A by B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A \u0026amp; B All number types Gives the result of bitwise AND of A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A B All number types   A ^ B All number types Gives the result of bitwise XOR of A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   ~A All number types Gives the result of bitwise NOT of A. The type of the result is the same as the type of A.    Logical Operators The following operators provide support for creating logical expressions. All of them return boolean TRUE, FALSE, or NULL depending upon the boolean values of the operands. NULL behaves as an \u0026ldquo;unknown\u0026rdquo; flag, so if the result depends on the state of an unknown, the result itself is unknown.\n   Operator Operand types Description     A AND B boolean TRUE if both A and B are TRUE, otherwise FALSE. NULL if A or B is NULL.   A OR B boolean TRUE if either A or B or both are TRUE, FALSE OR NULL is NULL, otherwise FALSE.   NOT A boolean TRUE if A is FALSE or NULL if A is NULL. Otherwise FALSE.   ! A boolean Same as NOT A.   A IN (val1, val2, \u0026hellip;) boolean TRUE if A is equal to any of the values. As of Hive 0.13 subqueries are supported in IN statements.   A NOT IN (val1, val2, \u0026hellip;) boolean TRUE if A is not equal to any of the values. As of Hive 0.13 subqueries are supported in NOT IN statements.   [NOT] EXISTS (subquery)  TRUE if the the subquery returns at least one row. Supported as of Hive 0.13.    String Operators    Operator Operand types Description     A  B    Complex Type Constructors The following functions construct instances of complex types.\n   Constructor Function Operands Description     map (key1, value1, key2, value2, \u0026hellip;) Creates a map with the given key/value pairs.   struct (val1, val2, val3, \u0026hellip;) Creates a struct with the given field values. Struct field names will be col1, col2, \u0026hellip;.   named_struct (name1, val1, name2, val2, \u0026hellip;) Creates a struct with the given field names and values. (As of Hive 0.8.0.)   array (val1, val2, \u0026hellip;) Creates an array with the given elements.   create_union (tag, val1, val2, \u0026hellip;) Creates a union type with the value that is being pointed to by the tag parameter.    Operators on Complex Types The following operators provide mechanisms to access elements in Complex Types.\n   Operator Operand types Description     A[n] A is an Array and n is an int Returns the nth element in the array A. The first element has index 0. For example, if A is an array comprising of [\u0026lsquo;foo\u0026rsquo;, \u0026lsquo;bar\u0026rsquo;] then A[0] returns \u0026lsquo;foo\u0026rsquo; and A[1] returns \u0026lsquo;bar\u0026rsquo;.   M[key] M is a Map\u0026lt;K, V\u0026gt; and key has type K Returns the value corresponding to the key in the map. For example, if M is a map comprising of {\u0026lsquo;f\u0026rsquo; -\u0026gt; \u0026lsquo;foo\u0026rsquo;, \u0026lsquo;b\u0026rsquo; -\u0026gt; \u0026lsquo;bar\u0026rsquo;, \u0026lsquo;all\u0026rsquo; -\u0026gt; \u0026lsquo;foobar\u0026rsquo;} then M[\u0026lsquo;all\u0026rsquo;] returns \u0026lsquo;foobar\u0026rsquo;.   S.x S is a struct Returns the x field of S. For example for the struct foobar {int foo, int bar}, foobar.foo returns the integer stored in the foo field of the struct.    Built-in Functions Mathematical Functions The following built-in mathematical functions are supported in Hive; most return NULL when the argument(s) are NULL:\n   Return Type Name (Signature) Description     DOUBLE round(DOUBLE a) Returns the rounded BIGINT value of a.   DOUBLE round(DOUBLE a, INT d) Returns a rounded to d decimal places.   DOUBLE bround(DOUBLE a) Returns the rounded BIGINT value of a using HALF_EVEN rounding mode (as of Hive 1.3.0, 2.0.0). Also known as Gaussian rounding or bankers' rounding. Example: bround(2.5) = 2, bround(3.5) = 4.   DOUBLE bround(DOUBLE a, INT d) Returns a rounded to d decimal places using HALF_EVEN rounding mode (as of Hive 1.3.0, 2.0.0). Example: bround(8.25, 1) = 8.2, bround(8.35, 1) = 8.4.   BIGINT floor(DOUBLE a) Returns the maximum BIGINT value that is equal to or less than a.   BIGINT ceil(DOUBLE a), ceiling(DOUBLE a) Returns the minimum BIGINT value that is equal to or greater than a.   DOUBLE rand(), rand(INT seed) Returns a random number (that changes from row to row) that is distributed uniformly from 0 to 1. Specifying the seed will make sure the generated random number sequence is deterministic.   DOUBLE exp(DOUBLE a), exp(DECIMAL a) Returns ea where e is the base of the natural logarithm. Decimal version added in Hive 0.13.0.   DOUBLE ln(DOUBLE a), ln(DECIMAL a) Returns the natural logarithm of the argument a. Decimal version added in Hive 0.13.0.   DOUBLE log10(DOUBLE a), log10(DECIMAL a) Returns the base-10 logarithm of the argument a. Decimal version added in Hive 0.13.0.   DOUBLE log2(DOUBLE a), log2(DECIMAL a) Returns the base-2 logarithm of the argument a. Decimal version added in Hive 0.13.0.   DOUBLE log(DOUBLE base, DOUBLE a)log(DECIMAL base, DECIMAL a) Returns the base-base logarithm of the argument a. Decimal versions added in Hive 0.13.0.   DOUBLE pow(DOUBLE a, DOUBLE p), power(DOUBLE a, DOUBLE p) Returns ap.   DOUBLE sqrt(DOUBLE a), sqrt(DECIMAL a) Returns the square root of a. Decimal version added in Hive 0.13.0.   STRING bin(BIGINT a) Returns the number in binary format (see http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_bin).   STRING hex(BIGINT a) hex(STRING a) hex(BINARY a) If the argument is an INT or binary, hex returns the number as a STRING in hexadecimal format. Otherwise if the number is a STRING, it converts each character into its hexadecimal representation and returns the resulting STRING. (See http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function_hex, BINARY version as of Hive 0.12.0.)   BINARY unhex(STRING a) Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of the number. (BINARY version as of Hive 0.12.0, used to return a string.)   STRING conv(BIGINT num, INT from_base, INT to_base), conv(STRING num, INT from_base, INT to_base) Converts a number from a given base to another (see http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function_conv).   DOUBLE abs(DOUBLE a) Returns the absolute value.   INT or DOUBLE pmod(INT a, INT b), pmod(DOUBLE a, DOUBLE b) Returns the positive value of a mod b.   DOUBLE sin(DOUBLE a), sin(DECIMAL a) Returns the sine of a (a is in radians). Decimal version added in Hive 0.13.0.   DOUBLE asin(DOUBLE a), asin(DECIMAL a) Returns the arc sin of a if -1\u0026lt;=a\u0026lt;=1 or NULL otherwise. Decimal version added in Hive 0.13.0.   DOUBLE cos(DOUBLE a), cos(DECIMAL a) Returns the cosine of a (a is in radians). Decimal version added in Hive 0.13.0.   DOUBLE acos(DOUBLE a), acos(DECIMAL a) Returns the arccosine of a if -1\u0026lt;=a\u0026lt;=1 or NULL otherwise. Decimal version added in Hive 0.13.0.   DOUBLE tan(DOUBLE a), tan(DECIMAL a) Returns the tangent of a (a is in radians). Decimal version added in Hive 0.13.0.   DOUBLE atan(DOUBLE a), atan(DECIMAL a) Returns the arctangent of a. Decimal version added in Hive 0.13.0.   DOUBLE degrees(DOUBLE a), degrees(DECIMAL a) Converts value of a from radians to degrees. Decimal version added in Hive 0.13.0.   DOUBLE radians(DOUBLE a), radians(DOUBLE a) Converts value of a from degrees to radians. Decimal version added in Hive 0.13.0.   INT or DOUBLE positive(INT a), positive(DOUBLE a) Returns a.   INT or DOUBLE negative(INT a), negative(DOUBLE a) Returns -a.   DOUBLE or INT sign(DOUBLE a), sign(DECIMAL a) Returns the sign of a as \u0026lsquo;1.0\u0026rsquo; (if a is positive) or \u0026lsquo;-1.0\u0026rsquo; (if a is negative), \u0026lsquo;0.0\u0026rsquo; otherwise. The decimal version returns INT instead of DOUBLE. Decimal version added in Hive 0.13.0.   DOUBLE e() Returns the value of e.   DOUBLE pi() Returns the value of pi.   BIGINT factorial(INT a) Returns the factorial of a (as of Hive 1.2.0). Valid a is [0..20].   DOUBLE cbrt(DOUBLE a) Returns the cube root of a double value (as of Hive 1.2.0).   INTBIGINT shiftleft(TINYINT SMALLINT   INTBIGINT shiftright(TINYINT SMALLINT   INTBIGINT shiftrightunsigned(TINYINT SMALLINT   T greatest(T v1, T v2, \u0026hellip;) Returns the greatest value of the list of values (as of Hive 1.1.0). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with \u0026ldquo;\u0026gt;\u0026rdquo; operator (as of Hive 2.0.0).   T least(T v1, T v2, \u0026hellip;) Returns the least value of the list of values (as of Hive 1.1.0). Fixed to return NULL when one or more arguments are NULL, and strict type restriction relaxed, consistent with \u0026ldquo;\u0026lt;\u0026rdquo; operator (as of Hive 2.0.0).   INT width_bucket(NUMERIC expr, NUMERIC min_value, NUMERIC max_value, INT num_buckets) Returns an integer between 0 and num_buckets+1 by mapping expr into the ith equally sized bucket. Buckets are made by dividing [min_value, max_value] into equally sized regions. If expr \u0026lt; min_value, return 1, if expr \u0026gt; max_value return num_buckets+1. See https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions214.htm (as of Hive 3.0.0)    Mathematical Functions and Operators for Decimal Datatypes Version\nThe decimal datatype was introduced in Hive 0.11.0 (HIVE-2693).\nAll regular arithmetic operators (such as +, -, *, /) and relevant mathematical UDFs (Floor, Ceil, Round, and many more) have been updated to handle decimal types. For a list of supported UDFs, see Mathematical UDFs in Hive Data Types.\nCollection Functions The following built-in collection functions are supported in Hive:\n   Return Type Name(Signature) Description     int size(Map\u0026lt;K.V\u0026gt;) Returns the number of elements in the map type.   int size(Array) Returns the number of elements in the array type.   array map_keys(Map\u0026lt;K.V\u0026gt;) Returns an unordered array containing the keys of the input map.   array map_values(Map\u0026lt;K.V\u0026gt;) Returns an unordered array containing the values of the input map.   boolean array_contains(Array, value) Returns TRUE if the array contains value.   array sort_array(Array) Sorts the input array in ascending order according to the natural ordering of the array elements and returns it (as of version 0.9.0).    Type Conversion Functions The following type conversion functions are supported in Hive:\n   Return Type Name(Signature) Description     binary binary(string binary)   Expected \u0026ldquo;=\u0026rdquo; to follow \u0026ldquo;type\u0026rdquo; cast(expr as ) Converts the results of the expression expr to . For example, cast(\u0026lsquo;1\u0026rsquo; as BIGINT) will convert the string \u0026lsquo;1\u0026rsquo; to its integral representation. A null is returned if the conversion does not succeed. If cast(expr as boolean) Hive returns true for a non-empty string.    Date Functions The following built-in date functions are supported in Hive:\n   Return Type Name(Signature) Description     string from_unixtime(bigint unixtime[, string pattern]) Converts a number of seconds since epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current time zone(using config \u0026ldquo;hive.local.time.zone\u0026rdquo;) using the specified pattern. If the pattern is missing the default is used (\u0026lsquo;uuuu-MM-dd HH:mm:ss\u0026rsquo; or yyyy-MM-dd HH:mm:ss'). Example: from_unixtime(0)=1970-01-01 00:00:00 (hive.local.time.zone=Etc/GMT)As of Hive 4.0.0 (HIVE-25576), the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation, and as a consequence the accepted patterns and their behavior. Prior versions always used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter.   bigint unix_timestamp() Gets current Unix timestamp in seconds. This function is not deterministic and its value is not fixed for the scope of a query execution, therefore prevents proper optimization of queries - this has been deprecated since 2.0 in favour of CURRENT_TIMESTAMP constant.   bigint unix_timestamp(string date) Converts a datetime string to unix time (seconds since epoch) using the default pattern(s). The default accepted patterns depend on the underlying formatter implementation. The datetime string does not contain a timezone so the conversion uses the local time zone as specified by \u0026ldquo;hive.local.time.zone\u0026rdquo; property. Returns null when the conversion fails. Example: unix_timestamp(\u0026lsquo;2009-03-20 11:30:01\u0026rsquo;) = 1237573801As of Hive 4.0.0 (HIVE-25576), the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation, and as a consequence the accepted patterns and their behavior. Prior versions always used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter.   bigint unix_timestamp(string date, string pattern) Converts a datetime string to unix time (seconds since epoch) using the specified pattern. The accepted patterns and their behavior depend on the underlying formatter implementation. Returns null when the conversion fails. Example: unix_timestamp(\u0026lsquo;2009-03-20\u0026rsquo;, \u0026lsquo;uuuu-MM-dd\u0026rsquo;) = 1237532400As of Hive 4.0.0 (HIVE-25576), the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation, and as a consequence the accepted patterns and their behavior. Prior versions always used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter.   pre 2.1.0: string2.1.0 on: date to_date(string timestamp) Returns the date part of a timestamp string (pre-Hive 2.1.0): to_date(\u0026ldquo;1970-01-01 00:00:00\u0026rdquo;) = \u0026ldquo;1970-01-01\u0026rdquo;. As of Hive 2.1.0, returns a date object.Prior to Hive 2.1.0 (HIVE-13248) the return type was a String because no Date type existed when the method was created.   int year(string date) Returns the year part of a date or a timestamp string: year(\u0026ldquo;1970-01-01 00:00:00\u0026rdquo;) = 1970, year(\u0026ldquo;1970-01-01\u0026rdquo;) = 1970.   int quarter(date/timestamp/string) Returns the quarter of the year for a date, timestamp, or string in the range 1 to 4 (as of Hive 1.3.0). Example: quarter(\u0026lsquo;2015-04-08\u0026rsquo;) = 2.   int month(string date) Returns the month part of a date or a timestamp string: month(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 11, month(\u0026ldquo;1970-11-01\u0026rdquo;) = 11.   int day(string date) dayofmonth(date) Returns the day part of a date or a timestamp string: day(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 1, day(\u0026ldquo;1970-11-01\u0026rdquo;) = 1.   int hour(string date) Returns the hour of the timestamp: hour(\u0026lsquo;2009-07-30 12:58:59\u0026rsquo;) = 12, hour(\u0026lsquo;12:58:59\u0026rsquo;) = 12.   int minute(string date) Returns the minute of the timestamp.   int second(string date) Returns the second of the timestamp.   int weekofyear(string date) Returns the week number of a timestamp string: weekofyear(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 44, weekofyear(\u0026ldquo;1970-11-01\u0026rdquo;) = 44.   int extract(field FROM source) Retrieve fields such as days or hours from source (as of Hive 2.2.0). Source must be a date, timestamp, interval or a string that can be converted into either a date or timestamp. Supported fields include: day, dayofweek, hour, minute, month, quarter, second, week and year.Examples:1. select extract(month from \u0026ldquo;2016-10-20\u0026rdquo;) results in 10.   2. select extract(hour from \u0026ldquo;2016-10-20 05:06:07\u0026rdquo;) results in 5.     3. select extract(dayofweek from \u0026ldquo;2016-10-20 05:06:07\u0026rdquo;) results in 5.     4. select extract(month from interval \u0026lsquo;1-3\u0026rsquo; year to month) results in 3.     5. select extract(minute from interval \u0026lsquo;3 12:20:30\u0026rsquo; day to second) results in 20.          int datediff(string enddate, string startdate) Returns the number of days from startdate to enddate: datediff(\u0026lsquo;2009-03-01\u0026rsquo;, \u0026lsquo;2009-02-27\u0026rsquo;) = 2.   pre 2.1.0: string2.1.0 on: date date_add(date/timestamp/string startdate, tinyint/smallint/int days) Adds a number of days to startdate: date_add(\u0026lsquo;2008-12-31\u0026rsquo;, 1) = \u0026lsquo;2009-01-01\u0026rsquo;.Prior to Hive 2.1.0 (HIVE-13248) the return type was a String because no Date type existed when the method was created.   pre 2.1.0: string2.1.0 on: date date_sub(date/timestamp/string startdate, tinyint/smallint/int days) Subtracts a number of days to startdate: date_sub(\u0026lsquo;2008-12-31\u0026rsquo;, 1) = \u0026lsquo;2008-12-30\u0026rsquo;.Prior to Hive 2.1.0 (HIVE-13248) the return type was a String because no Date type existed when the method was created.   timestamp from_utc_timestamp({any primitive type} ts, string timezone) Converts a timestamp* in UTC to a given timezone (as of Hive 0.8.0).* timestamp is a primitive type, including timestamp/date, tinyint/smallint/int/bigint, float/double and decimal.Fractional values are considered as seconds. Integer values are considered as milliseconds. For example, from_utc_timestamp(2592000.0,\u0026lsquo;PST\u0026rsquo;), from_utc_timestamp(2592000000,\u0026lsquo;PST\u0026rsquo;) and from_utc_timestamp(timestamp \u0026lsquo;1970-01-30 16:00:00\u0026rsquo;,\u0026lsquo;PST\u0026rsquo;) all return the timestamp 1970-01-30 08:00:00.   timestamp to_utc_timestamp({any primitive type} ts, string timezone) Converts a timestamp* in a given timezone to UTC (as of Hive 0.8.0).* timestamp is a primitive type, including timestamp/date, tinyint/smallint/int/bigint, float/double and decimal.Fractional values are considered as seconds. Integer values are considered as milliseconds. For example, to_utc_timestamp(2592000.0,\u0026lsquo;PST\u0026rsquo;), to_utc_timestamp(2592000000,\u0026lsquo;PST\u0026rsquo;) and to_utc_timestamp(timestamp \u0026lsquo;1970-01-30 16:00:00\u0026rsquo;,\u0026lsquo;PST\u0026rsquo;) all return the timestamp 1970-01-31 00:00:00.   date current_date Returns the current date at the start of query evaluation (as of Hive 1.2.0). All calls of current_date within the same query return the same value.   timestamp current_timestamp Returns the current timestamp at the start of query evaluation (as of Hive 1.2.0). All calls of current_timestamp within the same query return the same value.   string add_months(string start_date, int num_months, output_date_format) Returns the date that is num_months after start_date (as of Hive 1.1.0). start_date is a string, date or timestamp. num_months is an integer. If start_date is the last day of the month or if the resulting month has fewer days than the day component of start_date, then the result is the last day of the resulting month. Otherwise, the result has the same day component as start_date. The default output format is \u0026lsquo;yyyy-MM-dd\u0026rsquo;.Before Hive 4.0.0, the time part of the date is ignored.As of Hive 4.0.0, add_months supports an optional argument output_date_format, which accepts a String that represents a valid date format for the output. This allows to retain the time format in the output.For example :add_months(\u0026lsquo;2009-08-31\u0026rsquo;, 1) returns \u0026lsquo;2009-09-30\u0026rsquo;.add_months(\u0026lsquo;2017-12-31 14:15:16\u0026rsquo;, 2, \u0026lsquo;YYYY-MM-dd HH:mm:ss\u0026rsquo;) returns \u0026lsquo;2018-02-28 14:15:16\u0026rsquo;.   string last_day(string date) Returns the last day of the month which the date belongs to (as of Hive 1.1.0). date is a string in the format \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo; or \u0026lsquo;yyyy-MM-dd\u0026rsquo;. The time part of date is ignored.   string next_day(string start_date, string day_of_week) Returns the first date which is later than start_date and named as day_of_week (as of Hive 1.2.0). start_date is a string/date/timestamp. day_of_week is 2 letters, 3 letters or full name of the day of the week (e.g. Mo, tue, FRIDAY). The time part of start_date is ignored. Example: next_day(\u0026lsquo;2015-01-14\u0026rsquo;, \u0026lsquo;TU\u0026rsquo;) = 2015-01-20.   string trunc(string date, string format) Returns date truncated to the unit specified by the format (as of Hive 1.2.0). Supported formats: MONTH/MON/MM, YEAR/YYYY/YY. Example: trunc(\u0026lsquo;2015-03-17\u0026rsquo;, \u0026lsquo;MM\u0026rsquo;) = 2015-03-01.   double months_between(date1, date2) Returns number of months between dates date1 and date2 (as of Hive 1.2.0). If date1 is later than date2, then the result is positive. If date1 is earlier than date2, then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always an integer. Otherwise the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2. date1 and date2 type can be date, timestamp or string in the format \u0026lsquo;yyyy-MM-dd\u0026rsquo; or \u0026lsquo;yyyy-MM-dd HH:mm:ss\u0026rsquo;. The result is rounded to 8 decimal places. Example: months_between(\u0026lsquo;1997-02-28 10:30:00\u0026rsquo;, \u0026lsquo;1996-10-30\u0026rsquo;) = 3.94959677   string date_format(date/timestamp/string ts, string pattern) Converts a date/timestamp/string to a value of string using the specified pattern (as of Hive 1.2.0). The accepted patterns and their behavior depend on the underlying formatter implementation. The pattern argument should be constant. Example: date_format(\u0026lsquo;2015-04-08\u0026rsquo;, \u0026lsquo;y\u0026rsquo;) = \u0026lsquo;2015\u0026rsquo;.date_format can be used to implement other UDFs, e.g.:* dayname(date) is date_format(date, \u0026lsquo;EEEE\u0026rsquo;)     dayofyear(date) is date_format(date, \u0026lsquo;D\u0026rsquo;)  As of Hive 4.0.0 (\nHIVE-27673 Configurable datetime formatter for date_format Closed\n), the \u0026ldquo;hive.datetime.formatter\u0026rdquo; property can be used to control the underlying formatter implementation, and as a consequence the accepted patterns and their behavior. Prior versions always used https://docs.oracle.com/javase/8/docs/api/java/text/SimpleDateFormat.html as the underlying formatter. |\nConditional Functions    Return Type Name(Signature) Description     T if(boolean testCondition, T valueTrue, T valueFalseOrNull) Returns valueTrue when testCondition is true, returns valueFalseOrNull otherwise.   boolean isnull( a ) Returns true if a is NULL and false otherwise.   boolean isnotnull ( a ) Returns true if a is not NULL and false otherwise.   T nvl(T value, T default_value) Returns default value if value is null else returns value (as of HIve 0.11).   T COALESCE(T v1, T v2, \u0026hellip;) Returns the first v that is not NULL, or NULL if all v\u0026rsquo;s are NULL.   T CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END When a = b, returns c; when a = d, returns e; else returns f.   T CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END When a = true, returns b; when c = true, returns d; else returns e.   T nullif( a, b ) Returns NULL if a=b; otherwise returns a (as of Hive 2.3.0).Shorthand for: CASE WHEN a = b then NULL else a   void assert_true(boolean condition) Throw an exception if \u0026lsquo;condition\u0026rsquo; is not true, otherwise return null (as of Hive 0.8.0). For example, select assert_true (2\u0026lt;1).    String Functions The following built-in String functions are supported in Hive:\n   Return Type Name(Signature) Description     int ascii(string str) Returns the numeric value of the first character of str.   string base64(binary bin) Converts the argument from binary to a base 64 string (as of Hive 0.12.0).   int character_length(string str) Returns the number of UTF-8 characters contained in str (as of Hive 2.2.0). The function char_length is shorthand for this function.   string chr(bigint double A)   string concat(string binary A, string   array\u0026lt;struct\u0026lt;string,double\u0026raquo; context_ngrams(array\u0026lt;array\u0026gt;, array, int K, int pf) Returns the top-k contextual N-grams from a set of tokenized sentences, given a string of \u0026ldquo;context\u0026rdquo;. See StatisticsAndDataMining for more information.   string concat_ws(string SEP, string A, string B\u0026hellip;) Like concat() above, but with custom separator SEP.   string concat_ws(string SEP, array) Like concat_ws() above, but taking an array of strings. (as of Hive 0.9.0)   string decode(binary bin, string charset) Decodes the first argument into a String using the provided character set (one of \u0026lsquo;US-ASCII\u0026rsquo;, \u0026lsquo;ISO-8859-1\u0026rsquo;, \u0026lsquo;UTF-8\u0026rsquo;, \u0026lsquo;UTF-16BE\u0026rsquo;, \u0026lsquo;UTF-16LE\u0026rsquo;, \u0026lsquo;UTF-16\u0026rsquo;). If either argument is null, the result will also be null. (As of Hive 0.12.0.)   string elt(N int,str1 string,str2 string,str3 string,\u0026hellip;) Return string at index number. For example elt(2,\u0026lsquo;hello\u0026rsquo;,\u0026lsquo;world\u0026rsquo;) returns \u0026lsquo;world\u0026rsquo;. Returns NULL if N is less than 1 or greater than the number of arguments.(see https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_elt)   binary encode(string src, string charset) Encodes the first argument into a BINARY using the provided character set (one of \u0026lsquo;US-ASCII\u0026rsquo;, \u0026lsquo;ISO-8859-1\u0026rsquo;, \u0026lsquo;UTF-8\u0026rsquo;, \u0026lsquo;UTF-16BE\u0026rsquo;, \u0026lsquo;UTF-16LE\u0026rsquo;, \u0026lsquo;UTF-16\u0026rsquo;). If either argument is null, the result will also be null. (As of Hive 0.12.0.)   int field(val T,val1 T,val2 T,val3 T,\u0026hellip;) Returns the index of val in the val1,val2,val3,\u0026hellip; list or 0 if not found. For example field(\u0026lsquo;world\u0026rsquo;,\u0026lsquo;say\u0026rsquo;,\u0026lsquo;hello\u0026rsquo;,\u0026lsquo;world\u0026rsquo;) returns 3.All primitive types are supported, arguments are compared using str.equals(x). If val is NULL, the return value is 0.(see https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_field)   int find_in_set(string str, string strList) Returns the first occurance of str in strList where strList is a comma-delimited string. Returns null if either argument is null. Returns 0 if the first argument contains any commas. For example, find_in_set(\u0026lsquo;ab\u0026rsquo;, \u0026lsquo;abc,b,ab,c,def\u0026rsquo;) returns 3.   string format_number(number x, int d) Formats the number X to a format like \u0026lsquo;#,###,###.##\u0026rsquo;, rounded to D decimal places, and returns the result as a string. If D is 0, the result has no decimal point or fractional part. (As of Hive 0.10.0; bug with float types fixed in Hive 0.14.0, decimal type support added in Hive 0.14.0)   string get_json_object(string json_string, string path) Extracts json object from a json string based on json path specified, and returns json string of the extracted json object. It will return null if the input json string is invalid. NOTE: The json path can only have the characters [0-9a-z_], i.e., no upper-case or special characters. Also, the keys cannot start with numbers. This is due to restrictions on Hive column names.   boolean in_file(string str, string filename) Returns true if the string str appears as an entire line in filename.   int instr(string str, string substr) Returns the position of the first occurrence of substr in str. Returns null if either of the arguments are null and returns 0 if substr could not be found in str. Be aware that this is not zero based. The first character in str has index 1.   int length(string A) Returns the length of the string.   int locate(string substr, string str[, int pos]) Returns the position of the first occurrence of substr in str after position pos.   string lower(string A) lcase(string A) Returns the string resulting from converting all characters of B to lower case. For example, lower(\u0026lsquo;fOoBaR\u0026rsquo;) results in \u0026lsquo;foobar\u0026rsquo;.   string lpad(string str, int len, string pad) Returns str, left-padded with pad to a length of len. If str is longer than len, the return value is shortened to len characters. In case of empty pad string, the return value is null.   string ltrim(string A) Returns the string resulting from trimming spaces from the beginning(left hand side) of A. For example, ltrim(' foobar \u0026lsquo;) results in \u0026lsquo;foobar \u0026lsquo;.   array\u0026lt;struct\u0026lt;string,double\u0026raquo; ngrams(array\u0026lt;array\u0026gt;, int N, int K, int pf) Returns the top-k N-grams from a set of tokenized sentences, such as those returned by the sentences() UDAF. See StatisticsAndDataMining for more information.   int octet_length(string str) Returns the number of octets required to hold the string str in UTF-8 encoding (since Hive 2.2.0). Note that octet_length(str) can be larger than character_length(str).   string parse_url(string urlString, string partToExtract [, string keyToExtract]) Returns the specified part from the URL. Valid values for partToExtract include HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO. For example, parse_url(\u0026lsquo;http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1', \u0026lsquo;HOST\u0026rsquo;) returns \u0026lsquo;facebook.com\u0026rsquo;. Also a value of a particular key in QUERY can be extracted by providing the key as the third argument, for example, parse_url(\u0026lsquo;http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1', \u0026lsquo;QUERY\u0026rsquo;, \u0026lsquo;k1\u0026rsquo;) returns \u0026lsquo;v1\u0026rsquo;.   string printf(String format, Obj\u0026hellip; args) Returns the input formatted according do printf-style format strings (as of Hive 0.9.0).   string quote(String text) Returns the quoted string (Includes escape character for any single quotes HIVE-4.0.0)       Input Output     NULL NULL   DONT \u0026lsquo;DONT\u0026rsquo;   DON\u0026rsquo;T \u0026lsquo;DON'T\u0026rsquo;       Return Type Name(Signature) Description     string regexp_extract(string subject, string pattern, int index) Returns the string extracted using the pattern. For example, regexp_extract(\u0026lsquo;foothebar\u0026rsquo;, \u0026lsquo;foo(.*?)(bar)\u0026rsquo;, 2) returns \u0026lsquo;bar.\u0026rsquo; Note that some care is necessary in using predefined character classes: using \u0026lsquo;\\s\u0026rsquo; as the second argument will match the letter s; \u0026lsquo;\\s\u0026rsquo; is necessary to match whitespace, etc. The \u0026lsquo;index\u0026rsquo; parameter is the Java regex Matcher group() method index. See docs/api/java/util/regex/Matcher.html for more information on the \u0026lsquo;index\u0026rsquo; or Java regex group() method.   string regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT) Returns the string resulting from replacing all substrings in INITIAL_STRING that match the java regular expression syntax defined in PATTERN with instances of REPLACEMENT. For example, regexp_replace(\u0026ldquo;foobar\u0026rdquo;, \u0026ldquo;oo   string repeat(string str, int n) Repeats str n times.   string replace(string A, string OLD, string NEW) Returns the string A with all non-overlapping occurrences of OLD replaced with NEW (as of Hive 1.3.0 and 2.1.0). Example: select replace(\u0026ldquo;ababab\u0026rdquo;, \u0026ldquo;abab\u0026rdquo;, \u0026ldquo;Z\u0026rdquo;); returns \u0026ldquo;Zab\u0026rdquo;.   string reverse(string A) Returns the reversed string.   string rpad(string str, int len, string pad) Returns str, right-padded with pad to a length of len. If str is longer than len, the return value is shortened to len characters. In case of empty pad string, the return value is null.   string rtrim(string A) Returns the string resulting from trimming spaces from the end(right hand side) of A. For example, rtrim(\u0026rsquo; foobar \u0026lsquo;) results in ' foobar\u0026rsquo;.   array\u0026lt;array\u0026gt; sentences(string str, string lang, string locale) Tokenizes a string of natural language text into words and sentences, where each sentence is broken at the appropriate sentence boundary and returned as an array of words. The \u0026lsquo;lang\u0026rsquo; and \u0026lsquo;locale\u0026rsquo; are optional arguments. For example, sentences(\u0026lsquo;Hello there! How are you?') returns ( (\u0026ldquo;Hello\u0026rdquo;, \u0026ldquo;there\u0026rdquo;), (\u0026ldquo;How\u0026rdquo;, \u0026ldquo;are\u0026rdquo;, \u0026ldquo;you\u0026rdquo;) ).   string space(int n) Returns a string of n spaces.   array split(string str, string pat) Splits str around pat (pat is a regular expression).   map\u0026lt;string,string\u0026gt; str_to_map(text[, delimiter1, delimiter2]) Splits text into key-value pairs using two delimiters. Delimiter1 separates text into K-V pairs, and Delimiter2 splits each K-V pair. Default delimiters are \u0026lsquo;,\u0026rsquo; for delimiter1 and \u0026lsquo;:\u0026rsquo; for delimiter2.   string substr(string binary A, int start) substring(string   string substr(string binary A, int start, int len) substring(string   string substring_index(string A, string delim, int count) Returns the substring from string A before count occurrences of the delimiter delim (as of Hive 1.3.0). If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. Substring_index performs a case-sensitive match when searching for delim. Example: substring_index(\u0026lsquo;www.apache.org\u0026rsquo;, \u0026lsquo;.\u0026rsquo;, 2) = \u0026lsquo;www.apache\u0026rsquo;.   string translate(string char   string trim(string A) Returns the string resulting from trimming spaces from both ends of A. For example, trim(\u0026rsquo; foobar \u0026lsquo;) results in \u0026lsquo;foobar\u0026rsquo;   binary unbase64(string str) Converts the argument from a base 64 string to BINARY. (As of Hive 0.12.0.)   string upper(string A) ucase(string A) Returns the string resulting from converting all characters of A to upper case. For example, upper(\u0026lsquo;fOoBaR\u0026rsquo;) results in \u0026lsquo;FOOBAR\u0026rsquo;.   string initcap(string A) Returns string, with the first letter of each word in uppercase, all other letters in lowercase. Words are delimited by whitespace. (As of Hive 1.1.0.)   int levenshtein(string A, string B) Returns the Levenshtein distance between two strings (as of Hive 1.2.0). For example, levenshtein(\u0026lsquo;kitten\u0026rsquo;, \u0026lsquo;sitting\u0026rsquo;) results in 3.   string soundex(string A) Returns soundex code of the string (as of Hive 1.2.0). For example, soundex(\u0026lsquo;Miller\u0026rsquo;) results in M460.    Data Masking Functions The following built-in data masking functions are supported in Hive:\n   Return Type Name(Signature) Description     string mask(string str[, string upper[, string lower[, string number]]]) Returns a masked version of str (as of Hive 2.1.0). By default, upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example mask(\u0026ldquo;abcd-EFGH-8765-4321\u0026rdquo;) results in xxxx-XXXX-nnnn-nnnn. You can override the characters used in the mask by supplying additional arguments: the second argument controls the mask character for upper case letters, the third argument for lower case letters and the fourth argument for numbers. For example, mask(\u0026ldquo;abcd-EFGH-8765-4321\u0026rdquo;, \u0026ldquo;U\u0026rdquo;, \u0026ldquo;l\u0026rdquo;, \u0026ldquo;#\u0026quot;) results in llll-UUUU-####-####.   string mask_first_n(string str[, int n]) Returns a masked version of str with the first n values masked (as of Hive 2.1.0). Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_first_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in nnnn-5678-8765-4321.   string mask_last_n(string str[, int n]) Returns a masked version of str with the last n values masked (as of Hive 2.1.0). Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_last_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in 1234-5678-8765-nnnn.   string mask_show_first_n(string str[, int n]) Returns a masked version of str, showing the first n characters unmasked (as of Hive 2.1.0). Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_show_first_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in 1234-nnnn-nnnn-nnnn.   string mask_show_last_n(string str[, int n]) Returns a masked version of str, showing the last n characters unmasked (as of Hive 2.1.0). Upper case letters are converted to \u0026ldquo;X\u0026rdquo;, lower case letters are converted to \u0026ldquo;x\u0026rdquo; and numbers are converted to \u0026ldquo;n\u0026rdquo;. For example, mask_show_last_n(\u0026ldquo;1234-5678-8765-4321\u0026rdquo;, 4) results in nnnn-nnnn-nnnn-4321.   string mask_hash(string char    Misc. Functions    Return Type Name(Signature) Description     varies java_method(class, method[, arg1[, arg2..]]) Synonym for reflect. (As of Hive 0.9.0.)   varies reflect(class, method[, arg1[, arg2..]]) Calls a Java method by matching the argument signature, using reflection. (As of Hive 0.7.0.) See Reflect (Generic) UDF for examples.   int hash(a1[, a2\u0026hellip;]) Returns a hash value of the arguments. (As of Hive 0.4.)   string current_user() Returns current user name from the configured authenticator manager (as of Hive 1.2.0). Could be the same as the user provided when connecting, but with some authentication managers (for example HadoopDefaultAuthenticator) it could be different.   string logged_in_user() Returns current user name from the session state (as of Hive 2.2.0). This is the username provided when connecting to Hive.   string current_database() Returns current database name (as of Hive 0.13.0).   string md5(string/binary) Calculates an MD5 128-bit checksum for the string or binary (as of Hive 1.3.0). The value is returned as a string of 32 hex digits, or NULL if the argument was NULL. Example: md5(\u0026lsquo;ABC\u0026rsquo;) = \u0026lsquo;902fbdd2b1df0c4f70b4a5d23525e932\u0026rsquo;.   string sha1(string/binary)sha(string/binary) Calculates the SHA-1 digest for string or binary and returns the value as a hex string (as of Hive 1.3.0). Example: sha1(\u0026lsquo;ABC\u0026rsquo;) = \u0026lsquo;3c01bdbb26f358bab27f267924aa2c9a03fcfdb8\u0026rsquo;.   bigint crc32(string/binary) Computes a cyclic redundancy check value for string or binary argument and returns bigint value (as of Hive 1.3.0). Example: crc32(\u0026lsquo;ABC\u0026rsquo;) = 2743272264.   string sha2(string/binary, int) Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2(\u0026lsquo;ABC\u0026rsquo;, 256) = \u0026lsquo;b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78\u0026rsquo;.   binary aes_encrypt(input string/binary, key string/binary) Encrypt input using AES (as of Hive 1.3.0). Key lengths of 128, 192 or 256 bits can be used. 192 and 256 bits keys can be used if Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are installed. If either argument is NULL or the key length is not one of the permitted values, the return value is NULL. Example: base64(aes_encrypt(\u0026lsquo;ABC\u0026rsquo;, \u0026lsquo;1234567890123456\u0026rsquo;)) = \u0026lsquo;y6Ss+zCYObpCbgfWfyNWTw==\u0026rsquo;.   binary aes_decrypt(input binary, key string/binary) Decrypt input using AES (as of Hive 1.3.0). Key lengths of 128, 192 or 256 bits can be used. 192 and 256 bits keys can be used if Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files are installed. If either argument is NULL or the key length is not one of the permitted values, the return value is NULL. Example: aes_decrypt(unbase64(\u0026lsquo;y6Ss+zCYObpCbgfWfyNWTw=='), \u0026lsquo;1234567890123456\u0026rsquo;) = \u0026lsquo;ABC\u0026rsquo;.   string version() Returns the Hive version (as of Hive 2.1.0). The string contains 2 fields, the first being a build number and the second being a build hash. Example: \u0026ldquo;select version();\u0026rdquo; might return \u0026ldquo;2.1.0.2.5.0.0-1245 r027527b9c5ce1a3d7d0b6d2e6de2378fb0c39232\u0026rdquo;. Actual results will depend on your build.   bigint surrogate_key([write_id_bits, task_id_bits]) Automatically generate numerical Ids for rows as you enter data into a table. Can only be used as default value for acid or insert-only tables.    xpath The following functions are described in LanguageManual XPathUDF:\n xpath, xpath_short, xpath_int, xpath_long, xpath_float, xpath_double, xpath_number, xpath_string  get_json_object A limited version of JSONPath is supported:\n $ : Root object . : Child operator [] : Subscript operator for array   : Wildcard for []    Syntax not supported that\u0026rsquo;s worth noticing:\n : Zero length string as key .. : Recursive descent @ : Current object/element () : Script expression ?() : Filter (script) expression. [,] : Union operator [start:end.step] : array slice operator  Example: src_json table is a single column (json), single row table:\n+----+ json +----+ {\u0026quot;store\u0026quot;: {\u0026quot;fruit\u0026quot;:\\[{\u0026quot;weight\u0026quot;:8,\u0026quot;type\u0026quot;:\u0026quot;apple\u0026quot;},{\u0026quot;weight\u0026quot;:9,\u0026quot;type\u0026quot;:\u0026quot;pear\u0026quot;}], \u0026quot;bicycle\u0026quot;:{\u0026quot;price\u0026quot;:19.95,\u0026quot;color\u0026quot;:\u0026quot;red\u0026quot;} }, \u0026quot;email\u0026quot;:\u0026quot;amy@only_for_json_udf_test.net\u0026quot;, \u0026quot;owner\u0026quot;:\u0026quot;amy\u0026quot; } +----+ The fields of the json object can be extracted using these queries:\nhive\u0026gt; SELECT get_json_object(src_json.json, '$.owner') FROM src_json; amy hive\u0026gt; SELECT get_json_object(src_json.json, '$.store.fruit\\[0]') FROM src_json; {\u0026quot;weight\u0026quot;:8,\u0026quot;type\u0026quot;:\u0026quot;apple\u0026quot;} hive\u0026gt; SELECT get_json_object(src_json.json, '$.non_exist_key') FROM src_json; NULL Built-in Aggregate Functions (UDAF) The following built-in aggregate functions are supported in Hive:\n   Return Type Name(Signature) Description     BIGINT count(*), count(expr), count(DISTINCT expr[, expr\u0026hellip;]) count(*) - Returns the total number of retrieved rows, including rows containing NULL values.count(expr) - Returns the number of rows for which the supplied expression is non-NULL.count(DISTINCT expr[, expr]) - Returns the number of rows for which the supplied expression(s) are unique and non-NULL. Execution of this can be optimized with hive.optimize.distinct.rewrite.   DOUBLE sum(col), sum(DISTINCT col) Returns the sum of the elements in the group or the sum of the distinct values of the column in the group.   DOUBLE avg(col), avg(DISTINCT col) Returns the average of the elements in the group or the average of the distinct values of the column in the group.   DOUBLE min(col) Returns the minimum of the column in the group.   DOUBLE max(col) Returns the maximum value of the column in the group.   DOUBLE variance(col), var_pop(col) Returns the variance of a numeric column in the group.   DOUBLE var_samp(col) Returns the unbiased sample variance of a numeric column in the group.   DOUBLE stddev_pop(col) Returns the standard deviation of a numeric column in the group.   DOUBLE stddev_samp(col) Returns the unbiased sample standard deviation of a numeric column in the group.   DOUBLE covar_pop(col1, col2) Returns the population covariance of a pair of numeric columns in the group.   DOUBLE covar_samp(col1, col2) Returns the sample covariance of a pair of a numeric columns in the group.   DOUBLE corr(col1, col2) Returns the Pearson coefficient of correlation of a pair of a numeric columns in the group.   DOUBLE percentile(BIGINT col, p) Returns the exact pth percentile of a column in the group (does not work with floating point types). p must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral.   array percentile(BIGINT col, array(p1 [, p2]\u0026hellip;)) Returns the exact percentiles p1, p2, \u0026hellip; of a column in the group (does not work with floating point types). pi must be between 0 and 1. NOTE: A true percentile can only be computed for integer values. Use PERCENTILE_APPROX if your input is non-integral.   DOUBLE percentile_approx(DOUBLE col, p [, B]) Returns an approximate pth percentile of a numeric column (including floating point types) in the group. The B parameter controls approximation accuracy at the cost of memory. Higher values yield better approximations, and the default is 10,000. When the number of distinct values in col is smaller than B, this gives an exact percentile value.   array percentile_approx(DOUBLE col, array(p1 [, p2]\u0026hellip;) [, B]) Same as above, but accepts and returns an array of percentile values instead of a single one.   double regr_avgx(independent, dependent) Equivalent to avg(dependent). As of Hive 2.2.0.   double regr_avgy(independent, dependent) Equivalent to avg(independent). As of Hive 2.2.0.   double regr_count(independent, dependent) Returns the number of non-null pairs used to fit the linear regression line. As of Hive 2.2.0.   double regr_intercept(independent, dependent) Returns the y-intercept of the linear regression line, i.e. the value of b in the equation dependent = a * independent + b. As of Hive 2.2.0.   double regr_r2(independent, dependent) Returns the coefficient of determination for the regression. As of Hive 2.2.0.   double regr_slope(independent, dependent) Returns the slope of the linear regression line, i.e. the value of a in the equation dependent = a * independent + b. As of Hive 2.2.0.   double regr_sxx(independent, dependent) Equivalent to regr_count(independent, dependent) * var_pop(dependent). As of Hive 2.2.0.   double regr_sxy(independent, dependent) Equivalent to regr_count(independent, dependent) * covar_pop(independent, dependent). As of Hive 2.2.0.   double regr_syy(independent, dependent) Equivalent to regr_count(independent, dependent) * var_pop(independent). As of Hive 2.2.0.   array\u0026lt;struct {'x','y'}\u0026gt; histogram_numeric(col, b) Computes a histogram of a numeric column in the group using b non-uniformly spaced bins. The output is an array of size b of double-valued (x,y) coordinates that represent the bin centers and heights   array collect_set(col) Returns a set of objects with duplicate elements eliminated.   array collect_list(col) Returns a list of objects with duplicates. (As of Hive 0.13.0.)   INTEGER ntile(INTEGER x) Divides an ordered partition into x groups called buckets and assigns a bucket number to each row in the partition. This allows easy calculation of tertiles, quartiles, deciles, percentiles and other common summary statistics. (As of Hive 0.11.0.)    Built-in Table-Generating Functions (UDTF) Normal user-defined functions, such as concat(), take in a single input row and output a single output row. In contrast, table-generating functions transform a single input row to multiple output rows.\n   Row-set columns types Name(Signature) Description     T explode(ARRAYa) Explodes an array to multiple rows. Returns a row-set with a single column (col), one row for each element from the array.   Tkey,Tvalue explode(MAP\u0026lt;Tkey,Tvalue\u0026gt; m) Explodes a map to multiple rows. Returns a row-set with a two columns (key,value) , one row for each key-value pair from the input map. (As of Hive 0.8.0.).   int,T posexplode(ARRAYa) Explodes an array to multiple rows with additional positional column of int type (position of items in the original array, starting with 0). Returns a row-set with two columns (pos,val), one row for each element from the array.   T1,\u0026hellip;,Tn inline(ARRAY\u0026lt;STRUCTf1:T1,...,fn:Tn\u0026gt; a) Explodes an array of structs to multiple rows. Returns a row-set with N columns (N = number of top level elements in the struct), one row per struct from the array. (As of Hive 0.10.)   T1,\u0026hellip;,Tn/r stack(int r,T1 V1,\u0026hellip;,Tn/r Vn) Breaks up n values V1,\u0026hellip;,Vn into r rows. Each row will have n/r columns. r must be constant.        string1,\u0026hellip;,stringn json_tuple(string jsonStr,string k1,\u0026hellip;,string kn) Takes JSON string and a set of n keys, and returns a tuple of n values. This is a more efficient version of the get_json_object UDF because it can get multiple keys with just one call.   string 1,\u0026hellip;,stringn parse_url_tuple(string urlStr,string p1,\u0026hellip;,string pn) Takes URL string and a set of n URL parts, and returns a tuple of n values. This is similar to the parse_url() UDF but can extract multiple parts at once out of a URL. Valid part names are: HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, USERINFO, QUERY:.    Usage Examples explode (array) select explode(array('A','B','C')); select explode(array('A','B','C')) as col; select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf; select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf as col; explode (map) select explode(map('A',10,'B',20,'C',30)); select explode(map('A',10,'B',20,'C',30)) as (key,value); select tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf; select tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf as key,value; posexplode (array) select posexplode(array('A','B','C')); select posexplode(array('A','B','C')) as (pos,val); select tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf; select tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf as pos,val;  inline (array of structs) select inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))); select inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) as (col1,col2,col3); select tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf; select tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf as col1,col2,col3; stack (values) select stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01'); select stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') as (col0,col1,col2); select tf.* from (select 0) t lateral view stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') tf; select tf.* from (select 0) t lateral view stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') tf as col0,col1,col2;  Using the syntax \u0026ldquo;SELECT udtf(col) AS colAlias\u0026hellip;\u0026rdquo; has a few limitations:\n No other expressions are allowed in SELECT  SELECT pageid, explode(adid_list) AS myCol\u0026hellip; is not supported   UDTF\u0026rsquo;s can\u0026rsquo;t be nested  SELECT explode(explode(adid_list)) AS myCol\u0026hellip; is not supported   GROUP BY / CLUSTER BY / DISTRIBUTE BY / SORT BY is not supported  SELECT explode(adid_list) AS myCol \u0026hellip; GROUP BY myCol is not supported    Please see LanguageManual LateralView for an alternative syntax that does not have these limitations.\nAlso see Writing UDTFs if you want to create a custom UDTF.\nexplode explode() takes in an array (or a map) as an input and outputs the elements of the array (map) as separate rows. UDTFs can be used in the SELECT expression list and as a part of LATERAL VIEW.\nAs an example of using explode() in the SELECT expression list, consider a table named myTable that has a single column (myCol) and two rows:\n   ArraymyCol     [100,200,300]   [400,500,600]    Then running the query:\nSELECT explode(myCol) AS myNewCol FROM myTable; will produce:\n   (int) myNewCol     100   200   300   400   500   600    The usage with Maps is similar:\nSELECT explode(myMap) AS (myMapKey, myMapValue) FROM myMapTable; posexplode Version\nAvailable as of Hive 0.13.0. See HIVE-4943.\nposexplode() is similar to explode but instead of just returning the elements of the array it returns the element as well as its position in the original array.\nAs an example of using posexplode() in the SELECT expression list, consider a table named myTable that has a single column (myCol) and two rows:\n   ArraymyCol     [100,200,300]   [400,500,600]    Then running the query:\nSELECT posexplode(myCol) AS pos, myNewCol FROM myTable; will produce:\n   (int) pos (int) myNewCol     1 100   2 200   3 300   1 400   2 500   3 600    json_tuple A new json_tuple() UDTF is introduced in Hive 0.7. It takes a set of names (keys) and a JSON string, and returns a tuple of values using one function. This is much more efficient than calling GET_JSON_OBJECT to retrieve more than one key from a single JSON string. In any case where a single JSON string would be parsed more than once, your query will be more efficient if you parse it once, which is what JSON_TUPLE is for. As JSON_TUPLE is a UDTF, you will need to use the LATERAL VIEW syntax in order to achieve the same goal.\nFor example,\nselect a.timestamp, get_json_object(a.appevents, '$.eventid'), get_json_object(a.appenvets, '$.eventname') from log a; should be changed to:\nselect a.timestamp, b.* from log a lateral view json_tuple(a.appevent, 'eventid', 'eventname') b as f1, f2; parse_url_tuple The parse_url_tuple() UDTF is similar to parse_url(), but can extract multiple parts of a given URL, returning the data in a tuple. Values for a particular key in QUERY can be extracted by appending a colon and the key to the partToExtract argument, for example, parse_url_tuple(\u0026lsquo;http://facebook.com/path1/p.php?k1=v1\u0026amp;k2=v2#Ref1', \u0026lsquo;QUERY:k1\u0026rsquo;, \u0026lsquo;QUERY:k2\u0026rsquo;) returns a tuple with values of \u0026lsquo;v1\u0026rsquo;,\u0026lsquo;v2\u0026rsquo;. This is more efficient than calling parse_url() multiple times. All the input parameters and output column types are string.\nSELECT b.* FROM src LATERAL VIEW parse_url_tuple(fullurl, 'HOST', 'PATH', 'QUERY', 'QUERY:id') b as host, path, query, query_id LIMIT 1; GROUPing and SORTing on f(column) A typical OLAP pattern is that you have a timestamp column and you want to group by daily or other less granular date windows than by second. So you might want to select concat(year(dt),month(dt)) and then group on that concat(). But if you attempt to GROUP BY or SORT BY a column on which you\u0026rsquo;ve applied a function and alias, like this:\nselect f(col) as fc, count(*) from table_name group by fc; you will get an error:\nFAILED: Error in semantic analysis: line 1:69 Invalid Table Alias or Column Reference fc because you are not able to GROUP BY or SORT BY a column alias on which a function has been applied. There are two workarounds. First, you can reformulate this query with subqueries, which is somewhat complicated:\nselect sq.fc,col1,col2,...,colN,count(*) from (select f(col) as fc,col1,col2,...,colN from table_name) sq group by sq.fc,col1,col2,...,colN; Or you can make sure not to use a column alias, which is simpler:\nselect f(col) as fc, count(*) from table_name group by f(col); Contact Tim Ellis (tellis) at RiotGames dot com if you would like to discuss this in further detail.\nUtility Functions    Function Name Return Type Description To Run     version String Provides the Hive version Details (Package built version) select version();   buildversion String Extension of the Version function which includes the checksum select buildversion();    UDF internals The context of a UDF\u0026rsquo;s evaluate method is one row at a time. A simple invocation of a UDF like\nSELECT length(string_col) FROM table_name; would evaluate the length of each of the string_col\u0026rsquo;s values in the map portion of the job. The side effect of the UDF being evaluated on the map-side is that you can\u0026rsquo;t control the order of rows which get sent to the mapper. It is the same order in which the file split sent to the mapper gets deserialized. Any reduce side operation (such as SORT BY, ORDER BY, regular JOIN, etc.) would apply to the UDFs output as if it is just another column of the table. This is fine since the context of the UDF\u0026rsquo;s evaluate method is meant to be one row at a time.\nIf you would like to control which rows get sent to the same UDF (and possibly in what order), you will have the urge to make the UDF evaluate during the reduce phase. This is achievable by making use of DISTRIBUTE BY, DISTRIBUTE BY + SORT BY, CLUSTER BY. An example query would be:\nSELECT reducer_udf(my_col, distribute_col, sort_col) FROM (SELECT my_col, distribute_col, sort_col FROM table_name DISTRIBUTE BY distribute_col SORT BY distribute_col, sort_col) t However, one could argue that the very premise of your requirement to control the set of rows sent to the same UDF is to do aggregation in that UDF. In such a case, using a User Defined Aggregate Function (UDAF) is a better choice. You can read more about writing a UDAF here. Alternatively, you can user a custom reduce script to accomplish the same using Hive\u0026rsquo;s Transform functionality. Both of these options would do aggregations on the reduce side.\nCreating Custom UDFs For information about how to create a custom UDF, see Hive Plugins and Create Function.\nselect explode(array(\u0026lsquo;A\u0026rsquo;,\u0026lsquo;B\u0026rsquo;,\u0026lsquo;C\u0026rsquo;));select explode(array(\u0026lsquo;A\u0026rsquo;,\u0026lsquo;B\u0026rsquo;,\u0026lsquo;C\u0026rsquo;)) as col;select tf.* from (select 0) t lateral view explode(array(\u0026lsquo;A\u0026rsquo;,\u0026lsquo;B\u0026rsquo;,\u0026lsquo;C\u0026rsquo;)) tf;select tf.* from (select 0) t lateral view explode(array(\u0026lsquo;A\u0026rsquo;,\u0026lsquo;B\u0026rsquo;,\u0026lsquo;C\u0026rsquo;)) tf as col;\nAttachments: attachments/27362046/62696447-html (text/html)\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-udf_27362046/","tags":null,"title":"Apache Hive : LanguageManual UDF"},{"categories":null,"contents":"Apache Hive : LanguageManual Union  Union Syntax  UNION within a FROM Clause Unions in DDL and Insert Statements Applying Subclauses Column Aliases for Schema Matching Column Type Conversion Version Information    Union Syntax select_statement UNION [ALL | DISTINCT] select_statement UNION [ALL | DISTINCT] select_statement ... UNION is used to combine the result from multiple SELECT statements into a single result set.\n Hive versions prior to 1.2.0 only support UNION ALL (bag union), in which duplicate rows are not eliminated. In Hive 1.2.0 and later, the default behavior for UNION is that duplicate rows are removed from the result. The optional DISTINCT keyword has no effect other than the default because it also specifies duplicate-row removal. With the optional ALL keyword, duplicate-row removal does not occur and the result includes all matching rows from all the SELECT statements.  You can mix UNION ALL and UNION DISTINCT in the same query. Mixed UNION types are treated such that a DISTINCT union overrides any ALL union to its left. A DISTINCT union can be produced explicitly by using UNION DISTINCT or implicitly by using UNION with no following DISTINCT or ALL keyword.\nThe number and names of columns returned by each select_statement have to be the same. Otherwise, a schema error is thrown.\nUNION within a FROM Clause If some additional processing has to be done on the result of the UNION, the entire statement expression can be embedded in a FROM clause like below:\nSELECT * FROM ( select_statement UNION ALL select_statement ) unionResult For example, if we suppose there are two different tables that track which user has published a video and which user has published a comment, the following query joins the results of a UNION ALL with the user table to create a single annotated stream for all the video publishing and comment publishing events:\n SELECT u.id, actions.date FROM ( SELECT av.uid AS uid FROM action_video av WHERE av.date = '2008-06-03' UNION ALL SELECT ac.uid AS uid FROM action_comment ac WHERE ac.date = '2008-06-03' ) actions JOIN users u ON (u.id = actions.uid) Unions in DDL and Insert Statements Unions can be used in views, inserts, and CTAS (create table as select) statements. A query can contain multiple UNION clauses, as shown in the syntax above.\nApplying Subclauses To apply ORDER BY, SORT BY, CLUSTER BY, DISTRIBUTE BY or LIMIT to an individual SELECT, place the clause inside the parentheses that enclose the SELECT:\nSELECT key FROM (SELECT key FROM src ORDER BY key LIMIT 10)subq1 UNION SELECT key FROM (SELECT key FROM src1 ORDER BY key LIMIT 10)subq2 To apply an ORDER BY, SORT BY, CLUSTER BY, DISTRIBUTE BY or LIMIT clause to the entire UNION result, place the ORDER BY, SORT BY, CLUSTER BY, DISTRIBUTE BY or LIMIT after the last one. The following example uses both ORDER BY and LIMIT clauses:\nSELECT key FROM src UNION SELECT key FROM src1 ORDER BY key LIMIT 10 Column Aliases for Schema Matching UNION expects the same schema on both sides of the expression list. As a result, the following query may fail with an error message such as \u0026ldquo;FAILED: SemanticException 4:47 Schema of both sides of union should match.\u0026rdquo;\nINSERT OVERWRITE TABLE target_table SELECT name, id, category FROM source_table_1 UNION ALL SELECT name, id, \u0026quot;Category159\u0026quot; FROM source_table_2 In such cases, column aliases can be used to force equal schemas:\nINSERT OVERWRITE TABLE target_table SELECT name, id, category FROM source_table_1 UNION ALL SELECT name, id, \u0026quot;Category159\u0026quot; as category FROM source_table_2 Column Type Conversion Before HIVE-14251 in release 2.2.0, Hive tries to perform implicit conversion across Hive type groups. With the change of HIVE-14251, Hive will only perform implicit conversion within each type group including string group, number group or date group, not across groups. In order to union the types from different groups such as a string type and a date type, an explicit cast from string to date or from date to string is needed in the query.\n SELECT name, id, cast('2001-01-01' as date) d FROM source_table_1 UNION ALL SELECT name, id, hiredate as d FROM source_table_2  Version Information Version information\nIn Hive 0.12.0 and earlier releases, unions can only be used within a subquery such as \u0026ldquo;SELECT * FROM (select_statement UNION ALL select_statement UNION ALL \u0026hellip;) unionResult\u0026rdquo;.\nAs of Hive 0.13.0, unions can also be used in a top-level query: \u0026ldquo;select_statement UNION ALL select_statement UNION ALL \u0026hellip;\u0026rdquo;. (See HIVE-6189.)\nBefore Hive 1.2.0, only UNION ALL (bag union) is supported. UNION (or UNION DISTINCT) is supported since Hive 1.2.0. (See HIVE-9039.)\n  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-union_27362049/","tags":null,"title":"Apache Hive : LanguageManual Union"},{"categories":null,"contents":"Apache Hive : LanguageManual VariableSubstitution    Introduction Using Variables Substitution During Query Construction Disabling Variable Substitution  Introduction Hive is used for batch and interactive queries. Variable Substitution allows for tasks such as separating environment-specific configuration variables from code.\nThe Hive variable substitution mechanism was designed to avoid some of the code that was getting baked into the scripting language on top of Hive.\nExamples such as the following shell commands may (inefficiently) be used to set variables within a script:\n$ a=b $ hive -e \u0026quot; describe $a \u0026quot; This is frustrating as Hive becomes closely coupled with scripting languages. The Hive startup time of a couple seconds is non-trivial when doing thousands of manipulations such as multiple hive -e invocations.\nHive Variables combine the set capability you know and love with some limited yet powerful substitution ability.\nThe following example:\n$ bin/hive --hiveconf a=b -e 'set a; set hiveconf:a; \\ create table if not exists b (col int); describe ${hiveconf:a}' results in:\nHive history file=/tmp/edward/hive_job_log_edward_201011240906_1463048967.txt a=b hiveconf:a=b OK Time taken: 5.913 seconds OK col\tint\tTime taken: 0.754 seconds For general information about Hive command line options, see Hive CLI.\nVersion information\nThe hiveconf option was added in version 0.7.0 (JIRA HIVE-1096). Version 0.8.0 added the options define and hivevar (JIRA HIVE-2020), which are equivalent and are not described here. They create custom variables in a namespace that is separate from the hiveconf, system, and env namespaces.\nUsing Variables There are three namespaces for variables – hiveconf, system, and env. (Custom variables can also be created in a separate namespace with the define or hivevar option in Hive 0.8.0 and later releases.)\nThe hiveconf variables are set as normal:\nset x=myvalue However they are retrieved using:\n${hiveconf:x} Annotated examples of usage from the test case ql/src/test/queries/clientpositive/set_processor_namespaces.q:\nset zzz=5; -- sets zzz=5 set zzz; set system:xxx=5; set system:xxx; -- sets a system property xxx to 5 set system:yyy=${system:xxx}; set system:yyy; -- sets yyy with value of xxx set go=${hiveconf:zzz}; set go; -- sets go base on value on zzz set hive.variable.substitute=false; set raw=${hiveconf:zzz}; set raw; -- disable substitution set a value to the literal set hive.variable.substitute=true; EXPLAIN SELECT * FROM src where key=${hiveconf:zzz}; SELECT * FROM src where key=${hiveconf:zzz}; --use a variable in a query set a=1; set b=a; set c=${hiveconf:${hiveconf:b}}; set c; --uses nested variables. set jar=../lib/derby.jar; add file ${hiveconf:jar}; list file; delete file ${hiveconf:jar}; list file; Substitution During Query Construction Hive substitutes the value for a variable when a query is constructed with the variable.\n If you run two different Hive sessions, variable values will not be mixed across sessions. If you set variables with the same name in the same Hive session, a query uses the last set value.  Disabling Variable Substitution Variable substitution is on by default (hive.variable.substitute=true). If this causes an issue with an already existing script, disable it using the following command:\nset hive.variable.substitute=false; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-variablesubstitution_30754722/","tags":null,"title":"Apache Hive : LanguageManual VariableSubstitution"},{"categories":null,"contents":"Apache Hive : LanguageManual VirtualColumns  Virtual Columns  Simple Examples    Virtual Columns Hive 0.8.0 provides support for two virtual columns:\nOne is INPUT__FILE__NAME, which is the input file\u0026rsquo;s name for a mapper task.\nthe other is BLOCK__OFFSET__INSIDE__FILE, which is the current global file position.\nFor block compressed file, it is the current block\u0026rsquo;s file offset, which is the current block\u0026rsquo;s first byte\u0026rsquo;s file offset.\nSince Hive 0.8.0 the following virtual columns have been added:\n ROW__OFFSET__INSIDE__BLOCK RAW__DATA__SIZE ROW__ID GROUPING__ID  It is important to note, that all of the virtual columns listed here cannot be used for any other purpose (i.e. table creation with columns having a virtual column will fail with \u0026ldquo;SemanticException Error 10328: Invalid column name..\u0026quot;)\nSimple Examples select INPUT__FILE__NAME, key, BLOCK__OFFSET__INSIDE__FILE from src;\nselect key, count(INPUT__FILE__NAME) from src group by key order by key;\nselect * from src where BLOCK__OFFSET__INSIDE__FILE \u0026gt; 12000 order by key;\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-virtualcolumns_27362048/","tags":null,"title":"Apache Hive : LanguageManual VirtualColumns"},{"categories":null,"contents":"Apache Hive : LanguageManual WindowingAndAnalytics Windowing and Analytics Functions  Windowing and Analytics Functions  Enhancements to Hive QL Examples  PARTITION BY with one partitioning column, no ORDER BY or window specification PARTITION BY with two partitioning columns, no ORDER BY or window specification PARTITION BY with one partitioning column, one ORDER BY column, and no window specification PARTITION BY with two partitioning columns, two ORDER BY columns, and no window specification PARTITION BY with partitioning, ORDER BY, and window specification WINDOW clause LEAD using default 1 row lead and not specifying default value LAG specifying a lag of 3 rows and default value of 0 Distinct counting for each partition      Enhancements to Hive QL Version\nIntroduced in Hive version 0.11.\nThis section introduces the Hive QL enhancements for windowing and analytics functions. See \u0026ldquo;Windowing Specifications in HQL\u0026rdquo; (attached to HIVE-4197) for details. HIVE-896 has more information, including links to earlier documentation in the initial comments.\nAll of the windowing and analytics functions operate as per the SQL standard.\nThe current release supports the following functions for windowing and analytics:\n Windowing functions  LEAD  The number of rows to lead can optionally be specified. If the number of rows to lead is not specified, the lead is one row. Returns null when the lead for the current row extends beyond the end of the window.   LAG  The number of rows to lag can optionally be specified. If the number of rows to lag is not specified, the lag is one row. Returns null when the lag for the current row extends before the beginning of the window.   FIRST_VALUE  This takes at most two parameters. The first parameter is the column for which you want the first value, the second (optional) parameter must be a boolean which is false by default. If set to true it skips null values.   LAST_VALUE  This takes at most two parameters. The first parameter is the column for which you want the last value, the second (optional) parameter must be a boolean which is false by default. If set to true it skips null values.     The OVER clause   OVER with standard aggregates:\n COUNT SUM MIN MAX AVG    OVER with a PARTITION BY statement with one or more partitioning columns of any primitive datatype.\n  OVER with PARTITION BY and ORDER BY with one or more partitioning and/or ordering columns of any datatype.\n OVER with a window specification. Windows can be defined separately in a WINDOW clause. Window specifications support the following formats:  (ROWS | RANGE) BETWEEN (UNBOUNDED | [num]) PRECEDING AND ([num] PRECEDING | CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING) (ROWS | RANGE) BETWEEN CURRENT ROW AND (CURRENT ROW | (UNBOUNDED | [num]) FOLLOWING) (ROWS | RANGE) BETWEEN [num] FOLLOWING AND (UNBOUNDED | [num]) FOLLOWING  en ORDER BY is specified with missing WINDOW clause, the WINDOW specification defaults to `RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.` en both ORDER BY and WINDOW clauses are missing, the WINDOW specification defaults to `ROW BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING.` e OVER clause supports the following functions, but it does not support a window with them (see [HIVE-4797](https://issues.apache.org/jira/browse/HIVE-4797)): nking functions: Rank, NTile, DenseRank, CumeDist, PercentRank. ad and Lag functions.    Analytics functions  RANK ROW_NUMBER DENSE_RANK CUME_DIST PERCENT_RANK NTILE   Distinct support in Hive 2.1.0 and later (see HIVE-9534)  Distinct is supported for aggregation functions including SUM, COUNT and AVG, which aggregate over the distinct values within each partition. Current implementation has the limitation that no ORDER BY or window specification can be supported in the partitioning clause for performance reason. The supported syntax is as follows.\nCOUNT(DISTINCT a) OVER (PARTITION BY c) ORDER BY and window specification is supported for distinct in Hive 2.2.0 (see HIVE-13453). An example is as follows.\nCOUNT(DISTINCT a) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING) Aggregate functions in OVER clause support in Hive 2.1.0 and later (see HIVE-13475)  Support to reference aggregate functions within the OVER clause has been added. For instance, currently we can use the SUM aggregation function within the OVER clause as follows.\nSELECT rank() OVER (ORDER BY sum(b)) FROM T GROUP BY a; Examples This section provides examples of how to use the Hive QL windowing and analytics functions in SELECT statements. See HIVE-896 for additional examples.\nPARTITION BY with one partitioning column, no ORDER BY or window specification SELECT a, COUNT(b) OVER (PARTITION BY c) FROM T; PARTITION BY with two partitioning columns, no ORDER BY or window specification SELECT a, COUNT(b) OVER (PARTITION BY c, d) FROM T; PARTITION BY with one partitioning column, one ORDER BY column, and no window specification SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d) FROM T; PARTITION BY with two partitioning columns, two ORDER BY columns, and no window specification SELECT a, SUM(b) OVER (PARTITION BY c, d ORDER BY e, f) FROM T; PARTITION BY with partitioning, ORDER BY, and window specification SELECT a, SUM(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) FROM T; SELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 3 PRECEDING AND CURRENT ROW) FROM T; SELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING) FROM T; SELECT a, AVG(b) OVER (PARTITION BY c ORDER BY d ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING) FROM T;  There can be multiple OVER clauses in a single query. A single OVER clause only applies to the immediately preceding function call. In this example, the first OVER clause applies to COUNT(b) and the second OVER clause applies to SUM(b):\nSELECT a, COUNT(b) OVER (PARTITION BY c), SUM(b) OVER (PARTITION BY c) FROM T;  Aliases can be used as well, with or without the keyword AS:\nSELECT a, COUNT(b) OVER (PARTITION BY c) AS b_count, SUM(b) OVER (PARTITION BY c) b_sum FROM T; WINDOW clause SELECT a, SUM(b) OVER w FROM T WINDOW w AS (PARTITION BY c ORDER BY d ROWS UNBOUNDED PRECEDING); LEAD using default 1 row lead and not specifying default value SELECT a, LEAD(a) OVER (PARTITION BY b ORDER BY C) FROM T; LAG specifying a lag of 3 rows and default value of 0 SELECT a, LAG(a, 3, 0) OVER (PARTITION BY b ORDER BY C) FROM T; Distinct counting for each partition SELECT a, COUNT(distinct a) OVER (PARTITION BY b) FROM T; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-windowingandanalytics_31819589/","tags":null,"title":"Apache Hive : LanguageManual WindowingAndAnalytics"},{"categories":null,"contents":"Apache Hive : LanguageManual XPathUDF Documentation for Built-In User-Defined Functions Related To XPath\nUDFs xpath, xpath_short, xpath_int, xpath_long, xpath_float, xpath_double, xpath_number, xpath_string  Functions for parsing XML data using XPath expressions. Since version: 0.6.0  Overview The xpath family of UDFs are wrappers around the Java XPath library javax.xml.xpath provided by the JDK. The library is based on the XPath 1.0 specification. Please refer to http://java.sun.com/javase/6/docs/api/javax/xml/xpath/package-summary.html for detailed information on the Java XPath library.\nAll functions follow the form: xpath_*(xml_string, xpath_expression_string). The XPath expression string is compiled and cached. It is reused if the expression in the next input row matches the previous. Otherwise, it is recompiled. So, the xml string is always parsed for every input row, but the xpath expression is precompiled and reused for the vast majority of use cases.\nBackward axes are supported. For example:\n \u0026gt; select xpath ('\u0026lt;a\u0026gt;\u0026lt;b id=\u0026quot;1\u0026quot;\u0026gt;\u0026lt;c/\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;b id=\u0026quot;2\u0026quot;\u0026gt;\u0026lt;c/\u0026gt;\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;','/descendant::c/ancestor::b/@id') from t1 limit 1 ; [1\u0026quot;,\u0026quot;2] Each function returns a specific Hive type given the XPath expression:\n xpath returns a Hive array of strings. xpath_string returns a string. xpath_boolean returns a boolean. xpath_short returns a short integer. xpath_int returns an integer. xpath_long returns a long integer. xpath_float returns a floating point number. xpath_double,xpath_number returns a double-precision floating point number (xpath_number is an alias for xpath_double).  The UDFs are schema agnostic - no XML validation is performed. However, malformed xml (e.g., \u0026lt;a\u0026gt;\u0026lt;b\u0026gt;1\u0026lt;/b\u0026gt;\u0026lt;/aa\u0026gt;) will result in a runtime exception being thrown.\nFollowing are specifics on each xpath UDF variant.\nxpath The xpath() function always returns a hive array of strings. If the expression results in a non-text value (e.g., another xml node) the function will return an empty array. There are 2 primary uses for this function: to get a list of node text values or to get a list of attribute values.\nExamples:\nNon-matching XPath expression:\n \u0026gt; select xpath('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;','a/*') from src limit 1 ; [] Get a list of node text values:\n \u0026gt; select xpath('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;','a/*/text()') from src limit 1 ; [b1\u0026quot;,\u0026quot;b2] Get a list of values for attribute \u0026lsquo;id\u0026rsquo;:\n \u0026gt; select xpath('\u0026lt;a\u0026gt;\u0026lt;b id=\u0026quot;foo\u0026quot;\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b id=\u0026quot;bar\u0026quot;\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;','//@id') from src limit 1 ; [foo\u0026quot;,\u0026quot;bar] Get a list of node texts for nodes where the \u0026lsquo;class\u0026rsquo; attribute equals \u0026lsquo;bb\u0026rsquo;:\n \u0026gt; SELECT xpath ('\u0026lt;a\u0026gt;\u0026lt;b class=\u0026quot;bb\u0026quot;\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;b3\u0026lt;/b\u0026gt;\u0026lt;c class=\u0026quot;bb\u0026quot;\u0026gt;c1\u0026lt;/c\u0026gt;\u0026lt;c\u0026gt;c2\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'a/*[@class=\u0026quot;bb\u0026quot;]/text()') FROM src LIMIT 1 ; [b1\u0026quot;,\u0026quot;c1] xpath_string The xpath_string() function returns the text of the first matching node.\nGet the text for node \u0026lsquo;a/b\u0026rsquo;:\n \u0026gt; SELECT xpath_string ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;bb\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;cc\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'a/b') FROM src LIMIT 1 ; bb Get the text for node \u0026lsquo;a\u0026rsquo;. Because \u0026lsquo;a\u0026rsquo; has children nodes with text, the result is a composite of text from the children.\n \u0026gt; SELECT xpath_string ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;bb\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;cc\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'a') FROM src LIMIT 1 ; bbcc Non-matching expression returns an empty string:\n \u0026gt; SELECT xpath_string ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;bb\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;cc\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'a/d') FROM src LIMIT 1 ; Gets the text of the first node that matches \u0026lsquo;//b\u0026rsquo;:\n \u0026gt; SELECT xpath_string ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', '//b') FROM src LIMIT 1 ; b1 Gets the second matching node:\n \u0026gt; SELECT xpath_string ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', 'a/b[2]') FROM src LIMIT 1 ; b2 Gets the text from the first node that has an attribute \u0026lsquo;id\u0026rsquo; with value \u0026lsquo;b_2\u0026rsquo;:\n \u0026gt; SELECT xpath_string ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b1\u0026lt;/b\u0026gt;\u0026lt;b id=\u0026quot;b_2\u0026quot;\u0026gt;b2\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', 'a/b[@id=\u0026quot;b_2\u0026quot;]') FROM src LIMIT 1 ; b2 xpath_boolean Returns true if the XPath expression evaluates to true, or if a matching node is found.\nMatch found:\n \u0026gt; SELECT xpath_boolean ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', 'a/b') FROM src LIMIT 1 ; true No match found:\n \u0026gt; SELECT xpath_boolean ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', 'a/c') FROM src LIMIT 1 ; false Match found:\n \u0026gt; SELECT xpath_boolean ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;b\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', 'a/b = \u0026quot;b\u0026quot;') FROM src LIMIT 1 ; true No match found:\n \u0026gt; SELECT xpath_boolean ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;10\u0026lt;/b\u0026gt;\u0026lt;/a\u0026gt;', 'a/b \u0026lt; 10') FROM src LIMIT 1 ; false xpath_short, xpath_int, xpath_long These functions return an integer numeric value, or the value zero if no match is found, or a match is found but the value is non-numeric.\nMathematical operations are supported. In cases where the value overflows the return type, then the maximum value for the type is returned.\nNo match:\n \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;b\u0026lt;/a\u0026gt;', 'a = 10') FROM src LIMIT 1 ; 0 Non-numeric match:\n \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;this is not a number\u0026lt;/a\u0026gt;', 'a') FROM src LIMIT 1 ; 0 \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;this 2 is not a number\u0026lt;/a\u0026gt;', 'a') FROM src LIMIT 1 ; 0 Adding values:\n \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;\u0026lt;b class=\u0026quot;odd\u0026quot;\u0026gt;1\u0026lt;/b\u0026gt;\u0026lt;b class=\u0026quot;even\u0026quot;\u0026gt;2\u0026lt;/b\u0026gt;\u0026lt;b class=\u0026quot;odd\u0026quot;\u0026gt;4\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;8\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'sum(a/*)') FROM src LIMIT 1 ; 15 \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;\u0026lt;b class=\u0026quot;odd\u0026quot;\u0026gt;1\u0026lt;/b\u0026gt;\u0026lt;b class=\u0026quot;even\u0026quot;\u0026gt;2\u0026lt;/b\u0026gt;\u0026lt;b class=\u0026quot;odd\u0026quot;\u0026gt;4\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;8\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'sum(a/b)') FROM src LIMIT 1 ; 7 \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;\u0026lt;b class=\u0026quot;odd\u0026quot;\u0026gt;1\u0026lt;/b\u0026gt;\u0026lt;b class=\u0026quot;even\u0026quot;\u0026gt;2\u0026lt;/b\u0026gt;\u0026lt;b class=\u0026quot;odd\u0026quot;\u0026gt;4\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;8\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'sum(a/b[@class=\u0026quot;odd\u0026quot;])') FROM src LIMIT 1 ; 5 Overflow:\n \u0026gt; SELECT xpath_int ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;2000000000\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;40000000000\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'a/b * a/c') FROM src LIMIT 1 ; 2147483647 xpath_float, xpath_double, xpath_number Similar to xpath_short, xpath_int and xpath_long but with floating point semantics. Non-matches result in zero. However,\nnon-numeric matches result in NaN. Note that xpath_number() is an alias for xpath_double().\nNo match:\n \u0026gt; SELECT xpath_double ('\u0026lt;a\u0026gt;b\u0026lt;/a\u0026gt;', 'a = 10') FROM src LIMIT 1 ; 0.0 Non-numeric match:\n \u0026gt; SELECT xpath_double ('\u0026lt;a\u0026gt;this is not a number\u0026lt;/a\u0026gt;', 'a') FROM src LIMIT 1 ; NaN A very large number:\n SELECT xpath_double ('\u0026lt;a\u0026gt;\u0026lt;b\u0026gt;2000000000\u0026lt;/b\u0026gt;\u0026lt;c\u0026gt;40000000000\u0026lt;/c\u0026gt;\u0026lt;/a\u0026gt;', 'a/b * a/c') FROM src LIMIT 1 ; 8.0E19 UDAFs UDTFs ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/languagemanual-xpathudf_27362051/","tags":null,"title":"Apache Hive : LanguageManual XPathUDF"},{"categories":null,"contents":"Apache Hive : Links Motivation Today, the infrastructure provided by Hive allows for the setup of a single shared warehouse and the authorization model allows for access control within this warehouse if needed. Growth beyond a single warehouse (when datacenter capacity limits are reached) OR separation of capacity usage and allocation requires the creation of multiple warehouses with each warehouse mapping to it\u0026rsquo;s own Hive metastore. Let\u0026rsquo;s define the term physical warehouse to map to a single Hive metastore, the Hadoop cluster it maps to and the data in it.\nIn organizations with a large number of teams needing a warehouse, there is a need to be able to:\n Maximize sharing of physical clusters to keep operational costs low Clearly identify and track capacity usage by teams in the data warehouse  One way to do this is to use a single shared warehouse as we do today, but this has the below issues:\n When the warehouse reaches datacenter capacity limits, it is hard to identify self-contained pieces that can be migrated out. Capacity tracking and management becomes an issue.  An alternative is to create a new physical warehouse per team (1:1 mapping), but this is not optimal since the physical resources are not shared across teams, and the operational cost is high. Further, data may not be cleanly partition-able and end up being replicated in multiple physical warehouses.\nTo provide context, in Facebook, we expect to have 20+ partitions of the warehouse, so operating each in their own physical warehouse will be impractical from an operational perspective.\nRequirements Introduce the notion of a virtual warehouse (namespace) in Hive with the below key properties:\n Can be housed in the same physical warehouse with other virtual warehouses (multi-tenancy). Portable (so it can be moved from one physical warehouse to another). Being self-contained is a necessary condition for portability (all queries on this namespace operate only on data available in the namespace). Unit of capacity tracking and capacity allocation. This is a nice side effect of creating self-contained namespaces and allows capacity planning based on the virtual warehouse growth.  Mapping many namespaces to 1 physical warehouse keeps the operational cost low. If a physical warehouse reaches capacity limits, portability will allow seamless migration of the namespace to another physical warehouse.\nNote that users can operate on multiple namespaces simultaneously although they are likely to most often operate within one namespace. So namespaces are not trying to solve the problem of ensuring that users only have access to a subset of data in the warehouse.\nFrom Hive, therefore the requirements are:\n Provide metadata to identify tables and queries that belong to one namespace. Provide controls to prevent operating on tables outside the namespace. Provide commands to explicitly request that tables/partitions in namespace1 be made available in namespace2 (since some tables/partitions may be needed across multiple namespaces). Avoid making copies of tables/partitions for this.  Design The design that is proposed is:\n Modeling namespaces as databases. No explicit accounting/tracking of tables/partitions/views that belong to a namespace is needed since a database provides that already. Prevent access using two part name syntax (Y.T). This ensures the database is self-contained. Modeling table/partition imports across namespaces using a new concept called Links in Hive. There will be commands to create Links to tables in other databases, alter and drop them. Links do not make copies of the table/partition and hence avoid data duplication in the same physical warehouse.  Let’s take a concrete example:\n Namespace A resides in database A, namespace B in database B. Access across these namespace using A.T or B.T syntax is disabled in ‘namespace’ mode. The user is importing table T1 from B into A . The user issues a CREATE LINK command, which creates metadata in the target namespace A for the table + metadata to indicate which object is linked. The ALTER LINK ADD PARTITION command is used to add partitions to the link. These partitions are modeled by replicating partition-level metadata in the target database A for the accessible partitions. The Link can be dynamic, which means it is kept updated as the source table gets new partitions or drops partitions.  There are 3 alternatives to implementing these ideas in open-source hive and Facebook extensions:\n Implement links as a first-class concept in Hive, and use a Facebook hook to disable Y.T access unless there is a link to the table Y.T. Implement links as a first-class concept, and introduce a new syntax T@Y to access linked content. Use a Facebook hook to disable Y.T access. Implement links as a first-class concept, and introduce a new syntax T@Y to access linked content. Disable cross database access using a new privilege. All these changes will be in Open Source Hive.  Links to JIRAS for these features:\n HIVE-3047 Add a privilege for cross database access HIVE-2989 Adding Table Links to Hive  A basic tenet of our design is that a Hive instance does not operate across physical warehouses. We are building a namespace service external to Hive that has metadata on namespace location across the Hive instances, and allows importing data across Hive instances using replication.\nAlternate design options Modeling Namespace as a Role in Hive (using the authorization model) The idea here is to create a Role for each namespace and users operating in that namespace belong to that Role. Access to data outside the namespace is made possible by granting permissions to the foreign table/view to the Role for the namespace.\nIssues\n A user who belongs to multiple namespaces (and hence multiple roles) will be able to operate on all data across those namespaces at any point in time, so namespaces are no longer self-contained. Imagine the situation of user A who has access to namespaces N1 and N2 running a query on both simultaneously. Either of those queries will be able to access data across both N1 and N2 although this shouldn’t be allowed. Capacity tracking is more complex Operations like show tables, and show partitions do not work without changes.  Modeling Namespace by tagging objects The idea here is to tag tables/partitions with the namespaces that they belong to. To handle the requirements:\n Tags will have to be explicitly created by a user of the warehouse (through a command) Commands like show tables and show partitions will need changes to understand tags. Capacity tracking and management will be more complex than using databases for this purpose. Data migration is more complex since the data is not contained with the root folder of one database  None of these are insurmountable problems, but using databases to model namespaces is a cleaner approach.\n(Taking this idea further, a database in Hive could itself have been implemented using tags in a single global namespace which would have not been as elegant as the current implementation of a database being a first class concept in Hive.)\nModeling Namespace as a database but using views for imports  The view would be a simple select * using Y.T syntax. It’s a degenerate case of view. We would need a registry of all views which import tables/partitions from other databases for namespace accounting. This requires adding metadata to these views to distinguish them from other user-created views. It would be harder to single instance imports using views (same table/partitions imported twice into the same namespace). Views are too opaque.  Using partitioned views:\nBy definition, there isn\u0026rsquo;t a one-one mapping between a view partition and a table partition. In fact, hive today does not even know about this dependency between view partitions and table partitions. Partitioned views is just a metadata concept - it is not something that the query layer understands. For e.g: if a view V partitioned on ds had 2 partitions: 1 and 2, then a query like select … from V where ds = 3 may still give valid results if the ds=3 is satisfied by the table underlying V. This means that:\n View metadata doesn’t stay in sync with source partitions (as partitions get added and dropped). The user has to explicitly do this, which won\u0026rsquo;t work for our case. We would like to differentiate the set of partitions that are available for the same imported tables across namespaces. This would require partition pruning based on the view partitions in query rewrite which is not how it works today.  The above notes make it clear that what we are trying to build is a very special case of a degenerate view, and it would be cleaner to introduce a new concept in Hive to model these ‘imports’.\nComments:            Questions from Ashutosh Chauhan (with inline responses):      What exactly is contained in tracking capacity usage. One is disk space. That I presume you are going to track via summing size under database directory. Are you also thinking of tracking resource usage in terms of CPU/memory/network utilization for different teams? Right now the capacity usage in Hive we will track is the disk space (managed tables that belong to the namespace + imported tables). We will track the mappers and reducers that the namepace utilizes directly from Hadoop.\n  Each namespace (ns) will have exactly one database. If so, then users are not allowed to create/use databases in such deployment? Not necessarily a problem, just trying to understand design. This is correct – this is a limitation of the design. Introducing a new concept seemed heavyweight, so we re-used databases for namespaces. But this approach means that a given namespace cannot have sub-databases in it.\n  How are you going to keep metadata consistent across two ns? If metadata gets updated in remote ns, will it get automatically updated in user\u0026rsquo;s local ns? If yes, how will this be implemented? If no, then every time user need to use data from remote ns, she has to bring metadata uptodate in her ns. How will she do it? Metadata will be kept in sync for linked tables. We will make alter table on the remote table (source of the link) cause an update to the target of the link. Note that from a Hive perspective, the metadata for the source and target of a link is in the same metastore.\n  Is it even required that metadata of two linked tables to be consistent? Seems like user has to run \u0026ldquo;alter link add partition\u0026rdquo; herself for each partition. She can choose only to add few partitions. In this case, tables in two ns have different number of partitions and thus data. What you say above is true for static links. For dynamic links, add and drop partition on the source of the link will cause the target to get those partitions as well (we trap alter table add/drop partition to provide this behavior).\n  Who is allowed to create links? Any user on the database who has create/all privileges on the database. We could potentially create a new privilege for this, but I think create privilege should suffice. We can similarly map alter, drop privileges to the appropriate operations.\n  Once user creates a link, who can use it? If everyone is allowed to access, then I don\u0026rsquo;t see how is it different from the problem that you are outlining in first alternative design option, wherein user having an access to two ns via roles has access to data on both ns. The link creates metadata in the target database. So you can only access data that has been linked into this database (access is via the T@Y or Y.T syntax depending on the chosen design option). Note that this is different than having a role that a user maps to since in that case, there is no local metadata in the target database specifying if the imported data is accessible from this database.\n  If links are first class concepts, then authorization model also needs to understand them? I don\u0026rsquo;t see any mention of that. Yes, we need to account for the authorization model.\n  I see there is a hdfs jira for implementing hard links of files in hdfs layer, so that takes care of linking physical data on hdfs. What about tables whose data is stored in external systems. For example, hbase. Does hbase also needs to implement feature of hard-linking their table for hive to make use of this feature? What about other storage handlers like cassandra, mongodb etc. The link does not create a link on HDFS. It just points to the source table/partitions. One can think of it as a Hive-level link so there is no need for any changes/features from the other storage handlers.\n  Migration will involve two step process of distcp\u0026rsquo;ing data from one cluster to another and then replicating one mysql instance to another. Are there any other steps? Do you plan to (later) build tools to automate this process of migration. We will be building tools to enable migration of a namespace to another cluster. Migration will involve replicating the metadata and the data as you mention above.\n  When migrating ns from one datacenter to another, will links be dropped or they are also preserved? We will preserve them – by copying the data for the links to the other datacenter.\n  Posted by sambavi at May 22, 2012 02:10 | | The first draft of this proposal is very hard to decipher because it relies on terms that aren\u0026rsquo;t well defined. For example, here\u0026rsquo;s the second sentence from the motivations section: Growth beyond a single warehouse (or) separation of capacity usage and allocation requires the creation of multiple physical warehouses, i.e., separate Hive instances. What\u0026rsquo;s the difference between a warehouse and a physical warehouse? How do you define a Hive instance? In the requirements section the term virtual warehouse is introduced and equated to a namespace, but clearly it\u0026rsquo;s more than that because otherwise DBs/Schemas would suffice. Can you please update the proposal to include definitions of these terms?\nPosted by cwsteinbach at May 22, 2012 18:35 | | Prevent access using two part name syntax (Y.T) if namespaces feature is \u0026ldquo;on\u0026rdquo; in a Hive instance. This ensures the database is self-contained. The cross-namespace Hiveconf ACL proposed in HIVE-3016 doesn\u0026rsquo;t prevent anyone from doing anything because there is no way to keep users from disabling it. I\u0026rsquo;m surprised to see this ticket mentioned here since three committers have already gone on record saying that this is the wrong approach, and one committer even -1\u0026rsquo;d it. If preventing cross-db references in queries is a requirement for this project, then I think Hive\u0026rsquo;s authorization mechanism will need to be extended to support this privilege/restriction.\nPosted by cwsteinbach at May 22, 2012 18:48 | | From the design section: We are building a namespace service external to Hive that has metadata on namespace location across the Hive instances, and allows importing data across Hive instances using replication. Does the work proposed in HIVE-2989 also include adding this Db/Table replication infrastructure to Hive?\nPosted by cwsteinbach at May 22, 2012 18:53 | | We mention the JIRA here for the sake of completeness. We are implementing this as a pre-execution hook for now, but support for namespaces will be incomplete without this control (since you can\u0026rsquo;t guarantee self-contained namespaces unless you prevent two-part name access). What extensions to the authorization system are you thinking of? One idea would be to set role for a session (corresponding to the namespace the user is operating in), so that a user operating in the context of that role can only see the data available to that role.\nPosted by sambavi at May 22, 2012 19:42 | | What extensions to the authorization system are you thinking of? Add a new privilege named something like \u0026ldquo;select_cross_db\u0026rdquo; and GRANT it to specific users as follows: GRANT select_cross_db ON DATABASE db TO USER x; GRANT select_cross_db ON DATABASE db to ROLE x; This privilege would be provided by default, but if absent, then the user would be prevented from referencing DBs outside of \u0026lsquo;db\u0026rsquo; while using \u0026lsquo;db\u0026rsquo; as the primary database.\nPosted by cwsteinbach at May 22, 2012 20:01 | | Thanks Carl - this is an interesting suggestion - for installations with namespaces, we would need to turn this privilege off by default and have no users. groups or roles be granted this privilege. We\u0026rsquo;ll discuss internally.\nPosted by sambavi at May 22, 2012 20:25 | | I\u0026rsquo;ve edited the main page to include a definition for physical warehouse and removed the term Hive instance to reduce ambiguity.\nPosted by sambavi at May 22, 2012 20:26 | | No Hive-2989 will not include the replication infrastructure. We plan to provide replication in the second half of the year.\nPosted by sambavi at May 23, 2012 10:30 | | I have opened a JIRA for adding a new privilege for cross database commands and resolved HIVE-3016. Thanks for the suggestion!\nPosted by sambavi at May 23, 2012 12:36 | | This proposal describes the DDL and metadata changes that are necessary to support DB Links, but it doesn\u0026rsquo;t include any details about the mechanics of replicating data across clusters (it\u0026rsquo;s probably a little more complicated than just running distcp). I think the proposal needs to include these details before it can be considered complete. No Hive-2989 will not include the replication infrastructure. We plan to provide replication in the second half of the year. The metadata extensions described in this proposal will require users to run metastore upgrade scripts, and the DDL extensions will become part of the public API. The former imposes a burden on users, and the latter constitutes a continuing maintenance burden on the people who contribute to this project. Taking this into account I think we need to be able to demonstrate that the new code tangibly benefits users before it appears in a Hive release. I don\u0026rsquo;t think it will be possible to demonstrate a tangible benefit to users until the replication/update mechanism is implemented and integrated into Hive.\nPosted by cwsteinbach at May 25, 2012 19:21 | | Some thoughts on this from our team:\n Even in a single physical warehouse, namespaces allow better quota management and isolation between independent team\u0026rsquo;s data/workload. This is independent of security and replication considerations. Further, replication as a feature is pretty big and will take a while to be built out. We can hide the table link feature behind a config parameter so that its not exposed to users who don\u0026rsquo;t need it until its completed. The only piece we cannot hide is the metastore changes, but the upgrade script for the metastore will just add few columns in few tables, and should not take more than a few minutes even for a pretty large warehouse (few thousand tables + ~100,000 partitions). In the meanwhile, users who do need this feature (like Facebook) can use it if they want to. If there was links support to start with in hive, we would have used it from the very beginning, and not gotten into the mess of one large warehouse with difficulty in dealing with multi-tenancy. We seriously believe that this is the right direction for the community, and all new users can design the warehouse in the right way from the very start, and learn from Facebook\u0026rsquo;s experience.  Posted by sambavi at May 25, 2012 21:37 | | Even in a single physical warehouse, namespaces allow better quota management and isolation between independent team\u0026rsquo;s data/workload. This is independent of security and replication considerations. Hive already provides support for namespaces in the form of databases/schemas. As far as I can tell the table link feature proposed here actually weakens the isolation guarantees provided by databases/schemas, and consequently will make quota and workload management between teams more complicated. In order to resolve this disagreement I think it would help if you provided some concrete examples of how the table link feature improves this situation.\nPosted by cwsteinbach at May 29, 2012 13:54 | | Suppose there are 2 teams which want to use the same physical warehouse. Team 1 wants to use the following: (let us say that each table is partitioned by date) T1 (all partitions) T2 (all partitions) T3 (all partitions) Team 2 wants to use the following: T1 (partitions for the last 3 days) T2 (partition for a fixed day: say May 29' 2012) T4 (all partitions) Using the current hive architecture, we can perform the following:\n Use a single database and have scripts for quota Use 2 databases and copy the data in both the databases (say, the databases are DB1 and DB2 respectively) Use 2 databases, and use views in database 2 (to be used to team 2).  The problems with these approaches is as follows:\n Table Discovery etc.becomes very messy. You can do that via tags, but then, all the functionality that is provided by databases can also be provided via tags. Duplication of data The user will have to perform the management himself. When a partition gets added to DB1.T1, the corresponding partition needs to be added to DB2.View1, and the 3 day old partition from DB2.View1 needs to be dropped. This has to be done outside hive, and makes the task of maintaining these partitions very difficult - how do you make sure this is atomic etc. User has to do lot more scripting.  Links is a degenerate case of views. With links, the above use case can be solved very easily. This is a real use case at Facebook today, and I think, there will be similar use cases for other users. Maybe, they are not solving it in the most optimal manner currently.\nPosted by namit.jain at May 29, 2012 18:33 | | Furthermore, databases don\u0026rsquo;t provide isolation today since two part name access is unrestricted. By introducing a trackable way of accessing content outside the current database (using table links), we get isolation for a namespace using Hive databases.\nPosted by sambavi at May 30, 2012 01:22 | | @Namit: Thanks for providing an example. I have a couple followup questions: Use a single database and have scripts for quota Can you provide some more details about how quota management works? For example, Team 1 and 2 both need access to a single partition in table T2, so who pays for the space occupied by this partition? Are both teams charged for it? Use 2 databases and copy the data in both the databases (say, the databases are DB1 and DB2 respectively) I\u0026rsquo;m not sure why this is listed as an option. What does this actually accomplish? Use 2 databases, and use views in database 2 (to be used to team 2). This seems like the most reasonable approach given my current understanding of your use case. You listed three problems with these approaches. Most of them don\u0026rsquo;t seem applicable to views: Table Discovery etc.becomes very messy. You can do that via tags, but then, all the functionality that is provided by databases can also be provided via tags. It\u0026rsquo;s hard for me to evaluate this claim since I\u0026rsquo;m not totally sure what is meant by \u0026ldquo;table discovery\u0026rdquo;. Can you please provide an example? However, my guess is that this is not a differentiator if you\u0026rsquo;re comparing the table links approach to views. Duplication of data Not applicable for views and table links. The user will have to perform the management himself. When a partition gets added to DB1.T1, the corresponding partition needs to be added to DB2.View1, and the 3 day old partition from DB2.View1 needs to be dropped. Based on the description of table links in HIVE-2989 it sounds like the user will still have to perform manual management even with table links, e.g. dropping the link that points to the partition from four days ago and adding a new link that points to the most recent partition. In this case views may actually work better since you can embed the filter condition (last three days) in the view definition instead of relying on external tools to update the table links. This has to be done outside hive, and makes the task of maintaining these partitions very difficult - how do you make sure this is atomic etc. User has to do lot more scripting. I don\u0026rsquo;t think table links make this process atomic, and as I mentioned above the process of maintaining this linked set of partitions actually seems easier if you use views instead. Links is a degenerate case of views. I agree that table links are a degenerate case of views. Since that\u0026rsquo;s the case, why is it necessary to implement table links? Why not leverage the functionality that is already provided with views?\nPosted by cwsteinbach at May 31, 2012 16:45 | | Furthermore, databases don\u0026rsquo;t provide isolation today since two part name access is unrestricted. DBs in conjunction with the authorization system provide strong isolation between different namespaces. Also, it should be possible to extend the authorization system to the two-part-name-access case that you described above (e.g. HIVE-3047). By introducing a trackable way of accessing content outside the current database (using table links), we get isolation for a namespace using Hive databases. I think you already get that by using views. If I\u0026rsquo;m wrong can you please explain how the view approach falls short? Thanks.\nPosted by cwsteinbach at May 31, 2012 16:59 | | Carl: I\u0026rsquo;ve addressed your questions below.\n  Can you provide some more details about how quota management works? For example, Team 1 and 2 both need access to a single partition in table T2, so who pays for the space occupied by this partition? Are both teams charged for it? If the partition is shared, it is accounted towards both their quota (base quota for the team that owns the partition, and imported quota for the team that imports it via a link). The reason for this is that when a namespace is moved to another datacenter, we have to account for all the quota (both imported and base) as belonging to the namespace (the data can no longer be shared directly via a link, and we will need to replicate it). Use 2 databases and copy the data in both the databases (say, the databases are DB1 and DB2 respectively) I\u0026rsquo;m not sure why this is listed as an option. What does this actually accomplish? It was just one way of achieving the same data being available in the two namespaces. You can ignore this one (smile) It\u0026rsquo;s hard for me to evaluate this claim since I\u0026rsquo;m not totally sure what is meant by \u0026ldquo;table discovery\u0026rdquo;. Can you please provide an example? However, my guess is that this is not a differentiator if you\u0026rsquo;re comparing the table links approach to views. I think Namit meant this in reference to the design option of using a single database and using scripts for quota management. In the case of views, due to views being opaque, it will be hard to see which tables are imported into the namespace. Based on the description of table links in HIVE-2989 it sounds like the user will still have to perform manual management even with table links, e.g. dropping the link that points to the partition from four days ago and adding a new link that points to the most recent partition. In this case views may actually work better since you can embed the filter condition (last three days) in the view definition instead of relying on external tools to update the table links. Maybe the description was unclear. Table links have two types: static and dynamic. Static links behave the way you describe, but dynamic links will have addition and drop of partitions when the source table (of the link) has partitions added or removed from it. I don\u0026rsquo;t think table links make this process atomic, and as I mentioned above the process of maintaining this linked set of partitions actually seems easier if you use views instead. Addressed this above - table links do keep the links updated when the source of the link has partitions added or dropped. This will be atomic since it is done in one metastore operation during an ALTER TABLE ADD/DROP PARTITION command. I agree that table links are a degenerate case of views. Since that\u0026rsquo;s the case, why is it necessary to implement table links? Why not leverage the functionality that is already provided with views? Table links allow for better accounting of imported data (views are opaque), single instancing of imports and partition pruning when the imports only have some of the partitions of the source table of the link. Given this, it seems ideal to introduce table links as a concept rather than overload views.\n  Posted by sambavi at May 31, 2012 18:09 | | Hi Carl, I explained how views fall short in the post below (response to your comments on Namit\u0026rsquo;s post). Please add any more questions you have - I can explain further if unclear.\nPosted by sambavi at May 31, 2012 18:11 | | I think Namit meant this in reference to the design option of using a single database and using scripts for quota management. In the case of views, due to views being opaque, it will be hard to see which tables are imported into the namespace. Views are not opaque. DESCRIBE FORMATTED currently includes the following information:\n# View Information\tView Original Text: SELECT value FROM src WHERE key=86\tView Expanded Text: SELECT `src`.`value` FROM `src` WHERE `src`.`key`=86\tCurrently the metastore only tracks the original and expanded text view query, but it would be straightforward to also extract and store the list of source tables that are referenced in the query when the view is created (in fact, there\u0026rsquo;s already a JIRA ticket for this (HIVE-1073), and the information is already readily available internally as described here). If we assume that it\u0026rsquo;s possible to make this change then from a quota management standpoint I don\u0026rsquo;t think the table links makes quota management easier. Maybe the description was unclear. Table links have two types: static and dynamic. Static links behave the way you describe, but dynamic links will have addition and drop of partitions when the source table (of the link) has partitions added or removed from it. I don\u0026rsquo;t think dynamic table links satisfy the use case covered by Team 2\u0026rsquo;s access requirements for table T1. Team 2 wants to see only the most recent three partitions in table T1, and my understanding of dynamic table links is that once the link is created, Team 2 will subsequently see every new partition that is added to the source table. In order to satisfy Team 2\u0026rsquo;s requirements I think you\u0026rsquo;re going to have to manually add and drop partitions from the link using the ALTER LINK ADD/DROP PARTITION command, which doesn\u0026rsquo;t sound any easier than using partitioned views. The functionality provided by dynamic links does make sense in some contexts, but the same is true for dynamic partitioned views. Why not extend the partitioned view feature to support dynamic partitions? Table links allow for better accounting of imported data (views are opaque), single instancing of imports and partition pruning when the imports only have some of the partitions of the source table of the link. Given this, it seems ideal to introduce table links as a concept rather than overload views. I addressed the \u0026ldquo;views are opaque\u0026rdquo; argument above. I\u0026rsquo;m having trouble following the rest of the sentence. What does \u0026ldquo;single instancing of imports\u0026rdquo; mean? If possible can you provide an example in terms of table links and partitioned views?\nPosted by cwsteinbach at May 31, 2012 22:27 | | Going back to my example: Suppose there are 2 teams which want to use the same physical warehouse. Team 1 wants to use the following: (let us say that each table is partitioned by date) T1 (all partitions) T2 (all partitions) T3 (all partitions) Team 2 wants to use the following: T1 (partitions for the last 3 days) T2 (partition for a fixed day: say May 29' 2012) T4 (all partitions) Using the current hive architecture, we can perform the following:\n Use a single database and have scripts for quota Use 2 databases and copy the data in both the databases (say, the databases are DB1 and DB2 respectively) Use 2 databases, and use views in database 2 (to be used to team 2).  We have discarded the first 2 approaches above, so let us discuss how will we use approach 3 (specifically for T1). Team 2 will create the view: create view V1T1 as select * from DB1.T1 Now, whenever a partition gets added in DB1.T1, someone (a hook or something - outside hive) needs to add the corresponding partition in V1T1. That extra layer needs to make sure that the new partition in V1T1 is part of the inputs (may be necessary for auditing etc.) Hive metastore has no knowledge of this dependency (view partition -\u0026gt; table partition), and it is maintained in multiple places (for possibly different teams). The same argument applies when a partition gets dropped from DB1.T1. By design, there is no one-to-one dependency between a table partition and a view partition, and we do not want to create such a dependency. The view may depend on multiple tables/partitions. The views in hive are not updatable. By design, the schema of the view and the underlying table(s) can be different. Links provide the above functionality. If I understand right, you are proposing to extend views to support the above functionality. We will end up with a very specific model for a specific type of views, which are not like normal hive views. That would be more confusing, in my opinion.\nPosted by namit.jain at Jun 01, 2012 14:25 | | Please comment - we haven\u0026rsquo;t gotten any updates on the wiki as well as the jira https://issues.apache.org/jira/browse/HIVE-2989\nPosted by namit.jain at Jun 02, 2012 19:35 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/links_27847416/","tags":null,"title":"Apache Hive : Links"},{"categories":null,"contents":"Apache Hive : ListBucketing  Goal  Basic Partitioning List Bucketing Skewed Table vs. List Bucketing Table List Bucketing Validation  DDL DML Alter Table Concatenate   Hive Enhancements  Create Table Alter Table  Alter Table Skewed Alter Table Not Skewed Alter Table Not Stored as Directories Alter Table Set Skewed Location   Design     Implementation  Goal The top level problem is as follows:\nThere are many tables of the following format:\n create table T(a, b, c, ....., x) partitioned by (ds);  and the following queries need to be performed efficiently:\n select ... from T where x = 10;  The cardinality of \u0026lsquo;x\u0026rsquo; is in 1000\u0026rsquo;s per partition of T. Moreover, there is a skew for the values of \u0026lsquo;x\u0026rsquo;. In general, there are ~10 values of \u0026lsquo;x\u0026rsquo; which have a very large skew, and the remaining values of \u0026lsquo;x\u0026rsquo; have a small cardinality. Also, note that this mapping (values of \u0026lsquo;x\u0026rsquo; with a high cardinality) can change daily.\nThe above requirement can be solved in the following ways:\nBasic Partitioning Create a partition per value of \u0026lsquo;x\u0026rsquo;.\n create table T(a,b,c, .......) partitioned by (ds, x); Advantages  Existing Hive is good enough.   Disadvantages  HDFS scalability: Number of files in HDFS increases. HDFS scalability: Number of intermediate files in HDFS increases. For example if there are 1000 mappers and 1000 partitions, and each mapper gets at least 1 row for each key, we will end up creating 1 million intermediate files. Metastore scalability: Will the metastore scale with the number of partitions.    List Bucketing The basic idea here is as follows: Identify the keys with a high skew. Have one directory per skewed key, and the remaining keys go into a separate directory. This mapping is maintained in the metastore at a table or partition level, and is used by the Hive compiler to do input pruning. The list of skewed keys is stored at the table level. (Note that initially this list can be supplied by the client periodically, and eventually it can be updated when a new partition is being loaded.)\nFor example, the table maintains the list of skewed keys for \u0026lsquo;x\u0026rsquo;: 6, 20, 30, 40. When a new partition is being loaded, it will create 5 directories (4 directories for skewed keys + 1 default directory for all the remaining keys). The table/partition that got loaded will have the following mapping: 6,20,30,40,others. This is similar to hash bucketing currently, where the bucket number determines the file number. Since the skewed keys need not be consecutive, the entire list of skewed keys need be stored in each table/partition.\nWhen a query of the form\n select ... from T where ds = '2012-04-15' and x = 30;  is issued, the Hive compiler will only use the directory corresponding to x=30 for the map-reduce job.\nFor a query of the form\n select ... from T where ds = '2012-04-15' and x = 50;  the Hive compiler will only use the file corresponding to x=others for the map-reduce job.\nThis approach is good under the following assumptions:\n Each partition\u0026rsquo;s skewed keys account for a significant percentage of the total data. In the above example, if the skewed keys (6,20,30 and 40) only occupy a small percentage of the data (say 20%), the queries of the form x=50 will still need to scan the remaining data (~80%). The number of skewed keys per partition is fairly small. This list is stored in the metastore, so it does not make sense to store 1 million skewed keys per partition in the metastore.  This approach can be extended to the scenario when there are more than one clustered key. Say we want to optimize the queries of the form\n select ... from T where x = 10 and y = 'b'; Extend the above approach. For each skewed value of (x,y), store the file offset. So, the metastore will have the mapping like: (10, \u0026lsquo;a\u0026rsquo;) -\u0026gt; 1, (10, \u0026lsquo;b\u0026rsquo;) -\u0026gt; 2, (20, \u0026lsquo;c\u0026rsquo;) -\u0026gt; 3, (others) -\u0026gt; 4.  A query with all the clustering keys specified can be optimized easily. However, queries with some of the clustering keys specified:\n   select ... from T where x = 10;  select ... from T where y = 'b';      can only be used to prune very few directories. It does not really matter if the prefix of the clustering keys is specified or not. For example for x=10, the Hive compiler can prune the file corresponding to (20, \u0026lsquo;c\u0026rsquo;). And for y=\u0026lsquo;b\u0026rsquo;, the files corresponding to (10, \u0026lsquo;a\u0026rsquo;) and (20, \u0026lsquo;c\u0026rsquo;) can be pruned. Hashing for others does not really help, when the complete key is not specified.\nThis approach does not scale in the following scenarios:\n The number of skewed keys is very large. This creates a problem for metastore scalability. In most of the cases, the number of clustered keys is more than one, and in the query, all the clustered keys are not specified.  Skewed Table vs. List Bucketing Table  Skewed Table is a table which has skewed information. List Bucketing Table is a skewed table. In addition, it tells Hive to use the list bucketing feature on the skewed table: create sub-directories for skewed values.  A normal skewed table can be used for skewed join, etc. (See the Skewed Join Optimization design document.) You don\u0026rsquo;t need to define it as a list bucketing table if you don\u0026rsquo;t use the list bucketing feature.\nList Bucketing Validation Mainly due to its sub-directory nature, list bucketing can\u0026rsquo;t coexist with some features.\nDDL Compilation error will be thrown if list bucketing table coexists with\n normal bucketing (clustered by, tablesample, etc.) external table \u0026ldquo;load data ...\u0026rdquo; CTAS (Create Table As Select) queries  DML Compilation error will be thrown if list bucketing table coexists with\n \u0026ldquo;insert into\u0026rdquo; normal bucketing (clustered by, tablesample, etc.) external table non-RCfile due to merge non-partitioned table  Partitioning value should not be the same as a default list bucketing directory name.\nAlter Table Concatenate Compilation error will be thrown if list bucketing table coexists with\n non-RCfile external table for alter table  Hive Enhancements Hive needs to be extended to support the following:\nCreate Table CREATE TABLE \u0026lt;T\u0026gt; (SCHEMA) SKEWED BY (keys) ON ('c1', 'c2') [STORED AS DIRECTORIES]; The table will be a skewed table. Skewed information will be created for all partitions.\nFor example:\n create table T (c1 string, c2 string) skewed by (c1) on ('x1') stored as directories; create table T (c1 string, c2 string, c3 string) skewed by (c1, c2) on (('x1', 'x2'), ('y1', 'y2')) stored as directories;  \u0026lsquo;STORED AS DIRECTORIES\u0026rsquo; is an optional parameter. It tells Hive that it is not only a skewed table but also the list bucketing feature should apply: create sub-directories for skewed values.\nAlter Table Alter Table Skewed ALTER TABLE \u0026lt;T\u0026gt; (SCHEMA) SKEWED BY (keys) ON ('c1', 'c2') [STORED AS DIRECTORIES]; The above is supported in table level only and not partition level.\nIt will\n convert a table from a non-skewed table to a skewed table, or else alter a skewed table\u0026rsquo;s skewed column names and/or skewed values.  It will impact\n partitions created after the alter statement but not partitions created before the alter statement.  Alter Table Not Skewed ALTER TABLE \u0026lt;T\u0026gt; (SCHEMA) NOT SKEWED; The above will\n turn off the \u0026ldquo;skewed\u0026rdquo; feature from a table make a table non-skewed turn off the \u0026ldquo;list bucketing\u0026rdquo; feature since a list bucketing table is a skewed table also.  It will impact\n partitions created after the alter statement but not partitions created before the alter statement.  Alter Table Not Stored as Directories ALTER TABLE \u0026lt;T\u0026gt; (SCHEMA) NOT STORED AS DIRECTORIES; The above will\n turn off \u0026ldquo;list bucketing\u0026rdquo; not turn off the \u0026ldquo;skewed\u0026rdquo; feature from table since a \u0026ldquo;skewed\u0026rdquo; table can be a normal \u0026ldquo;skewed\u0026rdquo; table without list bucketing.  Alter Table Set Skewed Location ALTER TABLE \u0026lt;T\u0026gt; (SCHEMA) SET SKEWED LOCATION (key1=\u0026quot;loc1\u0026quot;, key2=\u0026quot;loc2\u0026quot;); The above will change the list bucketing location map.\nDesign When such a table is being loaded, it would be good to create a sub-directory per skewed key. The infrastructure similar to dynamic partitions can be used.\nAlter table \u0026lt;T\u0026gt; partition \u0026lt;P\u0026gt; concatenate; needs to be changed to merge files per directory.\nImplementation Version information\nList bucketing was added in Hive 0.10.0 and 0.11.0.\nHIVE-3026 is the root JIRA ticket for the list bucketing feature. It has links to additional JIRA tickets which implement list bucketing in Hive, including:  HIVE-3554: Hive List Bucketing – Query logic (release 0.10.0) HIVE-3649: Hive List Bucketing - enhance DDL to specify list bucketing table (release 0.10.0) HIVE-3072: Hive List Bucketing - DDL support (release 0.10.0) HIVE-3073: Hive List Bucketing - DML support (release 0.11.0)  For more information, see Skewed Tables in the DDL document.\nComments:            Does this feature require any changes to the metastore? If so can you please describe them? Thanks.    Posted by cwsteinbach at Jun 11, 2012 15:13 | | Please also describe any changes that will be made to public APIs including the following:\n The metastore and/or HiveServer Thrift interfaces (note that this includes overloading functions that are already included in the current Thrift interfaces, as well as modifying or adding new Thrift structs/objects). Hive Query Language, including new commands, extensions to existing commands, or changes to the output generated by commands (e.g. DESCRIBE FORMATTED TABLE). New configuration properties. Modifications to any of the public plugin APIs including SerDes and Hook/Listener interfaces,  Also, if this feature requires any changes to the Metastore schema, those changes should be described in this document. Finally, please describe your plan for implementing this feature and getting it committed. Will it go in as a single patch or be split into several different patches.\nPosted by cwsteinbach at Jun 12, 2012 01:47 | | Yes, it requires metastore change. We want to store the following information in metastore:\n skewed column names skewed column values mappings from skewed column value to directories. The above 3 will be added to MStorageDescriptor.java etc  Posted by gangtimliu at Jun 14, 2012 12:47 | | Yes, I will update document with any changes in the areas you mention. Here is plan:\n Implement End-to-end feature for single skewed column (DDL+DML) and go in as a single patch. Implement End-to-end feature for multiple skewed columns (DDL+DML) and go in as a single patch. Implement follow-ups and go in as a single patch. The #3 is a slot for those not critical but nice to have and not in #1 \u0026amp; #2 due to resource constraints etc.  Posted by gangtimliu at Jun 14, 2012 12:55 | | It wasn\u0026rsquo;t clear to me from this wiki page what the benefit is of storing the skewed values \u0026ldquo;as directories\u0026rdquo; over just storing them as files as regular skew tables do? Tim, could you please elaborate on that?\nPosted by mgrover@oanda.com at Nov 07, 2012 11:23 | | Different terms but refer to the same thing: create sub directory for skewed value and store record in file. Note that regular skew table doesn\u0026rsquo;t create sub directory. It\u0026rsquo;s different from non-skewed table because it has meta-data of skewed column name and values so that feature like skewed join can leverage it. Only list bucketing table creates sub directory for skewed-value. We use \u0026ldquo;stored as directories\u0026rdquo; to mark it. Hope it helps.\nPosted by gangtimliu at Nov 07, 2012 12:49 | | Tim, thanks for responding but I am still missing something. I re-read the wiki page and here is my understanding. Please correct me if I am wrong. Let\u0026rsquo;s take a hand-wavy example. Skewed table: create table t1 (x string) skewed by (error) on (\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;) partitioned by dt location \u0026lsquo;/user/hive/warehouse/t1\u0026rsquo;; will create the following files: /user/hive/warehouse/t1/dt=something/x=a.txt /user/hive/warehouse/t1/dt=something/x=b.txt /user/hive/warehouse/t1/dt=something/default List bucketing table: create table t2 (x string) skewed by (error) on (\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;) partitioned by dt location \u0026lsquo;/user/hive/warehouse/t2\u0026rsquo; ; will create the following files: /user/hive/warehouse/t2/dt=something/x=a/data.txt /user/hive/warehouse/t2/dt=something/x=b/data.txt /user/hive/warehouse/t2/dt=something/default/data.txt Is that correct? In that case, why would a user ever choose to create sub-directories? Skewed joins would perform just well for regular skewed tables or list bucketing tables. Given that list bucketing introduces sub-directories it imposes restrictions on what other things users can and cannot do while regular skewed tables don\u0026rsquo;t. So what would be someone\u0026rsquo;s motivation to choose list bucketing over skewed tables?\nPosted by mgrover@oanda.com at Nov 09, 2012 00:11 | | sorry for confusion. wiki requires polish to make it clear. I assume t2 has stored as directories. t1 doesn\u0026rsquo;t have sub-directories but t2 has sub-directories. Directory structure looks like: /user/hive/warehouse/t1/dt=something/data.txt /user/hive/warehouse/t2/dt=something/x=a/data.txt /user/hive/warehouse/t2/dt=something/x=b/data.txt /user/hive/warehouse/t2/dt=something/default/data.txt \u0026ldquo;stored as directories\u0026rdquo; tells hive to create sub-directories. what\u0026rsquo;s use case of t1? t1 can be used for skewed join since t1 has skewed column and value information.\nPosted by gangtimliu at Nov 09, 2012 01:55 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/listbucketing_27846854/","tags":null,"title":"Apache Hive : ListBucketing"},{"categories":null,"contents":"Apache Hive : Literals Literals Integral types Integral literals are assumed to be INT by default, unless the number exceeds the range of INT in which case it is interpreted as a BIGINT, or if one of the following postfixes is present on the number.\n   Type Postfix Example     TINYINT Y 100Y   SMALLINT S 100S   BIGINT L 100L    String types String literals can be expressed with either single quotes (') or double quotes (\u0026quot;). Hive uses C-style escaping within the strings.\nFloating point types Floating point literals are assumed to be DOUBLE. Scientific notation is not yet supported.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/literals_27829682/","tags":null,"title":"Apache Hive : Literals"},{"categories":null,"contents":"Apache Hive : LLAP  Overview Persistent Daemon Execution Engine Query Fragment Execution I/O Caching Workload Management ACID Support Security Monitoring Web Services SLIDER on YARN Deployment LLAP Status Resources  Live Long And Process (LLAP) functionality was added in Hive 2.0 (HIVE-7926 and associated tasks). HIVE-9850 links documentation, features, and issues for this enhancement.\nFor configuration of LLAP, see the LLAP Section of Configuration Properties.\nOverview Hive has become significantly faster thanks to various features and improvements that were built by the community in recent years, including Tez and Cost-based-optimization. The following were needed to take Hive to the next level:\n Asynchronous spindle-aware IO Pre-fetching and caching of column chunks Multi-threaded JIT-friendly operator pipelines  Also known as Live Long and Process, LLAP provides a hybrid execution model. It consists of a long-lived daemon which replaces direct interactions with the HDFS DataNode, and a tightly integrated DAG-based framework.\nFunctionality such as caching, pre-fetching, some query processing and access control are moved into the daemon. Small/short queries are largely processed by this daemon directly, while any heavy lifting will be performed in standard YARN containers.\nSimilar to the DataNode, LLAP daemons can be used by other applications as well, especially if a relational view on the data is preferred over file-centric processing. The daemon is also open through optional APIs (e.g., InputFormat) that can be leveraged by other data processing frameworks as a building block.\nLast, but not least, fine-grained column-level access control – a key requirement for mainstream adoption of Hive – fits nicely into this model.\nThe diagram below shows an example execution with LLAP. Tez AM orchestrates overall execution. The initial stage of the query is pushed into LLAP. In the reduce stage, large shuffles are performed in separate containers. Multiple queries and applications can access LLAP concurrently.\nPersistent Daemon To facilitate caching and JIT optimization, and to eliminate most of the startup costs, a daemon runs on the worker nodes on the cluster. The daemon handles I/O, caching, and query fragment execution.\n These nodes are stateless. Any request to an LLAP node contains the data location and metadata. It processes local and remote locations; locality is the caller’s responsibility (YARN). Recovery/resiliency. Failure and recovery is simplified because any data node can still be used to process any fragment of the input data. The Tez AM can thus simply rerun failed fragments on the cluster. Communication between nodes. LLAP nodes are able to share data (e.g., fetching partitions, broadcasting fragments). This is realized with the same mechanisms used in Tez.  Execution Engine LLAP works within existing, process-based Hive execution to preserve the scalability and versatility of Hive. It does not replace the existing execution model but rather enhances it.\n The daemons are optional. Hive can work without them and also is able to bypass them even if they are deployed and operational. Feature parity with regard to language features is maintained. External orchestration and execution engines. LLAP is not an execution engine (like MapReduce or Tez). Overall execution is scheduled and monitored by an existing Hive execution engine (such as Tez) transparently over both LLAP nodes, as well as regular containers. Obviously, LLAP level of support depends on each individual execution engine (starting with Tez). MapReduce support is not planned, but other engines may be added later. Other frameworks like Pig also have the choice of using LLAP daemons. Partial execution. The result of the work performed by an LLAP daemon can either form part of the result of a Hive query, or be passed on to external Hive tasks, depending on the query. Resource Management. YARN remains responsible for the management and allocation of resources. The YARN container delegation model is used to allow the transfer of allocated resources to LLAP. To avoid the limitations of JVM memory settings, cached data is kept off-heap, as well as large buffers for processing (e.g., group by, joins). This way, the daemon can use a small amount of memory, and additional resources (i.e., CPU and memory) will be assigned based on workload.  Query Fragment Execution For partial execution as described above, LLAP nodes execute “query fragments” such as filters, projections, data transformations, partial aggregates, sorting, bucketing, hash joins/semi-joins, etc. Only Hive code and blessed UDFs are accepted in LLAP. No code is localized and executed on the fly. This is done for stability and security reasons.\n Parallel execution. An LLAP node allows parallel execution for multiple query fragments from different queries and sessions. Interface. Users can access LLAP nodes directly via client API. They are able to specify relational transformations and read data via record-oriented streams.  I/O The daemon off-loads I/O and transformation from compressed format to separate threads. The data is passed on to execution as it becomes ready, so the previous batches can be processed while the next ones are being prepared. The data is passed to execution in a simple RLE-encoded columnar format that is ready for vectorized processing; this is also the caching format, with the intent to minimize copying between I/O, cache, and execution.\n Multiple file formats. I/O and caching depend on some knowledge of the underlying file format (especially if it is to be done efficiently). Therefore, similar to Vectorization work, different file formats will be supported through plugins specific to each format (starting with ORC). Additionally, a generic, less-efficient plugin may be added that supports any Hive input format. The plugins have to maintain metadata and transform the raw data to column chunks. Predicates and bloom filters. SARGs and bloom filters are pushed down to storage layer, if they are supported.  Caching The daemon caches metadata for input files, as well as the data. The metadata and index information can be cached even for data that is not currently cached. Metadata is stored in process in Java objects; cached data is stored in the format described in the I/O section, and kept off-heap (see Resource management).\n Eviction policy. The eviction policy is tuned for analytical workloads with frequent (partial) table-scans. Initially, a simple policy like LRFU is used. The policy is pluggable. Caching granularity. Column-chunks are the unit of data in the cache. This achieves a compromise between low-overhead processing and storage efficiency. The granularity of the chunks depends on the particular file format and execution engine (Vectorized Row Batch size, ORC stripe, etc.).  A bloom filter is automatically created to provide Dynamic Runtime Filtering.\nWorkload Management YARN is used to obtain resources for different workloads. Once resources (CPU, memory, etc.) have been obtained from YARN for a specific workload, the execution engine can choose to delegate these resources to LLAP, or to launch Hive executors in separate processes. Resource enforcement via YARN has the advantage of ensuring that nodes do not get overloaded, either by LLAP or by other containers. The daemons themselves is under YARN’s control.\nACID Support LLAP is aware of transactions. The merging of delta files to produce a certain state of the tables is performed before the data is placed in cache.\nMultiple versions are possible and the request specifies which version is to be used. This has the benefit of doing the merge asynchronously and only once for cached data, thus avoiding the hit on the operator pipeline.\nSecurity LLAP servers are a natural place to enforce access control at a more fine-grained level than “per file”. Since the daemons know which columns and records are processed, policies on these objects can be enforced. This is not intended to replace the current mechanisms, but rather to enhance and open them up to other applications as well.\nMonitoring Configurations for LLAP monitoring are stored in resources.json, appConfig.json, metainfo.xml which are embedded into templates.py used by Slider. LLAP Monitor Daemon runs on YARN container, similar to LLAP Daemon, and listens on the same port. The LLAP Metrics Collection Server collects JMX metrics from all LLAP Daemons periodically. The list of LLAP Daemons are extracted from the Zookeeper server which launched in the cluster. Web Services HIVE-9814 introduces the following web services:\nJSON JMX data - /jmx\nJVM Stack Traces of all threads - /stacks\nXML Configuration from llap-daemon-site - /conf HIVE-13398 introduces the following web services:\nLLAP Status - /status\nLLAP Peers - /peers  /status example\ncurl localhost:15002/status { \u0026quot;status\u0026quot; : \u0026quot;STARTED\u0026quot;, \u0026quot;uptime\u0026quot; : 139093, \u0026quot;build\u0026quot; : \u0026quot;2.1.0-SNAPSHOT from 77474581df4016e3899a986e079513087a945674 by gopal source checksum a9caa5faad5906d5139c33619f1368bb\u0026quot; }  /peers example\ncurl localhost:15002/peers { \u0026quot;dynamic\u0026quot; : true, \u0026quot;identity\u0026quot; : \u0026quot;718264f1-722e-40f1-8265-ac25587bf336\u0026quot;, \u0026quot;peers\u0026quot; : [ { \u0026quot;identity\u0026quot; : \u0026quot;940d6838-4dd7-4e85-95cc-5a6a2c537c04\u0026quot;, \u0026quot;host\u0026quot; : \u0026quot;sandbox121.hortonworks.com\u0026quot;, \u0026quot;management-port\u0026quot; : 15004, \u0026quot;rpc-port\u0026quot; : 15001, \u0026quot;shuffle-port\u0026quot; : 15551, \u0026quot;resource\u0026quot; : { \u0026quot;vcores\u0026quot; : 24, \u0026quot;memory\u0026quot; : 128000 }, \u0026quot;host\u0026quot; : \u0026quot;sandbox121.hortonworks.com\u0026quot; }, ] }  SLIDER on YARN Deployment LLAP can be deployed via Slider, which bypasses node installation and related complexities (HIVE-9883).\nLLAP Status AMBARI-16149 introduces LLAP app status, available with HiveServer2.\nExample usage.\n/current/hive-server2-hive2/bin/hive --service llapstatus --name {llap_app_name} [-f] [-w] [-i] [-t]  -f,--findAppTimeout \u0026lt;findAppTimeout\u0026gt; Amount of time(s) that the tool will sleep to wait for the YARN application to start. negative values=wait forever, 0=Do not wait. default=20s -H,--help Print help information --hiveconf \u0026lt;property=value\u0026gt; Use value for given property. Overridden by explicit parameters -i,--refreshInterval \u0026lt;refreshInterval\u0026gt; Amount of time in seconds to wait until subsequent status checks in watch mode. Valid only for watch mode. (Default 1s) -n,--name \u0026lt;name\u0026gt; LLAP cluster name -o,--outputFile \u0026lt;outputFile\u0026gt; File to which output should be written (Default stdout) -r,--runningNodesThreshold \u0026lt;runningNodesThreshold\u0026gt; When watch mode is enabled (-w), wait until the specified threshold of nodes are running (Default 1.0 which means 100% nodes are running) -t,--watchTimeout \u0026lt;watchTimeout\u0026gt; Exit watch mode if the desired state is not attained until the specified timeout. (Default 300s) -w,--watch Watch mode waits until all LLAP daemons are running or subset of the nodes are running (threshold can be specified via -r option) (Default wait until all nodes are running) Version information\nThe findAppTimeout option was added in release 2.1.0 with HIVE-13643:-f or --``findAppTimeout.\nThe watch and running nodes options were added in release 2.2.0 with HIVE-15217 and HIVE-15651: -w or --watch, -i or --refreshInterval, -t or --watchTimeout, and -r or --runningNodesThreshold.\n Resources LLAP Design Document\nHive Contributor Meetup Presentation\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nSave\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/llap_62689557/","tags":null,"title":"Apache Hive : LLAP"},{"categories":null,"contents":"Apache Hive : Locking Hive Concurrency Model  Hive Concurrency Model  Use Cases Turn Off Concurrency Debugging Configuration   Locking in Hive Transactions  Use Cases Concurrency support (http://issues.apache.org/jira/browse/HIVE-1293) is a must in databases and their use cases are well understood. At a minimum, we want to support concurrent readers and writers whenever possible. It would be useful to add a mechanism to discover the current locks which have been acquired. There is no immediate requirement to add an API to explicitly acquire any locks, so all locks would be acquired implicitly.\nThe following lock modes will be defined in hive (Note that Intent lock is not needed).\n Shared (S) Exclusive (X)  As the name suggests, multiple shared locks can be acquired at the same time, whereas X lock blocks all other locks.\nThe compatibility matrix is as follows:\n| Lock Compatibility | Existing Lock | | S | X | | Requested Lock | S | True | False | | X | False | False |\nFor some operations, locks are hierarchical in nature \u0026ndash; for example for some partition operations, the table is also locked (to make sure that the table cannot be dropped while a new partition is being created).\nThe rational behind the lock mode to acquire is as follows:\nFor a non-partitioned table, the lock modes are pretty intuitive. When the table is being read, a S lock is acquired, whereas an X lock is acquired for all other operations (insert into the table, alter table of any kind etc.)\nFor a partitioned table, the idea is as follows:\nA \u0026lsquo;S\u0026rsquo; lock on table and relevant partition is acquired when a read is being performed. For all other operations, an \u0026lsquo;X\u0026rsquo; lock is taken on the partition. However, if the change is only applicable to the newer partitions, a \u0026lsquo;S\u0026rsquo; lock is acquired on the table, whereas if the change is applicable to all partitions, a \u0026lsquo;X\u0026rsquo; lock is acquired on the table. Thus, older partitions can be read and written into, while the newer partitions are being converted to RCFile. Whenever a partition is being locked in any mode, all its parents are locked in \u0026lsquo;S\u0026rsquo; mode.\nBased on this, the lock acquired for an operation is as follows:\n   Hive Command Locks Acquired     select .. T1 partition P1 S on T1, T1.P1   insert into T2(partition P2) select .. T1 partition P1 S on T2, T1, T1.P1 and X on T2.P2   insert into T2(partition P.Q) select .. T1 partition P1 S on T2, T2.P, T1, T1.P1 and X on T2.P.Q   alter table T1 rename T2 X on T1   alter table T1 add cols X on T1   alter table T1 replace cols X on T1   alter table T1 change cols X on T1   alter table T1 concatenate X on T1   alter table T1 add partition P1 S on T1, X on T1.P1   alter table T1 drop partition P1 S on T1, X on T1.P1   alter table T1 touch partition P1 S on T1, X on T1.P1   alter table T1 set serdeproperties S on T1   alter table T1 set serializer S on T1   alter table T1 set file format S on T1   alter table T1 set tblproperties X on T1   alter table T1 partition P1 concatenate X on T1.P1   drop table T1 X on T1    In order to avoid deadlocks, a very simple scheme is proposed here. All the objects to be locked are sorted lexicographically, and the required mode lock is acquired. Note that in some cases, the list of objects may not be known \u0026ndash; for example in case of dynamic partitions, the list of partitions being modified is not known at compile time \u0026ndash; so, the list is generated conservatively. Since the number of partitions may not be known, an exclusive lock is supposed to be taken (but currently not due to HIVE-3509 bug) on the table, or the prefix that is known.\nTwo new configurable parameters will be added to decide the number of retries for the lock and the wait time between each retry. If the number of retries are really high, it can lead to a live lock. Look at ZooKeeper recipes (http://hadoop.apache.org/zookeeper/docs/r3.1.2/recipes.html#sc_recipes_Locks) to see how read/write locks can be implemented using the zookeeper apis. Note that instead of waiting, the lock request will be denied. The existing locks will be released, and all of them will be retried after the retry interval.\nThe recipe listed above will not work as specified, because of the hierarchical nature of locks.\nThe \u0026lsquo;S\u0026rsquo; lock for table T is specified as follows:\n Call create( ) to create a node with pathname \u0026ldquo;/warehouse/T/read-\u0026rdquo;. This is the lock node used later in the protocol. Make sure to set the sequence and ephemeral flag. Call getChildren( ) on the lock node without setting the watch flag. If there is a child with a pathname starting with \u0026ldquo;write-\u0026rdquo; and a lower sequence number than the one obtained, the lock cannot be acquired. Delete the node created in the first step and return. Otherwise the lock is granted.  The \u0026lsquo;X\u0026rsquo; lock for table T is specified as follows:\n Call create( ) to create a node with pathname \u0026ldquo;/warehouse/T/write-\u0026rdquo;. This is the lock node used later in the protocol. Make sure to set the sequence and ephemeral flag. Call getChildren( ) on the lock node without setting the watch flag. If there is a child with a pathname starting with \u0026ldquo;read-\u0026rdquo; or \u0026ldquo;write-\u0026rdquo; and a lower sequence number than the one obtained, the lock cannot be acquired. Delete the node created in the first step and return. Otherwise the lock is granted.  The proposed scheme starves the writers for readers. In case of long readers, it may lead to starvation for writers.\nThe default Hive behavior will not be changed, and concurrency will not be supported.\nTurn Off Concurrency You can turn off concurrency by setting the following variable to false: hive.support.concurrency.\nDebugging You can see the locks on a table by issuing the following command:\n SHOW LOCKS \u0026lt;TABLE_NAME\u0026gt;; SHOW LOCKS \u0026lt;TABLE_NAME\u0026gt; EXTENDED; SHOW LOCKS \u0026lt;TABLE_NAME\u0026gt; PARTITION (\u0026lt;PARTITION_DESC\u0026gt;); SHOW LOCKS \u0026lt;TABLE_NAME\u0026gt; PARTITION (\u0026lt;PARTITION_DESC\u0026gt;) EXTENDED;  See also EXPLAIN LOCKS.\nConfiguration Configuration properties for Hive locking are described in Locking.\nLocking in Hive Transactions Hive 0.13.0 adds transactions with row-level ACID semantics, using a new lock manager. For more information, see:\n ACID and Transactions in Hive Lock Manager  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/locking_27362050/","tags":null,"title":"Apache Hive : Locking"},{"categories":null,"contents":"Apache Hive : Managed vs. External Tables Hive fundamentally knows two different types of tables:\n Managed (Internal) External  Introduction This document lists some of the differences between the two but the fundamental difference is that Hive assumes that it owns the data for managed tables. That means that the data, its properties and data layout will and can only be changed via Hive command. The data still lives in a normal file system and nothing is stopping you from changing it without telling Hive about it. If you do though it violates invariants and expectations of Hive and you might see undefined behavior.\nAnother consequence is that data is attached to the Hive entities. So, whenever you change an entity (e.g. drop a table) the data is also changed (in this case the data is deleted). This is very much like with traditional RDBMS where you would also not manage the data files on your own but use a SQL-based access to do so.\nFor external tables Hive assumes that it does not manage the data.\nManaged or external tables can be identified using the DESCRIBE FORMATTED table_name command, which will display either MANAGED_TABLE or EXTERNAL_TABLE depending on table type.\nStatistics can be managed on internal and external tables and partitions for query optimization. Feature comparison This means that there are lots of features which are only available for one of the two table types but not the other. This is an incomplete list of things:\n ARCHIVE/UNARCHIVE/TRUNCATE/MERGE/CONCATENATE only work for managed tables DROP deletes data for managed tables while it only deletes metadata for external ones ACID/Transactional only works for managed tables Query Results Caching only works for managed tables Only the RELY constraint is allowed on external tables Some Materialized View features only work on managed tables  Managed tables A managed table is stored under the hive.metastore.warehouse.dir path property, by default in a folder path similar to /user/hive/warehouse/databasename.db/tablename/. The default location can be overridden by the location property during table creation. If a managed table or partition is dropped, the data and metadata associated with that table or partition are deleted. If the PURGE option is not specified, the data is moved to a trash folder for a defined duration.\nUse managed tables when Hive should manage the lifecycle of the table, or when generating temporary tables.\nExternal tables An external table describes the metadata / schema on external files. External table files can be accessed and managed by processes outside of Hive. External tables can access data stored in sources such as Azure Storage Volumes (ASV) or remote HDFS locations. If the structure or partitioning of an external table is changed, an MSCK REPAIR TABLE table_name statement can be used to refresh metadata information.\nUse external tables when files are already present or in remote locations, and the files should remain even if the table is dropped.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/managed-vs-external-tables_95654003/","tags":null,"title":"Apache Hive : Managed vs. External Tables"},{"categories":null,"contents":"Apache Hive : Manual Installation  Installing, configuring and running Hive  Prerequisites Install the prerequisites  Java 8 Maven: Protobuf Hadoop Tez   Extra hadoop configurations to make everything working Installing Hive from a Tarball Installing from Source Code Installing with old version hadoop(greater than or equal 3.1.0) Next Steps Beeline CLI Hive Metastore HCatalog and WebHCat  HCatalog WebHCat (Templeton)      Installing, configuring and running Hive You can install a stable release of Hive by downloading and unpacking a tarball, or you can download the source code and build Hive using Maven (release 3.6.3 and later).\nPrerequisites  Java 8. Maven 3.6.3 Protobuf 2.5 Hadoop 3.3.6 (As a preparation, configure it in single-node cluster, pseudo-distributed mode) Tez. The default is MapReduce but we will change the execution engine to Tez. Hive is commonly used in production Linux environment. Mac is a commonly used development environment. The instructions in this document are applicable to Linux and Mac.  Install the prerequisites Java 8 Building Hive requires JDK 8 installed. Some notes in case you have ARM chipset (Apple M1 or later).\nYou will have to build protobuf 2.5 later. And it doesn\u0026rsquo;t compile with ARM JDK. So we will install intel architecture\u0026rsquo;s Java with brew and configure maven with this. It will enable us to compile protobuf. JDK install on apple ARM: brew install homebrew/cask-versions/adoptopenjdk8 --cask brew untap adoptopenjdk/openjdk Maven: Just install maven and configure the JAVA_HOME properly. Notes for arm: after a proper configuration, you should see something like this:\nmvn -version Apache Maven 3.6.3 (cecedd343002696d0abb50b32b541b8a6ba2883f) Maven home: /Users/yourusername/programs/apache-maven-3.6.3 Java version: 1.8.0_292, vendor: AdoptOpenJDK, runtime: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/jre Default locale: en_HU, platform encoding: UTF-8 OS name: \u0026quot;mac os x\u0026quot;, version: \u0026quot;10.16\u0026quot;, arch: \u0026quot;x86_64\u0026quot;, family: \u0026quot;mac\u0026quot; As you can see, even if it is an arm processor, maven thinks the architecture is Intel based.\nProtobuf You have to download and compile protobuf. And also, install it into the local maven repository. Protobuf 2.5.0 is not ready for ARM. On this chipset, you will need to do some extra steps.\nwget https://github.com/google/protobuf/releases/download/v2.5.0/protobuf-2.5.0.tar.bz2 tar -xvf protobuf-2.5.0.tar.bz2 cd protobuf-2.5.0 ./configure On ARM, edit the src/google/protobuf/stubs/platform_macros.h and add arm to the part, processor architecture detection, after the last elif branch: #elif defined(__arm64__) #define GOOGLE_PROTOBUF_ARCH_ARM 1 #define GOOGLE_PROTOBUF_ARCH_64_BIT 1 Now, you can compile and install protobuf:\nmake make check sudo make install You can validate your install:\nprotoc --version Hadoop Firstly, move through the instructions on the official documentation, single-node, pseudo-distributed configuration: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation.\nAfter that, set up HADOOP_HOME:\nexport HADOOP_HOME=/yourpathtohadoop/hadoop-3.3.6 Tez Tez will require some additional steps. Hadoop uses a tez tarball but it expects it in other compressed directory structure than it is released. So we will extract the tarball and compress again. And also, we will put the extracted jars into hdfs. After that we set the necessary environment variables.\nDownload tez, extract and re-compress the tar: wget https://dlcdn.apache.org/tez/0.10.3/apache-tez-0.10.3-bin.tar.gz tar -xzvf apache-tez-0.10.3-bin.tar.gz cd apache-tez-0.10.3-bin tar zcvf ../apache-tez-0.10.3-bin.tar.gz * \u0026amp;\u0026amp; cd .. Add the necessary tez files to hdfs\n$HADOOP_HOME/sbin/start-dfs.sh\t# start hdfs $HADOOP_HOME/bin/hadoop fs -mkdir -p /apps/tez $HADOOP_HOME/bin/hadoop fs -put apache-tez-0.10.3-bin.tar.gz /apps/tez # copy the tarball $HADOOP_HOME/bin/hadoop fs -put apache-tez-0.10.3-bin /apps/tez # copy the whole folder $HADOOP_HOME/bin/hadoop fs -ls /apps/tez # verify $HADOOP_HOME/sbin/stop-all.sh # stop hdfs Set up TEZ_HOME and HADOOP_CLASSPATH environment variables\nexport TEZ_HOME=/yourpathtotez/apache-tez-0.10.3-bin export HADOOP_CLASSPATH=$TEZ_HOME/*:$TEZ_HOME/conf Create a new config file for Tez: $TEZ_HOME/conf/tez-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.lib.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin.tar.gz,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin/lib,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Extra hadoop configurations to make everything working Modify $HADOOP_HOME/etc/hadoop/core-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;fs.defaultFS\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.yourusername.groups\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hadoop.proxyuser.yourusername.hosts\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Modify $HADOOP_HOME/etc/hadoop/hadoop-env.sh\n# JAVA_HOME export JAVA_HOME=/yourpathtojavahome/javahome # tez export TEZ_CONF_DIR=/yourpathtotezconf/conf export TEZ_JARS=/yourpathtotez/apache-tez-0.10.3-bin export HADOOP_CLASSPATH=${TEZ_CONF_DIR}:${TEZ_JARS}/*:${TEZ_JARS}/lib/*:${HADOOP_CLASSPATH}: ${JAVA_JDBC_LIBS}:${MAPREDUCE_LIBS} Modify $HADOOP_HOME/etc/hadoop/mapred-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.framework.name\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;yarn\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;mapreduce.application.classpath\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Modify $HADOOP_HOME/etc/hadoop/yarn-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;!-- Site specific YARN configuration properties --\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.aux-services\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;mapreduce_shuffle\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.env-whitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.resource.memory-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;4096\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.scheduler.minimum-allocation-mb\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2048\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;yarn.nodemanager.vmem-pmem-ratio\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;2.1\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Start hadoop\nIf you already started hadoop before, stop it. As we configured yarn and map-reduce, we have to restart it and make sure yarn is running:\n$HADOOP_HOME/sbin/stop-all.sh And start hadoop:\n$HADOOP_HOME/sbin/start-all.sh Installing Hive from a Tarball Start by downloading the most recent stable release of Hive from one of the Apache download mirrors (see Hive Releases).\nNext you need to unpack the tarball. This will result in the creation of a subdirectory named hive-x.y.z (where x.y.z is the release number):\nwget https://dlcdn.apache.org/hive/hive-4.0.0/apache-hive-4.0.0-bin.tar.gz tar -xzvf apache-hive-4.0.0-bin.tar.gz Set the environment variable HIVE_HOME to point to the installation directory:\ncd apache-hive-4.0.0-bin export HIVE_HOME=/yourpathtohive/apache-hive-4.0.0-bin Add $HIVE_HOME/bin to your PATH:\nexport PATH=$HIVE_HOME/bin:$PATH Create a directory for external tables: mkdir /yourpathtoexternaltables/warehouse Create a new config file for Hive: $HIVE_HOME/conf/hive-site.xml\n\u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.tez.container.size\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;1024\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.warehouse.external.dir\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/yourpathtowarehousedirectory/warehouse\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.execution.engine\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;tez\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.lib.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin.tar.gz,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin/lib,hdfs://localhost:9000/apps/tez/apache-tez-0.10.3-bin\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.configuration\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;/yourpathtotez/apache-tez-0.10.3-bin/conf/tez-site.xml\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.use.cluster.hadoop-libs\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; Initialize metastore schema. It will create a directore called metastore_db. It contains an embedded Derby database for metastore\n$HIVE_HOME/bin/schematool -dbType derby -initSchema --verbose Run HiveServer2\n$HIVE_HOME/bin/hiveserver2 Note: you can check if it is running in Hive Server web ui: http://localhost:10002/\nRun beeline:\n$HIVE_HOME/bin/beeline -u 'jdbc:hive2://localhost:10000/' -n yourusername As a test, create a table insert some value\ncreate table test (message string); insert into test values ('Hello, from Hive!'); Installing from Source Code Configuring is the same as when we do it from tarball. The only difference is that we have to build Hive for ourself and we will find the compiled binaries in a different directory.\nHive is available via Git at https://github.com/apache/hive. You can download it by running the following command.\ngit clone git@github.com:apache/hive.git In case you want to get a specific release branch, like 4.0.0, you can run that command: git clone -b branch-4.0 --single-branch git@github.com:apache/hive.git To build Hive, execute the following command on the base directory:\n $ mvn clean install -Pdist,iceberg -DskipTests It will create the subdirectory packaging/target/apache-hive-\u0026lt;release_string\u0026gt;-bin/apache-hive-\u0026lt;release_string\u0026gt;-bin/. That will be your HIVE_HOME directory.\nIt has a content like:\n bin/: directory containing all the shell scripts lib/: directory containing all required jar files conf/: directory with configuration files examples/: directory with sample input and query files  That directory should contain all the files necessary to run Hive. You can run it from there or copy it to a different location, if you prefer.\nFrom now, you can follow the steps described in the section Installing Hive from a Tarball\nInstalling with old version hadoop(greater than or equal 3.1.0) Although we normally require hive4 to rely on a hadoop 3.3.6+ cluster environment. However, in practice, in an ON YARN environment, we can package all the hadoop related dependencies into tez\u0026amp;hive so that they do not need to rely on the lib of the original hadoop cluster environment at runtime. In this way, we can run HIVE4 in a lower version of hadoop, provided that the base APIs of the hadoop 3.x series are common to each other.\nThe steps are as follows:\n1.Compile TEZ to get tez.tar.gz which contains all hadoop related dependencies(not tez minimal tarball), run mvn clean install -DskipTests=true -Dmaven.javadoc.skip=true -Pdist -Paws -Pazure. For more detail,see:https://tez.apache.org/install.html. After compiling to get tez.tar.gz, users should set the following properties in tez-site.xml:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.lib.uris\u0026lt;/name\u0026gt;\u0026lt;!--Example, replace with actual hdfs path--\u0026gt; \u0026lt;value\u0026gt;/apps/apache-tez-0.10.4-bin.tar.gz\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.lib.uris.classpath\u0026lt;/name\u0026gt; \u0026lt;!--only use tez self lib,do not use any old version hadoop cluster\u0026#39;s lib--\u0026gt; \u0026lt;value\u0026gt;$PWD/tezlib/*,$PWD/tezlib/lib/*\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.use.cluster.hadoop-libs\u0026lt;/name\u0026gt;\u0026lt;!--only use tez self lib,do not use any old version hadoop cluster\u0026#39;s lib--\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.am.launch.env\u0026lt;/name\u0026gt;\u0026lt;!--Example, replace with actual native-lib install path.Reuse old version hadoop cluster\u0026#39;s native lib is ok.--\u0026gt; \u0026lt;value\u0026gt;LD_LIBRARY_PATH=/usr/hadoop/3.1.0/hadoop/lib/native\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Users can set up environment variables individually, including but not limited to: JAVA_HOME, LD_LIBRARY_PATH.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;tez.task.launch.env\u0026lt;/name\u0026gt;\u0026lt;!--Example, replace with actual native-lib install path.Reuse old version hadoop cluster\u0026#39;s native lib is ok.--\u0026gt; \u0026lt;value\u0026gt;LD_LIBRARY_PATH=/usr/hadoop/3.1.0/hadoop/lib/native\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;Users can set up environment variables individually, including but not limited to: JAVA_HOME, LD_LIBRARY_PATH.\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; 2.Upload tez to the specified HDFS path in tez.lib.uris.(Please remember, do not use the minimal tarball for installation.)\n## DO not upload minimal tarball !!! [hadoop@hive opt]# hdfs dfs -put apache-tez-0.10.4-bin.tar.gz /apps/ 3.Download the high version of the Hadoop package(Please ensure that the HADOOP version on which TEZ depends is the same as the HADOOP version you have downloaded.).Unzip HIVE, HADOOP, and TEZ all in the installation path.\n## In this example, we have installed HIVE-4.0.1 and TEZ-0.10.4 on an Hadoop 3.1.0 cluster.users should install HIVE,HADOOP and TEZ into actual directories. [hadoop@hive opt]# cd /opt [hadoop@hive opt]# ll drwxr-xr-x 11 hive hadoop 4096 Nov 7 13:59 apache-hive-4.0.1-bin drwxr-xr-x 3 hive hadoop 4096 Nov 7 13:59 apache-tez-0.10.4-bin drwxr-xr-x 10 hive hadoop 4096 Nov 7 13:59 hadoop-3.3.6 lrwxrwxrwx 1 hive hadoop 30 Nov 7 13:59 hive-4.0.1 -\u0026gt; apache-hive-4.0.1-bin lrwxrwxrwx 1 hive hadoop 21 Nov 7 13:59 tez -\u0026gt; apache-tez-0.10.4-bin edit hive-env.sh\n# Set HADOOP_HOME to point to a specific hadoop install directory HADOOP_HOME=${HADOOP_HOME:-/opt/hadoop-3.3.6} export HIVE_HOME=${HIVE_HOME:-/opt/hive-4.0.1} export TEZ_HOME=/opt/tez Copy old version hadoop conf into hadoop3.3.6+:\ncp /usr/hadoop/3.1.0/hadoop/conf/* /opt/hadoop3.3.6/conf/ Put tez-site.xml into hive conf dir:\nmv tez-site.xml /opt/hive-4.0.1/conf/ After completing the above steps, users should be able to start the HMS service and HS2 service normally, and submit TEZ computing tasks without any issues.\nThrough the above steps, we can run Hive4+tez in any Hadoop3 environment. Users do not need to upgrade the cluster\u0026rsquo;s original hive/hadoop/tez.\nNext Steps You can begin using Hive as soon as it is installed, it should be work on you computer. There are some extra information in the following sections.\nBeeline CLI HiveServer2 has a CLI called Beeline (see Beeline – New Command Line Shell). To use Beeline, execute the following command in the Hive home directory:\n$ bin/beeline Hive Metastore Metadata is stored in a relational database. In our example (and as a default) it is a Derby database. By default, it\u0026rsquo;s location is ./metastore_db. (See conf/hive-default.xml). You can change it by modifying the configuration variable javax.jdo.option.ConnectionURL.\nUsing Derby in embedded mode allows at most one user at a time. To configure Derby to run in server mode, see Hive Using Derby in Server Mode.\nTo configure a database other than Derby for the Hive metastore, see Hive Metastore Administration.\nNext Step: Configuring Hive.\nHCatalog and WebHCat HCatalog If you install Hive from the binary tarball, the hcat command is available in the hcatalog/bin directory. However, most hcat commands can be issued as hive commands except for \u0026ldquo;hcat -g\u0026rdquo; and \u0026ldquo;hcat -p\u0026rdquo;. Note that the hcat command uses the -p flag for permissions but hive uses it to specify a port number. The HCatalog CLI is documented here and the Hive CLI is documented here.\nHCatalog installation is documented here.\nWebHCat (Templeton) If you install Hive from the binary tarball, the WebHCat server command webhcat_server.sh is in the hcatalog/webhcat/svr/src/main/bin/webhcat_server.sh directory.\nWebHCat installation is documented here.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/manual-installation_283118363/","tags":null,"title":"Apache Hive : Manual Installation"},{"categories":null,"contents":"Apache Hive : MapJoin and Partition Pruning  Overview  Problem Proposed Solution Possible Extensions   Optimization Details  Compile Time Runtime Pseudo Code    Overview In Hive, Map-Join is a technique that materializes data for all tables involved in the join except for the largest table and then large table is streamed over the materialized data from small tables. Map-Join is often a good join approach for star-schema joins where the fact table will be streamed over materialized dimension tables.\nProblem Map-Join predicates where the joining columns from big table (streamed table) are partition columns and corresponding columns from small table is not partitioned, the join would not prune the unnecessary partitions from big table. Since data for all small tables is materialized before big table is streamed, theoretically it would be possible to prune the unnecessary partitions from big table.\nHIVE-5119 has been created to track this feature improvement.\nProposed Solution Figure out the set of values from all small tables for each join column from big table (that is partition key). Using these set of values figure out the partitions from big table that should be scanned using metadata. Change the partitions to be scanned for big table before Map-Join starts streaming big table. This feature would be turned on only through an explicit configuration (name of that configuration is TBD).\nPossible Extensions • If LHS and RHS of join predicate are partitioned then for tables from inner side, Partitions can be decided statically at compile time.\n• Even if the Big Table columns are not partitioned, the set of values generated from small tables could be pushed down as a predicate on the big table. Storage Handlers like ORC, which can handle predicate push down, could take advantage of this.\nOptimization Details This optimization has compile time and run/execution time pieces to it. Compile time optimizations would happen as part of physical optimizer as one of the last optimizations (before inferring bucket sort). Run/Execution time optimizations would happen as part of MRLocalTask execution and before launching MapRedTask for Map-Join.\nCompile Time   Identify Map Join Operators that can participate in partition pruning.\n  For each of the Map-Join operator in the task, identify columns from big table that can participate in the partition pruning.\n   Columns that are identified from big table has following characteristics:\n • They are part of join condition\n • Big table is on the inner side of the join\n • Columns are not involved in any functions in the join conditions\n • Column value is not mutated (no function) before value reaches join condition from Table Scan.\n • Column is a partition column.\nIdentify small tables and columns from small table that can participate in partition pruning.   Columns that are identified from small table has following characteristics:\n • Column is the other side of predicate in the join condition and Big Table column is identified as a target for partition pruning.\n • Column is not part of any function on the join predicate.\n • Column is part of join in which big table is on the outer side.\nModify MapRedLocalTask to assemble set of values for each of the column from small tables that participate in partition pruning and to generate PartitionDesc for big table.  NOTE:\n• This requires adding a terminal operator to the operator DAG in the MapRedLocalTask.\n• Note that the new terminal operator would get tuples from all small tables of interest (just like HashTableSink Operator).\n• Cascading Map-Join operators (joining on different keys in the same task using same big table) would still use the same terminal operator in the MapRedLocalTask.\nRuntime   As tuples flow in to the new terminal operator in MapRedLocal task, it would extract columns of interest and would add it to a set of values for that column.\n  When close is called on the new terminal operator it would generate partitions of big table by consulting Meta Store (using values generated at #1).\n   NOTE:\n • Meta Store would need to answer queries with in clauses. Ex: give me all partitions for Table R where column x in (1,2,3) and column y in (5,6,7).\n • In case of cascading MapJoinOperators the big table would be pruned based on multiple keys (\u0026amp; hence set generation needs to handle it).\nModify the PartitionDesc for BigTable in the MapRedTask with the list from #2.  NOTE:\n • PartitionDesc from #2 should be merged with existing PartitionDesc for the Big Table by finding the intersection.\n • This modification of partition descriptor is designed as a prelaunch activity on each task. Task in turn would call prelaunch on associated work. Work may keep an ordered list of operators on which prelaunch needs to be called.\nAssumptions:\n• In HIVE currently Join predicates can only include conjunctions.\n• Hive only supports Equijoin\nPseudo Code   Walk through Task DAG looking for MapredTask. Perform #2 - #6 for each such MapRedTask.\n  Skip Task if it contains backup join plan (i.e if not MAPJOIN_ONLY_NOBACKUP or if backupTask is not null).\n  NOTE:\n This is aggressive; in my limited exposure to the hive code, it seemed like conditional tasks are currently set only for joins.\n With in the task Look for pattern “TS.*MAPJOIN”. Perform #4 - #6 for each MAPJOIN operator.\n  Flag a Map-Join Operator as candidate for Partition Pruning\n   4.1 Collect small tables that might participate in Big Table pruning\n a. Walk the join conditions. If Join Type is “outer” then check if big-table is on the outer side. If so then bailout.\n b. If big-table is on inner side then add the position of small table in to the set.\n 4.2 If set from #4.1 is empty then bailout. Otherwise collect join keys from big table which is not wrapped in a functions\n a) Get the join key from “MapJoinDesc.getKeys().get(MapJoinDesc .getPosBigTable)”\n b) Walk through list of “ExpressionNodeDesc”; if “ExprNodeDesc” is of type “ExprNodeGenericFuncDesc” then check if any of partition pruner candidate key is contained with in it (“ExprNodeDescUtils.containsPredicate”). If any candidate key is contained within the function then remove it from the partition-pruner-bigtable-candidate list.\n c) Create a pair of “ExprNodeColumnDesc position Integer within the list from #b” and “ExprNodecolumnDesc” and add to partition-pruner-bigtable-candidate list.\n4.3 If partition-pruner-bigtable-candidate list is empty then bailout. Otherwise find join keys from #4.1 that is not wrapped in function using partition pruner candidate set.\n a) Walk the set from 4.1\n b) Get the join key for each element from 4.1\n c) Walk the join key list from #b checking if any of it is a function\n d) If any of the element from #c is a function then check if it contains any element from partition-pruner-bigtable-candidate list. If yes then remove that element from partition-pruner-bigtable-candidate List and set-generation-key-map.\n e) Create a pair of table position and join key element from #d.\n f) Add element to set-generation-key-map where key is the position of element within the partition-pruner-bigtable-candidate list and value is element from #e.\n4.4 If partition-pruner-bigtable-candidate set is empty then bail out. Otherwise find BigTable Columns from partition-pruner-bigtable-candidate set that is partitioned.\n a) Construct list of “ExprNodeDesc” from the set of #4.2\n b) Find out the root table column descriptors for #a (“ExprNodeDescUtils.backtrack”)\n c) From Hive get Table metadata for big table\n d) Walk through the list from #b \u0026amp; check with Table meta data to see if any of those columns is partitioned (“Table.isPartitionKey”). If column is not partition key then remove it from the partition pruner candidate list.\n4.5 If partition-pruner-bigtable-candidate set is empty then bail out. Otherwise Check if any of the partition pruner element could potentially mutate the value before hitting the join conditions. We will have to introduce a new method to “ExprNodeDescUtil” similar to back track but checking if value could be mutated (ex functions).\n4.6 If partition-pruner-bigtable-candidate list from #4.5 is empty then bail out. Otherwise add partition-pruner-bigtable-candidate list and set-generation-key-map from #4.5 to the existing list of values in the PhysicalCtx.\n a) Create a pair of partition-pruner-bigtable-candidate list \u0026amp; set-generation-key-map.\n b) Add it to the existing list in the physical context (this is to handle cascading mapjoin operators in the same MapRedTask)\nIf partition-pruner-bigtable-candidate set and set-generation-keys are non empty then Modify corresponding LocalMRTask to introduce the new PartitionPrunerSink Operator (if not already).   a) Add to Physical Context a map of MapJoinOperator – HashTableSink Operator. This needs to happen during HashTableSink generation time.\n b) From physical context get the HashTableSinkOperator corresponding to the MapJoinOperator.\n c) From all the parents of MapJoin Operator identify the ones representing small tables in the set-generation-key-map.\n d) Create a new PartitionDescGenSinkOp (with set-generation-key-map)\n e) Add it as child of elements from #c.\nAssumption:\n Two different MapRedTask (that contains MapJoin Operators) would result in two different MapRedLocalTask even if they share the same set of small tables.\n Implementation of PartitionDescGenSink\n a) A map is maintained between BigTable column and HashSet.\n b) From each tuple extract values corresponding to each column with in set-generation-key.\n c) Add these to a HashSet\n d) On Close of PartitionDescGenSink consult Metadata to get partitions for the key columns corresponding. This requires potential enhancements to Hive Metadata handling to provide an api “Get all partitions where column1 has set1 of values, or column2 has set2 of values.\n e) Write the partition info to file. The file name \u0026amp; location needs to be finalized.\n In the MapRedTask corresponding to MapJoin, add TableScan for the bigTable to the prelaunch operator list. Add to the TS the location of corresponding PartitionDescGenSink output.\n  At execution time call prelaunch on each task. Task will call prelaunch on the work. Work will call prelaunch on the operators in the list in order. For TableScan, prelaunch will result in reading the PartitionDescriptor info and would find intersection of existing PartitionDesc and the new list produced by PartitionDescGenSink. Partition state info kept in MapWork would be updated with the new partitons (“MapWork.pathToAliases”, “MapWork.aliasToPartnInfo”, “MapWork.pathToPartitionInfo”). This would be then picked up by “ExecDriver.execute” to setup input paths for InputFormat.\n  NOTE:\n • In Mapwork, we may need to maintain a map of Table alias to List. One choice is to introduce a new “addPathToPartitionInfo” method and switch current callers to use the new convenience method; this method then could maintain a Map of table alias to list of PartitionDesc.\n • Current design assumes the partition descriptor info generated by Local Task would be communicated to MapRed Task through files. This is obviously sub optimal. As an enhancement different mechanisms can be brought in to pass this info.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/mapjoin-and-partition-pruning_34015666/","tags":null,"title":"Apache Hive : MapJoin and Partition Pruning"},{"categories":null,"contents":"Apache Hive : MapJoinOptimization Index\n 1. Map Join Optimization  1.1 Using Distributed Cache to Propagate Hashtable File 1.2 Removing JDBM 1.3 Performance Evaluation   2. Converting Join into Map Join Automatically  2.1 New Join Execution Flow 2.2 Resolving the Join Operation at Run Time 2.3 Backup Task 2.4 Performance Evaluation    1. Map Join Optimization 1.1 Using Distributed Cache to Propagate Hashtable File Previously, when 2 large data tables need to do a join, there will be 2 different Mappers to sort these tables based on the join key and emit an intermediate file, and the Reducer will take the intermediate file as input file and do the real join work. This join mechanism (referred to as Common Join) is perfect with two large data size. But if one the join tables is small enough to fit into the Mapper‚ A\u0026rsquo;s memory, then there is no need to launch the Reducer. Actually, the Reducer stage is very expensive for the performance because the Map/Reduce framework needs to sort and merge the intermediate files. So the basic idea of Map Join is to hold the data of small table in Mapper‚A\u0026rsquo;s memory and do the join work in Map stage, which saves the Reduce stage. The previous implementation (before optimization) is shown here:\nFig 1. The Previous Map Join Implementation\nIn Fig 1 above, the previous map join implementation does not scale well when the larger table is huge because each Mapper will directly read the small table data from HDFS. If the larger table is huge, there will be thousands of Mapper launched to read different records of the larger table. And those thousands of Mappers will read this small table data from HDFS into their memory, which can make access to the small table become the performance bottleneck; or, sometimes Mappers will get lots of time-outs for reading this small file, which may cause the task to fail.\n(HIVE-1641) has solved this problem, as shown in Fig2 below.\nFig 2. The Optimized Map Join\nThe basic idea is to create a new task, MapReduce Local Task, before the original Join Map/Reduce Task. This new task will read the small table data from HDFS to in-memory hashtable. After reading, it will serialize the in-memory hashtable into files on disk and compress the hashtable file into a tar file. In next stage, when the MapReduce task is launching, it will put this tar file to Hadoop Distributed Cache, which will populate the tar file to each Mapper‚ A\u0026rsquo;s local disk and decompress the file. So all the Mappers can deserialize the hashtable file back into memory and do the join work as before.\nObviously, the Local Task is a very memory intensive. So the query processor will launch this task in a child jvm, which has the same heap size as the Mapper\u0026rsquo;s. Since the Local Task may run out of memory, the query processor will measure the memory usage of the local task very carefully. Once the memory usage of the Local Task is higher than a threshold number. This Local Task will abort itself and tells the user that this table is too large to hold in the memory. User can change this threshold by set hive.mapjoin.localtask.max.memory.usage = 0.999;\n1.2 Removing JDBM Previously, Hive uses JDBM (HIVE-1293) as a persistent hashtable. Whenever the in-memory hashtable cannot hold data any more, it will swap the key/value into the JDBM table. However when profiling the Map Join, we found out this JDBM component takes more than 70 % CPU time as shown in Fig3. Also the persistent file JDBM generated is too large to put into the Distributed Cache. For example, if users put 67,000 simple integer key/value pairs into the JDBM, it will generate more 22M hashtable file. So the JDBM is too heavy weight for Map Join and it would better to remove this component from Hive. Map Join is designed for holding the small table\u0026rsquo;s data into memory. If the table is too large to hold, just run as a Common Join. There is no need to use a persistent hashtable any more. (HIVE-1754)\nFig 3. The Profiling Result of JDBM\n1.3 Performance Evaluation Here are some performance comparison results between the previous Map Join with the optimized Map Join\nTable 1: The Comparison between the previous map join with the new optimized map join\nAs shown in Table1, the optimized Map Join will be 12 ~ 26 times faster than the previous one. Most of map join performance improvement comes from removing the JDBM component.\n2. Converting Join into Map Join Automatically 2.1 New Join Execution Flow Since map join is faster than the common join, it would be better to run the map join whenever possible. Previously, Hive users need to give a hint in the query to assign which table the small table is. For example, select /+mapjoin(a)/ * from src1 x join src2y on x.key=y.key; It is not a good way for user experience and query performance, because sometimes user may give a wrong hint and also users may not give any hints. It would be much better to convert the Common Join into Map Join without users' hint.\n(HIVE-1642) has solved the problem by converting the Common Join into Map Join automatically. For the Map Join, the query processor should know which input table the big table is. The other input tables will be recognize as the small tables during the execution stage and these tables need to be held in the memory. However, in general, the query processor has no idea of input file size during compiling time (even with statistics) because some of the table may be intermediate tables generated from sub queries. So the query processor can only figure out the input file size during the execution time.\nRight now, users need to enable this feature by set hive.auto.convert.join = true;\nThis would become default in hive 0.11 with (HIVE-3297)\nFig 5: The Join Execution Flow\nAs shown in fig5, the left side shows the previous Common Join execution flow, which is very straightforward. On the other side, the right side is the new Common Join execution flow. During the compile time, the query processor will generate a Conditional Task, which contains a list of tasks and one of these tasks will be resolved to run during the execution time. It means the tasks in the Conditional Task\u0026rsquo;s list are the candidates and one of them will be chosen to run during the run time. First, the original Common Join Task should be put into the list. Also the query processor will generate a series of Map Join Task by assuming each of the input tables may be the big table. For example, select * from src1 x join src2y on x.key=y.key. Both table *_src2 _*and *_src1 _*may be the big table, so it will generate 2 Map Join Task. One is assuming src1 is the big table and the other is assuming src2 is the big table, as shown in Fig 6.\nFig 6Ôºö Create Map Join Task by Assuming One of the Input Table is the Big Table\n2.2 Resolving the Join Operation at Run Time During the execution stage, the Conditional Task can know exactly the file size of each input table, even the table is a intermediate table. If all the tables are too large to be converted into map join, then just run the Common Join Task as previously. If one of the tables is large and others are small enough to run Map Join, then the Conditional Task will pick the corresponding Map Join Local Task to run. By this mechanism, it can convert the Common Join into Map Join automatically and dynamically.\nCurrently, if the total size of small tables are large than 25M, then the Conditional Task will choose the original Common Join run. 25M is a very conservative number and user can change this number by set hive.mapjoin.smalltable.filesize = 30000000.\n2.3 Backup Task As mentioned above, the Local Task of Map Join is a very memory intensive. So the query processor will launch this task in a child jvm, which has the same heap size as the Mapper\u0026rsquo;s. Since the Local Task may run out of memory, the query processor will measure the memory usage of the local task very carefully. Once the memory usage of the Local Task is higher than a threshold. This Local Task will abort itself and it means the map join task fails. In this case, the query processor will launch the original Common Join task as a Backup Task to run, which is totally transparent to user. The basic idea is shown as Fig 7.\nFig7, Run the Original Common Join as a Backup Task\n2.4 Performance Evaluation Here are some performance comparison results between the previous Common Join with the optimized Common Join. All the benchmark queries here can be converted into Map Join.\nTable 2: The Comparison between the previous join with the new optimized join\n *  For the previous common join, the experiment only calculates the average time of map reduce task execution time. Because job finish time will include the job scheduling overhead. Sometimes it will wait for some time to start to run the job in the cluster. Also for the new optimized common join, the experiment only adds up the average time of local task execution time with the average time of map reduce execution time. So both of the results have avoided the job scheduling overhead.\nFrom the result, if the new common join can be converted into map join, it will get 57% ~163 % performance improvement.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/mapjoinoptimization_27362029/","tags":null,"title":"Apache Hive : MapJoinOptimization"},{"categories":null,"contents":"Apache Hive : Materialized views  Introduction  Objectives   Management of materialized views in Hive  Materialized views creation Other operations for materialized view management   Materialized view-based query rewriting  Example 1 Example 2 Example 3   Materialized view maintenance Materialized view lifecycle Open issues (JIRA)  Version information\nMaterialized views support is introduced in Hive 3.0.0.\nIntroduction This page documents the work done for the supporting materialized views in Apache Hive.\nObjectives Traditionally, one of the most powerful techniques used to accelerate query processing in data warehouses is the pre-computation of relevant summaries or materialized views.\nThe initial implementation introduced in Apache Hive 3.0.0 focuses on introducing materialized views and automatic query rewriting based on those materializations in the project. In particular, materialized views can be stored natively in Hive or in other systems such as Druid using custom storage handlers, and they can seamlessly exploit new exciting Hive features such as LLAP acceleration. Then the optimizer relies in Apache Calcite to automatically produce full and partial rewritings for a large set of query expressions comprising projections, filters, join, and aggregation operations.\nIn this document, we provide details about materialized view creation and management in Hive, describe the current coverage of the rewriting algorithm with some examples, and explain how Hive controls important aspects of the life cycle of the materialized views such as the freshness of their data.\nManagement of materialized views in Hive In this section, we present the main operations that are currently present in Hive for materialized views management.\nMaterialized views creation The syntax to create a materialized view in Hive is very similar to the CTAS statement syntax, supporting common features such as partition columns, custom storage handler, or passing table properties.\nCREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [DISABLE REWRITE] [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] [CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] AS \u0026lt;query\u0026gt;; When a materialized view is created, its contents will be automatically populated by the results of executing the query in the statement. The materialized view creation statement is atomic, meaning that the materialized view is not seen by other users until all query results are populated.\nBy default, materialized views are usable for query rewriting by the optimizer, while the DISABLE REWRITE option can be used to alter this behavior at materialized view creation time.\nThe default values for SerDe and storage format when they are not specified in the materialized view creation statement (they are optional) are specified using the configuration properties hive.materializedview.serde and hive.materializedview.fileformat, respectively.\nMaterialized views can be stored in external systems, e.g., Druid, using custom storage handlers. For instance, the following statement creates a materialized view that is stored in Druid:\nExample:\nCREATE MATERIALIZED VIEW druid_wiki_mv STORED AS 'org.apache.hadoop.hive.druid.DruidStorageHandler' AS SELECT __time, page, user, c_added, c_removed FROM src; Other operations for materialized view management Currently we support the following operations that aid at managing the materialized views in Hive:\n-- Drops a materialized view DROP MATERIALIZED VIEW [db_name.]materialized_view_name; -- Shows materialized views (with optional filters) SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards’]; -- Shows information about a specific materialized view DESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name; The functionality of these operations will be extended in the future and more operations may be added.\nMaterialized view-based query rewriting Once a materialized view has been created, the optimizer will be able to exploit its definition semantics to automatically rewrite incoming queries using materialized views, and hence, accelerate query execution. The rewriting algorithm can be enabled and disabled globally using the hive.materializedview.rewriting and hive.materializedview.rewriting.sql configuration properties (default value is true). In addition, users can selectively enable/disable materialized views for rewriting. Recall that, by default, materialized views are enabled for rewriting at creation time. To alter that behavior, the following statement can be used:\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE; Hive supports two types of rewriting algorithms:\n Algebraic: this is part of Apache Calcite and it supports queries containing TableScan, Project, Filter, Join, and Aggregate operators. More information about this rewriting coverage can be found here. In the following, we include a few examples that briefly illustrate different rewritings.  Example 1 Consider the database schema created by the following DDL statements:\nCREATE TABLE emps ( empid INT, deptno INT, name VARCHAR(256), salary FLOAT, hire_date TIMESTAMP) STORED AS ORC TBLPROPERTIES ('transactional'='true'); CREATE TABLE depts ( deptno INT, deptname VARCHAR(256), locationid INT) STORED AS ORC TBLPROPERTIES ('transactional'='true'); Assume we want to obtain frequently information about employees that were hired in different period granularities after 2016 and their departments. We may create the following materialized view:\nCREATE MATERIALIZED VIEW mv1 AS SELECT empid, deptname, hire_date FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2016-01-01'; Then, the following query extracting information about employees that were hired in Q1 2018 is issued to Hive:\nSELECT empid, deptname FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2018-01-01' AND hire_date \u0026lt;= '2018-03-31'; Hive will be able to rewrite the incoming query using the materialized view, including a compensation predicate on top of the scan over the materialization. Though the rewriting happens at the algebraic level, to illustrate this example, we include the SQL statement equivalent to the rewriting using the mv used by Hive to answer the incoming query:\nSELECT empid, deptname FROM mv1 WHERE hire_date \u0026gt;= '2018-01-01' AND hire_date \u0026lt;= '2018-03-31'; Example 2 For the second example, consider the star schema based on the SSB benchmark created by the following DDL statements:\nCREATE TABLE `customer`( `c_custkey` BIGINT, `c_name` STRING, `c_address` STRING, `c_city` STRING, `c_nation` STRING, `c_region` STRING, `c_phone` STRING, `c_mktsegment` STRING, PRIMARY KEY (`c_custkey`) DISABLE RELY) STORED AS ORC TBLPROPERTIES ('transactional'='true'); CREATE TABLE `dates`( `d_datekey` BIGINT, `d_date` STRING, `d_dayofweek` STRING, `d_month` STRING, `d_year` INT, `d_yearmonthnum` INT, `d_yearmonth` STRING, `d_daynuminweek` INT, `d_daynuminmonth` INT, `d_daynuminyear` INT, `d_monthnuminyear` INT, `d_weeknuminyear` INT, `d_sellingseason` STRING, `d_lastdayinweekfl` INT, `d_lastdayinmonthfl` INT, `d_holidayfl` INT, `d_weekdayfl`INT, PRIMARY KEY (`d_datekey`) DISABLE RELY) STORED AS ORC TBLPROPERTIES ('transactional'='true'); CREATE TABLE `part`( `p_partkey` BIGINT, `p_name` STRING, `p_mfgr` STRING, `p_category` STRING, `p_brand1` STRING, `p_color` STRING, `p_type` STRING, `p_size` INT, `p_container` STRING, PRIMARY KEY (`p_partkey`) DISABLE RELY) STORED AS ORC TBLPROPERTIES ('transactional'='true'); CREATE TABLE `supplier`( `s_suppkey` BIGINT, `s_name` STRING, `s_address` STRING, `s_city` STRING, `s_nation` STRING, `s_region` STRING, `s_phone` STRING, PRIMARY KEY (`s_suppkey`) DISABLE RELY) STORED AS ORC TBLPROPERTIES ('transactional'='true'); CREATE TABLE `lineorder`( `lo_orderkey` BIGINT, `lo_linenumber` int, `lo_custkey` BIGINT not null DISABLE RELY, `lo_partkey` BIGINT not null DISABLE RELY, `lo_suppkey` BIGINT not null DISABLE RELY, `lo_orderdate` BIGINT not null DISABLE RELY, `lo_ordpriority` STRING, `lo_shippriority` STRING, `lo_quantity` DOUBLE, `lo_extendedprice` DOUBLE, `lo_ordtotalprice` DOUBLE, `lo_discount` DOUBLE, `lo_revenue` DOUBLE, `lo_supplycost` DOUBLE, `lo_tax` DOUBLE, `lo_commitdate` BIGINT, `lo_shipmode` STRING, PRIMARY KEY (`lo_orderkey`) DISABLE RELY, CONSTRAINT fk1 FOREIGN KEY (`lo_custkey`) REFERENCES `customer_n1`(`c_custkey`) DISABLE RELY, CONSTRAINT fk2 FOREIGN KEY (`lo_orderdate`) REFERENCES `dates_n0`(`d_datekey`) DISABLE RELY, CONSTRAINT fk3 FOREIGN KEY (`lo_partkey`) REFERENCES `ssb_part_n0`(`p_partkey`) DISABLE RELY, CONSTRAINT fk4 FOREIGN KEY (`lo_suppkey`) REFERENCES `supplier_n0`(`s_suppkey`) DISABLE RELY) STORED AS ORC TBLPROPERTIES ('transactional'='true'); As you can observe, we declare multiple integrity constraints for the database, using the RELY keyword so they are visible to the optimizer. Now assume we want to create a materialization that denormalizes the database contents (consider dims to be the set of dimensions that we will be querying often):\nCREATE MATERIALIZED VIEW mv2 AS SELECT \u0026lt;dims\u0026gt;, lo_revenue, lo_extendedprice * lo_discount AS d_price, lo_revenue - lo_supplycost FROM customer, dates, lineorder, part, supplier WHERE lo_orderdate = d_datekey AND lo_partkey = p_partkey AND lo_suppkey = s_suppkey AND lo_custkey = c_custkey; The materialized view above may accelerate queries that execute joins among the different tables in the database. For instance, consider the following query:\nSELECT SUM(lo_extendedprice * lo_discount) FROM lineorder, dates WHERE lo_orderdate = d_datekey AND d_year = 2013 AND lo_discount between 1 and 3; Though the query does not use all tables present in the materialized view, it may be answered using the materialized view because the joins in mv2 preserve all the rows in the lineorder table (we know this because of the integrity constraints). Hence, the materialized view-based rewriting produced by the algorithm would be the following:\nSELECT SUM(d_price) FROM mv2 WHERE d_year = 2013 AND lo_discount between 1 and 3; Example 3 For the third example, consider the database schema with a single table that stores the edit events produced by a given website:\nCREATE TABLE `wiki` ( `time` TIMESTAMP, `page` STRING, `user` STRING, `characters_added` BIGINT, `characters_removed` BIGINT) STORED AS ORC TBLPROPERTIES ('transactional'='true'); For this example, we will use Druid to store the materialized view. Assume we want to execute queries over the table, however we are not interested on any information about the events at a higher time granularity level than a minute. We may create the following materialized view that rolls up the events by the minute:\nCREATE MATERIALIZED VIEW mv3 STORED BY 'org.apache.hadoop.hive.druid.DruidStorageHandler' AS SELECT floor(time to minute) as `__time`, page, SUM(characters_added) AS c_added, SUM(characters_removed) AS c_removed FROM wiki GROUP BY floor(time to minute), page; Then, assume we need to answer the following query that extracts the number of characters added per month:\nSELECT floor(time to month), SUM(characters_added) AS c_added FROM wiki GROUP BY floor(time to month); Hive will be able to rewrite the incoming query using mv3 by rolling up the data of the materialized view to month granularity and projecting the information needed for the query result:\nSELECT floor(time to month), SUM(c_added) FROM mv3 GROUP BY floor(time to month);  SQL text: The materialized view definition query text is compared to the incoming query text or it\u0026rsquo;s subquery text. It supports all kind of operators and aggregate functions.  Materialized view maintenance When data in the source tables used by a materialized view changes, e.g., new data is inserted or existing data is modified, we will need to refresh the contents of the materialized view to keep it up-to-date with those changes. Currently, the rebuild operation for a materialized view needs to be triggered by the user. In particular, the user should execute the following statement:\nALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD; Hive supports incremental view maintenance, i.e., only refresh data that was affected by the changes in the original source tables. Incremental view maintenance will decrease the rebuild step execution time. In addition, it will preserve LLAP cache for existing data in the materialized view.\nBy default, Hive will attempt to rebuild a materialized view incrementally, falling back to full rebuild if it is not possible.\nCurrent implementation only supports incremental rebuild when there were INSERT operations over the source tables, while UPDATE and DELETE operations will force a full rebuild of the materialized view.\nTo execute incremental maintenance, following conditions should be met if there were only INSERT operations over the source tables:\n The materialized view should only use transactional tables (the source tables must be transactional), either micromanaged or ACID or a storage format that supports snapshots (ex. Iceberg) If the materialized view definition contains a Group By clause, the materialized view should be stored in an ACID table or a storage format that supports snapshots (ex. Iceberg v2), since it needs to support MERGE operation. For materialized view definitions consisting of Scan-Project-Filter-Join, this restriction does not exist. If the materialized view definition contains a Group By clause the following aggregate functions are supported: COUNT, SUM, AVG (only if both COUNT and SUM defined for the same column), MIN, MAX  If there were UPDATE and DELETE operations over the source tables:\n The materialized view should only use transactional tables (the source tables must be transactional), either micromanaged or ACID The materialized view definition must contain a Group By clause and a COUNT(*) function call. The materialized view should be stored in an ACID table or a storage format that supports snapshots (ex. Iceberg v2), since it needs to support MERGE operation. The following aggregate functions are supported: COUNT, SUM, AVG (only if both COUNT and SUM defined for the same column)  A rebuild operation acquires an exclusive write lock over the materialized view, i.e., for a given materialized view, only one rebuild operation can be executed at a given time.\nMaterialized view lifecycle By default, once a materialized view contents are stale, the materialized view will not be used for automatic query rewriting.\nHowever, in some occasions it may be fine to accept stale data, e.g., if the materialized view uses non-transactional tables and hence we cannot verify whether its contents are outdated, however we still want to use the automatic rewriting. For those occasions, we can combine a rebuild operation run periodically, e.g., every 5minutes, and define the required freshness of the materialized view data using the hive.materializedview.rewriting.time.window configuration parameter, for instance:\nSET hive.materializedview.rewriting.time.window=10min; The parameter value can be also overridden by a concrete materialized view just by setting it as a table property when the materialization is created.\nOpen issues (JIRA)    Key Summary T Created Updated Due Assignee Reporter P Status Resolution     HIVE-14494 Add support for BUILD DEFERRED Improvement Aug 09, 2016 Feb 27, 2024  Ashish Sharma Jesús Camacho Rodríguez Major Open Unresolved   HIVE-14499 Add HMS metrics for materialized views Improvement Aug 09, 2016 Feb 27, 2024  John Sherman Jesús Camacho Rodríguez Major Open Unresolved   HIVE-18621 Replicate materialized views creation metadata with correct database name Sub-task Feb 05, 2018 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Minor Open Unresolved   HIVE-18960 Make Materialized view invalidation cache work with catalogs Sub-task Mar 14, 2018 Mar 14, 2018  Alan Gates Alan Gates Major Open Unresolved   HIVE-19114 MV rewriting not being triggered for last query in materialized_view_rewrite_4.q Bug Apr 05, 2018 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Critical Open Unresolved   HIVE-19407 Only support materialized views stored either as ACID or in selected custom storage handlers Improvement May 03, 2018 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-20543 Support replication of Materialized views Sub-task Sep 12, 2018 Aug 21, 2020  Aasha Medhi Sankar Hariappan Major Open Unresolved   HIVE-20773 Query result cache might contain stale MV data Bug Oct 18, 2018 Jun 23, 2021  Unassigned Oliver Draese Critical Open Unresolved   HIVE-21133 Support views with rewriting enabled useful for debugging Improvement Jan 18, 2019 Feb 27, 2024  Jesús Camacho Rodríguez Jesús Camacho Rodríguez Major Patch Available Unresolved   HIVE-21945 Enable sorted dynamic partitioning optimization for materialized views with custom data organization Bug Jul 02, 2019 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-21946 Consider data organization of a materialized view in transparent rewriting Bug Jul 02, 2019 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-21953 Enable CLUSTERED ON/DISTRIBUTED ON+SORTED ON in incremental rebuild of materialized views Bug Jul 04, 2019 Feb 27, 2024  Unassigned Jesús Camacho Rodríguez Major Open Unresolved   HIVE-22111 Materialized view based on replicated table might not get refreshed Bug Aug 14, 2019 Feb 27, 2024  Unassigned Peter Vary Minor Open Unresolved   HIVE-22253 General task tracking improvements for materialized views Task Sep 27, 2019 Jul 20, 2020  Unassigned Steve Carlin Major Open Unresolved   HIVE-22260 Materialized view rewriting does not support UNION operator, exact match can work under view Sub-task Sep 27, 2019 Sep 27, 2019  Unassigned Steve Carlin Major Open Unresolved   HIVE-22262 Aggregate pushdown through join may generate additional rewriting opportunities Sub-task Sep 27, 2019 Feb 27, 2024  Vineet Garg Steve Carlin Major Open Unresolved   HIVE-22264 Degenerate case where mv not being used: computing aggregate on group by field Sub-task Sep 27, 2019 Sep 27, 2019  Unassigned Steve Carlin Major Open Unresolved   HIVE-22265 Ordinals in view are not being picked up in materialized view Sub-task Sep 27, 2019 Feb 11, 2020  Unassigned Steve Carlin Critical Open Unresolved   HIVE-22921 materialized_view_partitioned_3.q relies on hive.optimize.sort.dynamic.partition property Test Feb 21, 2020 Feb 27, 2024  Vineet Garg Jesús Camacho Rodríguez Major Open Unresolved   HIVE-24335 RelOptMaterialization creates LogicalProject on top of HiveTableScan Bug Oct 30, 2020 Apr 04, 2024  Krisztian Kasa Krisztian Kasa Major Open Unresolved    Authenticate to retrieve your issues\nShowing 20 out of 24 issues\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/materialized-views_80447331/","tags":null,"title":"Apache Hive : Materialized views"},{"categories":null,"contents":"Apache Hive : Materialized views in Hive Objectives Traditionally, one of the most powerful techniques used to accelerate query processing in data warehouses is the pre-computation of relevant summaries or materialized views.\nThe initial implementation focuses on introducing materialized views and automatic query rewriting based on those materializations in the project. In particular, materialized views can be stored natively in Hive or in other systems such as Druid using custom storage handlers, and they can seamlessly exploit new exciting Hive features such as LLAP acceleration. Then the optimizer relies in Apache Calcite to automatically produce full and partial rewritings for a large set of query expressions comprising projections, filters, join, and aggregation operations.\nIn this document, we provide details about materialized view creation and management in Hive, describe the current coverage of the rewriting algorithm with some examples, and explain how Hive controls important aspects of the life cycle of the materialized views such as the freshness of their data.\nManagement of materialized views in Hive In this section, we present the main operations that are currently present in Hive for materialized views management.\nMaterialized views creation The syntax to create a materialized view in Hive is very similar to the CTAS statement syntax, supporting common features such as partition columns, custom storage handler, or passing table properties.\n| CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name``[DISABLE REWRITE]``[COMMENT materialized_view_comment]``[PARTITIONED ON (col_name, ...)]``[CLUSTERED ON (col_name, ...) | DISTRIBUTED ON (col_name, ...) SORTED ON (col_name, ...)]``[``[ROW FORMAT row_format]``[STORED AS file_format]``| STORED BY``'[storage.handler.class.name](http://storage.handler.class.name)' [WITH SERDEPROPERTIES (...)]``]``[LOCATION hdfs_path]``[TBLPROPERTIES (property_name=property_value, ...)]``AS``\u0026lt;query\u0026gt;; |\nWhen a materialized view is created, its contents will be automatically populated by the results of executing the query in the statement. The materialized view creation statement is atomic, meaning that the materialized view is not seen by other users until all query results are populated.\nBy default, materialized views are usable for query rewriting by the optimizer, while the DISABLE REWRITE option can be used to alter this behavior at materialized view creation time.\nThe default values for SerDe and storage format when they are not specified in the materialized view creation statement (they are optional) are specified using the configuration properties hive.materializedview.serde and hive.materializedview.fileformat, respectively.\nMaterialized views can be stored in external systems, e.g., Druid, using custom storage handlers. For instance, the following statement creates a materialized view that is stored in Druid:\nExample:\n| CREATE MATERIALIZED VIEW druid_wiki_mv``STORED AS``'org.apache.hadoop.hive.druid.DruidStorageHandler'``AS``SELECT __time, page, user, c_added, c_removed``FROM src; |\nOther operations for materialized view management Currently we support the following operations that aid at managing the materialized views in Hive:\n| -- Drops a materialized view``DROP MATERIALIZED VIEW [db_name.]materialized_view_name;``-- Shows materialized views (with optional filters)``SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards’];``-- Shows information about a specific materialized view``DESCRIBE [EXTENDED | FORMATTED] [db_name.]materialized_view_name; |\nThe functionality of these operations will be extended in the future and more operations may be added.\nMaterialized view-based query rewriting Once a materialized view has been created, the optimizer will be able to exploit its definition semantics to automatically rewrite incoming queries using materialized views, and hence, accelerate query execution. The rewriting algorithm can be enabled and disabled globally using the hive.materializedview.rewriting configuration property (default value is true). In addition, users can selectively enable/disable materialized views for rewriting. Recall that, by default, materialized views are enabled for rewriting at creation time. To alter that behavior, the following statement can be used:\n| ALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE|DISABLE REWRITE; |\nThe rewriting algorithm is part of Apache Calcite and it supports queries containing TableScan, Project, Filter, Join, and Aggregate operators. More information about the rewriting coverage can be found here. In the following, we include a few examples that briefly illustrate different rewritings.\nExample 1 Consider the database schema created by the following DDL statements:\n| CREATE TABLE emps (``empid INT,``deptno INT,``name VARCHAR(``256``),``salary FLOAT,``hire_date TIMESTAMP)``STORED AS ORC``TBLPROPERTIES (``'transactional'``=``'true'``); CREATE TABLE depts (``deptno INT,``deptname VARCHAR(``256``),``locationid INT)``STORED AS ORC``TBLPROPERTIES (``'transactional'``=``'true'``); |\nAssume we want to obtain frequently information about employees that were hired in different period granularities after 2016 and their departments. We may create the following materialized view:\n| CREATE MATERIALIZED VIEW mv1``AS``SELECT empid, deptname, hire_date``FROM emps JOIN depts``ON (emps.deptno = depts.deptno)``WHERE hire_date \u0026gt;=``'2016-01-01'``; |\nThen, the following query extracting information about employees that were hired in Q1 2018 is issued to Hive:\n| SELECT empid, deptname``FROM emps``JOIN depts``ON (emps.deptno = depts.deptno)``WHERE hire_date \u0026gt;=``'2018-01-01'``AND hire_date \u0026lt;=``'2018-03-31'``; |\nHive will be able to rewrite the incoming query using the materialized view, including a compensation predicate on top of the scan over the materialization. Though the rewriting happens at the algebraic level, to illustrate this example, we include the SQL statement equivalent to the rewriting using the mv used by Hive to answer the incoming query:\n| SELECT empid, deptname``FROM mv1``WHERE hire_date \u0026gt;=``'2018-01-01'``AND hire_date \u0026lt;=``'2018-03-31'``; |\nExample 2 For the second example, consider the star schema based on the SSB benchmark created by the following DDL statements:\n| CREATE TABLE customer(```c_custkey BIGINT,c_name` STRING,c_address STRING,```c_city STRING,c_nation` STRING,c_region STRING,```c_phone STRING,c_mktsegment` STRING,``PRIMARY KEY (`c_custkey`) DISABLE RELY)``STORED AS ORC``TBLPROPERTIES (``'transactional'``=``'true'``);` `CREATE TABLE `dates`(d_datekey BIGINT,```d_date STRING,d_dayofweek` STRING,d_month STRING,```d_year INT,d_yearmonthnum` INT,d_yearmonth STRING,```d_daynuminweek INT,d_daynuminmonth` INT,d_daynuminyear INT,```d_monthnuminyear INT,d_weeknuminyear` INT,d_sellingseason STRING,```d_lastdayinweekfl INT,d_lastdayinmonthfl` INT,d_holidayfl INT,```d_weekdayflINT,PRIMARY KEY (`d_datekey`) DISABLE RELY)STORED AS ORCTBLPROPERTIES (\u0026lsquo;transactional\u0026rsquo;=\u0026lsquo;true\u0026rsquo;);` `CREATE TABLE `part`(```p_partkey` BIGINT,```p_name` STRING,```p_mfgr` STRING,```p_category` STRING,```p_brand1` STRING,```p_color` STRING,```p_type` STRING,```p_size` INT,```p_container` STRING,PRIMARY KEY (p_partkey) DISABLE RELY)STORED AS ORCTBLPROPERTIES ('transactional'='true'); CREATE TABLE supplier(s_suppkey` BIGINT,s_name STRING,```s_address STRING,s_city` STRING,s_nation STRING,```s_region STRING,s_phone` STRING,``PRIMARY KEY (`s_suppkey`) DISABLE RELY)``STORED AS ORC``TBLPROPERTIES (``'transactional'``=``'true'``);` `CREATE TABLE `lineorder`(lo_orderkey BIGINT,```lo_linenumber```int``,```lo_custkey BIGINT notnull` `DISABLE RELY,```lo_partkey` BIGINT notnull DISABLE RELY,lo_suppkey` BIGINT not``null` `DISABLE RELY,lo_orderdate BIGINT not``null DISABLE RELY,```lo_ordpriority STRING,lo_shippriority` STRING,lo_quantity DOUBLE,```lo_extendedprice DOUBLE,lo_ordtotalprice` DOUBLE,lo_discount DOUBLE,```lo_revenue DOUBLE,lo_supplycost` DOUBLE,lo_tax DOUBLE,```lo_commitdate BIGINT,```lo_shipmode STRING,``PRIMARY KEY (lo_orderkey) DISABLE RELY,``CONSTRAINT fk1 FOREIGN KEY (lo_custkey) REFERENCES customer_n1(c_custkey) DISABLE RELY,``CONSTRAINT fk2 FOREIGN KEY (lo_orderdate) REFERENCES dates_n0(d_datekey) DISABLE RELY,``CONSTRAINT fk3 FOREIGN KEY (lo_partkey) REFERENCES ssb_part_n0(p_partkey) DISABLE RELY,``CONSTRAINT fk4 FOREIGN KEY (lo_suppkey) REFERENCES supplier_n0(s_suppkey) DISABLE RELY)``STORED AS ORC``TBLPROPERTIES (``'transactional'``=``'true'``); |\nAs you can observe, we declare multiple integrity constraints for the database, using the RELY keyword so they are visible to the optimizer. Now assume we want to create a materialization that denormalizes the database contents (consider dims to be the set of dimensions that we will be querying often):\n| CREATE MATERIALIZED VIEW mv2``AS``SELECT \u0026lt;dims\u0026gt;,``lo_revenue,``lo_extendedprice * lo_discount AS d_price,``lo_revenue - lo_supplycost``FROM customer, dates, lineorder, part, supplier``WHERE lo_orderdate = d_datekey``AND lo_partkey = p_partkey``AND lo_suppkey = s_suppkey``AND lo_custkey = c_custkey; |\nThe materialized view above may accelerate queries that execute joins among the different tables in the database. For instance, consider the following query:\n| SELECT SUM(lo_extendedprice * lo_discount)``FROM lineorder, dates``WHERE lo_orderdate = d_datekey``AND d_year =``2013``AND lo_discount between``1 and``3``; |\nThough the query does not use all tables present in the materialized view, it may be answered using the materialized view because the joins in mv2 preserve all the rows in the lineorder table (we know this because of the integrity constraints). Hence, the materialized view-based rewriting produced by the algorithm would be the following:\n| SELECT SUM(d_price)``FROM mv2``WHERE d_year =``2013``AND lo_discount between``1 and``3``; |\nExample 3 For the third example, consider the database schema with a single table that stores the edit events produced by a given website:\n| CREATE TABLE wiki (```time TIMESTAMP,page` STRING,user STRING,```characters_added BIGINT,```characters_removed BIGINT)``STORED AS ORC``TBLPROPERTIES (``'transactional'``=``'true'``); |\nFor this example, we will use Druid to store the materialized view. Assume we want to execute queries over the table, however we are not interested on any information about the events at a higher time granularity level than a minute. We may create the following materialized view that rolls up the events by the minute:\n| CREATE MATERIALIZED VIEW mv3``STORED BY``'org.apache.hadoop.hive.druid.DruidStorageHandler'``AS``SELECT floor(time to minute) as __time, page,``SUM(characters_added) AS c_added,``SUM(characters_removed) AS c_removed``FROM wiki``GROUP BY floor(time to minute), page; |\nThen, assume we need to answer the following query that extracts the number of characters added per month:\n| SELECT floor(time to month),``SUM(characters_added) AS c_added``FROM wiki``GROUP BY floor(time to month); |\nHive will be able to rewrite the incoming query using mv3 by rolling up the data of the materialized view to month granularity and projecting the information needed for the query result:\n| SELECT floor(time to month),``SUM(c_added)``FROM mv3``GROUP BY floor(time to month); |\nMaterialized view maintenance When data in the source tables used by a materialized view changes, e.g., new data is inserted or existing data is modified, we will need to refresh the contents of the materialized view to keep it up-to-date with those changes. Currently, the rebuild operation for a materialized view needs to be triggered by the user. In particular, the user should execute the following statement:\n| ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD; |\nHive supports incremental view maintenance, i.e., only refresh data that was affected by the changes in the original source tables. Incremental view maintenance will decrease the rebuild step execution time. In addition, it will preserve LLAP cache for existing data in the materialized view.\nBy default, Hive will attempt to rebuild a materialized view incrementally, falling back to full rebuild if it is not possible. Current implementation only supports incremental rebuild when there were INSERT operations over the source tables, while UPDATE and DELETE operations will force a full rebuild of the materialized view.\nTo execute incremental maintenance, following conditions should be met:\n The materialized view should only use transactional tables, either micromanaged or ACID. If the materialized view definition contains a Group By clause, the materialized view should be stored in an ACID table, since it needs to support MERGE operation. For materialized view definitions consisting of Scan-Project-Filter-Join, this restriction does not exist.  A rebuild operation acquires an exclusive write lock over the materialized view, i.e., for a given materialized view, only one rebuild operation can be executed at a given time.\nMaterialized view lifecycle By default, once a materialized view contents are stale, the materialized view will not be used for automatic query rewriting.\nHowever, in some occasions it may be fine to accept stale data, e.g., if the materialized view uses non-transactional tables and hence we cannot verify whether its contents are outdated, however we still want to use the automatic rewriting. For those occasions, we can combine a rebuild operation run periodically, e.g., every 5minutes, and define the required freshness of the materialized view data using the hive.materializedview.rewriting.time.window configuration parameter, for instance:\n| SET hive.materializedview.rewriting.time.window=10min; |\nThe parameter value can be also overridden by a concrete materialized view just by setting it as a table property when the materialization is created.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/materialized-views-in-hive_283118346/","tags":null,"title":"Apache Hive : Materialized views in Hive"},{"categories":null,"contents":"Apache Hive : MetaStore API Tests  IMetaStoreClient Tests  IMetaStoreClient Tests One option for Java clients to access the MetaStore is to connect through the IMetaStoreClient interface implementations.\nTo ensure that the IMetaStoreClient implementations provide the same API we created a set of tests to validate their workings.\nCurrently the following implementations are tested:\n EmbeddedMetaStore – when the MetaStore is running in the same thread, and in process communication is used. Represented by the EmbeddedMetaStoreForTests. RemoteMetaStore – when the MetaStore is running in a separate thread but in the same JVM, and in Thrift communication is used. Represented by the RemoteMetaStoreForTests. ClusterMetaStore – when the MetaStore is started separately from the tests. Currently it is not used in the code, but could be used to test against previous versions of the MetaStore to ensure backward compatibility.  MetaStoreFactoryForTests provides the list of these implementations which can be used in parametrized tests in @Parameterized.Parameters. Currently the returning list contains the following implementations:\n Embedded – Uses the same configuration as TestEmbeddedHiveMetaStore. Remote – Uses the same configuration as TestRemoteHiveMetaStore. Later multiple configurations could be added, like the ones in TestSetUGIOnBothClientServer, TestSetUGIOnOnlyClient, TestSetUGIOnOnlyServer. If the -Dtest.hms.client.configs is provided then a ClusterMetaStore is returned in the list too.  The following test files will check the API functionalities:\n TestDatabases – creating, querying, dropping databases TestFunctions – creating, querying, dropping functions \u0026hellip;     ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/metastore-api-tests_75958143/","tags":null,"title":"Apache Hive : MetaStore API Tests"},{"categories":null,"contents":"Apache Hive : Metastore TLP Proposal Summary of the Proposal from the Email Hive’s metastore has long been used by other projects in the Hadoop ecosystem to store and access metadata. Apache Impala, Apache Spark, Apache Drill, Presto, and other systems all use Hive’s metastore. Some, like Impala and Presto, can use it as their own metadata system with the rest of Hive not present.\nThis sharing is excellent for the ecosystem. Together with HDFS it allows users to use the tool of their choice while still accessing the same shared data. But having this shared metadata inside the Hive project limits the ability of other projects to contribute to the metastore. It also makes it harder for new systems that have similar but not identical metadata requirements (for example, stream processing systems on top of Apache Kafka) to use Hive’s metastore. This difficulty for other systems comes out in two ways. One, it is hard for non-Hive community members to participate in the project. Second, it adds operational cost since users are forced to deploy all of the Hive jars just to get the metastore to work.\nTherefore we propose to split Hive’s metastore out into a separate Apache project. This new project will continue to support the same Thrift API as the current metastore. It will continue to focus on being a high performance, fault tolerant, large scale, operational metastore for SQL engines and other systems that want to store schema information about their data. By making it a separate project we will enable other projects to join us in innovating on the metastore. It will simplify operations for non-Hive users that want to use the metastore as they will no longer need to install Hive just to get the metastore. And it will attract new projects that might otherwise feel the need to solve their metadata problems on their own.\nAny Hive PMC member or committer will be welcome to join the new project at the same level. We propose this project go straight to a top level project. Given that the initial PMC will be formed from experienced Hive PMC members we do not believe incubation will be necessary. (Note that the Apache board will need to approve this.)\nMore Details Use Cases As noted above, the metastore will continue to focus on being a metadata system for SQL systems like Hive and Impala. This system should also be able to store metadata for streaming systems such as data stored in Kafka. Additionally it should easily allow use by machine learning systems and others that need to access the data stored in SQL engines, streams, etc.\nA use case that we are not initially targeting is the larger area of a full data catalog, storing information such as lineage, user tags, etc. with support for end user discoverability and interaction.\nSupporting these uses cases will drive requirements such as:\n The ability to support various big data engines and frameworks, including relational, batch, and streaming The ability to scale to support a system with petabytes of data and thousands of users and their jobs High reliability and/or fault tolerance The ability to support multiple co-located systems (e.g. multiple Hive instances in one cloud or Impala and a streaming system in the same on-premise facility) Low response time (\u0026lt; 200ms) to support interactive and high throughput systems Support for transactional SQL systems Support for versioned schemas Ability to work on premise and in the cloud Maintaining backwards compatibility (including specifying public versus private APIs). This will be very important as the metastore already has a significant user community.  Details of Moving the Code Moving the code from Hive into a new project is not straightforward and will take some time. The following steps are proposed:\n A new TLP is established. As mentioned above, any existing Hive PMC members will be welcome to join the PMC, and any existing Hive committers will be granted committership in the new project. Hive begins the process of detangling the metastore code inside the Hive project. This will be done inside Hive to avoid a time where the code is in both Hive and the new project that would require double patching of any new features or bugs.\nIn order to enable the new project to begin adding layers around the core metastore and make releases, Hive can make source-only releases of only the metastore code during this interim period, similar to how the storage-api is released now. The new project can then depend on those releases. Once the detangling is complete and Hive is satisfied that the result works, the code will be moved from Hive to the new project.  There are many technical questions of how to separate out the code. These mainly center around which pieces of code should be moved into the new project, and whether the new project continues to depend on Hive’s storage-api (as ORC does today) or whether it copies any code that both it and Hive require (such as parts of the shim layer) in order to avoid any Hive dependencies. Also there are places where metastore \u0026ldquo;calls back\u0026rdquo; into QL via reflection (e.g. partition expression evaluation). We will need to determine how to continue this without pulling a dependency on all of Hive into the new project. Discussions and decisions on this will happen throughout the process via the normal methods.\nBackwards Compatibility There are already many users of Hive metastore outside of Hive. We do not want to break backwards compatibility for those users. Our goal will be to make sure there is a binary compatible metastore client available for these users that will support interoperation across versions of the metastore in Hive and as a stand alone system. Another possible approach is to assure that the Thrift interface continues to accept old clients (e.g. Hive 1.x and 2.x), rather than focusing on binary or source compatibility of the Hive client itself.## Project Name\nThe following have been suggested as a name for this project:\n Flora Honeycomb Metastore (NOTE: there are concerns that this would be too generic for Apache to defend the trademark and that it would not be clear enough to users that this was no longer just the Hive metastore) Omegastore Riven ZCatalog   ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/metastore-tlp-proposal_71013238/","tags":null,"title":"Apache Hive : Metastore TLP Proposal"},{"categories":null,"contents":"Apache Hive : MultiDelimitSerDe Introduction Introduced in HIVE-5871, MultiDelimitSerDe allows user to specify multiple-character string as the field delimiter when creating a table.\nVersion Hive 0.14.0 and later.\nHive QL Syntax You can use MultiDelimitSerDe in a create table statement like this:\nCREATE TABLE test ( id string, hivearray array\u0026lt;binary\u0026gt;, hivemap map\u0026lt;string,int\u0026gt;) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.MultiDelimitSerDe' WITH SERDEPROPERTIES (\u0026quot;field.delim\u0026quot;=\u0026quot;[,]\u0026quot;,\u0026quot;collection.delim\u0026quot;=\u0026quot;:\u0026quot;,\u0026quot;mapkey.delim\u0026quot;=\u0026quot;@\u0026quot;); where field.delim is the field delimiter, collection.delim and mapkey.delim is the delimiter for collection items and key value pairs, respectively. HIVE-20619 moved MultiDelimitSerDe to hive.serde2 in release 4.0.0, so user won\u0026rsquo;t have to install hive-contrib JAR into the HiveServer2 auxiliary directory.\nLimitations  Among the delimiters, field.delim is mandatory and can be of multiple characters, while collection.delim and mapkey.delim is optional and only support single character. Nested complex type is not supported, e.g. an Array. To use MultiDelimitSerDe prior to Hive release 4.0.0, you have to add the hive-contrib jar to the class path, e.g. with the add jar command.   Comments:            Thank you Lefty Leverenz    Posted by afan at Oct 05, 2018 06:18 | | And thanks for your contributions Alice Fan.\nPosted by leftyl at Oct 05, 2018 06:27 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/multidelimitserde_46631999/","tags":null,"title":"Apache Hive : MultiDelimitSerDe"},{"categories":null,"contents":"Apache Hive : OperatorsAndFunctions Hive Operators and Functions   Hive Plug-in Interfaces - User-Defined Functions and SerDes\n  Guide to Hive Operators and Functions\n Reflect UDF Generic UDAF Case Study Functions for Statistics and Data Mining    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/operatorsandfunctions_30754909/","tags":null,"title":"Apache Hive : OperatorsAndFunctions"},{"categories":null,"contents":"Apache Hive : OuterJoinBehavior Hive Outer Join Behavior  Hive Outer Join Behavior  Definitions Predicate Pushdown Rules  Hive Implementation   Examples  Case J1: Join Predicate on Preserved Row Table Case J2: Join Predicate on Null Supplying Table Case W1: Where Predicate on Preserved Row Table Case W2: Where Predicate on Null Supplying Table      This document is based on a writeup of DB2 Outer Join Behavior. The original HTML document is attached to the Hive Design Docs and can be downloaded here.\nDefinitions          Preserved Row table The table in an Outer Join that must return all rows. For left outer joins this is the Left table, for right outer joins it is the Right table, and for full outer joins both tables are Preserved Row tables.   Null Supplying table This is the table that has nulls filled in for its columns in unmatched rows. In the non-full outer join case, this is the other table in the Join. For full outer joins both tables are also Null Supplying tables.   During Join predicate A predicate that is in the JOIN ON clause. For example, in \u0026lsquo;R1 join R2 on R1.x = 5\u0026rsquo; the predicate \u0026lsquo;R1.x = 5\u0026rsquo; is a During Join predicate.   After Join predicate A predicate that is in the WHERE clause.    Predicate Pushdown Rules The logic can be summarized by these two rules:\n During Join predicates cannot be pushed past Preserved Row tables. After Join predicates cannot be pushed past Null Supplying tables.  This captured in the following table:\n     Preserved Row Table Null Supplying Table     Join Predicate Case J1: Not Pushed Case J2: Pushed   Where Predicate Case W1: Pushed Case W2: Not Pushed    See Examples below for illustrations of cases J1, J2, W1, and W2.\nHive Implementation Hive enforces the rules by these methods in the SemanticAnalyzer and JoinPPD classes:\nRule 1: During QBJoinTree construction in Plan Gen, the parseJoinCondition() logic applies this rule.\nRule 2: During JoinPPD (Join Predicate PushDown) the getQualifiedAliases() logic applies this rule.\nExamples Given Src(Key String, Value String) the following Left Outer Join examples show that Hive has the correct behavior.\nCase J1: Join Predicate on Preserved Row Table explain select s1.key, s2.key from src s1 left join src s2 on s1.key \u0026gt; '2'; STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stage STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: s1 TableScan alias: s1 Reduce Output Operator sort order: tag: 0 value expressions: expr: key type: string s2 TableScan alias: s2 Reduce Output Operator sort order: tag: 1 value expressions: expr: key type: string Reduce Operator Tree: Join Operator condition map: Left Outer Join0 to 1 condition expressions: 0 {VALUE._col0} 1 {VALUE._col0} filter predicates: 0 {(VALUE._col0 \u0026gt; '2')} 1 handleSkewJoin: false outputColumnNames: _col0, _col4 Select Operator expressions: expr: _col0 type: string expr: _col4 type: string outputColumnNames: _col0, _col1 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Stage: Stage-0 Fetch Operator limit: -1 Case J2: Join Predicate on Null Supplying Table explain select s1.key, s2.key from src s1 left join src s2 on s2.key \u0026gt; '2'; STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: s1 TableScan alias: s1 Reduce Output Operator sort order: tag: 0 value expressions: expr: key type: string s2 TableScan alias: s2 Filter Operator predicate: expr: (key \u0026gt; '2') type: boolean Reduce Output Operator sort order: tag: 1 value expressions: expr: key type: string Reduce Operator Tree: Join Operator condition map: Left Outer Join0 to 1 condition expressions: 0 {VALUE._col0} 1 {VALUE._col0} handleSkewJoin: false outputColumnNames: _col0, _col4 Select Operator expressions: expr: _col0 type: string expr: _col4 type: string outputColumnNames: _col0, _col1 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Stage: Stage-0 Fetch Operator limit: -1 Case W1: Where Predicate on Preserved Row Table explain select s1.key, s2.key from src s1 left join src s2 where s1.key \u0026gt; '2'; STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: s1 TableScan alias: s1 Filter Operator predicate: expr: (key \u0026gt; '2') type: boolean Reduce Output Operator sort order: tag: 0 value expressions: expr: key type: string s2 TableScan alias: s2 Reduce Output Operator sort order: tag: 1 value expressions: expr: key type: string Reduce Operator Tree: Join Operator condition map: Left Outer Join0 to 1 condition expressions: 0 {VALUE._col0} 1 {VALUE._col0} handleSkewJoin: false outputColumnNames: _col0, _col4 Select Operator expressions: expr: _col0 type: string expr: _col4 type: string outputColumnNames: _col0, _col1 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Stage: Stage-0 Fetch Operator limit: -1 Case W2: Where Predicate on Null Supplying Table explain select s1.key, s2.key from src s1 left join src s2 where s2.key \u0026gt; '2'; STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: s1 TableScan alias: s1 Reduce Output Operator sort order: tag: 0 value expressions: expr: key type: string s2 TableScan alias: s2 Reduce Output Operator sort order: tag: 1 value expressions: expr: key type: string Reduce Operator Tree: Join Operator condition map: Left Outer Join0 to 1 condition expressions: 0 {VALUE._col0} 1 {VALUE._col0} handleSkewJoin: false outputColumnNames: _col0, _col4 Filter Operator predicate: expr: (_col4 \u0026gt; '2') type: boolean Select Operator expressions: expr: _col0 type: string expr: _col4 type: string outputColumnNames: _col0, _col1 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Stage: Stage-0 Fetch Operator limit: -1 ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/outerjoinbehavior_35749927/","tags":null,"title":"Apache Hive : OuterJoinBehavior"},{"categories":null,"contents":"Apache Hive : Overview of Major Changes   Iceberg Integration  Advanced Snapshot management Branches \u0026amp; Tags support DML (insert/update/delete/merge) COW \u0026amp; MOR modes Vectorised Reads \u0026amp; Writes Table migration command LOAD DATA statements support Partition-level operations support Improved statistics (column stats support)    Hive ACID  Use sequences for TXN_ID generation (performance) Read-only transactions optimization Zero-wait readers Optimistic and Pessimistic concurrency control Lockless reads    Compaction  Rebalance compaction (Hive ACID) Compaction requests prioritization (compaction pooling) Iceberg compaction (Major)    Hive Metastore  API optimization (performance) Dynamic leader election External data sources support HMS support for Thrift over HTTP JWT authentication for Thrift over HTTP HMS metadata summary Use Zookeeper for service discovery    HiveServer2  Support SAML 2.0/JWT authentication mode Support both Kerberos and LDAP auth methods in parallel Graceful shutdown Easy access to the operation log through web UI    Hive Replication  Optimised Bootstrap Solution Support for Snapshot Based Replication for External Table Better Replication Tracking Metrics Support for Checkpointing during Replication    Security  Authorizations in alter table/view, UDFs, and Views created from Apache Spark. Authorizations on tables created based on storage handlers. Critical CVE fixes of transitive dependencies.    Compiler  Materialized view support for Iceberg tables Improvements to refresh materialized views Date/Timestamp fixes and improvements Anti join support Split update support Branch pruning Column histogram statistics support Calcite upgrade to 1.25 HPL/SQL improvements Scheduled query support    Miscl.  Support for ESRI GeopSpatial UDF\u0026rsquo;s Added support for Apache Ozone Support Hadoop-3.3.6 Supports Tez 0.10.3 Works with Aarch64 (ARM) New UDFs (Hive UDFs) Deprecated Hive on MR \u0026amp; Removed Hive on Spark Deprecated Hive CLI    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/overview-of-major-changes_283118379/","tags":null,"title":"Apache Hive : Overview of Major Changes"},{"categories":null,"contents":"Apache Hive : Parquet Version\nParquet is supported by a plugin in Hive 0.10, 0.11, and 0.12 and natively in Hive 0.13 and later.\n  Introduction Native Parquet Support  Hive 0.10, 0.11, and 0.12 Hive 0.13   HiveQL Syntax  Hive 0.10 - 0.12 Hive 0.13 and later   Versions and Limitations  Hive 0.13.0 Hive 0.14.0 Hive 1.1.0 Hive 1.2.0 Resources    Introduction Parquet (http://parquet.io/) is an ecosystem wide columnar format for Hadoop. Read Dremel made simple with Parquet for a good introduction to the format while the Parquet project has an in-depth description of the format including motivations and diagrams. At the time of this writing Parquet supports the follow engines and data description languages:\nEngines\n Apache Hive Apache Drill Cloudera Impala Apache Crunch Apache Pig Cascading Apache Spark  Data description\n Apache Avro Apache Thrift Google Protocol Buffers  The latest information on Parquet engine and data description support, please visit the Parquet-MR projects feature matrix.\nParquet Motivation\nWe created Parquet to make the advantages of compressed, efficient columnar data representation available to any project in the Hadoop ecosystem.\nParquet is built from the ground up with complex nested data structures in mind, and uses the record shredding and assembly algorithm described in the Dremel paper. We believe this approach is superior to simple flattening of nested name spaces.\nParquet is built to support very efficient compression and encoding schemes. Multiple projects have demonstrated the performance impact of applying the right compression and encoding scheme to the data. Parquet allows compression schemes to be specified on a per-column level, and is future-proofed to allow adding more encodings as they are invented and implemented.\nParquet is built to be used by anyone. The Hadoop ecosystem is rich with data processing frameworks, and we are not interested in playing favorites. We believe that an efficient, well-implemented columnar storage substrate should be useful to all frameworks without the cost of extensive and difficult to set up dependencies.\nNative Parquet Support Hive 0.10, 0.11, and 0.12 To use Parquet with Hive 0.10-0.12 you must download the Parquet Hive package from the Parquet project. You want the parquet-hive-bundle jar in Maven Central.\nHive 0.13 Native Parquet support was added (HIVE-5783). Please note that not all Parquet data types are supported in this version (see Versions and Limitations below).\nHiveQL Syntax A CREATE TABLE statement can specify the Parquet storage format with syntax that depends on the Hive version.\nHive 0.10 - 0.12 CREATE TABLE parquet_test ( id int, str string, mp MAP\u0026lt;STRING,STRING\u0026gt;, lst ARRAY\u0026lt;STRING\u0026gt;, strct STRUCT\u0026lt;A:STRING,B:STRING\u0026gt;) PARTITIONED BY (part string) ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat' OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'; Hive 0.13 and later CREATE TABLE parquet_test ( id int, str string, mp MAP\u0026lt;STRING,STRING\u0026gt;, lst ARRAY\u0026lt;STRING\u0026gt;, strct STRUCT\u0026lt;A:STRING,B:STRING\u0026gt;) PARTITIONED BY (part string) STORED AS PARQUET; Versions and Limitations Hive 0.13.0 Support was added for Create Table AS SELECT (CTAS \u0026ndash; HIVE-6375).\nHive 0.14.0 Support was added for timestamp (HIVE-6394), decimal (HIVE-6367), and char and varchar (HIVE-7735) data types. Support was also added for column rename with use of the flag parquet.column.index.access (HIVE-6938). Parquet column names were previously case sensitive (query had to use column case that matches exactly what was in the metastore), but became case insensitive (HIVE-7554).\nHive 1.1.0 Support was added for binary data types (HIVE-7073).\nHive 1.2.0 Support for remaining Parquet data types was added (HIVE-6384).\nResources  Parquet Website Format specification Feature Matrix The striping and assembly algorithms from the Dremel paper Dremel paper Dremel made simple with Parquet  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/parquet_38570914/","tags":null,"title":"Apache Hive : Parquet"},{"categories":null,"contents":"Apache Hive : Partition Filter Syntax Example: for a table having partition keys country and state, one could construct the following filter:\ncountry = \u0026quot;USA\u0026quot; AND (state = \u0026quot;CA\u0026quot; OR state = \u0026quot;AZ\u0026quot;)\nIn particular notice that it is possible to nest sub-expressions within parentheses.\nThe following operators are supported when constructing filters for partition columns (derived from HIVE-1862):\n = \u0026lt; \u0026lt;= \u0026gt; \u0026gt;= \u0026lt;\u0026gt; AND OR LIKE (on keys of type string only, supports literal string template with \u0026lsquo;.*' wildcard)  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/partition-filter-syntax_103092177/","tags":null,"title":"Apache Hive : Partition Filter Syntax"},{"categories":null,"contents":"Apache Hive : PartitionedViews  Use Cases  Approaches Syntax Metastore Strict Mode View Definition Changes   Hook Information  This is a followup to ViewDev for adding partition-awareness to views.\nUse Cases  An administrator wants to create a set of views as a table/column renaming layer on top of an existing set of base tables, without breaking any existing dependencies on those tables. To read-only users, the views should behave exactly the same as the underlying tables in every way. Among other things, this means users should be able to browse available partitions. A base table is partitioned on columns (ds,hr) for date and hour. Besides this fine-grained partitioning, users would also like to see a virtual table of coarse-grained (date-only) partitioning in which the partition for a given date only appears after all of the hour-level partitions of that day have been fully loaded. A view is defined on a complex join+union+aggregation of a number of underlying base tables and other views, all of which are themselves partitioned. The top-level view should also be partitioned accordingly, with a new partition not appearing until corresponding partitions have been loaded for all of the underlying tables.  Approaches  One possible approach mentioned in HIVE-1079 is to infer view partitions automatically based on the partitions of the underlying tables. A command such as SHOW PARTITIONS could then synthesize virtual partition descriptors on the fly. This is fairly easy to do for use case #1, but potentially very difficult for use cases #2 and #3. So for now, we are punting on this approach. Instead, per HIVE-1941, we will require users to explicitly declare view partitioning as part of CREATE VIEW, and explicitly manage partition metadata via ALTER VIEW ADD|DROP PARTITION. This allows all of the use cases to be satisfied (while placing more burden on the user, and taking up more metastore space). With this approach, there is no real connection between view partitions and underlying table partitions; it\u0026rsquo;s even possible to create a partitioned view on an unpartitioned table, or to have data in the view which is not covered by any view partition. One downside here is that a UI will not be able to show last access time and physical information such as file size when browsing available partitions. (In theory, stats could work via an explicit ANALYZE, but analyzing a view would need some work.)  Syntax  CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ] [COMMENT table_comment] [PARTITIONED ON (col1, col2, ...)] [TBLPROPERTIES ...] AS SELECT ... ALTER VIEW view_name ADD [IF NOT EXISTS] partition_spec partition_spec ... ALTER VIEW view_name DROP [IF EXISTS] partition_spec, partition_spec, ... partition_spec: : PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...) Notes:\n Whereas CREATE TABLE uses PARTITIONED BY, CREATE VIEW uses PARTITIONED ON. This difference is intentional because in CREATE TABLE, the PARTITIONED BY clause specifies additional column definitions which are appended to the non-partitioning columns. With CREATE VIEW, the PARTITIONED ON clause references (by name) columns already produced by the view definition. Only column names appear in PARTITIONED ON; no types etc. However, to match the CREATE TABLE convention of trailing partitioning columns, the columns referenced by the PARTITIONED ON clause must be the last columns in the view definition, and their order in the PARTITIONED ON clause must match their order in the view definition. The ALTER VIEW ADD/DROP partition syntax is identical to ALTER TABLE, except that it is illegal to specify a LOCATION clause. Other ALTER TABLE commands which operate on partitions (e.g. TOUCH/ARCHIVE) are not supported. (But maybe we need to support TOUCH?)  Metastore When storing view partition descriptors in the metastore, Hive omits the storage descriptor entirely. This is because there is no data associated with the view partition, so there is no need to keep track of partition-level column descriptors for table schema evolution, nor a partition location.\nStrict Mode Hive strict mode (enabled with hive.mapred.mode=strict) prevents execution of queries lacking a partition predicate. This only applies to base table partitions. What does this mean?\nSuppose you have table T1 partitioned on C1, and view V1 which selects FROM T1 WHERE C1=5. Then a query such as SELECT * FROM V1 will succeed even in strict mode, since the predicate inside of the view constrains C1.\nLikewise, suppose you have view V2 which selects from T1 (with no WHERE clause) and is partitioned on C2. Then a query such as SELECT * FROM V2 WHERE C2=3 will fail; even though the view partition column is constrained, there is no predicate on the underlying T1\u0026rsquo;s partition column C1.\nView Definition Changes Currently, changing a view definition requires dropping the view and recreating it. This implies dropping and recreating all existing partitions as well, which could be very expensive.\nThis implies that followup support for CREATE OR REPLACE VIEW is very important, and that it needs to preserve existing partitions (after validating that they are still compatible with the new view definition).\nHook Information Although there is currently no connection between the view partition and underlying table partitions, Hive does provide dependency information as part of the hook invocation for ALTER VIEW ADD PARTITION. It does this by compiling an internal query of the form\n SELECT * FROM view_name WHERE view_partition_col1 = 'val1' AND view_partition_col=2 = 'val2' ... and then capturing the table/partition inputs for this query and passing them on to the ALTER VIEW ADD PARTITION hook results.\nThis allows applications to track the dependencies themselves. In the future, Hive will automatically populate these dependencies into the metastore as part of HIVE-1073.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/partitionedviews_27362053/","tags":null,"title":"Apache Hive : PartitionedViews"},{"categories":null,"contents":"Apache Hive : Performance YourKit Java Profiler Collaboration To measure Hive\u0026rsquo;s internal performance, we use the YourKit Java Profiler. YourKit LLC is kindly supporting open source projects with its full-featured Java Profiler. The Hive project has been granted YourKit open source licenses to be used by its developers. For more information on this collaboration, ask on the developers mailing list.\n Benchmarks Here are some JIRA issues about benchmarks for Hive:\n Hive performance benchmarks (HIVE-396) Running TPC-H queries on Hive (HIVE-600)    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/performance_27362052/","tags":null,"title":"Apache Hive : Performance"},{"categories":null,"contents":"Apache Hive : Permission Inheritance in Hive This document describes how attributes (permission, group, extended ACL\u0026rsquo;s) of files representing Hive data are determined.\nHDFS Background  When a file or directory is created, its owner is the user identity of the client process, and its group is inherited from parent (the BSD rule). Permissions are taken from default umask. Extended Acl\u0026rsquo;s are taken from parent unless they are set explicitly.  Goals To reduce need to set fine-grain file security props after every operation, users may want the following Hive warehouse file/dir to auto-inherit security properties from their directory parents:\n Directories created by new database/table/partition/bucket Files added to tables via load/insert Table directories exported/imported (open question of whether exported table inheriting perm from new parent needs another flag)  What is inherited:\n Basic file permission Groups (already done by HDFS for new directories) Extended ACL\u0026rsquo;s (already done by HDFS for new directories)   This inheritance of extended ACL\u0026rsquo;s is literal, all extended ACL\u0026rsquo;s are copied to children as is, including ACL\u0026rsquo;s for the defaultGroup.\nOne room for improvement may be to follow HDFS semantics for the defaultGroup, which is as follows:\n\u0026ldquo;When a new file or sub-directory is created, it automatically copies the default ACL of its parent into its own access ACL. A new sub-directory also copies it to its own default ACL. In this way, the default ACL will be copied down through arbitrarily deep levels of the file system tree as new sub-directories get created.\u0026rdquo; (https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#ACLs_Access_Control_Lists)\nSee HIVE-11481.\nBehavior  When \u0026ldquo;hive.warehouse.subdir.inherit.perms\u0026rdquo; flag is enabled in Hive, Hive will try to do all the following inheritances.  Database directory inherits from warehouse directory. Table directory inherits from database directory, or from warehouse directory if it is part of the default database. External table directory inherits from parent directory. Partition directory inherits from table directory. (As of Hive 1.1.0.) Data files inherit from table or partition directory.   Failure by Hive to inherit will not cause operation to fail. Rule of thumb of when security-prop inheritance will happen is the following:  To run chmod, a user must be the owner of the file, or else a super-user. To run chgrp, a user must be the owner of files, or else a super-user. Hence, user that hive runs as (either \u0026lsquo;hive\u0026rsquo; or the logged-in user in case of impersonation), must be super-user or owner of the file whose security properties are going to be changed    Version Information Most of this functionality was added as of Hive 0.14. See umbrella JIRA HIVE-6892 for details.\nhive.warehouse.subdir.inherit.perms was removed in Hive 3.0.0. The feature is no longer needed in Hive as the traditional permission model has largely been replaced by external security systems such as Ranger and Sentry. A user may choose SQLStdAuth which ships with Hive if user doesn\u0026rsquo;t want to use an external security system.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/permission-inheritance-in-hive_48203008/","tags":null,"title":"Apache Hive : Permission Inheritance in Hive"},{"categories":null,"contents":"Apache Hive : PluginDeveloperKit Hive Plugin Developer Kit This page explains Apache Hive\u0026rsquo;s Plugin Developer Kit, or PDK. This allows developers to build and test Hive plugins without having to set up a Hive source build; only a Hive binary release is needed.\nThe PDK is planned for inclusion in the Hive 0.8.0 release; until that is available, please download a recent snapshot build from Jenkins; make sure it includes HIVE-2244.\nCurrently, the PDK is only targeted at user defined functions (including UDAF\u0026rsquo;s and UDTF\u0026rsquo;S), although it may be possible to use it for building other kinds of plugins such as serdes, input/output formats, storage handlers and index handlers. The PDK\u0026rsquo;s test framework currently only supports automated testing of UDF\u0026rsquo;s.\nExample Plugin To demonstrate the PDK in action, the Hive release includes an examples/test-plugin directory. You can build the test plugin by changing to that directory and running\nant -Dhive.install.dir=../.. This will create a build subdirectory containing the compiled plugin: pdk-test-udf-0.1.jar. There\u0026rsquo;s also a build/metadata directory containing add-jar.sql (demonstrating the command to use to load the plugin jar) and class-registration.sql (demonstrating the commands to use for loading the UDF\u0026rsquo;s from the plugin). The .sql files can be passed via the Hive CLI\u0026rsquo;s -i command-line parameter in order to be run as initialization scripts.\nYou can run the tests associated with the plugin via\nant -Dhive.install.dir=../.. test If all is well, you should see output like\nBuildfile: /hive-0.8.0-SNAPSHOT/examples/test-plugin/build.xml get-class-list: test: [junit] Running org.apache.hive.pdk.PluginTest [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 38.955 sec BUILD SUCCESSFUL The example plugin is also built and tested as part of the main Hive build in order to verify that the PDK is operating as expected.\nYour Own Plugin To create your own plugin, you can follow the patterns from the example plugin. Let\u0026rsquo;s take a closer look at it. First, the build.xml:\n\u0026lt;project name=\u0026quot;pdktest\u0026quot; default=\u0026quot;package\u0026quot;\u0026gt; \u0026lt;property name=\u0026quot;plugin.libname\u0026quot; value=\u0026quot;pdk-test-udf\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;plugin.title\u0026quot; value=\u0026quot;Hive PDK Test UDF Library\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;plugin.version\u0026quot; value=\u0026quot;0.1\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;plugin.vendor\u0026quot; value=\u0026quot;Apache Software Foundation\u0026quot;/\u0026gt; \u0026lt;property name=\u0026quot;function.sql.prefix\u0026quot; value=\u0026quot;tp_\u0026quot;/\u0026gt; \u0026lt;import file=\u0026quot;${hive.install.dir}/scripts/pdk/build-plugin.xml\u0026quot;/\u0026gt; \u0026lt;/project\u0026gt; All this buildfile does is define some variable settings and then import a build script from the PDK, which does the rest (including defining the package and test targets used for building and testing the plugin). So for your own plugin, change the variable settings accordingly, and set hive.install.dir to the location where you\u0026rsquo;ve installed the Hive release.\nThe imported PDK buildfile assumes a few things about the structure of your plugin source structure:\n your-plugin-root  build.xml src  Java source files   test  setup.sql cleanup.sql any datafiles needed by your tests      For the example plugin, a datafile onerow.txt contains a single row of data; setup.sql creates a table named onerow and loads the datafile, whereas cleanup.sql drops the onerow table. The onerow table is convenient for testing UDF\u0026rsquo;s.\nAnnotations Now let\u0026rsquo;s take a look at the source code for a UDF.\npackage org.apache.hive.pdktest; import org.apache.hive.pdk.HivePdkUnitTest; import org.apache.hive.pdk.HivePdkUnitTests; import org.apache.hadoop.hive.ql.exec.Description; import org.apache.hadoop.hive.ql.exec.UDF; import org.apache.hadoop.io.Text; /** * Example UDF for rot13 transformation. */ @Description(name = \u0026quot;rot13\u0026quot;, value = \u0026quot;_FUNC_(str) - Returns str with all characters transposed via rot13\u0026quot;, extended = \u0026quot;Example:\\n\u0026quot; + \u0026quot; \u0026gt; SELECT _FUNC_('Facebook') FROM src LIMIT 1;\\n\u0026quot; + \u0026quot; 'Snprobbx'\u0026quot;) @HivePdkUnitTests( setup = \u0026quot;create table rot13_data(s string); \u0026quot; + \u0026quot;insert overwrite table rot13_data select 'Facebook' from onerow;\u0026quot;, cleanup = \u0026quot;drop table if exists rot13_data;\u0026quot;, cases = { @HivePdkUnitTest( query = \u0026quot;SELECT tp_rot13('Mixed Up!') FROM onerow;\u0026quot;, result = \u0026quot;Zvkrq Hc!\u0026quot;), @HivePdkUnitTest( query = \u0026quot;SELECT tp_rot13(s) FROM rot13_data;\u0026quot;, result = \u0026quot;Snprobbx\u0026quot;) } ) public class Rot13 extends UDF { private Text t = new Text(); public Rot13() { } public Text evaluate(Text s) { StringBuilder out = new StringBuilder(s.getLength()); char[] ca = s.toString().toCharArray(); for (char c : ca) { if (c \u0026gt;= 'a' \u0026amp;\u0026amp; c \u0026lt;= 'm') { c += 13; } else if (c \u0026gt;= 'n' \u0026amp;\u0026amp; c \u0026lt;= 'z') { c -= 13; } else if (c \u0026gt;= 'A' \u0026amp;\u0026amp; c \u0026lt;= 'M') { c += 13; } else if (c \u0026gt;= 'N' \u0026amp;\u0026amp; c \u0026lt;= 'Z') { c -= 13; } out.append(c); } t.set(out.toString()); return t; } } The annotations are interpreted by the PDK as follows:\n @Description: provides metadata to Hive about a UDF\u0026rsquo;s syntax and usage. Only classes with this annotation will be included in the generated class-registration.sql @HivePdkUnitTests: enumerates one or more test cases, and also specifies optional setup and cleanup commands to run before and after the test cases. @HivePdkUnitTest: specifies one test case, consisting of the query to run and the expected result  Annotations allow the code and tests to be kept close together. This is good for small tests; if your tests are very complicated, you may want to set up your own scripting around the Hive CLI.\nTest Execution The PDK executes tests as follows:\n Run top-level cleanup.sql (in case a previous test failed in the middle) Run top-level setup.sql For each class with @HivePdkUnitTests annotation  Run class cleanup (if any) Run class setup (if any) For each @HivePdkUnitTest annotation, run query and verify that actual result matches expected result Run class cleanup (if any)   Run top-level cleanup.sql  If you encounter problems during test execution, look in the file TEST-org.apache.hive.pdk.PluginTest.txt for details.\nFutures  support annotations for other plugin types add more annotations for automatically validating function parameters at runtime (instead of requiring the developer to write imperative Java code for this) add Eclipse support move Hive builtins to use PDK for more convenient testing (HIVE-2523) command-line option for invoking a single testcase  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/plugindeveloperkit_27820324/","tags":null,"title":"Apache Hive : PluginDeveloperKit"},{"categories":null,"contents":"Apache Hive : PoweredBy Applications and organizations using Hive include (alphabetically):\nBizo We use Hive for reporting and ad hoc queries.\nChitika We use Hive for data mining and analysis on our 435M monthly global users.\nCNET We use Hive for data mining, internal log analysis and ad hoc queries.\nDigg We use Hive for data mining, internal log analysis, R\u0026amp;D, and reporting/analytics.\neHarmony We use Hadoop to store copies of internal log and dimension data sources and use it as a source for reporting/analytics and machine learning. Currently have a 640 machine cluster with ~5000 cores and 2PB raw storage. Each (commodity) node has 8 cores and 4 TB of storage.\nGrooveshark We use Hive for user analytics, dataset cleaning, and machine learning R\u0026amp;D.\nhi5 We use Hive for analytics, machine learning and social graph analysis.\nHubSpot We use Hive as part of a larger Hadoop pipeline to serve near-realtime web analytics.\nLast.fm We use Hive for various ad hoc queries.\nMedHelp Find a Doctor We implemented Hive to analyse large amounts of doctors across the United States, and for internal analytics for over 1M pageview/day.\nNexR  Replacing RDB\u0026amp;DW with Hadoop and Hive  We use hive for replacing Oracle DW, big data analysis and integrating R. We develop the enterprise Hive.\nPapertrail  Log analytics with Hadoop and Hive  We use Hive as a customer-facing analysis destination for our hosted syslog and app log management service.\nRocket Fuel We use Hive to host all our fact and dimension data. Off this warehouse, we do reporting, analytics, machine learning and model building, and various ad hoc queries.\nSaaSPulse We use Hive for analytics, machine learning and customer interaction analysis of web applications.\nScribd We use hive for machine learning, data mining, ad-hoc querying, and both internal and user-facing analytics\nTaoBao We use Hive for data mining, internal log analysis and ad-hoc queries. We also do some extensively developing work on Hive.\nTrending Topics Hot Wikipedia Topics, Served Fresh Daily. Powered by Cloudera Hadoop Distribution \u0026amp; Hive on EC2. We use Hive for log data normalization and building sample datasets for trend detection R\u0026amp;D.\nVideoEgg We use Hive as the core database for our data warehouse where we track and analyze all the usage data of the ads across our network.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/poweredby_27362055/","tags":null,"title":"Apache Hive : PoweredBy"},{"categories":null,"contents":"Apache Hive : Presentations   Hive Meetups  January 2016 Hive User Group Meetup November 2015 Hive Contributor Meetup April 2015 Hive Contributor Meetup Presentations February 2015 Hive User Meetup Presentation November 2013 Hive Contributors Meetup Presentations June 2013 Hadoop Summit Hive Meetup Presentations February 2013 Hive User Group Meetup June 2012 Hadoop Summit Hive Meetup Presentations November 2011 NYC Hive Meetup Presentations   Older Hive Presentations Related Work  Hive Meetups January 2016 Hive User Group Meetup  attachments/27362054/61337098-pptx attachments/27362054/61337312-ppsx attachments/27362054/61337398-ppsx Hive on Spark: now and future - Xuefu Zhang  November 2015 Hive Contributor Meetup  attachments/27362054/61329032-pptx attachments/27362054/61329033-pptx attachments/27362054/61329034-pptx attachments/27362054/61329036-pptx attachments/27362054/61329038-pptx attachments/27362054/61329039.pdf attachments/27362054/61329040-pptx attachments/27362054/61329311-ppsx  April 2015 Hive Contributor Meetup Presentations  SQLLine - The tale of the twins that everyone loved too much – Julian Hyde attachments/27362054/56131586.pdf attachments/27362054/55476525-pptx attachments/27362054/55476526-pptx  February 2015 Hive User Meetup Presentation  attachments/27362054/52036008-pptx (Szehon Ho) attachments/27362054/52036063.pdf (Ryan Desmond)  November 2013 Hive Contributors Meetup Presentations  attachments/27362054/35193152-pptx (Edward Capriolo) attachments/27362054/35193150-pptx (Gunther Hagleitner) attachments/27362054/35193153-pptx (Thejas Nair) attachments/27362054/35193154-pptx (Owen O\u0026rsquo;Malley) Apache Sentry (Brock Noland) attachments/27362054/35193151-pptx (John Pullokkaran) attachments/27362054/35193149-pptx (Ashutosh Chauhan)  June 2013 Hadoop Summit Hive Meetup Presentations  Hive Correlation Optimizer (Yin Huai)  February 2013 Hive User Group Meetup  attachments/27362054/30966692.pdf (Aaron Sun) attachments/27362054/30966693.pdf (Jerome Banks) attachments/27362054/30966691.pdf (Prasad Mujumdar) Case Study: Utilizing Windowing and Partitioned Table Functions with Hive (Murtaza Doctor) attachments/27362054/30966880.pdf (Ashutosh Chauhan)  June 2012 Hadoop Summit Hive Meetup Presentations  attachments/27362054/28017805.pdf (Ashutosh Chauhan, Hortonworks) attachments/27362054/28017803.pdf (Carl Steinbach, Cloudera) attachments/27362054/28017802.pdf (Shreepadma Venugopalan, Cloudera) attachments/27362054/28017801.pdf (Alan Gates, Hortonworks) attachments/27362054/28017807.pdf (Viral Bajaria, Hulu)  November 2011 NYC Hive Meetup Presentations  attachments/27362054/28016834.pdf (Edward Capriolo, Media 6 Degrees) HAWK: Performance Monitoring for Hive (JunHo Cho, NexR) RHive: Integrating R and Hive (JunHo Cho, NexR) attachments/27362054/28016835.pdf (Jim Falgout, Pervasive Software) attachments/27362054/28016838.pdf (Bennie Schut, eBuddy)  Older Hive Presentations  High Volume Updates in Hive from 2012 Hadoop Summit (Owen O\u0026rsquo;Malley) attachments/27362054/28016657.pdf from the 2011 Hadoop Summit (Liyin Tang, Namit Jain) Replacing an Oracle DB/DW with Hadoop/Hive from the 2011 Hadoop Summit Hive Contributor Meeting (JunHo Cho, NexR) RCFile Paper at ICDE 2011, Hannover, Germany (Yongqiang He (Facebook), Rubao Lee (OSU), Yin Huai (OSU), Zheng Shao (Facebook), Namit Jain (Facebook), Xiaodong Zhang (OSU), Zhiwei Xu (ICT, CAS)) Join Optimization in Hive (Liyin Tang, Facebook) Hive Presentation at ApacheCon NA 2010 (John Sichi, Facebook) Using Hadoop and Hive to Optimize Travel Search (Jonathan Seidman and Ramesh Venkataramaiah, Orbitz) HBase Meetup HUG10 - Hive/HBase Integration (John Sichi, Facebook) Hive User Group Presentation - Hive Quick Start Tutorial (Carl Steinbach, Cloudera) Hive User Group Presentation - New Features and APIs from Facebook (Ning Zhang, Yongqiang He, Namit Jain, John Sichi, Paul Yang, Zheng Shao, Facebook) Hive User Group Presentation from Netflix (Eva Tse and Jerome Boulon, Netflix) Hive Paper and Presentation at ICDE 2010, Long Beach, California (Raghotham Murthy and Namit Jain) Hive Presentation at QCon Nov 2009 (Ashish Thusoo and Namit Jain, Facebook) Hive Training – Motivations and Real World Use Cases, (Ning Zhang, Facebook, at Cloudera\u0026rsquo;s training session) User Defined Table Generating Functions, (Paul Yang, Facebook) Hive Anatomy – System \u0026amp; Pseudo-code level Architecture Review, (Ning Zhang, internal presentations, Facebook) Rethinking the Data Warehouse with Hadoop and Hive (Ashish Thusoo, Facebook at Hadoop World NYC 2009) Hive User Group Meeting August 2009, (Facebook) Hive Object Model, (Zheng Shao, Facebook) Hive: VLDB 2009, Lyon, France (Facebook) Hive: Hadoop Summit 2009, Santa Clara, CA, USA (Namit Jain, Zheng Shao, Facebook) Data Warehousing \u0026amp; Analytics on Hadoop, Percon Conference, Santa Clara, CA, USA (Ashish Thusoo, Prasad Chakka, Facebook) Large Scale Data Processing using commodity SW/HW, IIT-Delhi CS Dept., (Joydeep Sen Sarma, Facebook) An Introduction to Hive, Jeff Hammerbacher, Facebook Hive: Data Warehousing Analytics on Hadoop, UC Berkeley, (Joydeep Sarma, Namit Jain, Zheng Shao, Facebook) Hive: Data Warehousing with Hadoop, NYC Hadoop User Meetup (Jeff Hammerbacher, Cloudera) Facebook and Open Source, UIUC, (Zheng Shao, Facebook) Hive ApacheCon 2008, New Oreleans, LA (Ashish Thusoo, Facebook)  Related Work  Processing Theta-Joins using MapReduce (A. Okcan, M. Riedewald) Optimizing Joins in a Map-Reduce Environment (F. Afrati, J. Ullman) Efficient Parallel Set-Similarity Joins Using MapReduce (R. Vernica, M. Carey, C. Li) A Comparison of Join Algorithms for Log Processing in MapReduce (S. Blanas, J. Patel, V. Ercegovac, J. Rao, E. Shekita, Y. Tian) HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads (A. Abouzeid, K. Bajda-Pawlikowski, D. Abadi, A. Silberschatz, A. Rasin) Tenzing: A SQL Implementation On The MapReduce Framework Building a High-Level Dataflow System on top of Map-Reduce: The Pig Experience YSmart: Yet Another SQL-to-MapReduce Translator (R. Lee, T. Luo, Y. Huai, F. Wang, Y. He, X. Zhang) Query Optimization Using Column Statistics in Hive (A. Gruenheid, E. Omiecinski, L. Mark)   Attachments: attachments/27362054/28016657.pdf (application/pdf)\nattachments/27362054/28016834.pdf (application/pdf)\nattachments/27362054/28016835.pdf (application/pdf)\nattachments/27362054/28016838.pdf (application/pdf)\nattachments/27362054/28017801.pdf (application/pdf)\nattachments/27362054/28017802.pdf (application/pdf)\nattachments/27362054/28017803.pdf (application/pdf)\nattachments/27362054/28017805.pdf (application/pdf)\nattachments/27362054/28017807.pdf (application/pdf)\nattachments/27362054/30966691.pdf (application/pdf)\nattachments/27362054/30966693.pdf (application/pdf)\nattachments/27362054/30966692.pdf (application/pdf)\nattachments/27362054/30966880.pdf (application/pdf)\nattachments/27362054/35193149-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/35193150-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/35193151-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/35193152-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/35193153-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/35193154-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/52036008-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/52036063.pdf (application/pdf)\nattachments/27362054/55476524-key (application/x-iwork-keynote-sffkey)\nattachments/27362054/55476525-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/55476526-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/56131586.pdf (application/download)\nattachments/27362054/61329032-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61329033-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61329034-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61329036-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61329038-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61329039.pdf (application/pdf)\nattachments/27362054/61329040-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61329311-ppsx (application/vnd.openxmlformats-officedocument.presentationml.slideshow)\nattachments/27362054/61337098-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61337312-ppsx (application/vnd.openxmlformats-officedocument.presentationml.slideshow)\nattachments/27362054/61337313-pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nattachments/27362054/61337399-ppsx (application/vnd.openxmlformats-officedocument.presentationml.slideshow)\nattachments/27362054/61337398-ppsx (application/vnd.openxmlformats-officedocument.presentationml.slideshow)\nattachments/27362054/61337443.pdf (application/pdf)\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/presentations_27362054/","tags":null,"title":"Apache Hive : Presentations"},{"categories":null,"contents":"Apache Hive : Query ReExecution Query reexecution provides a facility to re-run the query multiple times in case of an unfortunate event happens.\n ReExecition strategies  Overlay Reoptimize Operator Matching   Configuration  Introduced in Hive 3.0 (HIVE-17626)\nReExecition strategies Overlay Enables to change the hive settings for all reexecutions which will be happening. It works by adding a configuration subtree as an overlay to the actual hive settings(reexec.overlay.*)\nExample\nset zzz=1; set reexec.overlay.zzz=2; set hive.query.reexecution.enabled=true; set hive.query.reexecution.strategies=overlay; create table t(a int); insert into t values (1); select assert_true(${hiveconf:zzz} \u0026gt; a) from t group by a; Every hive setting which has a prefix of \u0026ldquo;reexec.overlay\u0026rdquo; will be set for all reexecutions.\nA more real life example would be to disable join auto conversion for all reexecutions:\nset reexec.overlay.hive.auto.convert.join=false; Reoptimize During query execution; the actual number passing rows in every operator is tracked. This information is reused during re-planning which could result in a better plan.\nSituation in which this would be needed:\n missing statististics incorrect statistics many joins  It\u0026rsquo;s not that easy to craft queries which will lead to OOM situations; but to enable it:\nset hive.query.reexecution.strategies=overlay,reoptimize; Operator Matching Operator level statistics are matched to the new plan using operator subtree matching this also enables to match the information to a query which have \u0026ldquo;similar\u0026rdquo; parts.\nConfiguration    Configuration default      hive.query.reexecution.enabled true Feature enabler   hive.query.reexecution.strategies overlay,reoptimize reexecution plugins; currently overlay and reoptimize is supported   hive.query.reexecution.stats.persist.scope query runtime statistics can be persisted:* query: - only used during the reexecution hiveserver: persisted during the lifetime of the hiveserver metastore: persisted in the metastore; and loaded on hiveserver startup   hive.query.reexecution.max.count 1 number of reexecution that may happen   hive.query.reexecution.always.collect.operator.stats false Enable to gather runtime statistics on all queries.   hive.query.reexecution.stats.cache.batch.size -1 If runtime stats are stored in metastore; the maximal batch size per round during load.   hive.query.reexecution.stats.cache.size 100 000 Size of the runtime statistics cache. Unit is: OperatorStat entry; a query plan consist ~100.   runtime.stats.clean.frequency 3600s Frequency at which timer task runs to remove outdated runtime stat entries.   runtime.stats.max.age 3days Stat entries which are older than this are removed.    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/query-reexecution_87298873/","tags":null,"title":"Apache Hive : Query ReExecution"},{"categories":null,"contents":"Apache Hive : Query Results Caching (HIVE-18513) Introduction This document proposes the addition of a query results cache to Hive. Caching query results allows a previously computed query result to be re-used in the event that the same query is processed by Hive. This can save both time and resources spent running the cluster tasks required for the query.\nBackground Existing behavior for Hive query processing (very simplified):\n Hive query compilation takes the query string and produces a QueryPlan. The QueryPlan contains both the list of cluster tasks required for the query, as well as the FetchTask used to fetch results. The cluster tasks are configured to output the final results to a designated temp directory. The FetchTask contains a FetchOperator which is configured to read from the designated temp directory, as well as the input format, column information, and other information. During query execution (Driver.execute()), Hive will submit any cluster tasks (MR/Tez/Spark) required for the query. When the cluster tasks are finished the queryPlan’s FetchTask is used to read the query results from the temp directory (Driver.getResults()) Cleanup of the query results is handled by the Driver’s Context object. Several directory paths are cleaned up in this step, including the query results directory and parent directories of the results directory.  Proposed Changes New components: Results Cache\n Keeps a mapping of query string to cached results entry Used to lookup cache entries based on the query string Can receive query results from queries, for use by future queries Responsible for invalidating cache stale entries, and cleaning up resources used by the invalidated cache entries  Changes to query processing: During query compilation, check the results cache to see if it already has the query results. If there is a cache hit then the query plan will be set to a FetchTask that will read from the cached location.\nDuring query execution:\n If the results cache can be used for this query:   The query will simply be the FetchTask reading from the cached results directory. No cluster tasks will be required.   If the results cache cannot be used:   Run the cluster tasks as normal Check if the query results that have been computed are eligible to add to the results cache.   If results can be cached, the temporary results generated for the query will be saved to the results cache. Steps may need to be done here to ensure the query results directory is not deleted by query cleanup.  Considerations At what level of the query processing should results caching be applied There are several potential places the cache lookup can be done:\n Before parsing/compilation After parsing (with the AST available) but before full query optimization/planning After query compilation/planning  The current approach is for cache lookup to occur after AST generation but before full optimization. This can give us a chance to avoid full query optimization/planning. Access to the AST also allows Hive to fully qualify table references (to make sure the current query is using the same tables as the cached query), plus row filtering and column masking rewrites (from Ranger access rules) occur on the AST so this can still work with those.\n Matching the current query with entries in the results cache  Hive will need to check if the results for an incoming query are already saved in the results cache. A simple way to match the query with results cache entries would be to compare the incoming query text with the query text of the results cache entry. Many of the existing DBMS results caching solutions seem to cache results based on the query string.\nUnqualified table references need to be taken into account - depending on the current database at the time of the query, 2 queries running the exact same query string may access tables from completely different databases. So rather than performing the lookup based on the original SQL string, the query string should have any unqualified references resolved. This can be done using the AST and the UnparseTranslator.\nThere are similar issues with query string matching that apply to user-level security with row/column masking/filtering.\nIs the results cache independent or shared between different Hive instances? Currently each Hive instance has its own separate results cache. Originally the thinking was the query plan or AST would be part of the information saved in the cache, and might have been problematic to save and share between the different instances. But actually only the modified query string is saved for it which might make it possible to eventually have a shared cache. There are still a number of structures that are saved to properly set up the query (results schema, TableAccessInfo/ColumnAccessInfo, ReadEntities), so those would have to be saved somehow.\n ### Which queries can use the results cache\n Disallowed functions:\n * Non-deterministic functions (rand(), unix_timestamp(), \u0026hellip;)\n Runtime constant functions (current_date, current_timestamp, current_user(), current_database(), \u0026hellip;)   Disallowed tables:\n * Temporary tables\n External tables? If Hive relies on modifications to the underlying table to invalidate cached results, then external tables should not be allowed because these tables could be written outside of Hive.   Other conditions:\n * If the query has no cluster tasks (fetch-only query), no need to cache\nSaving query results from getting deleted by query cleanup To keep the query results for possible re-use, we need to make sure that the results directory is not deleted as part of query cleanup. Note that the Driver Context cleans up not only the query results directory but the parent directories of the results directory, which may complicate trying to save the query results directory while also performing the rest of the query cleanup.\nThe solution chosen will be to move the query results directory to a designated results cache directory to make sure that it does not get removed during query cleanup. This requires no changes to the existing query cleanup, and makes for simpler management of cached directories - all cached results can be under a single cache directory.\nColumn/Row Masking and Filtering Due to Ranger column/row masking, the query results from one user may look different that they would for a different user, even if the query text is the same. If results caching is enabled, then Hive will need to make sure that the cache lookup works correctly in this case, or else it will have to be disabled for masking/filtering.\nThe query rewrites that occur for Hive masking/filtering are done on the AST (SemanticAnalyzer.analyzeInternal()). So as long as the query string used for cache lookup can be derived from the rewritten AST (or the AST can be used during lookup), then query lookups should correctly distinguish cases where the query was modified from the original query text due to row/column filtering and masking.\n ### Invalidating results cache entries\n One issue with saving query results in a cache is that over time the entries in the results cache can become stale, if the underlying tables from the query are updated or dropped. A solution for this is to have a mechanism to invalidate the entries saved in the results cache. Some possible mechanisms for this:\n 1. Allow entries in the results cache to be valid for only a configured length of time. This is a simpler implementation which does not have to rely having to detect when the underlying tables of a cached query result have been changed. The disadvantage is that when the underlying tables are changed, any stale results would be served by the results cache until the cached result expires. 2. Add a command to clear the results cache. This can give the user a chance to prevent stale results from being served by the cache if they know that a table has been updated. Disadvantage is this requires manual intervention by the user to prevent stale results from being served. 3. Expire results cache entries if there are updates to the underlying tables. It may be possible to reuse the same mechanism that Materialized Views use to determine if any of the underlying tables of a cached query have been modified.\nCleanup of cache directories Each Hive instance can keep track of its cached results directories, and set them to be deleted on process exit. Under normal conditions this may be able to take care of cache directory cleanup, but in the case that the Hive process is terminated without a chance to perform graceful cleanup, these directories may still be left around.\nHive also has a cleanup mechanism for scratch directories (ClearDanglingScratchDir). It may be possible to reuse this for results cache cleanup. This cleanup works by creating a lockfile in the directory, and keeping the file open for the duration of the Hive instance. The cleanup thread will not delete the directory as long as the lock file is held by the Hive instance. This mechanism would work as long as each Hive instance is responsible for its own results cache directory (which is the current plan), as opposed to having a cache that is shared among different Hive instances.\nTable Locks At the start of query execution the Hive Driver will acquire locks for all tables being queried. This behavior should still remain even for queries that are using the cache - the read lock will prevent other users from being able to write to the underlying tables and invalidating the cache at the time the results cache is being checked for the query results.\n One possible problem is the fact that the cache lookup and decision to use the cache occurs during query compilation/semantic analysis, but the locks for the query are not acquired until query execution time. Between that period of time it might be possible for another query to have acquired the write lock and modified one of the tables used in the cache. When our query tries to acquire the read locks for the table, it will block until the write lock is done, and now when it is time for query execution the cache entry we found during compilation is now invalid due to the updates to the table. One solution to is would be to fail compilation with a RetryableException, so that the query can be retried - the next time the cache is checked the invalid entry will no longer be chosen. HIVE-17626 and its subtasks are supposed to allow queries to be retried in certain cases. This mechanism can be re-used here to allow the query to be re-run.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/75963441/","tags":null,"title":"Apache Hive : Query Results Caching (HIVE-18513)"},{"categories":null,"contents":"Apache Hive : RCFile RCFile (Record Columnar File) is a data placement structure designed for MapReduce-based data warehouse systems. Hive added the RCFile format in version 0.6.0.\nRCFile stores table data in a flat file consisting of binary key/value pairs. It first partitions rows horizontally into row splits, and then it vertically partitions each row split in a columnar way. RCFile stores the metadata of a row split as the key part of a record, and all the data of a row split as the value part.\nRCFile combines the advantages of both row-store and column-store to satisfy the need for fast data loading and query processing, efficient use of storage space, and adaptability to highly dynamic workload patterns.\n As row-store, RCFile guarantees that data in the same row are located in the same node. As column-store, RCFile can exploit column-wise data compression and skip unnecessary column reads.  A shell utility is available for reading RCFile data and metadata: see RCFileCat.\nFor details about the RCFile format, see:\n Javadoc for RCFile.java the 2011 ICDE conference paper \u0026ldquo;RCFile: A Fast and Space-efficient Data Placement Structure in MapReduce-based Warehouse Systems\u0026rdquo; by Yongqiang He, Rubao Lee, Yin Huai, Zheng Shao, Namit Jain, Xiaodong Zhang, and Zhiwei Xu.   ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/rcfile_58851803/","tags":null,"title":"Apache Hive : RCFile"},{"categories":null,"contents":"Apache Hive : RCFileCat RCFileCat  RCFileCat  Data Metadata    $HIVE_HOME/bin/hive \u0026ndash;rcfilecat is a shell utility which can be used to print data or metadata from RC files.\nData Prints out the rows stored in an RCFile, columns are tab separated and rows are newline separated.\nUsage:\nhive --rcfilecat [--start=start_offset] [--length=len] [--verbose] fileName --start=start_offset Start offset to begin reading in the file --length=len Length of data to read from the file --verbose Prints periodic stats about the data read, how many records, how many bytes, scan rate Metadata New in 0.11.0\nUsage:\nhive --rcfilecat [--column-sizes | --column-sizes-pretty] fileName With the \u0026ndash;column-sizes option set, instead of printing the data in the RC file, prints rows with 3 columns.\nThe sizes of the columns are the aggregated sizes of the column in the entire file taken from the RC file headers.\nWith the \u0026ndash;column-sizes-pretty option set prints the same data as is printed with the \u0026ndash;column-sizes option but with a more human friendly format.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/rcfilecat_30748712/","tags":null,"title":"Apache Hive : RCFileCat"},{"categories":null,"contents":"Apache Hive : Rebalance compaction In order to improve performance, Hive under the hood creates bucket files even for non-explicitly bucketed tables. Depending on the usage, the data loaded into these non-explicitly bucketed full-acid ORC tables may lead to unbalanced distribution, where some of the buckets are much larger (\u0026gt; 100 times) than the others. Unbalanced tables has performance penalty, as larger buckets takes more time to read. Rebalance compaction addresses this issue by equally redistributing the data among the implicit bucket files.\n Rebalance compaction is never initiated automatically. Unlike other compaction types, rebalance compaction puts an exclusive write lock on the table. SQL example:  ALTER TABLE table_name COMPACT 'REBALANCE'; Number of implicit buckets It is possible to set the desired number of implicit buckects during a rebalance compaction.\nSQL example:\nALTER TABLE table_name COMPACT 'REBALANCE' CLUSTERED INTO n BUCKETS; If the bucket number is not supplied, the number of buckets will remain the same, only the data will be redistributed among them.\nOrdering Optionally, an order by expression can be supplied, to be able to re-order the data during the rebalance.\nSQL example:\nALTER TABLE table_name COMPACT 'REBALANCE' ORDER BY column_name DESC; If the order by expression is not set, the ordering of the data remains the same after the compaction.\nLimitations  Rebalance compaction can be done only within partitions. Rebalance compaction is supported only on implicitly bucketed tables, clustered tables are not supported. Rebalance compaction is supported only on full-acid tables, insert-only tables are not supported. Rebalance compaction is possible only via query-based compaction, MR based compaction is not supported.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/rebalance-compaction_240884502/","tags":null,"title":"Apache Hive : Rebalance compaction"},{"categories":null,"contents":"Apache Hive : ReflectUDF Reflect (Generic) UDF A Java class and method often exists to handle the exact function a user would like to use in Hive. Rather than having to write a wrapper UDF to call this method, the majority of these methods can be called using reflect UDF. Reflect uses Java reflection to instantiate and call methods of objects; it can also call static functions. The method must return a primitive type or a type that Hive knows how to serialize.\nSELECT reflect(\u0026quot;java.lang.String\u0026quot;, \u0026quot;valueOf\u0026quot;, 1), reflect(\u0026quot;java.lang.String\u0026quot;, \u0026quot;isEmpty\u0026quot;), reflect(\u0026quot;java.lang.Math\u0026quot;, \u0026quot;max\u0026quot;, 2, 3), reflect(\u0026quot;java.lang.Math\u0026quot;, \u0026quot;min\u0026quot;, 2, 3), reflect(\u0026quot;java.lang.Math\u0026quot;, \u0026quot;round\u0026quot;, 2.5), reflect(\u0026quot;java.lang.Math\u0026quot;, \u0026quot;exp\u0026quot;, 1.0), reflect(\u0026quot;java.lang.Math\u0026quot;, \u0026quot;floor\u0026quot;, 1.9) FROM src LIMIT 1; 1\ttrue\t3\t2\t3\t2.7182818284590455\t1.0 Version information\nAs of Hive 0.9.0, java_method() is a synonym for reflect(). See Misc. Functions in Hive Operators and UDFs.\nNote that Reflect UDF is non-deterministic since there is no guarantee what a specific method will return given the same parameters. So be cautious when using Reflect on the WHERE clause because that may invalidate Predicate Pushdown optimization.\nComments:            This doc comes from the Hive xdocs, with minor edits. It is included here because the xdocs are currently unavailable (Feb. 2013).    Posted by leftyl at Feb 21, 2013 09:30 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/reflectudf_30754716/","tags":null,"title":"Apache Hive : ReflectUDF"},{"categories":null,"contents":"Apache Hive : RelatedProjects Shark Shark is a fork of Apache Hive that uses Spark in place of MapReduce.\nApache Hivemall (incubating) Apache Hivemall is a scalable machine learning library for Apache Hive, Apache Spark, and Apache Pig.\nApache Sentry (incubating) Sentry is a role-based authorization system for Apache Hive.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/relatedprojects_34836686/","tags":null,"title":"Apache Hive : RelatedProjects"},{"categories":null,"contents":"Apache Hive : Replacing the Implementation of Hive CLI Using Beeline    Why Replace the Existing Hive CLI? Hive CLI Functionality Support  Hive CLI Options Support Examples Hive CLI Interactive Shell Commands Support Hive CLI Configuration Support   Performance Impacts  Why Replace the Existing Hive CLI? Hive CLI is a legacy tool which had two main use cases. The first is that it served as a thick client for SQL on Hadoop and the second is that it served as a command line tool for Hive Server (the original Hive server, now often referred to as \u0026ldquo;HiveServer1\u0026rdquo;). Hive Server has been deprecated and removed from the Hive code base as of Hive 1.0.0 (HIVE-6977) and replaced with HiveServer2 (HIVE-2935), so the second use case no longer applies. For the first use case, Beeline provides or is supposed to provide equal functionality, yet is implemented differently from Hive CLI.\nIdeally, Hive CLI should be deprecated as the Hive community has long recommended using the Beeline plus HiveServer2 configuration; however, because of the wide use of Hive CLI, we instead are replacing Hive CLI\u0026rsquo;s implementation with a new Hive CLI on top of Beeline plus embedded HiveServer2 (HIVE-10511) so that the Hive community only needs to maintain a single code path. In this way, the new Hive CLI is just an alias to Beeline at both the shell script level and the high code level. The goal is that no or minimal changes are required from existing user scripts using Hive CLI.\nHive CLI Functionality Support We use a new Hive CLI on top of Beeline to implement the Hive CLI functionality. Since some existing Hive CLI features are not supported in the new Hive CLI, we are using the old Hive client implementation by default. Use the following command to specify the new Beeline-based Hive CLI tool:\nexport USE_DEPRECATED_CLI=false Note that the log4j configuration file has been changed to \u0026ldquo;beeline-log4j.properties\u0026rdquo;. Hive CLI Options Support To get help, run \u0026ldquo;hive -H\u0026rdquo; or \u0026ldquo;hive --help\u0026rdquo;.\nusage: hive -d,--define \u0026lt;key=value\u0026gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --database \u0026lt;databasename\u0026gt; Specify the database to use -e \u0026lt;quoted-query-string\u0026gt; SQL from command line -f \u0026lt;filename\u0026gt; SQL from files -H,--help Print help information --hiveconf \u0026lt;property=value\u0026gt; Use value for given property --hivevar \u0026lt;key=value\u0026gt; Variable subsitution to apply to hive commands. e.g. --hivevar A=B -i \u0026lt;filename\u0026gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console) Examples  Example of running a query from the command line  $HIVE_HOME/bin/hive -e 'select a.foo from pokes a'  Example of setting Hive configuration variables  $HIVE_HOME/bin/hive -e 'select a.foo from pokes a' --hiveconf hive.exec.scratchdir=/opt/my/hive_scratch --hiveconf mapred.reduce.tasks=1  Example of dumping data out from a query into a file using silent mode  $HIVE_HOME/bin/hive -S -e 'select a.foo from pokes a' \u0026gt; a.txt  Example of running a script non-interactively from local disk  $HIVE_HOME/bin/hive -f /home/my/hive-script.sql  Example of running a script non-interactively from a Hadoop supported filesystem (starting in Hive 0.14)  $HIVE_HOME/bin/hive -f hdfs://\u0026lt;namenode\u0026gt;:\u0026lt;port\u0026gt;/hive-script.sql Hive CLI Interactive Shell Commands Support When $HIVE_HOME/bin/hive is run without either the -e or -f option, it enters interactive shell mode.\nUse \u0026ldquo;;\u0026rdquo; (semicolon) to terminate commands. Comments in scripts can be specified using the \u0026ldquo;\u0026ndash;\u0026rdquo; prefix.\n   Command Description     quit exit Use quit or exit to leave the interactive shell.   reset Resets the configuration to the default values (as of Hive 0.10: see HIVE-3202).   set = Sets the value of a particular configuration variable (key). Note: If you misspell the variable name, the CLI will not show an error.   set Prints a list of configuration variables that are overridden by the user or Hive.   set -v Prints all Hadoop and Hive configuration variables.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * Adds one or more files, jars, or archives to the list of resources in the distributed cache. See Hive Resources for more information.   add FILE[S] * add JAR[S] * add ARCHIVE[S] * As of Hive 1.2.0, adds one or more files, jars or archives to the list of resources in the distributed cache using an Ivy URL of the form ivy://group:module:version?query_string. See Hive Resources for more information.   list FILE[S] list JAR[S] list ARCHIVE[S] Lists the resources already added to the distributed cache. See Hive Resources for more information.   list FILE[S] * list JAR[S] * list ARCHIVE[S] * Checks whether the given resources are already added to the distributed cache or not. See Hive Resources for more information.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * Removes the resource(s) from the distributed cache.   delete FILE[S] * delete JAR[S] * delete ARCHIVE[S] * As of Hive 1.2.0, removes the resource(s) which were added using the from the distributed cache. See Hive Resources for more information.   !  Executes a shell command from the Hive shell.   dfs  Executes a dfs command from the Hive shell.    Executes a Hive query and prints results to standard output.   source FILE  Executes a script file inside the CLI.    Examples of shell commands:\nhive\u0026gt; source /root/test.sql; hive\u0026gt; show tables; test1 test2 hive\u0026gt; exit; hive\u0026gt; quit; hive\u0026gt; set; hive\u0026gt; set hive.cli.print.header=true; hive\u0026gt; set -v; hive\u0026gt; reset; hive\u0026gt; add file /opt/a.txt; Added resources: [/opt/a.txt] hive\u0026gt; list files; /opt/a.txt hive\u0026gt; delete file /opt/a.txt; hive\u0026gt; add jar /usr/share/vnc/classes/vncviewer.jar; Added [/usr/share/vnc/classes/vncviewer.jar]to class path Added resources:[/usr/share/vnc/classes/vncviewer.jar] hive\u0026gt; list jars; /usr/share/vnc/classes/vncviewer.jar hive\u0026gt; delete jar /usr/share/vnc/classes/vncviewer.jar; hive\u0026gt; !ls; bin conf hive\u0026gt; dfs -ls / ; Found 2 items drwx-wx-wx - root supergroup 0 2015-08-12 19:06 /tmp drwxr-xr-x - root supergroup 0 2015-08-12 19:43 /user hive\u0026gt; select * from pokes; OK pokes.foo pokes.bar 238 val_238 86 val_86 311 val_311 hive\u0026gt;source /opt/s.sql; Hive CLI Configuration Support    Configuration Name Supported in New Hive CLI Description     hive.cli.print.header Yes Whether to print the names of the columns in query output. HIVE-11624   hive.cli.errors.ignore Yes Whether to force execution of a script when errors occurred. HIVE-11191   hive.cli.prompt Yes Command line prompt configuration value. Other hiveconf can be used in this configuration value. HIVE-11226   hive.cli.pretty.output.num.cols Yes The number of columns to use when formatting output generated by the DESCRIBE PRETTY table_name command. HIVE-11779   hive.cli.print.current.db Yes Whether to include the current database in the Hive prompt. HIVE-11637    Performance Impacts Using the JMH to measure the average time cost when retrieving a data set, we have the following results.\nBenchmark Mode Samples Score Error Units o.a.h.b.c.CliBench.BeeLineDriverBench.testSQLWithInitialFile avgt 1 1713326099.000 ? NaN ns/op o.a.h.b.c.CliBench.CliDriverBench.testSQLWithInitialFile avgt 1 1852995786.000 ? NaN ns/op The lower the score the better since we are evaluating the time cost. There is no clear performance gap in terms of retrieving data.\n  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/replacing-the-implementation-of-hive-cli-using-beeline_61311909/","tags":null,"title":"Apache Hive : Replacing the Implementation of Hive CLI Using Beeline"},{"categories":null,"contents":"Apache Hive : Replication  Overview Potential Uses Prerequisites Limitations Configuration Typical Mode of Operation Replication to AWS/EMR/S3  Overview Hive Replication builds on the metastore event and ExIm features to provide a framework for replicating Hive metadata and data changes between clusters. There is no requirement for the source cluster and replica to run the same Hadoop distribution, Hive version, or metastore RDBMS. The replication system has a fairly \u0026lsquo;light touch\u0026rsquo;, exhibiting a low degree of coupling and using the Hive-metastore Thrift service as an integration point. However, the current implementation is not an \u0026lsquo;out of the box\u0026rsquo; solution. In particular it is necessary to provide some kind of orchestration service that is responsible for requesting replication tasks and executing them.\nSee HiveReplicationDevelopment for information on the design of replication in Hive.\n A more advanced replication mechanism is being implemented in Hive to address some of the limitations of this mode. See HiveReplicationv2Development for details.\nPotential Uses  Disaster recovery clusters. Copying data into clouds for off-premise processing.  Prerequisites  You must be running Hive 1.1.0 or later at your replication source (for DbNotificationListener support). You must be running Hive 0.8.0 or later at your replication destination (for IMPORT support). You\u0026rsquo;ll require Hive 1.2.0 or later JAR dependencies to instantiate and execute ReplicationTasks. This is not a cluster requirement; it is needed only for the service orchestrating the replication. You will initially require administration privileges on the source cluster to enable the writing of notifications to the metastore database.  Limitations  While the metastore events feature allows the sinking of notifications to anything implementing MetaStoreEventListener, the implementation of the replication feature can only source events from the metastore database and hence the DbNotificationListener must be used. Data appended to tables or partitions using the HCatalogWriters will not be automatically replicated as they do not currently generate metastore notifications (HIVE-9577). This is likely only a consideration if data is being written to table by processes outside of Hive.  Configuration To configure the persistence of metastore notification events it is necessary to set the following [hive-site.xml](#hive-site-xml) properties on the source cluster. A restart of the metastore service will be required for the settings to take effect.\nhive-site.xml Configuration for Replication\n \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.event.listeners\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hive.hcatalog.listener.DbNotificationListener\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.event.db.listener.timetolive\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;86400s\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; The system uses the org.apache.hive.hcatalog.api.repl.exim.EximReplicationTaskFactory by default. This uses EXPORT and IMPORT commands to capture, move, and ingest the metadata and data that need to be replicated. However, it is possible to provide custom implementations by setting the hive.repl.task.factory Hive configuration property.\nTypical Mode of Operation  With the metastore event configuration in place on the source cluster, the NOTIFICATION_LOG table in the metastore will be populated with events on the successful execution of metadata operations such as CREATE, ALTER, and DROP. These events can be read and converted into ReplicationTasks using org.apache.hive.hcatalog.api.HCatClient.getReplicationTasks(long, int, String, String). ReplicationTasks encapsulate a set of commands to execute on the source Hive instance (typically to export data) and another set to execute on the replica instance (typically to import data). The commands are provided as Hive SQL strings. The ReplicationTask also serves as a place where database and table name mappings can be declared and StagingDirectoryProvider implementations configured for the resolution of paths at both the source and destination:  org.apache.hive.hcatalog.api.repl.ReplicationTask.withDbNameMapping(Function\u0026lt;String, String\u0026gt;) org.apache.hive.hcatalog.api.repl.ReplicationTask.withTableNameMapping(Function\u0026lt;String, String\u0026gt;) org.apache.hive.hcatalog.api.repl.ReplicationTask.withSrcStagingDirProvider(StagingDirectoryProvider) org.apache.hive.hcatalog.api.repl.ReplicationTask.withDstStagingDirProvider(StagingDirectoryProvider)   The Hive SQL commands provided by the tasks must then be executed against the source Hive and then the destination (aka the replica). One way of doing this is to open up a JDBC connection to the respective HiveServer and submit the task\u0026rsquo;s Hive SQL queries. It is necessary to maintain the position within the notification log so that replication tasks are applied only once. This can be achieved by maintaining a record of the last successfully executed event\u0026rsquo;s id (task.getEvent().getEventId()) and providing this as an offset when sourcing the next batch of events. To avoid losing or missing events that require replication, it may be wise to poll for replication tasks at a frequency significantly greater than derived from the hive.metastore.event.db.listener.timetolive property. If notifications are not consumed in a timely manner they may be purged from the table before they can be actioned by the replication service.  Replication to AWS/EMR/S3 At this time it is not possible to replicate to tables on EMR that have a path location in S3. This is due to a bug in the dependency of the IMPORT command in the EMR distribution (checked in AMI-4.2.0). Also, if using the EximReplicationTaskFactory you may need to add the relevant S3 protocols to your Hive configurations:\nHiveConf Configuration for ExIm on S3\n \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.exim.uri.scheme.whitelist\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hdfs,s3a\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt;  Save\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/replication_61336919/","tags":null,"title":"Apache Hive : Replication"},{"categories":null,"contents":"Apache Hive : Roadmap Before adding to the list below, please check JIRA to see if a ticket has already been opened for the feature. If not, please open a ticket on the Hive JIRA and also update the following list.\nFeatures to be added Major Recent Changes  Table Statistics Archiving Indexing First Cut Concurrency Conversion to Map-Join at Runtime Support for Multiple Distincts Remove Partition Filtering Conditions INSERT INTO statement Block-level merge HAVING clause support Cross-database queries Bitmap Index Use Filter Pushdown for Automatically Accessing Indexes Remove Duplicate Filters Authentication Authorization  Current Projects  Bloom Filters TIMESTAMP data type  Up For Grabs Priorities are denoted as P0 \u0026gt; P1 \u0026gt; \u0026hellip;\nQuery Optimization  P0 Optimizing JOIN followed by GROUP BY  A lot of analytics queries are JOINs followed by GROUP BY (join keys and group by keys may or may not be the same or related). We need a better optimization for this kind of query (optimize number of MapReduce Jobs vs. optimize data transfer size etc.)   P0 Optimize JOINs using Bloom Filters  This is to optimize the case where two big tables are joined but the results are small.   P1 Column-level statistics  We already have UDAFs for percentile, histogram etc. We need to figure out a smart way to compute column-level (approximate) stats in a streaming fashion (during/piggy-backing the scan/loading data process) without firing a new query.   P1 Determining whether to use skew for a count distinct in group by  Need column level stats or a sample task before the real query.   P1 Exploit clustering information in the input data  Some data are generated using \u0026lsquo;GROUP BY\u0026rsquo;, \u0026lsquo;CLUSTER BY\u0026rsquo;, \u0026lsquo;ORDER BY\u0026rsquo; etc. that implicit ordering in the data. We should exploit this metadata to do better join in choosing joins (e.g., map-side sort-merge join).   Cost-based Optimization  Query Execution  P0 Native Support for types of numbers, datetime, and IP addresses  Currently most numbers, datetimes, and IP addresses are treated as strings. If we know their data types we should store these data types in a more effective manner (see also \u0026ldquo;Binary Storage and SerDe\u0026rdquo;).   P0 Create a URL data type that is more friendly to data compression.  URLs are very long and can contribute to large portion of storage. They are treated as generic strings and use string compression algorithms. We should investigate if there are ways to treate URL data types in a clever way so that the compression algorithm (may be a customized compression algorithm) can compress the data better in small enough overhead.   P1 Binary storage format and SerDe  The idea is to store data in their native format (rather than converting to UTF-8 string type) before stored on disk. This can potentially save a lot of CPU/IO cost in data conversion and object creations. Also investigating the compression technics used in other column-stores that does not require decompression before query (filtering). Revisit the LazyBinarySerDe and see if they can be reused or extended to this storage format. Make this storage format amenable to mmap() (or FileChannel in Java) so that the system can skip I/O if memory random access can skip part of data (e.g., columns)   Support for IN, exists and correlated subqueries More native types - Enums, timestamp Persistent UDF\u0026rsquo;s SQL/OLAP Storage handler improvements System views JDBC/ODBC improvements mapred to mapreduce transition (no longer needed since mapred got undeprecated)  Metadata Management  P0 Reducing the size of the metastore ‚Äì whatever it takes: some ideas were to not store COLUMNS etc.  One idea here is to introduce something like a schema object that contains the column objects. Partitions would inherit the schema object, which would only change when the table schema changes. Also work would be involved to migrate the existing setup. This would greatly reduce the number of rows in columns (current in the billions).   P1 View improvements  Test, Error Messages and Debugging  P0 Heavy-duty test infrastructure Automated code coverage reports Hive CLI improvement/Error messages HiveServer robustness Debuggability / Resumability:  Show users the last portion of the data that caused the task to fail Restart a job with a particular mapper (that failed earlier, for debugging purposes) Resume at map-reduce job level.    ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/roadmap_27362057/","tags":null,"title":"Apache Hive : Roadmap"},{"categories":null,"contents":"Apache Hive : Running Yetus Overview Yetus is added to Hive in release 3.0.0 to run checks on the new patches. See HIVE-15051.\nThere are several rules already defined by the community, but most of them are not enforced.\nYetus helps us by checking these rules for newly introduced errors. Note that Yetus checks only the changed part of the code. If any unchanged code contains errors, then Yetus will not report them, but all of the new code should conform to the rules.\nThe following Yetus plugins are used in the Hive personality:\n asflicense – Rat check to validate ASF headers. author – Checks that there is not @author tag in the files. checkstyle – Runs checkstyle. findbugs – Runs findbugs. compile – Shows compile warnings. javadoc – Shows javadoc problems. whitespace – Checks for extra whitespaces. xml – Checks xml validity.  Findbugs To run findbugs checks, the findbugs binary should be installed on the computer and the FINDBUGS_HOME environment variable should be set. The binary can be downloaded here.\nexport FINDBUGS_HOME=~/dev/upstream/findbugs-3.0.1/ Running Yetus First checkout a copy of the branch you are targeting without your commits.\nThen run the checks with the following command:\n./dev-support/test-patch.sh ~/Downloads/HIVE-16345.2.patch ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/running-yetus_71012969/","tags":null,"title":"Apache Hive : Running Yetus"},{"categories":null,"contents":"Apache Hive : Scheduled Queries  Maintaining scheduled queries  Create Scheduled query syntax Alter Scheduled query syntax Drop syntax scheduleSpecification syntax  CRON based schedule syntax EVERY based schedule syntax   ExecutedAs syntax enableSpecification syntax Defined AS syntax executeSpec syntax   System tables/views  information_schema.scheduled_queries information_schema.scheduled_executions  Execution states     Configuration  Hive metastore related configuration HiveServer2 related configuration   Examples  Example 1 – basic example of using schedules Example 2 – analyze external table periodically Example 3 – materialized view rebuild Example 4 – Ingestion    Intro\nExecuting statements periodically can be usefull in\n Pulling informations from external systems Periodically updating column statistics Rebuilding materialized views  Overview\n The metastore maintains the scheduled queries in the metastore database Hiveserver(s) periodically polls the metastore for a scheduled query to be executed   During execution informations about ongoing/finished executions are kept in the metastore  Scheduled queries were added in Hive 4.0 (HIVE-21884)\nHive has it’s scheduled query interface built into the language itself for easy access:\nMaintaining scheduled queries Create Scheduled query syntax CREATE SCHEDULED QUERY \u0026lt;scheduled_query_name\u0026gt;\n [ ]\n[]\n\nAlter Scheduled query syntax ALTER SCHEDULED QUERY \u0026lt;scheduled_query_name\u0026gt; (||||);\nDrop syntax DROP SCHEDULED QUERY \u0026lt;scheduled_query_name\u0026gt;;\nscheduleSpecification syntax Schedules can be specified using CRON expressions or for common cases there is a simpler form; in any case the schedule is stored as Quartz cron expression.\nCRON based schedule syntax CRON \u0026lt;quartz_schedule_expression\u0026gt;\nwhere quartz_schedule_expression is quoted schedule in the Quartz format\nhttps://www.freeformatter.com/cron-expression-generator-quartz.html\nFor example the CRON '0 */10 * * * ? *' expression will fire every 10 minutes.\nEVERY based schedule syntax To give a more readable way to declare schedules EVERY can be used.\nEVERY [] (SECOND|MINUTE|HOUR) [(OFFSET BY|AT) ]\nthe format makes it possible to declare schedules in a more readable way:\nEVERY 2 MINUTES EVERY HOUR AT \u0026lsquo;0:07:30\u0026rsquo;\nEVERY DAY AT \u0026lsquo;11:35:30\u0026rsquo;\nExecutedAs syntax EXECUTED AS \u0026lt;user_name\u0026gt;\nScheduled queries are executed as the declaring user by default; but people with admin privileges might be able to change the executing user.\nenableSpecification syntax (ENABLE[D] | DISABLE[D])\nCan be used to enable/disable a schedule.\nFor CREATE SCHEDULED QUERY statements the default behaviour is set by the configuration key hive.scheduled.queries.create.as.enabled\nIn case there are in-flight scheduled executions at the time when the corresponding schedule is disabled - the already running executions will still finish. But no more executions will be triggered.\nDefined AS syntax [DEFINED] AS \nThe “query” is a single statement expression to be scheduled for execution.\nexecuteSpec syntax EXECUTE\nChanges the schedules next execution time to be now. Could be useful during debugging/development.\nSystem tables/views Informations about scheduled queries/executions can be obtain by using the information_schema or the sysdb - recommended way is to use the information_schema; sysdb is tables are there to build the information_schema level views - and for debugging.\ninformation_schema.scheduled_queries Suppose we have a scheduled query defined by:\n**create scheduled query sc1 cron \u0026lsquo;0 /10 * * * ? \u0026rsquo; as select 1;\nLet’s take a look at it in the information_schema.scheduled_queries table by using\nselect * from information_schema.scheduled_queries;\nI will transpose the resultset to describe each column\n| scheduled_query_id | 1 | Internally, every scheduled query also has a numeric id | | schedule_name | sc1 | The name of the schedule | | enabled | true | True if the schedule is enabled | | cluster_namespace | hive | The namespace thes scheduled query belongs to | | schedule | 0 */10 * * * ? * | The schedule described in QUARTZ cron format | | user | dev | The owner/executor of the query | | query | select 1 | The query being scheduled | | next_execution | 2020-01-29 16:50:00 | Technical column; shows when the next execution should happen |\n(schedule_name,cluster_namespace) is unique\ninformation_schema.scheduled_executions This view can be used to get information about recent scheduled query executions.\nselect * from information_schema.scheduled_executions;\nOne record in this view has the following informations:\n| scheduled_execution_id | 13 | Every scheduled query execution has a unique numeric id | | schedule_name | sc1 | The schedule name to which this execution belongs | | executor_query_id | dev_20200131103008_c9a39b8d-e26b-44cd-b8ae-9d054204dc07 | The query id assigned by the execution engine for the given scheduled execution | | state | FINISHED | State of the execution; can be | | start_time | 2020-01-31 10:30:06 | Start time of execution | | end_time | 2020-01-31 10:30:08 | End time of execution | | elapsed | 2 | (computed) end_time-start_time | | error_message | NULL | In case the query is FAILED the error message is shown here | | last_update_time | NULL | During execution the last update time the executor provided informations about the state |\nExecution states | INITED | The scheduled execution record is created at the time an executor is assigned to run it;The INITED state is retained until the first update from the executor comes in. | | EXECUTING | Queries in executing state are being processed by the executor; during this phase the executor reports the progress of the query in intervals defined by: hive.scheduled.queries.executor.progress.report.interval | | FAILED | The query execution stoped by an error code(or an exception) when this state is set the error_message is also filled. | | FINISHED | The query finished without problems | | TIMED_OUT | An execution is considered timed out when it’s being executed for more than metastore.scheduled.queries.execution.timeout.The scheduled queries maintenance task checks for any timed out executions. |\nHow long are execution informations are retained?\nThe scheduled query maintenance task removes older than metastore.scheduled.queries.execution.max.age entries.\nConfiguration Hive metastore related configuration  metastore.scheduled.queries.enabled (default: true) Controls the metastore side support for scheduled queries; forces all HMS scheduled query related endpoints to return with an error metastore.scheduled.queries.execution.timeout (default: 2 minutes) In case a scheduled execution is not updated for at least this amount of time; it’s state will be changed to TIMED_OUT by the cleaner task metastore.scheduled.queries.execution.maint.task.frequency (default: 1 minute)\nInterval of scheduled query maintenance task. Which removes executions above max age; and marks executions as TIMED_OUT if the condition is met metastore.scheduled.queries.execution.max.age (default: 30 days)\nMaximal age of a scheduled query execution entry before it is removed.  HiveServer2 related configuration  hive.scheduled.queries.executor.enabled (default: true) Controls whether HS2 will run scheduled query executor. hive.scheduled.queries.namespace (default: \u0026ldquo;hive\u0026rdquo;) Sets the scheduled query namespace to be used. New scheduled queries are created in this namespace; and execution is also bound to the namespace hive.scheduled.queries.executor.idle.sleep.time (default: 1 minute) Time to sleep between querying for the presence of a scheduled query. hive.scheduled.queries.executor.progress.report.interval (default: 1 minute) While scheduled queries are in flight; a background update happens periodically to report the actual state of the query. hive.scheduled.queries.create.as.enabled (default: true) This option sets the default behaviour of newly created scheduled queries. hive.security.authorization.scheduled.queries.supported (default: false) Enable this if the configured authorizer is able to handle scheduled query related calls.  Examples Example 1 – basic example of using schedules create table t (a integer); -- create a scheduled query; every 10 minute insert a new row create scheduled query sc1 cron '0 */10 * * * ? *' as insert into t values (1); -- depending on hive.scheduled.queries.create.as.enabled the query might get create in disabled mode -- it can be enabled using: alter scheduled query sc1 enabled; -- inspect scheduled queries using the information_schema select * from information_schema.scheduled_queries s where schedule_name='sc1'; +-----------------------+------------------+------------+----------------------+-------------------+---------+-----------+----------------------+ | s.scheduled_query_id | s.schedule_name | s.enabled | s.cluster_namespace | s.schedule | s.user | s.query | s.next_execution | +-----------------------+------------------+------------+----------------------+-------------------+---------+-----------+----------------------+ | 1 | sc1 | true | hive | 0 */10 * * * ? * | dev | select 1 | 2020-02-03 15:10:00 | +-----------------------+------------------+------------+----------------------+-------------------+---------+-----------+----------------------+ -- wait 10 minutes or execute by issuing: alter scheduled query sc1 execute; select * from information_schema.scheduled_executions s where schedule_name='sc1' order by scheduled_execution_id desc limit 1; +---------------------------+------------------+----------------------------------------------------+-----------+----------------------+----------------------+------------+------------------+---------------------+ | s.scheduled_execution_id | s.schedule_name | s.executor_query_id | s.state | s.start_time | s.end_time | s.elapsed | s.error_message | s.last_update_time | +---------------------------+------------------+----------------------------------------------------+-----------+----------------------+----------------------+------------+------------------+---------------------+ | 496 | sc1 | dev_20200203152025_bdf3deac-0ca6-407f-b122-c637e50f99c8 | FINISHED | 2020-02-03 15:20:23 | 2020-02-03 15:20:31 | 8 | NULL | NULL | +---------------------------+------------------+----------------------------------------------------+-----------+----------------------+----------------------+------------+------------------+---------------------+ Example 2 – analyze external table periodically Suppose you have an external table - the contents of it is slowly changing\u0026hellip;which will eventually lead that Hive will utilize outdated statistics during planning time\n-- create external table create external table t (a integer); -- see where the table lives: desc formatted t; [...] | Location: | file:/data/hive/warehouse/t | NULL | [...] -- in a terminal; load some data into the table directory: seq 1 10 \u0026gt; /data/hive/warehouse/t/f1 -- back in hive you will see that select count(1) from t; 10 -- meanwhile basic stats show that the table has \u0026quot;0\u0026quot; rows desc formatted t; [...] | | numRows | 0 | [...] create scheduled query t_analyze cron '0 */1 * * * ? *' as analyze table t compute statistics for columns; -- wait some time or execute by issuing: alter scheduled query t_analyze execute; select * from information_schema.scheduled_executions s where schedule_name='ex_analyze' order by scheduled_execution_id desc limit 3; +---------------------------+------------------+----------------------------------------------------+------------+----------------------+----------------------+------------+------------------+----------------------+ | s.scheduled_execution_id | s.schedule_name | s.executor_query_id | s.state | s.start_time | s.end_time | s.elapsed | s.error_message | s.last_update_time | +---------------------------+------------------+----------------------------------------------------+------------+----------------------+----------------------+------------+------------------+----------------------+ | 498 | t_analyze | dev_20200203152640_a59bc198-3ed3-4ef2-8f63-573607c9914e | FINISHED | 2020-02-03 15:26:38 | 2020-02-03 15:28:01 | 83 | NULL | NULL | +---------------------------+------------------+----------------------------------------------------+------------+----------------------+----------------------+------------+------------------+----------------------+ -- and the numrows have been updated desc formatted t; [...] | | numRows | 10 | [...] -- we don't want this running every minute anymore... alter scheduled query t_analyze disable; Example 3 – materialized view rebuild -- some settings...they might be there already set hive.support.concurrency=true; set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager; set hive.strict.checks.cartesian.product=false; set hive.stats.fetch.column.stats=true; set hive.materializedview.rewriting=true; -- create some tables CREATE TABLE emps ( empid INT, deptno INT, name VARCHAR(256), salary FLOAT, hire_date TIMESTAMP) STORED AS ORC TBLPROPERTIES ('transactional'='true'); CREATE TABLE depts ( deptno INT, deptname VARCHAR(256), locationid INT) STORED AS ORC TBLPROPERTIES ('transactional'='true'); -- load data insert into emps values (100, 10, 'Bill', 10000, 1000), (200, 20, 'Eric', 8000, 500), (150, 10, 'Sebastian', 7000, null), (110, 10, 'Theodore', 10000, 250), (120, 10, 'Bill', 10000, 250), (1330, 10, 'Bill', 10000, '2020-01-02'); insert into depts values (10, 'Sales', 10), (30, 'Marketing', null), (20, 'HR', 20); insert into emps values (1330, 10, 'Bill', 10000, '2020-01-02'); -- create mv CREATE MATERIALIZED VIEW mv1 AS SELECT empid, deptname, hire_date FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2016-01-01 00:00:00'; EXPLAIN SELECT empid, deptname FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2018-01-01'; -- create a schedule to rebuild mv create scheduled query mv_rebuild cron '0 */10 * * * ? *' defined as alter materialized view mv1 rebuild; -- from this expalin it will be seen that the mv1 is being used EXPLAIN SELECT empid, deptname FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2018-01-01'; -- insert a new record insert into emps values (1330, 10, 'Bill', 10000, '2020-01-02'); -- the source tables are scanned EXPLAIN SELECT empid, deptname FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2018-01-01'; -- wait 10 minutes or execute alter scheduled query mv_rebuild execute; -- run it again...the view should be rebuilt EXPLAIN SELECT empid, deptname FROM emps JOIN depts ON (emps.deptno = depts.deptno) WHERE hire_date \u0026gt;= '2018-01-01'; Example 4 – Ingestion drop table if exists t; drop table if exists s; -- suppose that this table is an external table or something -- which supports the pushdown of filter condition on the id column create table s(id integer, cnt integer); -- create an internal table and an offset table create table t(id integer, cnt integer); create table t_offset(offset integer); insert into t_offset values(0); -- pretend that data is added to s insert into s values(1,1); -- run an ingestion... from (select id==offset as first,* from s join t_offset on id\u0026gt;=offset) s1 insert into t select id,cnt where first = false insert overwrite table t_offset select max(s1.id); -- configure to run ingestion every 10 minutes create scheduled query ingest every 10 minutes defined as from (select id==offset as first,* from s join t_offset on id\u0026gt;=offset) s1 insert into t select id,cnt where first = false insert overwrite table t_offset select max(s1.id); -- add some new values insert into s values(2,2),(3,3); -- pretend that a timeout have happened alter scheduled query ingest execute; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/scheduled-queries_145724128/","tags":null,"title":"Apache Hive : Scheduled Queries"},{"categories":null,"contents":"Apache Hive : Security This page collects some resources and pointers for various efforts underway to add security features to Hive and related projects.\nAuthorization modes\nThe links below refer to the original Hive authorization mode. See Authorization for an overview of authorization modes, which include storage based authorization and SQL standards based authorization.\n Thoughts on security from Venkatesh Howl\u0026rsquo;s approach for persisting and validating DDL authorization via HDFS permissions HIVE-1264: Hadoop security integration THRIFT-889: allow Kerberos authentication over Thrift HTTP THRIFT-876: SASL integration Howl Authorization Proposal Hive Authorization Proposal  Note that Howl was the precursor to HCatalog.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/security_27362056/","tags":null,"title":"Apache Hive : Security"},{"categories":null,"contents":"Apache Hive : SerDe  SerDe Overview  Built-in and Custom SerDes  Built-in SerDes Custom SerDes   HiveQL for SerDes   Input Processing Output Processing Additional Notes  SerDe Overview SerDe is short for Serializer/Deserializer. Hive uses the SerDe interface for IO. The interface handles both serialization and deserialization and also interpreting the results of serialization as individual fields for processing.\nA SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format. Anyone can write their own SerDe for their own data formats.\nSee Hive SerDe for an introduction to SerDes.\nBuilt-in and Custom SerDes The Hive SerDe library is in org.apache.hadoop.hive.serde2. (The old SerDe library in org.apache.hadoop.hive.serde is deprecated.)\nBuilt-in SerDes  Avro (Hive 0.9.1 and later) ORC (Hive 0.11 and later) RegEx Thrift Parquet (Hive 0.13 and later) CSV (Hive 0.14 and later) JsonSerDe (Hive 0.12 and later in hcatalog-core)  Note: For Hive releases prior to 0.12, Amazon provides a JSON SerDe available at s3://elasticmapreduce/samples/hive-ads/libs/jsonserde.jar.\nCustom SerDes For information about custom SerDes, see How to Write Your Own SerDe in the Developer Guide.\nHiveQL for SerDes For the HiveQL statements that specify SerDes and their properties, see Create Table (particularly Row Formats \u0026amp; SerDe) and Alter Table (Add SerDe Properties).\nInput Processing  Hive\u0026rsquo;s execution engine (referred to as just engine henceforth) first uses the configured InputFormat to read in a record of data (the value object returned by the RecordReader of the InputFormat). The engine then invokes Serde.deserialize() to perform deserialization of the record. There is no real binding that the deserialized object returned by this method indeed be a fully deserialized one. For instance, in Hive there is a LazyStruct object which is used by the LazySimpleSerDe to represent the deserialized object. This object does not have the bytes deserialized up front but does at the point of access of a field. The engine also gets hold of the ObjectInspector to use by invoking Serde.getObjectInspector(). This has to be a subclass of structObjectInspector since a record representing a row of input data is essentially a struct type. The engine passes the deserialized object and the object inspector to all operators for their use in order to get the needed data from the record. The object inspector knows how to construct individual fields out of a deserialized record. For example, StructObjectInspector has a method called getStructFieldData() which returns a certain field in the record. This is the mechanism to access individual fields. For instance ExprNodeColumnEvaluator class which can extract a column from the input row uses this mechanism to get the real column object from the serialized row object. This real column object in turn can be a complex type (like a struct). To access sub fields in such complex typed objects, an operator would use the object inspector associated with that field (The top level StructObjectInspector for the row maintains a list of field level object inspectors which can be used to interpret individual fields).  For UDFs the new GenericUDF abstract class provides the ObjectInspector associated with the UDF arguments in the initialize() method. So the engine first initializes the UDF by calling this method. The UDF can then use these ObjectInspectors to interpret complex arguments (for simple arguments, the object handed to the udf is already the right primitive object like LongWritable/IntWritable etc).\nOutput Processing Output is analogous to input. The engine passes the deserialized Object representing a record and the corresponding ObjectInspector to Serde.serialize(). In this context serialization means converting the record object to an object of the type expected by the OutputFormat which will be used to perform the write. To perform this conversion, the serialize() method can make use of the passed ObjectInspector to get the individual fields in the record in order to convert the record to the appropriate type.\nAdditional Notes  The owner of an object (either a row, a column, a sub field of a column, or the return value of a UDF) is the code that creates it, and the life time of an object expires when the corresponding object for the next row is created. That means several things:  We should not directly cache any object. In both group-by and join, we copy the object and then put it into a HashMap. SerDe, UDF, etc can reuse the same object for the same column in different rows. That means we can get rid of most of the object creations in the data pipeline, which is a huge performance boost.   Settable ObjectInspectors (for write and object creation).  ObjectInspector allows us to \u0026ldquo;get\u0026rdquo; fields, but not \u0026ldquo;set\u0026rdquo; fields or \u0026ldquo;create\u0026rdquo; objects. Settable ObjectInspectors allows that. We can convert an object with JavaIntObjectInspector to an object with WritableIntObjectInspector (which is, from Integer to IntWritable) easily with the help of Settable ObjectInspectors. In UDFs (non-GenericUDFs), we use Java reflection to get the type of the parameters/return values of a function (like IntWritable in case of UDFOPPlus), and then infer the ObjectInspector for that using ObjectInspectorUtils.getStandardObjectInspectors. Given the ObjectInspector of an Object that is passed to a UDF, and the ObjectInspector of the type of the parameter of the UDF, we will construct a ObjectInspectorConverter, which uses the SettableObjectInspector interface to convert the object. The converters are called in GenericUDF and GenericUDAF.    In short, Hive will automatically convert objects so that Integer will be converted to IntWritable (and vice versa) if needed. This allows people without Hadoop knowledge to use Java primitive classes (Integer, etc), while hadoop users/experts can use IntWritable which is more efficient.\nBetween map and reduce, Hive uses LazyBinarySerDe and BinarySortableSerDe \u0026rsquo;s serialize methods. SerDe can serialize an object that is created by another serde, using ObjectInspector.\nComments:            I noticed that there are \u0026lsquo;!\u0026rsquo;s in the text, but didn\u0026rsquo;t figure out why.    Posted by xuefu at Feb 22, 2014 20:17 | | The exclamation marks also appear in two sections of the Developer Guide:* Hive SerDe\n ObjectInspector  I asked about them in a comment on HIVE-5380. If they aren\u0026rsquo;t escape characters, could they be leftovers from a previous formatting style?\nPosted by leftyl at Feb 23, 2014 08:47 | | Yes, they are artifacts of the old MoinMoin Wiki syntax and can be removed.\nPosted by larsfrancke at Feb 23, 2014 09:09 | | And they\u0026rsquo;re gone, gone, solid gone. Thanks Lars.\nPosted by leftyl at Feb 25, 2014 09:19 | | Lefty Leverenz I added JsonSerDe to the list of built-in serdes and created new page for Json Serde. Can you review it?\nPosted by apivovarov at Dec 15, 2015 01:43 | | Great! Thanks Alexander Pivovarov, I\u0026rsquo;ll just make a few minor edits.\nPosted by leftyl at Jan 06, 2016 03:17 | | Alexander Pivovarov, in the Json SerDe doc you have a code box with the title \u0026ldquo;Create table, specify CSV properties\u0026rdquo; but I don\u0026rsquo;t see anything about CSV in the code – should it be \u0026ldquo;Create table, specify JsonSerDe\u0026rdquo; instead?\nPosted by leftyl at Jan 07, 2016 08:31 | | Alexander Pivovarov, pinging about \u0026ldquo;CSV\u0026rdquo; in the Json SerDe doc\u0026rsquo;s code box (see my reply to your comment on the SerDe doc).\nPosted by leftyl at Mar 19, 2016 08:33 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/serde_27362059/","tags":null,"title":"Apache Hive : SerDe"},{"categories":null,"contents":"Apache Hive : Setting Up Hive with Docker Introduction  Run Apache Hive inside docker container in pseudo-distributed mode\nSTEP 1: Pull the image  Pull the 4.0.0 image from Hive DockerHub  docker pull apache/hive:4.0.0 STEP 2: Export the Hive version export HIVE_VERSION=4.0.0 STEP 3: Launch the HiveServer2 with an embedded Metastore. This is lightweight and for a quick setup, it uses Derby as metastore db.\ndocker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hive4 apache/hive:${HIVE_VERSION} STEP 4: Connect to beeline docker exec -it hiveserver2 beeline -u 'jdbc:hive2://hiveserver2:10000/' Note: Launch Standalone Metastore To use standalone Metastore with Derby,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --name metastore-standalone apache/hive:${HIVE_VERSION} Advanced Setup Run services - Metastore For a quick start, launch the Metastore with Derby,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --name metastore-standalone apache/hive:4.0.0 Everything would be lost when the service is down. In order to save the Hive table\u0026rsquo;s schema and data, start the container with an external Postgres and Volume to keep them,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \\ --env SERVICE_OPTS=\u0026quot;-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password\u0026quot; \\ --mount source=warehouse,target=/opt/hive/data/warehouse \\ --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \\ --name metastore-standalone apache/hive:4.0.0 If you want to use your own hdfs-site.xml or yarn-site.xml for the service, you can provide the environment variable HIVE_CUSTOM_CONF_DIR for the command. For instance, put the custom configuration file under the directory /opt/hive/conf, then run,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \\ -v /opt/hive/conf:/hive_custom_conf --env HIVE_CUSTOM_CONF_DIR=/hive_custom_conf \\ --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \\ --name metastore apache/hive:4.0.0 For Hive releases before 4.0, if you want to upgrade the existing external Metastore schema to the target version, then add --env SCHEMA_COMMAND=upgradeSchema to the command. To skip schematool initialisation or upgrade for metastore use --env `IS_RESUME=\u0026quot;true\u0026quot;`, and for verbose logging set --env `VERBOSE=\u0026quot;true\u0026quot;.` - HiveServer2 Launch the HiveServer2 with an embedded Metastore,\n docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hiveserver2-standalone apache/hive:4.0.0 or specify a remote Metastore if it\u0026rsquo;s available,\n docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 \\ --env SERVICE_OPTS=\u0026quot;-Dhive.metastore.uris=thrift://metastore:9083\u0026quot; \\ --env IS_RESUME=\u0026quot;true\u0026quot; \\ --name hiveserver2-standalone apache/hive:4.0.0 To save the data between container restarts, you can start the HiveServer2 with a Volume,\n docker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 \\ --env SERVICE_OPTS=\u0026quot;-Dhive.metastore.uris=thrift://metastore:9083\u0026quot; \\ --mount source=warehouse,target=/opt/hive/data/warehouse \\ --env IS_RESUME=\u0026quot;true\u0026quot; \\ --name hiveserver2 apache/hive:4.0.0 - HiveServer2, Metastore To get a quick overview of both HiveServer2 and Metastore, there is a docker-compose.yml placed under packaging/src/docker for this purpose, specify the POSTGRES_LOCAL_PATH first:\nexport POSTGRES_LOCAL_PATH=your_local_path_to_postgres_driver Example:\nmvn dependency:copy -Dartifact=\u0026quot;org.postgresql:postgresql:42.5.1\u0026quot; \u0026amp;\u0026amp; \\ export POSTGRES_LOCAL_PATH=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar If you don\u0026rsquo;t install maven or have problem in resolving the postgres driver, you can always download this jar yourself, change the POSTGRES_LOCAL_PATH to the path of the downloaded jar.\nThen,\ndocker compose up -d HiveServer2, Metastore and Postgres services will be started as a consequence. Volumes are used to persist data generated by Hive inside Postgres and HiveServer2 containers,\n hive_db  The volume persists the metadata of Hive tables inside Postgres container.\n warehouse  The volume stores tables' files inside HiveServer2 container.\nTo stop/remove them all,\ndocker compose down Usage  HiveServer2 web  Accessed on browser at http://localhost:10002/   Beeline:   docker exec -it hiveserver2 beeline -u 'jdbc:hive2://hiveserver2:10000/' # If beeline is installed on host machine, HiveServer2 can be simply reached via: beeline -u 'jdbc:hive2://localhost:10000/'  Run some queries   show tables; create table hive_example(a string, b int) partitioned by(c int); alter table hive_example add partition(c=1); insert into hive_example partition(c=1) values('a', 1), ('a', 2),('b',3); select count(distinct a) from hive_example; select sum(b) from hive_example; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/setting-up-hive-with-docker_282102281/","tags":null,"title":"Apache Hive : Setting Up Hive with Docker"},{"categories":null,"contents":"Apache Hive : Setting Up HiveServer2 HiveServer2  HiveServer2  How to Configure  Configuration Properties in the hive-site.xml File Running in HTTP Mode  Cookie Based Authentication   Optional Global Init File Logging Configuration   How to Start  Usage Message   Authentication/Security Configuration  Configuration Impersonation Integrity/Confidentiality Protection SSL Encryption  Setting up SSL with self-signed certificates Selectively disabling SSL protocol versions   Pluggable Authentication Modules (PAM) Setting up HiveServer2 job credential provider   Scratch Directory Management  Configuration Properties ClearDanglingScratchDir Tool   Web UI for HiveServer2 Python Client Driver Ruby Client Driver    HiveServer2 (HS2) is a server interface that enables remote clients to execute queries against Hive and retrieve the results (a more detailed intro here). The current implementation, based on Thrift RPC, is an improved version of HiveServer and supports multi-client concurrency and authentication. It is designed to provide better support for open API clients like JDBC and ODBC.\n The Thrift interface definition language (IDL) for HiveServer2 is available at https://github.com/apache/hive/blob/trunk/service/if/TCLIService.thrift. Thrift documentation is available at http://thrift.apache.org/docs/.  This document describes how to set up the server. How to use a client with this server is described in the HiveServer2 Clients document.\nVersion\nIntroduced in Hive version 0.11. See HIVE-2935.\nHow to Configure Configuration Properties in the hive-site.xml File hive.server2.thrift.min.worker.threads – Minimum number of worker threads, default 5.\nhive.server2.thrift.max.worker.threads – Maximum number of worker threads, default 500.\nhive.server2.thrift.port – TCP port number to listen on, default 10000.\nhive.server2.thrift.bind.host – TCP interface to bind to.\nSee HiveServer2 in the Configuration Properties document for additional properties that can be set for HiveServer2.\nOptional Environment Settings\nHIVE_SERVER2_THRIFT_BIND_HOST – Optional TCP host interface to bind to. Overrides the configuration file setting.\nHIVE_SERVER2_THRIFT_PORT – Optional TCP port number to listen on, default 10000. Overrides the configuration file setting.\nRunning in HTTP Mode HiveServer2 provides support for sending Thrift RPC messages over HTTP transport (Hive 0.13 onward, see HIVE-4752). This is particularly useful to support a proxying intermediary between the client and the server (for example, for load balancing or security reasons). Currently, you can run HiveServer2 in either TCP mode or the HTTP mode, but not in both. For the corresponding JDBC URL check this link: HiveServer2 Clients \u0026ndash; JDBC Connection URLs. Use the following settings to enable and configure HTTP mode:\n   Setting Default Description     hive.server2.transport.mode binary Set to http to enable HTTP transport mode   hive.server2.thrift.http.port 10001 HTTP port number to listen on   hive.server2.thrift.http.max.worker.threads 500 Maximum worker threads in the server pool   hive.server2.thrift.http.min.worker.threads 5 Minimum worker threads in the server pool   hive.server2.thrift.http.path cliservice The service endpoint    Cookie Based Authentication HIVE-9709 and HIVE-9710 introduced cookie based authentication for HiveServer2 in HTTP mode. The HiveServer2 parameters (hive.server2.thrift.http.cookie.*) associated with this change can be found here.\nOptional Global Init File A global init file can be placed in the configured hive.server2.global.init.file.location location (Hive 0.14 onward, see HIVE-5160, HIVE-7497, and HIVE-8138). This can be either the path to the init file itself, or a directory where an init file named \u0026ldquo;.hiverc\u0026rdquo; is expected.\nThe init file lists a set of commands that will run for users of this HiveServer2 instance, such as register a standard set of jars and functions.\nLogging Configuration HiveServer2 operation logs are available for Beeline clients (Hive 0.14 onward). These parameters configure logging:\n hive.server2.logging.operation.enabled hive.server2.logging.operation.log.location hive.server2.logging.operation.verbose (Hive 0.14 to 1.1) hive.server2.logging.operation.level (Hive 1.2 onward)  How to Start $HIVE_HOME/bin/hiveserver2 OR\n$HIVE_HOME/bin/hive --service hiveserver2 Usage Message The -H or --help option displays a usage message, for example:\n$HIVE_HOME/bin/hive --service hiveserver2 -H Starting HiveServer2 usage: hiveserver2 -H,--help Print help information --hiveconf \u0026lt;property=value\u0026gt; Use value for given property Authentication/Security Configuration HiveServer2 supports Anonymous (no authentication) with and without SASL, Kerberos (GSSAPI), pass through LDAP, Pluggable Custom Authentication and Pluggable Authentication Modules (PAM, supported Hive 0.13 onwards).\nConfiguration Authentication mode:\nhive.server2.authentication – Authentication mode, default NONE. Options are NONE (uses plain SASL), NOSASL, KERBEROS, LDAP, PAM and CUSTOM.\nSet following for KERBEROS mode:\nhive.server2.authentication.kerberos.principal – Kerberos principal for server.\nhive.server2.authentication.kerberos.keytab – Keytab for server principal.\nSet following for LDAP mode:\nhive.server2.authentication.ldap.url – LDAP URL (for example, ldap://hostname.com:389).\nhive.server2.authentication.ldap.baseDN – LDAP base DN. (Optional for AD.)\nhive.server2.authentication.ldap.Domain – LDAP domain. (Hive 0.12.0 and later.)\nSee User and Group Filter Support with LDAP Atn Provider in HiveServer2 for other LDAP configuration parameters in Hive 1.3.0 and later.\nSet following for CUSTOM mode:\nhive.server2.custom.authentication.class – Custom authentication class that implements the org.apache.hive.service.auth.PasswdAuthenticationProvider interface.\nFor PAM mode, see details in section on PAM below.\nImpersonation By default HiveServer2 performs the query processing as the user who submitted the query. But if the following parameter is set to false, the query will run as the user that the hiveserver2 process runs as.\nhive.server2.enable.doAs – Impersonate the connected user, default true.\nTo prevent memory leaks in unsecure mode, disable file system caches by setting the following parameters to true (see HIVE-4501):\nfs.hdfs.impl.disable.cache – Disable HDFS filesystem cache, default false.\nfs.file.impl.disable.cache – Disable local filesystem cache, default false.\nIntegrity/Confidentiality Protection Integrity protection and confidentiality protection (beyond just the default of authentication) for communication between the Hive JDBC driver and HiveServer2 are enabled (Hive 0.12 onward, see HIVE-4911). You can use the SASL QOP property to configure this.\n This is only when Kerberos is used for the HS2 client (JDBC/ODBC application) authentication with HiveServer2. hive.server2.thrift.sasl.qop in hive-site.xml has to be set to one of the valid QOP values (\u0026lsquo;auth\u0026rsquo;, \u0026lsquo;auth-int\u0026rsquo; or \u0026lsquo;auth-conf\u0026rsquo;).  SSL Encryption Support is provided for SSL encryption (Hive 0.13 onward, see HIVE-5351). To enable, set the following configurations in hive-site.xml:\nhive.server2.use.SSL – Set this to true.\nhive.server2.keystore.path – Set this to your keystore path.\nhive.server2.keystore.password – Set this to your keystore password.\nNote\nWhen hive.server2.transport.mode is binary and hive.server2.authentication is KERBEROS, SSL encryption did not work until Hive 2.0. Set hive.server2.thrift.sasl.qop to auth-conf to enable encryption. See HIVE-14019 for details.\nSetting up SSL with self-signed certificates Use the following steps to create and verify self-signed SSL certificates for use with HiveServer2:\n Create the self signed certificate and add it to a keystore file using: keytool -genkey -alias example.com -keyalg RSA -keystore keystore.jks -keysize 2048 Ensure the name used in the self signed certificate matches the hostname where HiveServer2 will run. List the keystore entries to verify that the certificate was added. Note that a keystore can contain multiple such certificates: keytool -list -keystore keystore.jks Export this certificate from keystore.jks to a certificate file: keytool -export -alias example.com -file example.com.crt -keystore keystore.jks Add this certificate to the client\u0026rsquo;s truststore to establish trust: keytool -import -trustcacerts -alias example.com -file example.com.crt -keystore truststore.jks Verify that the certificate exists in truststore.jks: keytool -list -keystore truststore.jks Then start HiveServer2, and try to connect with beeline using: jdbc:hive2://:/;ssl=true;sslTrustStore=;trustStorePassword=  Selectively disabling SSL protocol versions To disable specific SSL protocol versions, use the following steps:\n Run openssl ciphers -v (or the corresponding command if not using openssl) to view all protocol versions. In addition to 1, an additional step of going over the HiveServer2 logs may be required to see all the protocols that the node running HiveServer2 is supporting. For that, search for \u0026ldquo;SSL Server Socket Enabled Protocols:\u0026rdquo; in the HiveServer2 log file. Add all the SSL protocols that need to be disabled to hive.ssl.protocol.blacklist. Ensure that the property in hiveserver2-site.xml does not override that in hive-site.xml.  Pluggable Authentication Modules (PAM) Warning\nJPAM library that is used to provide the PAM authentication mode can cause HiveServer2 to go down if a user\u0026rsquo;s password has expired. This happens because of segfault/core dumps from native code invoked by JPAM. Some users have also reported crashes during logins in other cases as well. Use of LDAP or KERBEROS is recommended.\n Support is provided for PAM (Hive 0.13 onward, see HIVE-6466). To configure PAM:\n Download the JPAM native library for the relevant architecture. Unzip and copy libjpam.so to a directory () on the system. Add the directory to the LD_LIBRARY_PATH environment variable like so:export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:\u0026lt;libjmap-directory\u0026gt; For some PAM modules, you\u0026rsquo;ll have to ensure that your /etc/shadow and /etc/login.defs files are readable by the user running the HiveServer2 process.  Finally, set the following configurations in hive-site.xml:\nhive.server2.authentication – Set this to PAM.\nhive.server2.authentication.pam.services – Set this to a list of comma-separated PAM services that will be used. Note that a file with the same name as the PAM service must exist in /etc/pam.d.\nSetting up HiveServer2 job credential provider Starting Hive 2.2.0 onwards (see HIVE-14822) Hiveserver2 supports job specific hadoop credential provider for MR and Spark jobs. When using encrypted passwords via the Hadoop Credential Provider, HiveServer2 needs to forward enough information to the job configuration so that jobs launched across cluster can read those secrets. Additionally, HiveServer2 may have secrets that the job should not have such as the Hive Metastore database password. If your job needs to access such secrets, like S3 credentials, then you can configure them using the configuration steps below:\n Create a job-specific keystore using Hadoop Credential Provider API at a secure location in HDFS. This keystore should contain the encrypted key/value pairs of the configurations needed by jobs. Eg: in case of S3 credentials the keystore should contain fs.s3a.secret.key and fs.s3a.access.key with their corresponding values. The password to decrypt the keystore should be set as a HiveServer2 environment variable called HIVE_JOB_CREDSTORE_PASSWORD Set hive.server2.job.credential.provider.path to URL pointing to the type and location of keystore created in (1) above. If there is no job-specific keystore, HiveServer2 will use the one set using hadoop.credential.provider.path in core-site.xml if available. If the password using environment variable set in step 2 is not provided, HiveServer2 will use HADOOP_CREDSTORE_PASSWORD environment variable if available. HiveServer2 will now modify the job configuration of the jobs launched using MR or Spark execution engines to include the job credential provider so that job tasks can access the encrypted keystore with the secrets.  hive.server2.job.credential.provider.path – Set this to your job-specific hadoop credential provider. Eg: jceks://hdfs/user/hive/secret/jobcreds.jceks.\nHIVE_JOB_CREDSTORE_PASSWORD – Set this HiveServer2 environment variable to your job specific Hadoop credential provider password set above.\nScratch Directory Management HiveServer2 allows the configuration of various aspects of scratch directories, which are used by Hive to store temporary output and plans.\nConfiguration Properties The following are the properties that can be configured related to scratch directories:\n hive.scratchdir.lock hive.exec.scratchdir hive.scratch.dir.permission hive.start.cleanup.scratchdir  ClearDanglingScratchDir Tool The tool cleardanglingscratchdir can be run to clean up any dangling scratch directories that might be left over from improper shutdowns of Hive, such as when a virtual machine restarts and leaves no chance for Hive to run the shutdown hook.\nhive --service cleardanglingscratchdir [-r] [-v] [-s scratchdir] -r dry-run mode, which produces a list on console -v verbose mode, which prints extra debugging information -s if you are using non-standard scratch directory The tool tests if a scratch directory is in use, and if not, will remove it. This relies on HDFS write locks to detect if a scratch directory is in use. An HDFS client opens an HDFS file ($scratchdir/inuse.lck) for writing and only closes it at the time that the session is closed. cleardanglingscratchdir will try to open $scratchdir/inuse.lck for writing to test if the corresponding HiveCli/HiveServer2 is still running. If the lock is in use, the scratch directory will not be cleared. If the lock is available, the scratch directory will be cleared. Note that it might take NameNode up to 10 minutes to reclaim the lease on scratch file locks from a dead HiveCli/HiveServer2, at which point cleardanglingscratchdir will be able to remove it if run again**.**\nWeb UI for HiveServer2 Version\nIntroduced in Hive 2.0.0. See HIVE-12338 and its sub-tasks.\nA Web User Interface (UI) for HiveServer2 provides configuration, logging, metrics and active session information. The Web UI is available at port 10002 (127.0.0.1:10002) by default.  Configuration properties for the Web UI can be customized in hive-site.xml, including hive.server2.webui.host, hive.server2.webui.port, hive.server2.webui.max.threads, and others. Hive Metrics can by viewed by using the \u0026ldquo;Metrics Dump\u0026rdquo; tab. Logscan be viewed by using the \u0026ldquo;Local logs\u0026rdquo; tab.  The interface is currently under development with HIVE-12338.\nPython Client Driver A Python client driver for HiveServer2 is available at https://github.com/BradRuderman/pyhs2 (thanks, Brad). It includes all the required packages such as SASL and Thrift wrappers.\nThe driver has been certified for use with Python 2.6 and newer.\nTo use the pyhs2 driver:\npip install pyhs2 and then:\nimport pyhs2 with pyhs2.connect(host='localhost', port=10000, authMechanism=\u0026quot;PLAIN\u0026quot;, user='root', password='test', database='default') as conn: with conn.cursor() as cur: #Show databases print cur.getDatabases() #Execute query cur.execute(\u0026quot;select * from table\u0026quot;) #Return column info from query print cur.getSchema() #Fetch table results for i in cur.fetch(): print i You can discuss this driver on the user@hive.apache.org mailing list.\nRuby Client Driver A Ruby client driver is available on github at https://github.com/forward3d/rbhive.\n  Attachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/setting-up-hiveserver2_30758712/","tags":null,"title":"Apache Hive : Setting Up HiveServer2"},{"categories":null,"contents":"Apache Hive : Skewed Join Optimization Optimizing Skewed Joins The Problem A join of 2 large data tables is done by a set of MapReduce jobs which first sorts the tables based on the join key and then joins them. The Mapper gives all rows with a particular key to the same Reducer.\ne.g., Suppose we have table A with a key column, \u0026ldquo;id\u0026rdquo; which has values 1, 2, 3 and 4, and table B with a similar column, which has values 1, 2 and 3.\nWe want to do a join corresponding to the following query\n select A.id from A join B on A.id = B.id  A set of Mappers read the tables and gives them to Reducers based on the keys. e.g., rows with key 1 go to Reducer R1, rows with key 2 go to Reducer R2 and so on. These Reducers do a cross product of the values from A and B, and write the output. The Reducer R4 gets rows from A, but will not produce any results.\nNow let\u0026rsquo;s assume that A was highly skewed in favor of id = 1. Reducers R2 and R3 will complete quickly but R1 will continue for a long time, thus becoming the bottleneck. If the user has information about the skew, the bottleneck can be avoided manually as follows:\nDo two separate queries\n select A.id from A join B on A.id = B.id where A.id \u0026lt;\u0026gt; 1; select A.id from A join B on A.id = B.id where A.id = 1 and B.id = 1;  The first query will not have any skew, so all the Reducers will finish at roughly the same time. If we assume that B has only few rows with B.id = 1, then it will fit into memory. So the join can be done efficiently by storing the B values in an in-memory hash table. This way, the join can be done by the Mapper itself and the data do not have to go to a Reducer. The partial results of the two queries can then be merged to get the final results.\n Advantages  If a small number of skewed keys make up for a significant percentage of the data, they will not become bottlenecks.   Disadvantages  The tables A and B have to be read and processed twice. Because of the partial results, the results also have to be read and written twice. The user needs to be aware of the skew in the data and manually do the above process.    We can improve this further by trying to reduce the processing of skewed keys. First read B and store the rows with key 1 in an in-memory hash table. Now run a set of mappers to read A and do the following:\n If it has key 1, then use the hashed version of B to compute the result. For all other keys, send it to a reducer which does the join. This reducer will get rows of B also from a mapper.  This way, we end up reading only B twice. The skewed keys in A are only read and processed by the Mapper, and not sent to the reducer. The rest of the keys in A go through only a single Map/Reduce.\nThe assumption is that B has few rows with keys which are skewed in A. So these rows can be loaded into the memory.\nHive Enhancements Original plan: ~~The skew data will be obtained from list bucketing (see the List Bucketing~~design document). There will be no additions to the Hive grammar.\nImplementation: Starting in Hive 0.10.0, tables can be created as skewed or altered to be skewed (in which case partitions created after the ALTER statement will be skewed). In addition, skewed tables can use the list bucketing feature by specifying the STORED AS DIRECTORIES option. See the DDL documentation for details: Create Table, Skewed Tables, and Alter Table Skewed or Stored as Directories.\nComments:            Is this proposal ready for review?    Posted by cwsteinbach at May 31, 2012 21:27 | | yes\nPosted by namit.jain at Jun 01, 2012 21:07 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/skewed-join-optimization_27847852/","tags":null,"title":"Apache Hive : Skewed Join Optimization"},{"categories":null,"contents":"Apache Hive : Spatial queries Overview Hadoop-GIS is a scalable and high performance spatial data warehousing system for running large-scale spatial queries on Hadoop. Hadoop-GIS relies on RESQUE for spatial query processing. RESQUE is a internally developed tile based spatial query engine which is written in C++ and deployed as shared library.\nHive****SP: we integrate Hadoop-GIS with Hive, to support both structured queries and spatial queries with a unified query language (HQL) and interface (Hive Shell).\nQuery Language At the language layer, Hadoop-GIS extends HQL to support spatial data types and query constructs.\nJOIN operator – we kept JOIN keyword for backward compatibility. However, whenever there is a spatial operator in the join predicate, the query is considered as spatial join query and a spatial join query processing pipeline is applied to process this query.\ne.g SELECT * FROM a JOIN b on ST_INTERSECTS (a.spatialcolumn ,b.spatialcolumn) = TRUE ;\nData Type\nWe will add a spatial data type in Hive: GEOMETRY. Geometry is an extension of String type with special serialization/deserialization, and operation support. For example, users can create a table with spatial column as shown in following example:\nCREATE TABLE IF NOT EXISTS spatial_table ( tile_id STRING, d_id STRING, rec_id STRING, outline GEOMETRY) ROW FORMAT DELIMITED FIELDS TERMINATED BY \u0026lsquo;\\t\u0026rsquo; ;\nQuery Pipelines\nWe have created efficient query pipelines between Hive and RESQUE, to support various spatial queries. Again, with the philosophy of “minimum change to Hive”, current query pipelines implemented as set of “custom MapReduce codes” which interacts with Hive via TRANSFORM mechanism.\nSpecifically, a spatial SQL query will be intercepted at the query translation phase to translate the query into a Hive executable query operators. Basically, the spatial query processing part will be translated into set of custom Map and Reduce scripts which will interact with Hive via STDIN and STDOUT.\nBefore Hive submits the spatial query operator to the RESQUE for processing, it will use appropriate serialization method to transform data into a format that RESQUE can recognize. Then after REQUE processing, RESQUE will desterilize the data into a format that can be recognized by Hive.\nSpatial Predicates At this moment, Hadoop-GIS support the following spatial predicates which implemented as Hive UDF. More predicates being developed and will be integrate in future.\n st_intersects st_touches st_crosses st_contains st_adjacent st_disjoint st_equals st_dwithin st_within st_overlaps  We can use the spatial query just like using standard HQL in Hive shell. For example, if we want to spatially join two tables (say ta and tb), we can issue following HQL sentence in Hive Shell:\n SELECT ta.rec_id, tb.rec_id FROM ta JOIN tb ON (st_intersects(ta.outline, tb.outline) = TRUE);  We will get the following output for the above st_intersects query:\n …… Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1 2013-09-08 01:23:18,838 Stage-1 map = 0%, reduce = 0% 2013-09-08 01:23:24,889 Stage-1 map = 100%, reduce = 0% 2013-09-08 01:23:33,954 Stage-1 map = 100%, reduce = 100% Ended Job = job_201309080121_0001 MapReduce Jobs Launched: Job 0: Map: 2 Reduce: 1 HDFS Read: 187984 HDFS Write: 40 SUCCESS Total MapReduce CPU Time Spent: 0 msec OK 1 1 1 2 1 3 1 4 15 87 34 78 54 74 61 54 Time taken: 28.207 seconds  Changes in Hive We have tried to make minimum change to Hive to not to compromise the compatibility.\nChanges are mostly at the language, and query optimization layer.\nLanague layer: Hive.g is changed to add data types and other spatial language support.\nParsing/Analyzing: Mostly the SemanticAnalyzer is changed (by adding functions) to generate an executable query plan.\nOptimization: The generated query plan is optimized with a function which can produce optimal query plan according to the spatial predicate and table information.\nThe RESQUE library will be deployed as shared library, and a path to this library will be provided to hive to invoke functions in the library via RANSFORM mechanism. ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/spatial-queries_34022710/","tags":null,"title":"Apache Hive : Spatial queries"},{"categories":null,"contents":"Apache Hive : SQL Standard Based Hive Authorization  Status of Hive Authorization before Hive 0.13 SQL Standards Based Hive Authorization (New in Hive 0.13)  Restrictions on Hive Commands and Statements Privileges Objects Object Ownership Users and Roles  Names of Users and Roles Role Management Commands   Managing Object Privileges  Object Privilege Commands Examples of Managing Object Privileges   Privileges Required for Hive Operations Configuration  For Hive 0.13.x For Hive 0.14 and Newer   Known Issues  Hive 0.13 Hive 0.13.1   References Troubleshooting    Status of Hive Authorization before Hive 0.13 The default authorization in Hive is not designed with the intent to protect against malicious users accessing data they should not be accessing. It only helps in preventing users from accidentally doing operations they are not supposed to do. It is also incomplete because it does not have authorization checks for many operations including the grant statement. The authorization checks happen during Hive query compilation. But as the user is allowed to execute dfs commands, user-defined functions and shell commands, it is possible to bypass the client security checks.\nHive also has support for storage based authorization, which is commonly used to add authorization to metastore server API calls (see Storage Based Authorization in the Metastore Server). As of Hive 0.12.0 it can be used on the client side as well. While it can protect the metastore against changes by malicious users, it does not support fine grained access control (column or row level).\nThe default authorization model in Hive can be used to provide fine grained access control by creating views and granting access to views instead of the underlying tables.\nSQL Standards Based Hive Authorization (New in Hive 0.13) The SQL standards based authorization option (introduced in Hive 0.13) provides a third option for authorization in Hive. This is recommended because it allows Hive to be fully SQL compliant in its authorization model without causing backward compatibility issues for current users. As users migrate to this more secure model, the current default authorization could be deprecated.\nFor an overview of this authorization option, see SQL Standards Based Authorization in HiveServer2.\nThis authorization mode can be used in conjunction with storage based authorization on the metastore server. Like the current default authorization in Hive, this will also be enforced at query compilation time. To provide security through this option, the client will have to be secured. This can be done by allowing users access only through Hive Server2, and by restricting the user code and non-SQL commands that can be run. The checks will happen against the user who submits the request, but the query will run as the Hive server user. The directories and files for input data would have read access for this Hive server user. For users who don’t have the need to protect against malicious users, this could potentially be supported through the Hive command line as well.\nThe goal of this work has been to comply with the SQL standard as far as possible, but there are deviations from the standard in the implementation. Some deviations were made to make it easier for existing Hive users to migrate to this authorization model, and some were made considering ease of use (in such cases we also looked at what many widely used databases do).\nUnder this authorization model, users who have access to the Hive CLI, HDFS commands, Pig command line, \u0026lsquo;hadoop jar\u0026rsquo; command, etc., are considered privileged users. In an organization, it is typically only the teams that work on ETL workloads that need such access. These tools don\u0026rsquo;t access the data through HiveServer2, and as a result their access is not authorized through this model. For Hive CLI, Pig, and MapReduce users access to Hive tables can be controlled using storage based authorization enabled on the metastore server.\nMost users such as business analysts tend to use SQL and ODBC/JDBC through HiveServer2 and their access can be controlled using this authorization model.\nRestrictions on Hive Commands and Statements Commands such as dfs, add, delete, compile, and reset are disabled when this authorization is enabled.\nThe set commands used to change Hive configuration are restricted to a smaller safe set. This is controlled using the hive.security.authorization.sqlstd.confwhitelist configuration parameter. If this set needs to be customized, the HiveServer2 administrator can set a value for this configuration parameter in its hive-site.xml.\nPrivileges to add or drop functions and macros are restricted to the admin role.\nTo enable users to use functions, the ability to create permanent functions has been added. A user in the admin role can run commands to create these functions, which all users can then use.\nThe Hive transform clause is also disabled when this authorization is enabled.\nPrivileges ● SELECT privilege – gives read access to an object.\n● INSERT privilege – gives ability to add data to an object (table).\n● UPDATE privilege – gives ability to run update queries on an object (table).\n● DELETE privilege – gives ability to delete data in an object (table).\n● ALL PRIVILEGES – gives all privileges (gets translated into all the above privileges).\nObjects  The privileges apply to table and views. The above privileges are not supported on databases. Database ownership is considered for certain actions. URI is another object in Hive, as Hive allows the use of URI in SQL syntax. The above privileges are not applicable on URI objects. URI used are expected to point to a file/directory in a file system. Authorization is done based on the permissions the user has on the file/directory.  Object Ownership For certain actions, the ownership of the object (table/view/database) determines if you are authorized to perform the action.\nThe user who creates the table, view or database becomes its owner. In the case of tables and views, the owner gets all the privileges with grant option.\nA role can also be the owner of a database. The \u0026ldquo;[alter database](#alter-database)\u0026rdquo; command can be used to set the owner of a database to a role.\nUsers and Roles Privileges can be granted to users as well as roles.\nUsers can belong to one or more roles.\nThere are two roles with special meaning – public and admin.\nAll users belong to the public role. You use this role in your grant statement to grant a privilege to all users.\nWhen a user runs a Hive query or command, the privileges granted to the user and her \u0026ldquo;current roles\u0026rdquo; are checked. The current roles can be seen using the \u0026ldquo;show current roles;\u0026rdquo; command. All of the user\u0026rsquo;s roles except for the admin role will be in the current roles by default, although you can use the \u0026ldquo;set role\u0026rdquo; command to set a specific role as the current role. See the command descriptions for details.\nUsers who do the work of a database administrator are expected to be added to the admin role.\nThey have privileges for running additional commands such as \u0026ldquo;create role\u0026rdquo; and \u0026ldquo;drop role\u0026rdquo;. They can also access objects that they haven’t been given explicit access to. However, a user who belongs to the admin role needs to run the \u0026ldquo;set role\u0026rdquo; command before getting the privileges of the admin role, as this role is not in current roles by default.\nNames of Users and Roles Role names are case insensitive. That is, “marketing” and “MarkEting” refer to same role.\nUser names are case sensitive. This is because, unlike role names, user names are not managed within Hive. The user can be any user that the hiveserver2 authentication mode supports.\nQuoted Identifiers User and role names may optionally be surrounded by backtick characters () when the configuration parameter hive.support.quoted.identifiers is set to column (default value). All [Unicode](http://en.wikipedia.org/wiki/List_of_Unicode_characters) characters are permitted in the quoted identifiers, with double backticks (``) representing a backtick character. However when hive.support.quoted.identifiers is set to none`, only alphanumeric and underscore characters are permitted in user names and role names.\nFor details, see HIVE-6013 and Supporting Quoted Identifiers in Column Names.\nAs of Hive 0.14, user may be optionally surrounded by backtick characters () irrespective of the hive.support.quoted.identifiers` setting.\nRole Management Commands Create Role CREATE ROLE role_name; Creates a new role. Only the admin role has privilege for this.\nThe role names ALL, DEFAULT and NONE are reserved.\nDrop Role DROP ROLE role_name; Drops the given role. Only the admin role has privilege for this.\nShow Current Roles SHOW CURRENT ROLES; Shows the list of the user\u0026rsquo;s current roles. All actions of the user are authorized by looking at the privileges of the user and all current roles of the user.\nThe default current roles has all roles for the user except for the admin role (even if the user belongs to the admin role as well).\nAny user can run this command.\nSet Role SET ROLE (role_name|ALL|NONE); If a role_name is specified, then that role becomes the only role in current roles.\nSetting role_name to ALL refreshes the list of current roles (in case new roles were granted to the user) and sets them to the default list of roles.\nSetting role_name to NONE will remove all current roles from the current user. (It\u0026rsquo;s introduced in HIVE-11780 and will be included in the upcoming versions 1.3.0 and 1.2.2.)\nIf a role the user does not belong to is specified as the role_name, it will result in an error.\nShow Roles SHOW ROLES; List all currently existing roles.\nOnly the admin role has privilege for this.\nGrant Role GRANT role_name [, role_name] ... TO principal_specification [, principal_specification] ... [ WITH ADMIN OPTION ]; principal_specification : USER user | ROLE role Grant one or more roles to other roles or users.\nIf “WITH ADMIN OPTION” is specified, then the user gets privileges to grant the role to other users/roles.\nIf the grant statement ends up creating a cycling relationship between roles, the command will fail with an error.\nRevoke Role REVOKE [ADMIN OPTION FOR] role_name [, role_name] ... FROM principal_specification [, principal_specification] ... ; principal_specification : USER user | ROLE role Revokes the membership of the roles from the user/roles in the FROM clause.\nAs of Hive 0.14.0, revoking just the ADMIN OPTION is possible with the use of REVOKE ADMIN OPTION FOR (HIVE-6252).\nShow Role Grant SHOW ROLE GRANT (USER|ROLE) principal_name; where principal_name is the name of a user or role.\nLists all roles the given user or role has been granted.\nCurrently any user can run this command. But this is likely to change in future to allow users to see only their own role grants, and additional privileges would be needed to see role grants of other users.\nExample of Show Role Grant 0: jdbc:hive2://localhost:10000\u0026gt; GRANT role1 TO USER user1; No rows affected (0.058 seconds) 0: jdbc:hive2://localhost:10000\u0026gt; SHOW ROLE GRANT USER user1; +---------+---------------+----------------+----------+ | role | grant_option | grant_time | grantor | +---------+---------------+----------------+----------+ | public | false | 0 | | | role1 | false | 1398284083000 | uadmin | +---------+---------------+----------------+----------+ Show Principals SHOW PRINCIPALS role_name; Lists all roles and users who belong to this role.\nOnly the admin role has privilege for this.\nExample of Show Principals 0: jdbc:hive2://localhost:10000\u0026gt; SHOW PRINCIPALS role1; +-----------------+-----------------+---------------+----------+---------------+----------------+ | principal_name | principal_type | grant_option | grantor | grantor_type | grant_time | +-----------------+-----------------+---------------+----------+---------------+----------------+ | role2 | ROLE | false | uadmin | USER | 1398285926000 | | role3 | ROLE | true | uadmin | USER | 1398285946000 | | user1 | USER | false | uadmin | USER | 1398285977000 | +-----------------+-----------------+---------------+----------+---------------+----------------+ Managing Object Privileges Object Privilege Commands Grant GRANT priv_type [, priv_type ] ... ON table_or_view_name TO principal_specification [, principal_specification] ... [WITH GRANT OPTION]; Revoke REVOKE [GRANT OPTION FOR] priv_type [, priv_type ] ... ON table_or_view_name FROM principal_specification [, principal_specification] ... ; principal_specification : USER user | ROLE role priv_type : INSERT | SELECT | UPDATE | DELETE | ALL If a user is granted a privilege WITH GRANT OPTION on a table or view, then the user can also grant/revoke privileges of other users and roles on those objects. As of Hive 0.14.0, the grant option for a privilege can be removed while still keeping the privilege by using REVOKE GRANT OPTION FOR (HIVE-7404).\nNote that in case of the REVOKE statement, the DROP-BEHAVIOR option of CASCADE is not currently supported (which is in SQL standard). As a result, the revoke statement will not drop any dependent privileges. For details on CASCADE behavior, you can check the Postgres revoke documentation.\nExamples:\n0: jdbc:hive2://localhost:10000/default\u0026gt; grant select on table secured_table to role my_role; No rows affected (0.046 seconds) 0: jdbc:hive2://localhost:10000/default\u0026gt; revoke update, select on table secured_table from role my_role; No rows affected (0.028 seconds) Notice that in Hive, unlike in standard SQL, USER or ROLE must be specified in the principal_specification.\nShow Grant SHOW GRANT [principal_specification] ON (ALL | [TABLE] table_or_view_name); principal_specification : USER user | ROLE role Currently any user can run this command. But this is likely to change in the future to allow users to see only their own privileges, and additional privileges would be needed to see privileges of other users.\nExamples of Managing Object Privileges Find out the privileges user ashutosh has on table hivejiratable:\n0: jdbc:hive2://localhost:10000\u0026gt; show grant user ashutosh on table hivejiratable; +-----------+----------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ | database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor | +-----------+----------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ | default | hivejiratable | | | ashutosh | USER | DELETE | false | 1398303419000 | thejas | | default | hivejiratable | | | ashutosh | USER | SELECT | false | 1398303407000 | thejas | +-----------+----------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+  Find out the privileges user ashutosh has on all objects:\n0: jdbc:hive2://localhost:10000\u0026gt; show grant user ashutosh on all; +-----------+-------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ | database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor | +-----------+-------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ | default | hivecontributors | | | ashutosh | USER | DELETE | false | 1398303576000 | thejas | | default | hivecontributors | | | ashutosh | USER | INSERT | false | 1398303576000 | thejas | | default | hivecontributors | | | ashutosh | USER | SELECT | false | 1398303576000 | thejas | | default | hivejiratable | | | ashutosh | USER | DELETE | false | 1398303419000 | thejas | | default | hivejiratable | | | ashutosh | USER | SELECT | false | 1398303407000 | thejas | +-----------+-------------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+  Find out the privileges all users have on table hivejiratable:\n0: jdbc:hive2://localhost:10000\u0026gt; show grant on table hivejiratable; +-----------+----------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ | database | table | partition | column | principal_name | principal_type | privilege | grant_option | grant_time | grantor | +-----------+----------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ | default | hivejiratable | | | ashutosh | USER | DELETE | false | 1398303419000 | thejas | | default | hivejiratable | | | ashutosh | USER | SELECT | false | 1398303407000 | thejas | | default | hivejiratable | | | navis | USER | INSERT | false | 1398303650000 | thejas | | default | hivejiratable | | | navis | USER | SELECT | false | 1398303650000 | thejas | | default | hivejiratable | | | public | ROLE | SELECT | false | 1398303481000 | thejas | | default | hivejiratable | | | thejas | USER | DELETE | true | 1398303380000 | thejas | | default | hivejiratable | | | thejas | USER | INSERT | true | 1398303380000 | thejas | | default | hivejiratable | | | thejas | USER | SELECT | true | 1398303380000 | thejas | | default | hivejiratable | | | thejas | USER | UPDATE | true | 1398303380000 | thejas | +-----------+----------------+------------+---------+-----------------+-----------------+------------+---------------+----------------+----------+ Privileges Required for Hive Operations Codes\nY: Privilege required.\nY + G: Privilege \u0026ldquo;WITH GRANT OPTION\u0026rdquo; required.\n   Action Select Insert Update Delete Ownership Admin URI Privilege (RWX Permission + Ownership)     CREATE TABLE     Y (of database)  Y (for create external table – the location)   DROP TABLE     Y     DESCRIBE TABLE Y         SHOW PARTITIONS Y         ALTER TABLE LOCATION     Y  Y (for new location)   ALTER PARTITION LOCATION     Y  Y (for new partition location)   ALTER TABLE ADD PARTITION  Y     Y (for partition location)   ALTER TABLE DROP PARTITION    Y      ALTER TABLE (all of them except the ones above)     Y     TRUNCATE TABLE     Y     CREATE VIEW Y + G         ALTER VIEW PROPERTIES     Y     ALTER VIEW RENAME     Y     DROP VIEW PROPERTIES     Y     DROP VIEW     Y     ANALYZE TABLE Y Y        SHOW COLUMNS Y         SHOW TABLE STATUS Y         SHOW TABLE PROPERTIES Y         CREATE TABLE AS SELECT Y (of input)    Y (of database)     CREATE INDEX     Y (of table)     DROP INDEX     Y     ALTER INDEX REBUILD     Y     ALTER INDEX PROPERTIES     Y     SELECT Y         INSERT  Y  Y (for OVERWRITE)      UPDATE   Y       DELETE    Y      LOAD  Y (output)  Y (output)   Y (input location)   SHOW CREATE TABLE Y+G         CREATE FUNCTION      Y    DROP FUNCTION      Y    CREATE MACRO      Y    DROP MACRO      Y    MSCK (metastore check)      Y    ALTER DATABASE      Y    CREATE DATABASE       Y (if custom location specified)   EXPLAIN Y         DROP DATABASE     Y      Version Information\nAs of Hive 3.0.0 (HIVE-12408), Ownership is not required for the URI Privilege.\nConfiguration For Hive 0.13.x Set the following in hive-site.xml:\n hive.server2.enable.doAs to false. hive.users.in.admin.role to the list of comma-separated users who need to be added to admin role. Note that a user who belongs to the admin role needs to run the \u0026ldquo;[set role](#set-role)\u0026rdquo; command before getting the privileges of the admin role, as this role is not in current roles by default.  Start HiveServer2 with the following additional command-line options:\n  -hiveconf hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory\n  -hiveconf hive.security.authorization.enabled=true\n  -hiveconf hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator\n  -hiveconf hive.metastore.uris=' '\n  For Hive 0.14 and Newer Set the following in hive-site.xml:\n hive.server2.enable.doAs to false. hive.users.in.admin.role to the list of comma-separated users who need to be added to admin role. Note that a user who belongs to the admin role needs to run the \u0026ldquo;[set role](#set-role)\u0026rdquo; command before getting the privileges of the admin role, as this role is not in current roles by default. Add org.apache.hadoop.hive.ql.security.authorization.MetaStoreAuthzAPIAuthorizerEmbedOnly to hive.security.metastore.authorization.manager. (It takes a comma separated list, so you can add it along with StorageBasedAuthorization parameter, if you want to enable that as well).\nThis setting disallows any of the authorization api calls to be invoked in a remote metastore. HiveServer2 can be configured to use embedded metastore, and that will allow it to invoke metastore authorization api. Hive cli and any other remote metastore users would be denied authorization when they try to make authorization api calls. This restricts the authorization api to privileged HiveServer2 process. You should also ensure that the metastore rdbms access is restricted to the metastore server and hiverserver2. hive.security.authorization.manager to org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdConfOnlyAuthorizerFactory. This will ensure that any table or views created by hive-cli have default privileges granted for the owner.  Set the following in hiveserver2-site.xml:\n  hive.security.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactory\n  hive.security.authorization.enabled=true\n  hive.security.authenticator.manager=org.apache.hadoop.hive.ql.security.SessionStateUserAuthenticator\n  hive.metastore.uris=' '\n  Known Issues Hive 0.13 HIVE-6985 – SQL std auth - privileges grants to public role not being honored\nHIVE-6919 – Hive sql std auth select query fails on partitioned tables\nHIVE-6921 – Index creation fails with SQL std auth turned on\nHIVE-6957 – SQL authorization does not work with HS2 binary mode and Kerberos auth\nattachments/40509928/42696874-txt - Export/Import statement not authorized. Hive 0.13.1 The known issues noted above under Hive 0.13.0 have been fixed in 0.13.1 release.\nReferences For information on the SQL standard for security see:\n ISO 9075 Part 1 Framework sections 4.2.6 (Roles), 4.6.11 (Privileges) ISO 9075 Part 2 Foundation sections 4.35 (Basic security model) and 12 (Access control)  Troubleshooting Problem: My user name is in hive.users.in.admin.role in hive-site.xml, but I still get the error that user is not an admin. What could be wrong?\nDo This: Ensure that you have restarted HiveServer2 after a configuration change and that you have used the HiveServer2 command line options as described in Configuration above.\nDo This: Ensure that you have run a \u0026lsquo;[set role](#set-role) admin;\u0026rsquo; command to get the admin role.\nAttachments: attachments/40509928/42696874-txt (text/plain)\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/sql-standard-based-hive-authorization_40509928/","tags":null,"title":"Apache Hive : SQL Standard Based Hive Authorization"},{"categories":null,"contents":"Apache Hive : StarRocks Integration StarRocks has the ability to setup a Hive catalog which enables you to query data from Hive without loading data into StarRocks or creating external tables. See here for more information. ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/starrocks-integration_272927528/","tags":null,"title":"Apache Hive : StarRocks Integration"},{"categories":null,"contents":"Apache Hive : StatisticsAndDataMining Statistics and Data Mining in Hive This page is the secondary documentation for the slightly more advanced statistical and data mining functions that are being integrated into Hive, and especially the functions that warrant more than one-line descriptions.\n Statistics and Data Mining in Hive  ngrams() and context_ngrams(): N-gram frequency estimation  Use Cases Usage Example   histogram_numeric(): Estimating frequency distributions  Use Cases Usage Example      ngrams() and context_ngrams(): N-gram frequency estimation N-grams are subsequences of length N drawn from a longer sequence. The purpose of the ngrams() UDAF is to find the k most frequent n-grams from one or more sequences. It can be used in conjunction with the sentences() UDF to analyze unstructured natural language text, or the collect() function to analyze more general string data.\nContextual n-grams are similar to n-grams, but allow you to specify a \u0026lsquo;context\u0026rsquo; string around which n-grams are to be estimated. For example, you can specify that you\u0026rsquo;re only interested in finding the most common two-word phrases in text that follow the context \u0026ldquo;I love\u0026rdquo;. You could achieve the same result by manually stripping sentences of non-contextual content and then passing them to ngrams(), but context_ngrams() makes it much easier.\nUse Cases   (ngrams) Find important topics in text in conjunction with a stopword list.\n  (ngrams) Find trending topics in text.\n  (context_ngrams) Extract marketing intelligence around certain words (e.g., \u0026ldquo;Twitter is ___\u0026quot;).\n  (ngrams) Find frequently accessed URL sequences.\n  (context_ngrams) Find frequently accessed URL sequences that start or end at a particular URL.\n  (context_ngrams) Pre-compute common search lookaheads.\n  Usage  SELECT context_ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter; The command above will return the top-100 bigrams (2-grams) from a hypothetical table called twitter. The tweet column is assumed to contain a string with arbitrary, possibly meaningless, text. The lower() UDF first converts the text to lowercase for standardization, and then sentences() splits up the text into arrays of words. The optional fourth argument is the precision factor that control the tradeoff between memory usage and accuracy in frequency estimation. Higher values will be more accurate, but could potentially crash the JVM with an OutOfMemory error. If omitted, sensible defaults are used.\n SELECT context_ngrams(sentences(lower(tweet)), array(\u0026quot;i\u0026quot;,\u0026quot;love\u0026quot;,null), 100, [, 1000]) FROM twitter; The command above will return a list of the top 100 words that follow the phrase \u0026ldquo;i love\u0026rdquo; in a hypothetical database of Twitter tweets. Each null specifies the position of an n-gram component to estimate; therefore, every query must contain at least one null in the context array.\nNote that the following two queries are identical, but ngrams() will be slightly faster in practice.\n SELECT ngrams(sentences(lower(tweet)), 2, 100 [, 1000]) FROM twitter; SELECT context_ngrams(sentences(lower(tweet)), array(null,null), 100, [, 1000]) FROM twitter; Example  SELECT explode(ngrams(sentences(lower(val)), 2, 10)) AS x FROM kafka; {\u0026quot;ngram\u0026quot;:[of\u0026quot;,\u0026quot;the],\u0026quot;estfrequency\u0026quot;:23.0} {\u0026quot;ngram\u0026quot;:[on\u0026quot;,\u0026quot;the],\u0026quot;estfrequency\u0026quot;:20.0} {\u0026quot;ngram\u0026quot;:[in\u0026quot;,\u0026quot;the],\u0026quot;estfrequency\u0026quot;:18.0} {\u0026quot;ngram\u0026quot;:[he\u0026quot;,\u0026quot;was],\u0026quot;estfrequency\u0026quot;:17.0} {\u0026quot;ngram\u0026quot;:[at\u0026quot;,\u0026quot;the],\u0026quot;estfrequency\u0026quot;:17.0} {\u0026quot;ngram\u0026quot;:[that\u0026quot;,\u0026quot;he],\u0026quot;estfrequency\u0026quot;:16.0} {\u0026quot;ngram\u0026quot;:[to\u0026quot;,\u0026quot;the],\u0026quot;estfrequency\u0026quot;:16.0} {\u0026quot;ngram\u0026quot;:[out\u0026quot;,\u0026quot;of],\u0026quot;estfrequency\u0026quot;:16.0} {\u0026quot;ngram\u0026quot;:[he\u0026quot;,\u0026quot;had],\u0026quot;estfrequency\u0026quot;:16.0} {\u0026quot;ngram\u0026quot;:[it\u0026quot;,\u0026quot;was],\u0026quot;estfrequency\u0026quot;:15.0}  SELECT explode(context_ngrams(sentences(lower(val)), array(\u0026quot;he\u0026quot;, null), 10)) AS x FROM kafka; {\u0026quot;ngram\u0026quot;:[was],\u0026quot;estfrequency\u0026quot;:17.0} {\u0026quot;ngram\u0026quot;:[had],\u0026quot;estfrequency\u0026quot;:16.0} {\u0026quot;ngram\u0026quot;:[thought],\u0026quot;estfrequency\u0026quot;:13.0} {\u0026quot;ngram\u0026quot;:[could],\u0026quot;estfrequency\u0026quot;:9.0} {\u0026quot;ngram\u0026quot;:[would],\u0026quot;estfrequency\u0026quot;:7.0} {\u0026quot;ngram\u0026quot;:[lay],\u0026quot;estfrequency\u0026quot;:5.0} {\u0026quot;ngram\u0026quot;:[s],\u0026quot;estfrequency\u0026quot;:4.0} {\u0026quot;ngram\u0026quot;:[wanted],\u0026quot;estfrequency\u0026quot;:4.0} {\u0026quot;ngram\u0026quot;:[did],\u0026quot;estfrequency\u0026quot;:4.0} {\u0026quot;ngram\u0026quot;:[felt],\u0026quot;estfrequency\u0026quot;:4.0} histogram_numeric(): Estimating frequency distributions Histograms represent frequency distributions from empirical data. The kind that is referred to here are histograms with variable-sized bins. Specifically, this UDAF will return a list of (x,y) pairs that represent histogram bin centers and heights. It\u0026rsquo;s up to you to then plot them in Excel / Gnuplot / Matlab / Mathematica to get a nice graphical display.\nUse Cases   Estimating the frequency distribution of a column, possibly grouped by other attributes.\n  Choosing discretization points in a continuous valued column.\n  Usage  SELECT histogram_numeric(age) FROM users GROUP BY gender; The command above is self-explanatory. Converting the output into a graphical display is a bit more involved. The following Gnuplot command should do it, assuming that you\u0026rsquo;ve parsed the output from histogram() into a text file of (x,y) pairs called data.txt.\n plot 'data.txt' u 1:2 w impulses lw 5 Example  SELECT explode(histogram_numeric(val, 10)) AS x FROM normal; {\u0026quot;x\u0026quot;:-3.6505464999999995,\u0026quot;y\u0026quot;:20.0} {\u0026quot;x\u0026quot;:-2.7514727901960785,\u0026quot;y\u0026quot;:510.0} {\u0026quot;x\u0026quot;:-1.7956678951954481,\u0026quot;y\u0026quot;:8263.0} {\u0026quot;x\u0026quot;:-0.9878507685761995,\u0026quot;y\u0026quot;:19167.0} {\u0026quot;x\u0026quot;:-0.2625338380837097,\u0026quot;y\u0026quot;:31737.0} {\u0026quot;x\u0026quot;:0.5057392319427763,\u0026quot;y\u0026quot;:31502.0} {\u0026quot;x\u0026quot;:1.2774146480311135,\u0026quot;y\u0026quot;:14526.0} {\u0026quot;x\u0026quot;:2.083955560712489,\u0026quot;y\u0026quot;:3986.0} {\u0026quot;x\u0026quot;:2.9209550254545484,\u0026quot;y\u0026quot;:275.0} {\u0026quot;x\u0026quot;:3.674835214285715,\u0026quot;y\u0026quot;:14.0} ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/statisticsanddatamining_27362058/","tags":null,"title":"Apache Hive : StatisticsAndDataMining"},{"categories":null,"contents":"Apache Hive : StatsDev Statistics in Hive  Statistics in Hive  Motivation Scope  Table and Partition Statistics Column Statistics Top K Statistics   Quick overview Implementation Usage  Configuration Variables Newly Created Tables Existing Tables – ANALYZE   Examples  ANALYZE TABLE CACHE METADATA   Current Status (JIRA)    This document describes the support of statistics for Hive tables (see HIVE-33).\nMotivation Statistics such as the number of rows of a table or partition and the histograms of a particular interesting column are important in many ways. One of the key use cases of statistics is query optimization. Statistics serve as the input to the cost functions of the optimizer so that it can compare different plans and choose among them. Statistics may sometimes meet the purpose of the users' queries. Users can quickly get the answers for some of their queries by only querying stored statistics rather than firing long-running execution plans. Some examples are getting the quantile of the users' age distribution, the top 10 apps that are used by people, and the number of distinct sessions.\nScope Table and Partition Statistics The first milestone in supporting statistics was to support table and partition level statistics. Table and partition statistics are now stored in the Hive Metastore for either newly created or existing tables. The following statistics are currently supported for partitions:\n Number of rows Number of files Size in Bytes  For tables, the same statistics are supported with the addition of the number of partitions of the table.\nVersion: Table and partition statistics\nTable and partition level statistics were added in Hive 0.7.0 by HIVE-1361.\nColumn Statistics The second milestone was to support column level statistics. See Column Statistics in Hive in the Design Documents.\nSupported column stats are:\n   BooleanColumnStatsData DoubleColumnStatsData LongColumnStatsData StringColumnStatsData BinaryColumnStatsData DecimalColumnStatsData Date DateColumnStatsData Timestamp TimestampColumnStatsData union ColumnStatisticsData     1: required i64 numTrues, 1: optional double lowValue, 1: optional i64 lowValue, 1: required i64 maxColLen, 1: required i64 maxColLen, 1: optional Decimal lowValue, 1: required i64 daysSinceEpoch 1: optional Date lowValue, 1: required i64 secondsSinceEpoch 1: optional Timestamp lowValue, 1: BooleanColumnStatsData booleanStats,   2: required i64 numFalses, 2: optional double highValue, 2: optional i64 highValue, 2: required double avgColLen, 2: required double avgColLen, 2: optional Decimal highValue,  2: optional Date highValue,  2: optional Timestamp highValue, 2: LongColumnStatsData longStats,   3: required i64 numNulls, 3: required i64 numNulls, 3: required i64 numNulls, 3: required i64 numNulls, 3: required i64 numNulls, 3: required i64 numNulls,  3: required i64 numNulls,  3: required i64 numNulls, 3: DoubleColumnStatsData doubleStats,   4: optional binary bitVectors 4: required i64 numDVs, 4: required i64 numDVs, 4: required i64 numDVs, 4: optional binary bitVectors 4: required i64 numDVs,  4: required i64 numDVs,  4: required i64 numDVs, 4: StringColumnStatsData stringStats,    5: optional binary bitVectors, 5: optional binary bitVectors, 5: optional binary bitVectors  5: optional binary bitVectors,  5: optional binary bitVectors,  5: optional binary bitVectors, 5: BinaryColumnStatsData binaryStats,    6: optional binary histogram 6: optional binary histogram   6: optional binary histogram  6: optional binary histogram  6: optional binary histogram 6: DecimalColumnStatsData decimalStats,             7: DateColumnStatsData dateStats,             8: TimestampColumnStatsData timestampStats    Version: Column statistics\nColumn level statistics were added in Hive 0.10.0 by HIVE-1362.\nTop K Statistics Column level top K statistics are still pending; see HIVE-3421.\nQuick overview    Description Stored in Collected by Since     Number of partition the dataset consists of Fictional metastore property: numPartitions computed during displaying the properties of a partitioned table Hive 2.3   Number of files the dataset consists of Metastore table property: numFiles Automatically during Metastore operations    Total size of the dataset as its seen at the filesystem level Metastore table property: totalSize     Uncompressed size of the dataset Metastore table property: rawDataSize Computed, these are the basic statistics. Calculated automatically when hive.stats.autogather is enabled.Can be collected manually by: ANALYZE TABLE \u0026hellip; COMPUTE STATISTICS Hive 0.8   Number of rows the dataset consist of Metastore table property: numRows     Column level statistics Metastore; TAB_COL_STATS table Computed, Calculated automatically when hive.stats.column.autogather is enabled.Can be collected manually by: ANALYZE TABLE \u0026hellip; COMPUTE STATISTICS FOR COLUMNS     Implementation The way the statistics are calculated is similar for both newly created and existing tables.\nFor newly created tables, the job that creates a new table is a MapReduce job. During the creation, every mapper while copying the rows from the source table in the FileSink operator, gathers statistics for the rows it encounters and publishes them into a Database (possibly MySQL). At the end of the MapReduce job, published statistics are aggregated and stored in the MetaStore.\nA similar process happens in the case of already existing tables, where a Map-only job is created and every mapper while processing the table in the TableScan operator, gathers statistics for the rows it encounters and the same process continues.\nIt is clear that there is a need for a database that stores temporary gathered statistics. Currently there are two implementations, one is using MySQL and the other is using HBase. There are two pluggable interfaces IStatsPublisher and IStatsAggregator that the developer can implement to support any other storage. The interfaces are listed below:\npackage org.apache.hadoop.hive.ql.stats; import org.apache.hadoop.conf.Configuration; /** * An interface for any possible implementation for publishing statics. */ public interface IStatsPublisher { /** * This method does the necessary initializations according to the implementation requirements. */ public boolean init(Configuration hconf); /** * This method publishes a given statistic into a disk storage, possibly HBase or MySQL. * * rowID : a string identification the statistics to be published then gathered, possibly the table name + the partition specs. * * key : a string noting the key to be published. Ex: \u0026quot;numRows\u0026quot;. * * value : an integer noting the value of the published key. * */ public boolean publishStat(String rowID, String key, String value); /** * This method executes the necessary termination procedures, possibly closing all database connections. */ public boolean terminate(); } package org.apache.hadoop.hive.ql.stats; import org.apache.hadoop.conf.Configuration; /** * An interface for any possible implementation for gathering statistics. */ public interface IStatsAggregator { /** * This method does the necessary initializations according to the implementation requirements. */ public boolean init(Configuration hconf); /** * This method aggregates a given statistic from a disk storage. * After aggregation, this method does cleaning by removing all records from the disk storage that have the same given rowID. * * rowID : a string identification the statistic to be gathered, possibly the table name + the partition specs. * * key : a string noting the key to be gathered. Ex: \u0026quot;numRows\u0026quot;. * * */ public String aggregateStats(String rowID, String key); /** * This method executes the necessary termination procedures, possibly closing all database connections. */ public boolean terminate(); } Usage Configuration Variables See Statistics in Configuration Properties for a list of the variables that configure Hive table statistics. Configuring Hive describes how to use the variables.\nNewly Created Tables For newly created tables and/or partitions (that are populated through the INSERT OVERWRITE command), statistics are automatically computed by default. The user has to explicitly set the boolean variable hive.stats.autogather to false so that statistics are not automatically computed and stored into Hive MetaStore.\nset hive.stats.autogather=false; The user can also specify the implementation to be used for the storage of temporary statistics setting the variable hive.stats.dbclass. For example, to set HBase as the implementation of temporary statistics storage (the default is jdbc:derby or fs, depending on the Hive version) the user should issue the following command:\nset hive.stats.dbclass=hbase; In case of JDBC implementations of temporary stored statistics (ex. Derby or MySQL), the user should specify the appropriate connection string to the database by setting the variable hive.stats.dbconnectionstring. Also the user should specify the appropriate JDBC driver by setting the variable hive.stats.jdbcdriver.\nset hive.stats.dbclass=jdbc:derby; set hive.stats.dbconnectionstring=\u0026quot;jdbc:derby:;databaseName=TempStatsStore;create=true\u0026quot;; set hive.stats.jdbcdriver=\u0026quot;org.apache.derby.jdbc.EmbeddedDriver\u0026quot;; Queries can fail to collect stats completely accurately. There is a setting hive.stats.reliable that fails queries if the stats can\u0026rsquo;t be reliably collected. This is false by default.\nExisting Tables – ANALYZE For existing tables and/or partitions, the user can issue the ANALYZE command to gather statistics and write them into Hive MetaStore. The syntax for that command is described below:\nANALYZE TABLE [db_name.]tablename [PARTITION(partcol1[=val1], partcol2[=val2], ...)] -- (Note: Fully support qualified table name since Hive 1.2.0, see HIVE-10007.) COMPUTE STATISTICS [FOR COLUMNS] -- (Note: Hive 0.10.0 and later.) [CACHE METADATA] -- (Note: Hive 2.1.0 and later.) [NOSCAN]; When the user issues that command, he may or may not specify the partition specs. If the user doesn\u0026rsquo;t specify any partition specs, statistics are gathered for the table as well as all the partitions (if any). If certain partition specs are specified, then statistics are gathered for only those partitions. When computing statistics across all partitions, the partition columns still need to be listed. As of Hive 1.2.0, Hive fully supports qualified table name in this command. User can only compute the statistics for a table under current database if a non-qualified table name is used.\nWhen the optional parameter NOSCAN is specified, the command won\u0026rsquo;t scan files so that it\u0026rsquo;s supposed to be fast. Instead of all statistics, it just gathers the following statistics:\n Number of files Physical size in bytes  Version 0.10.0: FOR COLUMNS\nAs of Hive 0.10.0, the optional parameter FOR COLUMNS computes column statistics for all columns in the specified table (and for all partitions if the table is partitioned). See Column Statistics in Hive for details.\nTo display these statistics, use DESCRIBE FORMATTED [db_name.]table_name column_name [PARTITION (partition_spec)].\nExamples Suppose table Table1 has 4 partitions with the following specs:\n Partition1: (ds=\u0026lsquo;2008-04-08\u0026rsquo;, hr=11) Partition2: (ds=\u0026lsquo;2008-04-08\u0026rsquo;, hr=12) Partition3: (ds=\u0026lsquo;2008-04-09\u0026rsquo;, hr=11) Partition4: (ds=\u0026lsquo;2008-04-09\u0026rsquo;, hr=12)  and you issue the following command:\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr=11) COMPUTE STATISTICS; then statistics are gathered for partition3 (ds=\u0026lsquo;2008-04-09\u0026rsquo;, hr=11) only.\nIf you issue the command:\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr=11) COMPUTE STATISTICS FOR COLUMNS; then column statistics are gathered for all columns for partition3 (ds=\u0026lsquo;2008-04-09\u0026rsquo;, hr=11). This is available in Hive 0.10.0 and later.\nIf you issue the command:\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr) COMPUTE STATISTICS; then statistics are gathered for partitions 3 and 4 only (hr=11 and hr=12).\nIf you issue the command:\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr) COMPUTE STATISTICS FOR COLUMNS; then column statistics for all columns are gathered for partitions 3 and 4 only (Hive 0.10.0 and later).\nIf you issue the command:\nANALYZE TABLE Table1 PARTITION(ds, hr) COMPUTE STATISTICS; then statistics are gathered for all four partitions.\nIf you issue the command:\nANALYZE TABLE Table1 PARTITION(ds, hr) COMPUTE STATISTICS FOR COLUMNS; then column statistics for all columns are gathered for all four partitions (Hive 0.10.0 and later).\nFor a non-partitioned table, you can issue the command:\nANALYZE TABLE Table1 COMPUTE STATISTICS; to gather statistics of the table.\nFor a non-partitioned table, you can issue the command:\nANALYZE TABLE Table1 COMPUTE STATISTICS FOR COLUMNS; to gather column statistics of the table (Hive 0.10.0 and later).\nIf Table1 is a partitioned table, then for basic statistics you have to specify partition specifications like above in the analyze statement. Otherwise a semantic analyzer exception will be thrown.\nHowever for column statistics, if no partition specification is given in the analyze statement, statistics for all partitions are computed.\nYou can view the stored statistics by issuing the DESCRIBE command. Statistics are stored in the Parameters array. Suppose you issue the analyze command for the whole table Table1, then issue the command:\nDESCRIBE EXTENDED TABLE1; then among the output, the following would be displayed:\n ... , parameters:{numPartitions=4, numFiles=16, numRows=2000, totalSize=16384, ...}, .... If you issue the command:\nDESCRIBE EXTENDED TABLE1 PARTITION(ds='2008-04-09', hr=11); then among the output, the following would be displayed:\n ... , parameters:{numFiles=4, numRows=500, totalSize=4096, ...}, .... If you issue the command:\ndesc formatted concurrent_delete_different partition(ds='tomorrow') name; the output would look like this:\n+-----------------+--------------------+-------+-------+------------+-----------------+--------------+--------------+------------+-------------+------------+----------+ | col_name | data_type | min | max | num_nulls | distinct_count | avg_col_len | max_col_len | num_trues | num_falses | bitvector | comment | +-----------------+--------------------+-------+-------+------------+-----------------+--------------+--------------+------------+-------------+------------+----------+ | col_name | name | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | data_type | varchar(50) | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | min | | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | max | | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | num_nulls | 0 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | distinct_count | 2 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | avg_col_len | 5.0 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | max_col_len | 5 | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | num_trues | | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | num_falses | | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | bitVector | | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | | comment | from deserializer | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | NULL | +-----------------+--------------------+-------+-------+------------+-----------------+--------------+--------------+------------+-------------+------------+----------+ If you issue the command:\nANALYZE TABLE Table1 PARTITION(ds='2008-04-09', hr) COMPUTE STATISTICS NOSCAN; then statistics, number of files and physical size in bytes are gathered for partitions 3 and 4 only.\nANALYZE TABLE \u0026lt;table1\u0026gt; CACHE METADATA Feature not implemented\nHive Metastore on HBase was discontinued and removed in Hive 3.0.0. See HBaseMetastoreDevelopmentGuide\nWhen Hive metastore is configured to use HBase, this command explicitly caches file metadata in HBase metastore. The goal of this feature is to cache file metadata (e.g. ORC file footers) to avoid reading lots of files from HDFS at split generation time, as well as potentially cache some information about splits (e.g. grouping based on location that would be good for some short time) to further speed up the generation and achieve better cache locality with consistent splits.\nANALYZE TABLE Table1 CACHE METADATA; See feature details in HBase Metastore Split Cache and (HIVE-12075)\nCurrent Status (JIRA)    T Key Summary Assignee Reporter P Status Resolution Created Updated Due     Improvement HIVE-28363 Improve heuristics of FilterStatsRule without column stats Shohei Okumiya Shohei Okumiya Major Resolved Fixed Jul 08, 2024 Sep 28, 2024    Bug HIVE-28124 Do not allow non-numeric values in Hive table stats during an alter table Miklos Szurap Miklos Szurap Major Open Unresolved Mar 18, 2024 Mar 18, 2024    Bug HIVE-27479 Incorrect filter selectivity of BETWEEN expressions when using histograms Ryan Johnson Ryan Johnson Major Resolved Fixed Jul 01, 2023 Jul 03, 2023    Bug HIVE-27142 Map Join not working as expected when joining non-native tables with native tables Syed Shameerur Rahman Syed Shameerur Rahman Major Open Unresolved Mar 15, 2023 Jul 10, 2024    Bug HIVE-27065 Exception in partition column statistics update with SQL Server db when histogram statistics is not enabled Venugopal Reddy K Venugopal Reddy K Major Closed Fixed Feb 10, 2023 Aug 15, 2023    Improvement HIVE-27000 Improve the modularity of the *ColumnStatsMerger classes Alessandro Solimando Alessandro Solimando Major Closed Fixed Jan 30, 2023 Aug 15, 2023    Improvement HIVE-26772 Add support for specific column statistics to ANALYZE TABLE command Unassigned Alessandro Solimando Major Open Unresolved Nov 23, 2022 Nov 23, 2022    Bug HIVE-26370 Check stats are up-to-date when getting materialized view state shahbaz Krisztian Kasa Major Resolved Won\u0026rsquo;t Fix Jul 05, 2022 Oct 21, 2022    Improvement HIVE-26313 Aggregate all column statistics into a single field in metastore Unassigned Alessandro Solimando Major In Progress Unresolved Jun 10, 2022 Mar 20, 2023    Sub-task HIVE-26297 Refactoring ColumnStatsAggregator classes to reduce warnings Alessandro Solimando Alessandro Solimando Minor Resolved Abandoned Jun 07, 2022 Dec 15, 2022    Bug HIVE-26277 NPEs and rounding issues in ColumnStatsAggregator classes Alessandro Solimando Alessandro Solimando Major Closed Fixed Jun 01, 2022 Nov 16, 2022    Improvement HIVE-26221 Add histogram-based column statistics Alessandro Solimando Alessandro Solimando Major Closed Fixed May 11, 2022 Aug 15, 2023    Task HIVE-26066 Remove deprecated GenericUDAFComputeStats Unassigned Alessandro Solimando Minor Resolved Duplicate Mar 24, 2022 Mar 24, 2022    Sub-task HIVE-25918 Invalid stats after multi inserting into the same partition Krisztian Kasa Krisztian Kasa Major Resolved Fixed Feb 01, 2022 Feb 22, 2022    Bug HIVE-25771 Stats may be incorrect under concurrent inserts if direct-insert is Off Krisztian Kasa Krisztian Kasa Major Open Unresolved Dec 03, 2021 Dec 03, 2021    Bug HIVE-25654 Stats of transactional table updated when transaction is rolled back Unassigned Krisztian Kasa Major Open Unresolved Oct 27, 2021 Oct 27, 2021    Improvement HIVE-24056 Column stats gather stage as part of import table command plan generation Ashish Sharma Ashish Sharma Major In Progress Unresolved Aug 21, 2020 Aug 21, 2020    Improvement HIVE-23901 Overhead of Logger in ColumnStatsMerger damage the performance Yu-Wen Lai Yu-Wen Lai Major Closed Fixed Jul 23, 2020 Nov 17, 2022    Bug HIVE-23887 Reset table level basic/column stats during import. Ashish Sharma Ashish Sharma Minor Closed Fixed Jul 21, 2020 Nov 17, 2022    Bug HIVE-23796 Multiple insert overwrite into a partitioned table doesn\u0026rsquo;t gather column statistics for all partitions Unassigned Yu-Wen Lai Major Open Unresolved Jul 02, 2020 Jul 02, 2020     Authenticate to retrieve your issues\nShowing 20 out of 306 issues\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/statsdev_27362062/","tags":null,"title":"Apache Hive : StatsDev"},{"categories":null,"contents":"Apache Hive : Storage API Release Proposal To enable faster and more direct integration of file formats like ORC and Parquet, Hive has separated out the Storage API as a distinct subproject and will release it independently of the rest of Hive. The storage-api source code will remain in the Hive git repository. The initial work on the pom files was done in HIVE-15419. The plan is to start the Storage API releases at 2.2.0, but number them independently of the Hive releases. I expect that Storage API will have more releases than Hive originally as we stabilize the subproject, but will eventually release less often than Hive.\nStorage API will have its own release branches and tags. The branches will have names of the form \u0026ldquo;storage-branch-X.Y\u0026rdquo; and the tags will have names of the form \u0026ldquo;storage-release-X.Y.Z\u0026rdquo;.\nThe master branch in the git repository will contain both snapshot versions for both Storage API and Hive and the Hive pom will build the Storage API automatically. Since this is pretty close to the current behavior, development on the master should be relatively unchanged.\nNaturally, Hive releases can\u0026rsquo;t refer to snapshot releases of Storage API and must use real releases. In the release branches of Hive, we\u0026rsquo;ll change the Hive pom to refer to a specific release of Storage API and remove storage-api from the list of modules to be built.\nDuring development of a fix for a Hive release branch that impacts both Storage API and Hive, the developer will\n Use \u0026ldquo;mvn clean install\u0026rdquo; in the storage-api directory to build and install the resulting jar into their m2 cache. Change the hive pom to refer to a snapshot version of Storage API. Build and test the rest of Hive normally  To commit the change, the developer will need to commit the Storage API change on the Storage API release branch and make a release candidate. Once the vote passes, they can commit the rest of their patch to Hive including changing the\n ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/storage-api-release-proposal_67635447/","tags":null,"title":"Apache Hive : Storage API Release Proposal"},{"categories":null,"contents":"Apache Hive : Storage Based Authorization in the Metastore Server  Storage Based Authorization in the Metastore Server  The Need for Metastore Server Security Storage Based Authorization Configuration Parameters for Metastore Security  Sample hive-site.xml: Default Settings      Storage Based Authorization in the Metastore Server The metastore server security feature with storage based authorization was added to Hive in release 0.10. This feature was introduced previously in HCatalog.\nVersion 0.10.0\nHIVE-3705 added metastore server security to Hive in release 0.10.0.\n For additional information about storage based authorization in the metastore server, see the HCatalog document Storage Based Authorization. For an overview of Hive authorization models and other security options, see the Authorization document.  The Need for Metastore Server Security When multiple clients access the same metastore in a backing database, such as MySQL, the database connection credentials may be visible in the hive-site.xml configuration file. A malicious or incompetent user could cause serious damage to metadata even though the underlying data is protected by HDFS access controls.\nAlso, when a Hive metastore server uses Thrift to communicate with clients and has a backing database for metadata storage and persistence, the authentication and authorization done on the client side cannot guarantee security on the metastore side. To provide security for metadata, Hive release 0.10 added authorization capability to the metastore. (See HIVE-3705.) Storage Based Authorization When metastore server security is configured to use Storage Based Authorization, it uses the file system permissions for folders corresponding to the different metadata objects as the source of truth for the authorization policy. Use of Storage Based Authorization in metastore is recommended.\nSee details in the HCatalog Storage Based Authorization document.\nStarting in Hive 0.14, storage based authorization authorizes read privilege on database and tables. The get_database api call needs database directory read privilege. The get_table_* calls that fetch table information and get_partition_* calls to list the partitions of a table require read privilege on the table directory. It is enabled by default with storage based authorization. See hive.security.metastore.authorization.auth.reads in the next section on configuration.\nConfiguration Parameters for Metastore Security To enable Hive metastore server security, set these parameters in hive-site.xml:\n hive.metastore.pre.event.listeners  Set to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener .\nThis turns on metastore-side security.\n hive.security.metastore.authorization.manager  Set to org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider .\nThis tells Hive which metastore-side authorization provider to use. The default setting uses DefaultHiveMetastoreAuthorizationProvider, which implements the standard Hive grant/revoke model. To use an HDFS permission-based model (recommended) to do your authorization, use StorageBasedAuthorizationProvider as instructed above.\nVersions 0.10.0 and 0.12.0\nThe StorageBasedAuthorizationProvider was introduced in Hive 0.10.0, running on the metastore side only (HIVE-3705). Starting in Hive 0.12.0 it also runs on the client side (HIVE-5048 and HIVE-5402).\n hive.security.metastore.authenticator.manager  Set to org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator .\n hive.security.metastore.authorization.auth.reads\nWhen this is set to true, Hive metastore authorization also checks for read access. It is set to true by default. Read authorization checks were introduced in Hive 0.14.0.  Sample hive-site.xml: Default Settings The snippet below shows the keys as they are in a default state in hive-site.xml (metastore-side security set up to use the default authorization/authentication, but disabled). Please edit in information as above to get the desired authorization behaviour:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.metastore.authorization.manager\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;authorization manager class name to be used in the metastore for authorization. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.security.metastore.authenticator.manager\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;authenticator manager class name to be used in the metastore for authentication. The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.pre.event.listeners\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt; \u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;pre-event listener classes to be loaded on the metastore side to run code whenever databases, tables, and partitions are created, altered, or dropped. Set to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener if metastore-side authorization is desired. \u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/storage-based-authorization-in-the-metastore-server_45876440/","tags":null,"title":"Apache Hive : Storage Based Authorization in the Metastore Server"},{"categories":null,"contents":"Apache Hive : StorageHandlers Hive Storage Handlers  Hive Storage Handlers  Introduction Terminology DDL Storage Handler Interface HiveMetaHook Interface Open Issues    Introduction This page documents the storage handler support being added to Hive as part of work on HBaseIntegration. The motivation is to make it possible to allow Hive to access data stored and managed by other systems in a modular, extensible fashion.\nBesides HBase, a storage handler implementation is also available for Hypertable, and others are being developed for Cassandra, Azure Table, JDBC (MySQL and others), MongoDB, ElasticSearch, Phoenix HBase, VoltDB and Google Spreadsheets. A Kafka handler demo is available.\nHive storage handler support builds on existing extensibility features in both Hadoop and Hive:\n input formats output formats serialization/deserialization libraries  Besides bundling these together, a storage handler can also implement a new metadata hook interface, allowing Hive DDL to be used for managing object definitions in both the Hive metastore and the other system\u0026rsquo;s catalog simultaneously and consistently.\nTerminology Before storage handlers, Hive already had a concept of managed vs external tables. A managed table is one for which the definition is primarily managed in Hive\u0026rsquo;s metastore, and for whose data storage Hive is responsible. An external table is one whose definition is managed in some external catalog, and whose data Hive does not own (i.e. it will not be deleted when the table is dropped).\nStorage handlers introduce a distinction between native and non-native tables. A native table is one which Hive knows how to manage and access without a storage handler; a non-native table is one which requires a storage handler.\nThese two distinctions (managed vs. external and native vs non-native) are orthogonal. Hence, there are four possibilities for base tables:\n managed native: what you get by default with CREATE TABLE external native: what you get with CREATE EXTERNAL TABLE when no STORED BY clause is specified managed non-native: what you get with CREATE TABLE when a STORED BY clause is specified; Hive stores the definition in its metastore, but does not create any files itself; instead, it calls the storage handler with a request to create a corresponding object structure external non-native: what you get with CREATE EXTERNAL TABLE when a STORED BY clause is specified; Hive registers the definition in its metastore and calls the storage handler to check that it matches the primary definition in the other system  Note that we avoid the term file-based in these definitions, since the form of storage used by the other system is irrelevant.\nDDL Storage handlers are associated with a table when it is created via the new STORED BY clause, an alternative to the existing ROW FORMAT and STORED AS clause:\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [col_comment], col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name, ...)] INTO num_buckets BUCKETS] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [AS select_statement] When STORED BY is specified, then row_format (DELIMITED or SERDE) and STORED AS cannot be specified, however starting from Hive 4.0, they can coexist to create the Iceberg table, this is the only exception. Optional SERDEPROPERTIES can be specified as part of the STORED BY clause and will be passed to the serde provided by the storage handler.\nSee CREATE TABLE and Row Format, Storage Format, and SerDe for more information.\nExample:\nCREATE TABLE hbase_table_1(key int, value string) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' WITH SERDEPROPERTIES ( \u0026quot;hbase.columns.mapping\u0026quot; = \u0026quot;cf:string\u0026quot;, \u0026quot;hbase.table.name\u0026quot; = \u0026quot;hbase_table_0\u0026quot; ); DROP TABLE works as usual, but ALTER TABLE is not yet supported for non-native tables.\nStorage Handler Interface The Java interface which must be implemented by a storage handler is reproduced below; for details, see the Javadoc in the code:\npackage org.apache.hadoop.hive.ql.metadata; import java.util.Map; import org.apache.hadoop.conf.Configurable; import org.apache.hadoop.hive.metastore.HiveMetaHook; import org.apache.hadoop.hive.ql.plan.TableDesc; import org.apache.hadoop.hive.serde2.SerDe; import org.apache.hadoop.mapred.InputFormat; import org.apache.hadoop.mapred.OutputFormat; public interface HiveStorageHandler extends Configurable { public Class\u0026lt;? extends InputFormat\u0026gt; getInputFormatClass(); public Class\u0026lt;? extends OutputFormat\u0026gt; getOutputFormatClass(); public Class\u0026lt;? extends SerDe\u0026gt; getSerDeClass(); public HiveMetaHook getMetaHook(); public void configureTableJobProperties( TableDesc tableDesc, Map\u0026lt;String, String\u0026gt; jobProperties); } The HiveMetaHook is optional, and described in the next section. If getMetaHook returns non-null, the returned object\u0026rsquo;s methods will be invoked as part of metastore modification operations.\nThe configureTableJobProperties method is called as part of planning a job for execution by Hadoop. It is the responsibility of the storage handler to examine the table definition and set corresponding attributes on jobProperties. At execution time, only these jobProperties will be available to the input format, output format, and serde.\nSee also FilterPushdownDev to learn how a storage handler can participate in filter evaluation (to avoid full-table scans).\nHiveMetaHook Interface The HiveMetaHook interface is reproduced below; for details, see the Javadoc in the code:\npackage org.apache.hadoop.hive.metastore; import org.apache.hadoop.hive.metastore.api.MetaException; import org.apache.hadoop.hive.metastore.api.Partition; import org.apache.hadoop.hive.metastore.api.Table; public interface HiveMetaHook { public void preCreateTable(Table table) throws MetaException; public void rollbackCreateTable(Table table) throws MetaException; public void commitCreateTable(Table table) throws MetaException; public void preDropTable(Table table) throws MetaException; public void rollbackDropTable(Table table) throws MetaException; public void commitDropTable(Table table, boolean deleteData) throws MetaException; Note that regardless of whether or not a remote Thrift metastore process is used in the Hive configuration, meta hook calls are always made from the Hive client JVM (never from the Thrift metastore server). This means that the jar containing the storage handler class needs to be available on the client, but not the thrift server.\nAlso note that there is no facility for two-phase commit in metadata transactions against the Hive metastore and the storage handler. As a result, there is a small window in which a crash during DDL can lead to the two systems getting out of sync.\nOpen Issues  The storage handler class name is currently saved to the metastore via table property storage_handler; this should probably be a first-class attribute on MStorageDescriptor instead Names of helper classes such as input format and output format are saved into the metastore based on what the storage handler returns during CREATE TABLE; it would be better to leave these null in case they are changed later as part of a handler upgrade A dummy location is currently set for a non-native table (the same as if it were a native table), even though no Hive files are created for it. We would prefer to store null, but first we need to fix the way data source information is passed to input formats (HIVE-1133) Storage handlers are currently set only at the table level. We may want to allow them to be specified per partition, including support for a table spanning different storage handlers. FileSinkOperator should probably be refactored, since non-native tables aren\u0026rsquo;t accessed as files, meaning a lot of the logic is irrelevant for them It may be a good idea to support temporary disablement of metastore hooks as part of manual catalog repair operations The CREATE TABLE grammar isn\u0026rsquo;t quite as strict as the one given above; some changes are needed in order to prevent STORED BY and row_format both being specified at once CREATE TABLE AS SELECT is currently prohibited for creating a non-native table. It should be possible to support this, although it may not make sense for all storage handlers. For example, for HBase, it won\u0026rsquo;t make sense until the storage handler is capable of automatically filling in column mappings.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/storagehandlers_27362063/","tags":null,"title":"Apache Hive : StorageHandlers"},{"categories":null,"contents":"Apache Hive : Streaming Data Ingest  Hive 3 Streaming API Hive HCatalog Streaming API  Streaming Mutation API   Streaming Requirements Limitations API Usage  Transaction and Connection Management  HiveEndPoint StreamingConnection TransactionBatch  Usage Guidelines   Notes about the HiveConf Object   I/O – Writing Data  RecordWriter DelimitedInputWriter StrictJsonWriter StrictRegexWriter AbstractRecordWriter   Error Handling   Example – Non-secure Mode Example – Secure Streaming Knowledge Base  Hive 3 Streaming API Hive 3 Streaming API Documentation - new API available in Hive 3\nHive HCatalog Streaming API Traditionally adding new data into Hive requires gathering a large amount of data onto HDFS and then periodically adding a new partition. This is essentially a “batch insertion”. Insertion of new data into an existing partition is not permitted. Hive Streaming API allows data to be pumped continuously into Hive. The incoming data can be continuously committed in small batches of records into an existing Hive partition or table. Once data is committed it becomes immediately visible to all Hive queries initiated subsequently.\nThis API is intended for streaming clients such as Flume and Storm, which continuously generate data. Streaming support is built on top of ACID based insert/update support in Hive (see Hive Transactions).\nThe Classes and interfaces part of the Hive streaming API are broadly categorized into two sets. The first set provides support for connection and transaction management while the second set provides I/O support. Transactions are managed by the metastore. Writes are performed directly to HDFS.\nStreaming to unpartitioned tables is also supported. The API supports Kerberos authentication starting in Hive 0.14.\nNote on packaging: The APIs are defined in the Java package org.apache.hive.hcatalog.streaming and part of the hive-hcatalog-streaming Maven module in Hive.\nStreaming Mutation API Starting in release 2.0.0, Hive offers another API for mutating (insert/update/delete) records into transactional tables using Hive’s ACID feature. See HCatalog Streaming Mutation API for details and a comparison with the streaming data ingest API that is described in this document.\nStreaming Requirements A few things are required to use streaming.\n The following settings are required in hive-site.xml to enable ACID support for streaming:  hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager hive.compactor.initiator.on = true(See more important details here) hive.compactor.cleaner.on = true (From Hive 4.0.0 onwards. See more important details here) hive.compactor.worker.threads \u0026gt; 0   “stored as orc” must be specified during table creation. Only ORC storage format is supported currently. tblproperties(\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) must be set on the table during creation. The Hive table must be bucketed, but not sorted. So something like “clustered by (colName) into 10 buckets” must be specified during table creation. The number of buckets is ideally the same as the number of streaming writers. User of the client streaming process must have the necessary permissions to write to the table or partition and create partitions in the table. (Temporary requirements) When issuing queries on streaming tables, the client needs to set  hive.vectorized.execution.enabled to false (for Hive version \u0026lt; 0.14.0) hive.input.format to org.apache.hadoop.hive.ql.io.HiveInputFormat    Limitations Out of the box, currently, the streaming API only provides support for streaming delimited input data (such as CSV, tab separated, etc.) and JSON (strict syntax) formatted data. Support for other input formats can be provided by additional implementations of the RecordWriter interface.\nCurrently only ORC is supported for the format of the destination table.\nAPI Usage Transaction and Connection Management HiveEndPoint The class HiveEndPoint describes a Hive End Point to connect to. This describes the database, table and partition names. Invoking the newConnection method on it establishes a connection to the Hive MetaStore for streaming purposes. It returns a StreamingConnection object. Multiple connections can be established on the same endpoint. StreamingConnection can then be used to initiate new transactions for performing I/O.\nIt is very likely that in a setup where data is being streamed continuously the data is added into new partitions periodically. Either the Hive admin can pre-create the necessary partitions or the streaming clients can create them as needed. HiveEndPoint.newConnection() accepts a boolean argument to indicate whether the partition should be auto created. Partition creation being an atomic action, multiple clients can race to create the partition, but only one will succeed, so streaming clients do not have to synchronize when creating a partition.\nTransactions are implemented slightly differently than traditional database systems. Each transaction has an id and multiple transactions are grouped into a “Transaction Batch”. This helps grouping records from multiple transactions into fewer files (rather than 1 file per transaction). After connection, a streaming client first requests a new batch of transactions. In response it receives a set of Transaction Ids that are part of the transaction batch. Subsequently the client proceeds to consume one transaction id at a time by initiating new Transactions. The client will write() one or more records per transaction and either commits or aborts the current transaction before switching to the next one. Each TransactionBatch.write() invocation automatically associates the I/O attempt with the current Txn ID. The user of the streaming client process, needs to have write permissions to the partition or table. Kerberos based authentication is required to acquire connections as a specific user. See secure streaming example below.\nConcurrency Note: I/O can be performed on multiple TransactionBatches concurrently. However the transactions within a transaction batch must be consumed sequentially.\nSee the Javadoc for HiveEndPoint for more information. Generally a user will establish the destination info with HiveEndPoint object and then calls newConnection to make a connection and get back a StreamingConnection object.\nStreamingConnection The StreamingConnection class is used to acquire batches of transactions. Once the connection has been provided by HiveEndPoint the application will generally enter a loop where it calls fetchTransactionBatch and writes a series of transactions. When closing down, the application should call close. See the Javadoc for more information. TransactionBatch TransactionBatch is used to write a series of transactions. There is one file created on HDFS per TxnBatch in each bucket. The API examines each record to decide which bucket it belongs to and writes it to the appropriate bucket. If the table has 5 buckets, there will be 5 files (some of them could be empty) for the TxnBatch (before compaction kicks in). Prior to Hive 1.3.0, a bug in the API\u0026rsquo;s bucket computation logic caused incorrect distribution of records into buckets, which could lead to incorrect data returned from queries using bucket join algorithms.\nFor each transaction in the TxnBatch, the application calls beginNextTransaction, write, and then commit or abort as appropriate. See the Javadoc for details. A Transaction cannot include data from more than one partition.\nTransactions in a TransactionBatch are eventually expired by the Metastore if not committed or aborted after hive.txn.timeout secs. TrasnactionBatch class provides a heartbeat() method to prolong the lifetime of unused transactions in the batch. A good rule of thumb is to send call heartbeat() at (hive.txn.timeout/2) intervals after creating a TransactionBatch. This is sufficient to keep an inactive transaction alive but not load the metastore unnecessarily.\nUsage Guidelines Generally, the more events are included in each transaction the more throughput can be achieved. It\u0026rsquo;s common commit either after a certain number of events or after a certain time interval, whichever comes first. The later ensures that when event flow rate is variable, transactions don\u0026rsquo;t stay open too long. There is no practical limit on how much data can be included in a single transaction. The only concern is amount of data which will need to be replayed if the transaction fails. The concept of a TransactionBatch serves to reduce the number of files created by SteramingAPI in HDFS. Since all transactions in a given batch write to the same physical file (per bucket), a partition can only be compacted up to the the level of the earliest transaction of any batch which contains an open transaction. Thus TranactionBatches should not be made excessively large. It makes sense to include a timer to close a TransactionBatch (even if it has unused transactions) after some amount of time.\nNote: Hive 1.3.0 onwards, invoking TxnBatch.close() will cause all unused transaction in the current TxnBatch to be aborted.\nNotes about the HiveConf Object HiveEndPoint.newConnection() accepts a HiveConf argument. This can either be set to null, or a pre-created HiveConf object can be provided. If this is null, a HiveConf object will be created internally and used for the connection. When a HiveConf object is instantiated, if the directory containing the hive-site.xml is part of the java classpath, then the HiveConf object will be initialized with values from it. If no hive-site.xml is found, then the object will be initialized with defaults. Pre-creating this object and reusing it across multiple connections may have a noticeable impact on performance if connections are being opened very frequently (for example several times a second). Secure connection relies on \u0026lsquo;hive.metastore.kerberos.principal\u0026rsquo; being set correctly in the HiveConf object.\nRegardless of what values are set in hive-site.xml or custom HiveConf, the API will internally override some settings in it to ensure correct streaming behavior. The below is the list of settings that are overridden:\n hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager hive.support.concurrency = true hive.metastore.execute.setugi = true hive.execution.engine = mr  I/O – Writing Data These classes and interfaces provide support for writing the data to Hive within a transaction.\nRecordWriter RecordWriter is the base interface implemented by all Writers. A Writer is responsible for taking a record in the form of a byte[] containing data in a known format (such as CSV) and writing it out in the format supported by Hive streaming. A RecordWriter may reorder or drop fields from the incoming record if necessary to map them to the corresponding columns in the Hive Table. A streaming client will instantiate an appropriate RecordWriter type and pass it to TransactionBatch. The streaming client does not directly interact with RecordWriter therafter. The TransactionBatch will thereafter use and manage the RecordWriter instance to perform I/O. See the Javadoc for details.\nA RecordWriter\u0026rsquo;s primary functions are:\n Modify input record: This may involve dropping fields from input data if they don’t have corresponding table columns, adding nulls in case of missing fields for certain columns, and changing the order of incoming fields to match the order of fields in the table. This task requires understanding of incoming data format. Not all formats (for example JSON, which includes field names in the data) need this step. Encode modified record: The encoding involves serialization using an appropriate Hive SerDe. Identify the bucket to which the record belongs Write encoded record to Hive using the AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket.  DelimitedInputWriter Class DelimitedInputWriter implements the RecordWriter interface. It accepts input records that in delimited formats (such as CSV) and writes them to Hive. It reorders the fields if needed, and converts the record into an Object using LazySimpleSerde, which is then passed on to the underlying AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket. See Javadoc.\nStrictJsonWriter Class StrictJsonWriter implements the RecordWriter interface. It accepts input records that in strict JSON format and writes them to Hive. It converts the JSON record directly into an Object using JsonSerde, which is then passed on to the underlying AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket. See Javadoc.\nStrictRegexWriter Class StrictRegexWriter implements the RecordWriter interface. It accepts input records, regex that in text format and writes them to Hive. It converts the text record using proper regex directly into an Object using RegexSerDe, which is then passed on to the underlying AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket. See Javadoc. Available in Hive 1.2.2+ and 2.3.0+.\nAbstractRecordWriter This is a base class that contains some of the common code needed by RecordWriter objects such as schema lookup and computing the bucket into which a record should belong.\nError Handling It\u0026rsquo;s imperative for proper functioning of the system that the client of this API handle errors correctly. Once a TransactionBatch is obtained, if any exception is thrown from TransactionBatch (except SerializationError) should cause the client to call TransactionBatch.abort() to abort current transaction and then TransactionBatch.close() and start a new batch to write more data and/or redo the work of the last transaction during which the failure occurred. Not following this may, in rare cases, cause file corruption. Furthermore, StreamingException should ideally cause the client to perform exponential back off before starting new batch. This will help the cluster stabilize since the most likely reason for these failures is HDFS overload.\nSerializationError indicates that a given tuple could not be parsed. The client may choose to throw away such tuples or send them to a dead letter queue. After seeing this exception, more data can be written to the current transaction and further transactions in the same TransactionBatch.\nExample – Non-secure Mode ///// Stream five records in two transactions ///// // Assumed HIVE table Schema: create table alerts ( id int , msg string ) partitioned by (continent string, country string) clustered by (id) into 5 buckets stored as orc tblproperties(\u0026quot;transactional\u0026quot;=\u0026quot;true\u0026quot;); // currently ORC is required for streaming //------- MAIN THREAD ------- // String dbName = \u0026quot;testing\u0026quot;; String tblName = \u0026quot;alerts\u0026quot;; ArrayList\u0026lt;String\u0026gt; partitionVals = new ArrayList\u0026lt;String\u0026gt;(2); partitionVals.add(\u0026quot;Asia\u0026quot;); partitionVals.add(\u0026quot;India\u0026quot;); String serdeClass = \u0026quot;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026quot;; HiveEndPoint hiveEP = new HiveEndPoint(\u0026quot;thrift://x.y.com:9083\u0026quot;, dbName, tblName, partitionVals); .. spin up threads .. //------- Thread 1 -------// StreamingConnection connection = hiveEP.newConnection(true); DelimitedInputWriter writer = new DelimitedInputWriter(fieldNames,\u0026quot;,\u0026quot;, hiveEP); TransactionBatch txnBatch = connection.fetchTransactionBatch(10, writer); ///// Batch 1 - First TXN txnBatch.beginNextTransaction(); txnBatch.write(\u0026quot;1,Hello streaming\u0026quot;.getBytes()); txnBatch.write(\u0026quot;2,Welcome to streaming\u0026quot;.getBytes()); txnBatch.commit(); if(txnBatch.remainingTransactions() \u0026gt; 0) { ///// Batch 1 - Second TXN txnBatch.beginNextTransaction(); txnBatch.write(\u0026quot;3,Roshan Naik\u0026quot;.getBytes()); txnBatch.write(\u0026quot;4,Alan Gates\u0026quot;.getBytes()); txnBatch.write(\u0026quot;5,Owen O’Malley\u0026quot;.getBytes()); txnBatch.commit(); txnBatch.close(); connection.close(); } txnBatch = connection.fetchTransactionBatch(10, writer); ///// Batch 2 - First TXN txnBatch.beginNextTransaction(); txnBatch.write(\u0026quot;6,David Schorow\u0026quot;.getBytes()); txnBatch.write(\u0026quot;7,Sushant Sowmyan\u0026quot;.getBytes()); txnBatch.commit(); if(txnBatch.remainingTransactions() \u0026gt; 0) { ///// Batch 2 - Second TXN txnBatch.beginNextTransaction(); txnBatch.write(\u0026quot;8,Ashutosh Chauhan\u0026quot;.getBytes()); txnBatch.write(\u0026quot;9,Thejas Nair\u0026quot; getBytes()); txnBatch.commit(); txnBatch.close(); } connection.close(); //------- Thread 2 -------// StreamingConnection connection2 = hiveEP.newConnection(true); DelimitedInputWriter writer2 = new DelimitedInputWriter(fieldNames,\u0026quot;,\u0026quot;, hiveEP); TransactionBatch txnBatch2= connection.fetchTransactionBatch(10, writer2); ///// Batch 1 - First TXN txnBatch2.beginNextTransaction(); txnBatch2.write(\u0026quot;21,Venkat Ranganathan\u0026quot;.getBytes()); txnBatch2.write(\u0026quot;22,Bowen Zhang\u0026quot;.getBytes()); txnBatch2.commit(); ///// Batch 1 - Second TXN txnBatch2.beginNextTransaction(); txnBatch2.write(\u0026quot;23,Venkatesh Seetaram\u0026quot;.getBytes()); txnBatch2.write(\u0026quot;24,Deepesh Khandelwal\u0026quot;.getBytes()); txnBatch2.commit(); txnBatch2.close(); connection.close(); txnBatch = connection.fetchTransactionBatch(10, writer); ///// Batch 2 - First TXN txnBatch.beginNextTransaction(); txnBatch.write(\u0026quot;26,David Schorow\u0026quot;.getBytes()); txnBatch.write(\u0026quot;27,Sushant Sowmyan\u0026quot;.getBytes()); txnBatch.commit(); txnBatch2.close(); connection2.close(); Example – Secure Streaming To connect via Kerberos to a secure Hive metastore, a UserGroupInformation (UGI) object is required. This UGI object must be acquired externally and passed as argument to the EndPoint.newConnection. All subsequent internal operations carried out using that connection object, such as acquiring transaction batch, writes and commits, will be will be automatically wrapped internally in a ugi.doAs block as necessary.\n**Important:**To connect using Kerberos, the \u0026lsquo;authenticatedUser\u0026rsquo; argument to EndPoint.newConnection() should have been used to do a Kerberos login. Additionally the \u0026lsquo;hive.metastore.kerberos.principal\u0026rsquo; setting should be set correctly either in hive-site.xml or in the \u0026lsquo;conf\u0026rsquo; argument (if not null). If using hive-site.xml, its directory should be included in the classpath.\nimport org.apache.hadoop.security.UserGroupInformation; HiveEndPoint hiveEP2 = ... ; UserGroupInformation ugi = .. authenticateWithKerberos(principal,keytab); StreamingConnection secureConn = hiveEP2.newConnection(true, null, ugi); DelimitedInputWriter writer3 = new DelimitedInputWriter(fieldNames, \u0026quot;,\u0026quot;, hiveEP2); TransactionBatch txnBatch3= secureConn.fetchTransactionBatch(10, writer3); ///// Batch 1 - First TXN – over secure connection txnBatch3.beginNextTransaction(); txnBatch3.write(\u0026quot;28,Eric Baldeschwieler\u0026quot;.getBytes()); txnBatch3.write(\u0026quot;29,Ari Zilka\u0026quot;.getBytes()); txnBatch3.commit(); txnBatch3.close(); secureConn.close(); Knowledge Base  Talks and Presentations Lessons learnt from NiFi streaming data to Hive transactional tables  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/streaming-data-ingest_40509746/","tags":null,"title":"Apache Hive : Streaming Data Ingest"},{"categories":null,"contents":"Apache Hive : Streaming Data Ingest V2 Starting in release Hive 3.0.0, Streaming Data Ingest is deprecated and is replaced by newer V2 API (HIVE-19205).  Hive Streaming API  Streaming Mutation API Deprecation and Removal   Streaming Requirements Limitations API Usage  Transaction and Connection Management  HiveStreamingConnection  Usage Guidelines   Notes about the HiveConf Object   I/O – Writing Data  RecordWriter StrictDelimitedInputWriter StrictJsonWriter StrictRegexWriter AbstractRecordWriter   Error Handling   Example  Hive Streaming API Traditionally adding new data into Hive requires gathering a large amount of data onto HDFS and then periodically adding a new partition. This is essentially a “batch insertion”. Hive Streaming API allows data to be pumped continuously into Hive. The incoming data can be continuously committed in small batches of records into an existing Hive partition or table. Once data is committed it becomes immediately visible to all Hive queries initiated subsequently.\nThis API is intended for streaming clients such as NiFi, Flume and Storm, which continuously generate data. Streaming support is built on top of ACID based insert/update support in Hive (see Hive Transactions).\nThe Classes and interfaces part of the Hive streaming API are broadly categorized into two sets. The first set provides support for connection and transaction management while the second set provides I/O support. Transactions are managed by the metastore. Writes are performed directly to destination filesystem defined by the table (HDFS, S3A etc.).\nStreaming to unpartitioned tables, partitioned table with static partitions and partitioned table with dynamic partitions are all supported. The API supports Kerberos authentication and Storage based authorization. The client user has to be logged in using kerberos before invoking the API. The logged in user should have appropriate storage permissions to write to destination partition or table location. It is recommended to use \u0026lsquo;hive\u0026rsquo; user in order for the hive queries to be able to read the data back (written by streaming API) with doAs set to false (query is run as hive user).\nNote on packaging: The APIs are defined in the Java package org.apache.hive.streaming and part of the hive-streaming Maven module in Hive.\nStreaming Mutation API Deprecation and Removal Starting in release 3.0.0, Hive deprecated Streaming Mutation API from hive-hcatalog-streaming module and will no longer be supported in future releases. The new hive-streaming module no longer support mutation API. Streaming Requirements A few things are required to use streaming.\n The following settings are required in hive-site.xml to enable ACID support for streaming:  hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager hive.compactor.initiator.on = true(See more important details here) hive.compactor.cleaner.on = true (From Hive 4.0.0 onwards. See more important details here) hive.compactor.worker.threads \u0026gt; 0   “stored as orc” must be specified during table creation. Only ORC storage format is supported currently. tblproperties(\u0026ldquo;transactional\u0026rdquo;=\u0026ldquo;true\u0026rdquo;) must be set on the table during creation. User of the client streaming process must have the necessary permissions to write to the table or partition and create partitions in the table.  Limitations Out of the box, currently, the streaming API only provides support for streaming delimited input data (such as CSV, tab separated, etc.), JSON and Regex formatted data. The records writers that are supported in Hive 3.0.0 release are\nStrictDelimitedInputWriter StrictJsonWriter StrictRegexWriter All these record writers expects strict schema match, meaning the schema of the records should exactly match with the table schema (Note: The writers does not perform schema check, it is up to the clients to make sure the record schema matches the table schema). Support for other input formats can be provided by additional implementations of the RecordWriter interface.\nCurrently only ORC is supported for the format of the destination table.\nAPI Usage Transaction and Connection Management HiveStreamingConnection The class HiveStreamingConnection describes the streaming connection related information. This describes the database, table, partition names, metastore URI to connect to and record writer to use for streaming records into destination partition or table. All these information can be specified via Builder API which then establishes a connection to the Hive MetaStore for streaming purposes. Invoking connect on the Builder API returns a StreamingConnection object. StreamingConnection can then be used to initiate new transactions for performing I/O.\n* CONCURRENCY NOTE: The streaming connection APIs and record writer APIs are not thread-safe. Streaming connection creation, * begin/commit/abort transactions, write and close has to be called in the same thread. If close() or * abortTransaction() has to be triggered from a separate thread it has to be co-ordinated via external variables or * synchronization mechanism HiveStreamingConnection API also supports 2 partitioning mode (static vs dynamic). In static partitioning mode, partition column values can be specified upfront via builder API. If the static partition exists (pre-created by the user or already existing partition in a table), the streaming connection will use it and if it does not exist the streaming connection will create a new static partition using the specified values. If the table is partitioned and if static partition values are not specified in builder API, hive streaming connection will use dynamic partitioning mode under which hive streaming connection expects the partition values to be the last columns in the record (similar to how hive dynamic partitioning works). For example, if table is partitioned by 2 columns (year and month), hive streaming connection will extract last 2 columns from the input records using which partitions will be created dynamically in metastore. Transactions are implemented slightly differently than traditional database systems. Each transaction has an id and multiple transactions are grouped into a “Transaction Batch”. This helps grouping records from multiple transactions into fewer files (rather than 1 file per transaction). During hive streaming connection creation, transaction batch size can be specified via builder API. Transaction management is completely hidden behind the API, in most cases users do not have to worry about tuning the transaction batch size (which is an expert level setting and might not be honored in future release). Also the API automatically rolls over to next transaction batch on beginTransaction() invocation if the current transaction batch is exhausted. The recommendation is to leave the transaction batch size at default value of 1 and group several thousands records together under a each transaction. Since each transaction corresponds to a delta directory in the filesystem, committing transaction too often can end up creating too many small directories. Transactions in a TransactionBatch are eventually expired by the Metastore if not committed or aborted after hive.txn.timeout secs. In order to keep the transactions alive, HiveStreamingConnection has a heartbeater thread which by default sends heartbeat after (hive.txn.timeout/2) intervals for all the open transactions. See the Javadoc for HiveStreamingConnection for more information. Usage Guidelines Generally, the more records are included in each transaction the more throughput can be achieved. It\u0026rsquo;s common to commit either after a certain number of records or after a certain time interval, whichever comes first. The later ensures that when event flow rate is variable, transactions don\u0026rsquo;t stay open too long. There is no practical limit on how much data can be included in a single transaction. The only concern is amount of data which will need to be replayed if the transaction fails. The concept of a TransactionBatch serves to reduce the number of files (and delta directories) created by HiveStreamingConnection API in the filesystem. Since all transactions in a given transaction batch write to the same physical file (per bucket), a partition can only be compacted up to the the level of the earliest transaction of any batch which contains an open transaction. Thus TransactionBatches should not be made excessively large. It makes sense to include a timer to close a TransactionBatch (even if it has unused transactions) after some amount of time.\nThe HiveStreamingConnection is highly optimized for write throughput (Delta Streaming Optimizations) and as a result the delta files generated by Hive streaming ingest have many of the ORC features disabled (dictionary encoding, indexes, compression, etc.) to facilitate high throughput writes. When the compactor kicks in, these delta files get rewritten into read- and storage-optimized ORC format (enable dictionary encoding, indexes and compression). So it is recommended to configure the compactor more aggressively/frequently (refer to Compactor) to generate compacted and optimized ORC files.\nNotes about the HiveConf Object HiveStreamingConnect builder API accepts a HiveConf argument. This can either be set to null, or a pre-created HiveConf object can be provided. If this is null, a HiveConf object will be created internally and used for the connection. When a HiveConf object is instantiated, if the directory containing the hive-site.xml is part of the java classpath, then the HiveConf object will be initialized with values from it. If no hive-site.xml is found, then the object will be initialized with defaults. Pre-creating this object and reusing it across multiple connections may have a noticeable impact on performance if connections are being opened very frequently (for example several times a second). Secure connection relies on \u0026lsquo;metastore.kerberos.principal\u0026rsquo; being set correctly in the HiveConf object.\nRegardless of what values are set in hive-site.xml or custom HiveConf, the API will internally override some settings in it to ensure correct streaming behavior. The below is the list of settings that are overridden:\n hive.txn.manager = org.apache.hadoop.hive.ql.lockmgr.DbTxnManager hive.support.concurrency = true hive.metastore.execute.setugi = true hive.exec.dynamic.partition.mode= nonstrict hive.exec.orc.delta.streaming.optimizations.enabled= true hive.metastore.client.cache.enabled= false  I/O – Writing Data These classes and interfaces provide support for writing the data to Hive within a transaction.\nRecordWriter RecordWriter is the base interface implemented by all Writers. A Writer is responsible for taking a record in the form of a byte[] (or InputStream with configurable line delimiter) containing data in a known format (such as CSV) and writing it out in the format supported by Hive streaming. A RecordWriter with Strict implementation expects record schema to exactly match as that of table schema. A RecordWriter writing in dynamic partitioning mode expects the partition columns to be the last columns in each record. If partition column value is empty or null, records will go into HIVE_DEFAULT_PARTITION. A streaming client will instantiate an appropriate RecordWriter type and pass it to HiveStreamingConnection builder API. The streaming client does not directly interact with RecordWriter thereafter. The StreamingConnection object will thereafter use and manage the RecordWriter instance to perform I/O. See the Javadoc for details.\nA RecordWriter\u0026rsquo;s primary functions are:\n Modify input record: This may involve dropping fields from input data if they don’t have corresponding table columns, adding nulls in case of missing fields for certain columns, and adding HIVE_DEFAULT_PARTITION if partition column value is null or empty. Dynamically creating partitions requires understanding of incoming data format to extract last columns to extract partition values. Encode modified record: The encoding involves serialization using an appropriate Hive SerDe. For bucketed tables, extract bucket column values from the record to identify the bucket where the record belongs. For partitioned tables, in dynamic partitioning mode, extract the partition column values from last N columns (where N is number of partitions) of the record to identify the partition where the record belongs. Write encoded record to Hive using the AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket.  StrictDelimitedInputWriter Class StrictDelimitedInputWriter implements the RecordWriter interface. It accepts input records that in delimited formats (such as CSV) and writes them to Hive. It expects the record schema to match the table schema and expects partition values at the last. The input records are converted into an Object using LazySimpleSerde to extract bucket and partition columns, which is then passed on to the underlying AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket. See Javadoc.\nStrictJsonWriter Class StrictJsonWriter implements the RecordWriter interface. It accepts input records that in strict JSON format and writes them to Hive. It converts the JSON record directly into an Object using JsonSerde, which is then passed on to the underlying AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket and partition. See Javadoc.\nStrictRegexWriter Class StrictRegexWriter implements the RecordWriter interface. It accepts input records, regex that in text format and writes them to Hive. It converts the text record using proper regex directly into an Object using RegexSerDe, which is then passed on to the underlying AcidOutputFormat\u0026rsquo;s record updater for the appropriate bucket. See Javadoc.\nAbstractRecordWriter This is a base class that contains some of the common code needed by RecordWriter objects such as schema lookup and computing the bucket and partition into which a record should belong.\nError Handling It\u0026rsquo;s imperative for proper functioning of the system that the client of this API handle errors correctly. The API just returns StreamingException, however there are several subclasses of StreamingException that are thrown under different situations.\nConnectionError - When a connection to metastore cannot be established or when HiveStreamingConnection connect API is used incorrectly\nInvalidTable - Thrown when destination table does not exists or is not ACID transactional table\nInvalidTransactionState - Thrown when the transaction batch gets to invalid state\nSerializationError - Thrown when SerDe throws any exception during serialization/deserialization/writing of records\nStreamingIOFailure - Thrown if destination partition cannot be created, if record updaters throws any IO errors during writing or flushing of records.\nTransactionError - Throw when internal transaction cannot be committed or aborted.\nIts up to the clients to decide which exceptions can be retried (typically with some backoff), ignored or rethrown. Typically connection related exceptions can be retried with exponential backoff. Serialization related errors can either be thrown or ignored (if some incoming records are incorrect/corrupt and can be dropped).\nExample ///// Stream five records in two transactions ///// // Assumed HIVE table Schema: create table alerts ( id int , msg string ) partitioned by (continent string, country string) clustered by (id) into 5 buckets stored as orc tblproperties(\u0026quot;transactional\u0026quot;=\u0026quot;true\u0026quot;); // currently ORC is required for streaming //------- MAIN THREAD ------- // String dbName = \u0026quot;testing\u0026quot;; String tblName = \u0026quot;alerts\u0026quot;; .. spin up thread 1 .. // static partition values ArrayList\u0026lt;String\u0026gt; partitionVals = new ArrayList\u0026lt;String\u0026gt;(2); partitionVals.add(\u0026quot;Asia\u0026quot;); partitionVals.add(\u0026quot;India\u0026quot;); // create delimited record writer whose schema exactly matches table schema StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder() .withFieldDelimiter(',') .build(); // create and open streaming connection (default.src table has to exist already) StreamingConnection connection = HiveStreamingConnection.newBuilder() .withDatabase(dbName) .withTable(tblName) .withStaticPartitionValues(partitionVals) .withAgentInfo(\u0026quot;example-agent-1\u0026quot;) .withRecordWriter(writer) .withHiveConf(hiveConf) .connect(); // begin a transaction, write records and commit 1st transaction connection.beginTransaction(); connection.write(\u0026quot;1,val1\u0026quot;.getBytes()); connection.write(\u0026quot;2,val2\u0026quot;.getBytes()); connection.commitTransaction(); // begin another transaction, write more records and commit 2nd transaction connection.beginTransaction(); connection.write(\u0026quot;3,val3\u0026quot;.getBytes()); connection.write(\u0026quot;4,val4\u0026quot;.getBytes()); connection.commitTransaction(); // close the streaming connection connection.close(); .. spin up thread 2 .. // dynamic partitioning // create delimited record writer whose schema exactly matches table schema StrictDelimitedInputWriter writer = StrictDelimitedInputWriter.newBuilder() .withFieldDelimiter(',') .build(); // create and open streaming connection (default.src table has to exist already) StreamingConnection connection = HiveStreamingConnection.newBuilder() .withDatabase(dbName) .withTable(tblName) .withAgentInfo(\u0026quot;example-agent-1\u0026quot;) .withRecordWriter(writer) .withHiveConf(hiveConf) .connect(); // begin a transaction, write records and commit 1st transaction connection.beginTransaction(); // dynamic partition mode where last 2 columns are partition values connection.write(\u0026quot;11,val11,Asia,China\u0026quot;.getBytes()); connection.write(\u0026quot;12,val12,Asia,India\u0026quot;.getBytes()); connection.commitTransaction(); // begin another transaction, write more records and commit 2nd transaction connection.beginTransaction(); connection.write(\u0026quot;13,val13,Europe,Germany\u0026quot;.getBytes()); connection.write(\u0026quot;14,val14,Asia,India\u0026quot;.getBytes()); connection.commitTransaction(); // close the streaming connection connection.close(); ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/streaming-data-ingest-v2_85477610/","tags":null,"title":"Apache Hive : Streaming Data Ingest V2"},{"categories":null,"contents":"Apache Hive : Subqueries in SELECT Problem Currently Hive doesn\u0026rsquo;t support subqueries in a SELECT statement, for example, the following query will not run on Hive:\nSELECT customer.customer_num, (SELECT SUM(ship_charge) FROM orders WHERE customer.customer_num = orders.customer_num ) AS total_ship_chg FROM customer Recently a lot of work has been done to extend support for subqueries (HIVE-15456). But this work primarily targeted extending subquery support in WHERE and HAVING clauses. We plan to continue the work done in HIVE-15456 to support subqueries in a select list (see HIVE-16091).\nAssumptions We plan to limit the scope with the following assumptions and limitations.\n Subqueries could only be top-level expressions in SELECT. That is, subqueries in complex expressions, aggregates, UDFs, etc. will not be supported for now. For example the following queries will not run on Hive:  Not Supported\n-- subquery in non-simple expression SELECT 1 + (SELECT SUM(ship_charge) FROM orders), customer.customer_num FROM customer -- subquery in CASE SELECT CASE WHEN (select count(*) from store_sales where ss_quantity between 1 and 20) \u0026gt; 409437 THEN (select avg(ss_ext_list_price) from store_sales where ss_quantity between 1 and 20) ELSE (select avg(ss_net_paid_inc_tax) from store_sales where ss_quantity between 1 and 20) end bucket1 FROM reason WHERE r_reason_sk = 1  Scalar subqueries can only return at most one row. Hive will check for this case at runtime and throw an error if not satisfied. For example the following query is invalid:  Not Supported\nSELECT customer.customer_num, (SELECT ship_charge FROM orders WHERE customer.customer_num = orders.customer_num ) AS total_ship_chg FROM customer  Scalar subqueries can only have one column. Hive will check for this case during compilation and throw an error. For example the following query is invalid:  Not Supported\nSELECT customer.customer_num, (SELECT ship_charge, customer_num FROM orders LIMIT 1 ) AS total_ship_chg FROM customer  Correlated variables are only permitted in a filter, that is, a WHERE or HAVING clause. For example the following query is invalid:  Not Supported\nSELECT customer.customer_num, (SELECT customer.customer_num FROM orders WHERE customer.customer_num = orders.customer_num ) AS total_ship_chg FROM customer  Subqueries with DISTINCT are not allowed. Since DISTINCT will be evaluated as GROUP BY , subqueries with DISTINCT are disallowed for now.  Design Given the assumptions above, the following kind of subqueries could be used in SELECT.  Scalar subqueries, for example:   SELECT customer.customer_num, (SELECT SUM(ship_charge) FROM orders WHERE customer.customer_num = orders.customer_num ) AS total_ship_chg FROM customer  IN subqueries, for example:  SELECT p_size IN ( SELECT MAX(p_size) FROM part) FROM part  EXISTS subqueries, for example:  SELECT EXISTS(SELECT p_size FROM part) FROM part All of the above queries could be correlated or uncorrelated.\nDesign for this will be similar to the work done in HIVE-15456.\n genLogicalPlan will go over the select list to do the following:  If subquery is not a top-level expression, throw an error. Otherwise, generate an appropriate plan by using RexSubquery to represent the subquery.   HiveSubqueryRemoveRule will then be applied to remove the RexSubquery node and rewrite the query into a join. HiveRelDecorrelator::decorrelateQuery will then be used to decorrelate correlated queries.   HIVE-16091 covers the initial work for supporting subqueries in SELECT.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/subqueries-in-select_68717850/","tags":null,"title":"Apache Hive : Subqueries in SELECT"},{"categories":null,"contents":"Apache Hive : Suggestion for DDL Commands in HMS schema upgrade scripts In this page, I would like to share the information I learned from Braintree\u0026rsquo;s Blog about how they handle DB schema migration while application is up and serving requests. I think this should benefits to developer who is working on HMS\u0026rsquo;s schema upgrade scripts. As for some DDL commands, they can lock out updates to a table for a long time and database operation that locks for more than a few seconds is indistinguishable from an outage for customers. Discussion here will be focused one using PostgreSQL as HMS DB and the following are the links for those useful articles:\n Braintree: Safe Operations For High Volume PostgreSQL: https://www.braintreepayments.com/blog/safe-operations-for-high-volume-postgresql/ PostgreSQL at Scale: Database Schema Changes Without Downtime: https://medium.com/braintree-product-technology/postgresql-at-scale-database-schema-changes-without-downtime-20d3749ed680 Understanding PostgreSQL locks: http://shiroyasha.io/understanding-postgresql-locks.html#:~:targetText=ROW%20SHARE%20%E2%80%94%20Acquired%20by%20the,of%20the%20alter%20table%20commands  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/suggestion-for-ddl-commands-in-hms-schema-upgrade-scripts_138022168/","tags":null,"title":"Apache Hive : Suggestion for DDL Commands in HMS schema upgrade scripts"},{"categories":null,"contents":"Apache Hive : Supported Features: Apache Hive 3.1 This table covers all mandatory features from SQL:2016 as well as optional features that Hive implements.\n   Feature ID Feature Name Implemented Mandatory Comments     E011 Numeric data types Yes Mandatory    E011-01 INTEGER and SMALLINT data types (including all spellings) Yes Mandatory    E011-02 REAL, DOUBLE PRECISON, and FLOAT data types Yes Mandatory    E011-03 DECIMAL and NUMERIC data types Yes Mandatory    E011-04 Arithmetic operators Yes Mandatory    E011-05 Numeric comparison Yes Mandatory    E011-06 Implicit casting among the numeric data types Yes Mandatory    E021 Character string types Yes Mandatory    E021-01 CHARACTER data type (including all its spellings) Partial Mandatory Only support CHAR, not CHARACTER   E021-02 CHARACTER VARYING data type (including all its spellings) Partial Mandatory Only support VARCHAR, not CHARACTER VARYING or CHAR VARYING   E021-03 Character literals Yes Mandatory    E021-04 CHARACTER_LENGTH function Yes Mandatory    E021-05 OCTET_LENGTH function Yes Mandatory    E021-06 SUBSTRING function Partial Mandatory Standard: SUBSTRING(val FROM startpos [FOR len]). Hive: SUBSTRING(val, startpos [, len])   E021-07 Character concatenation Yes Mandatory    E021-08 UPPER and LOWER functions Yes Mandatory    E021-09 TRIM function Yes Mandatory    E021-10 Implicit casting among the fixed-length and variable-length character string types Yes Mandatory    E021-11 POSITION function No Mandatory    E021-12 Character comparison Yes Mandatory    E031 Identifiers Partial Mandatory Unquoted identifiers use C syntax ([A-Za-z][A-Za-z0-9_]*). Quoted identifiers can have any character.   E031-01 Delimited identifiers Partial Mandatory Quoting done with ` rather than \u0026ldquo;, only supported for columns, not tables, views, etc.   E031-02 Lower case identifiers Yes Mandatory    E031-03 Trailing underscore Yes Mandatory    E051 Basic query specification Yes Mandatory    E051-01 SELECT DISTINCT Yes Mandatory    E051-02 GROUP BY clause Yes Mandatory    E051-04 GROUP BY can contain columns not in  Yes Mandatory    E051-05 Select list items can be renamed Yes Mandatory    E051-06 HAVING clause Yes Mandatory    E051-07 Qualified * in select list Yes Mandatory    E051-08 Correlation names in the FROM clause Yes Mandatory    E051-09 Rename columns in the FROM clause Yes Mandatory    E061 Basic predicates and search conditions Yes Mandatory    E061-01 Comparison predicate Yes Mandatory    E061-02 BETWEEN predicate Yes Mandatory    E061-03 IN predicate with list of values Yes Mandatory    E061-04 LIKE predicate Yes Mandatory    E061-05 LIKE predicate: ESCAPE clause Yes Mandatory    E061-06 NULL predicate Yes Mandatory    E061-07 Quantified comparison predicate No Mandatory    E061-08 EXISTS predicate Yes Mandatory    E061-09 Subqueries in comparison predicate No Mandatory    E061-11 Subqueries in IN predicate Yes Mandatory    E061-12 Subqueries in quantified comparison predicate No Mandatory    E061-13 Correlated subqueries Yes Mandatory    E061-14 Search condition Yes Mandatory    E071 Basic query expressions Yes Mandatory    E071-01 UNION DISTINCT table operator Yes Mandatory    E071-02 UNION ALL table operator Yes Mandatory    E071-03 EXCEPT DISTINCT table operator Yes Mandatory    E071-05 Columns combined via table operators need not have exactly the same data type. Yes Mandatory    E071-06 Table operators in subqueries Yes Mandatory    E081 Basic Privileges Yes Mandatory    E081-01 SELECT privilege at the table level Yes Mandatory    E081-02 DELETE privilege Yes Mandatory    E081-03 INSERT privilege at the table level Yes Mandatory    E081-04 UPDATE privilege at the table level Yes Mandatory    E081-05 UPDATE privilege at the column level Yes Mandatory    E081-06 REFERENCES privilege at the table level No Mandatory    E081-07 REFERENCES privilege at the column level No Mandatory    E081-08 WITH GRANT OPTION Yes Mandatory    E081-09 USAGE privilege No Mandatory    E081-10 EXECUTE privilege No Mandatory    E091 Set functions Yes Mandatory    E091-01 AVG Yes Mandatory    E091-02 COUNT Yes Mandatory    E091-03 MAX Yes Mandatory    E091-04 MIN Yes Mandatory    E091-05 SUM Yes Mandatory    E091-06 ALL quantifier Yes Mandatory    E091-07 DISTINCT quantifier Yes Mandatory    E101 Basic data manipulation Yes Mandatory    E101-01 INSERT statement Yes Mandatory    E101-03 Searched UPDATE statement Yes Mandatory    E101-04 Searched DELETE statement Yes Mandatory    E111 Single row SELECT statement No Mandatory    E121 Basic cursor support No Mandatory    E121-01 DECLARE CURSOR No Mandatory    E121-02 ORDER BY columns need not be in select list No Mandatory    E121-03 Value expressions in ORDER BY clause No Mandatory    E121-04 OPEN statement No Mandatory    E121-06 Positioned UPDATE statement No Mandatory    E121-07 Positioned DELETE statement No Mandatory    E121-08 CLOSE statement No Mandatory    E121-10 FETCH statement: implicit NEXT No Mandatory    E121-17 WITH HOLD cursors No Mandatory    E131 Null value support (nulls in lieu of values) Yes Mandatory    E141 Basic integrity constraints Partial Mandatory Don\u0026rsquo;t support UNIQUE (VALUE) constraints, don\u0026rsquo;t support UNIQUE over a list of columns. Unique constraints not enforced.Don\u0026rsquo;t support referencing periods, MATCH, or triggered actions in foreign key.Don\u0026rsquo;t support CHECK constraints.   E141-01 NOT NULL constraints Yes Mandatory    E141-02 UNIQUE constraints of NOT NULL columns Partial Mandatory UNIQUE constraints not enforced   E141-03 PRIMARY KEY constraints Partial Mandatory Primary keys not enforced   E141-04 Basic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action. Partial Mandatory Don\u0026rsquo;t support referencing periods, MATCH, or triggered actions. Foreign keys not enforced.   E141-06 CHECK constraints Yes Mandatory    E141-07 Column defaults Yes Mandatory    E141-08 NOT NULL inferred on PRIMARY KEY No Mandatory No need to declare NOT NULL with PRIMARY KEY or UNIQUE, but non-nullness not enforced.   E141-10 Names in a foreign key can be specified in any order No Mandatory    E151 Transaction support No Mandatory    E151-01 COMMIT statement No Mandatory    E151-02 ROLLBACK statement No Mandatory    E152 Basic SET TRANSACTION statement No Mandatory    E152-01 SET TRANSACTION state- ment: ISOLATION LEVEL SERIALIZABLE clause No Mandatory    E152-02 SET TRANSACTION state- ment: READ ONLY and READ WRITE clauses No Mandatory    E153 Updatable queries with subqueries No Mandatory    E161 SQL comments using leading double minus Yes Mandatory    E171 SQLSTATE support No Mandatory    F031 Basic schema manipulation Yes Mandatory    F031-01 CREATE TABLE statement to create persistent base tables Yes Mandatory    F031-02 CREATE VIEW statement Yes Mandatory    F031-03 GRANT statement Yes Mandatory    F031-04 ALTER TABLE statement: ADD COLUMN clause Yes Mandatory    F031-13 DROP TABLE statement: RESTRICT clause Yes Mandatory    F031-16 DROP VIEW statement: RESTRICT clause Yes Mandatory    F031-19 REVOKE statement: RESTRICT clause No Mandatory    F032 CASCADE drop behavior Yes Optional    F034 Extended REVOKE statement Yes Optional    F034-01 REVOKE statement performed by other than the owner of a schema object Yes Optional    F034-02 REVOKE statement: GRANT OPTION FOR clause Yes Optional    F034-03 REVOKE statement to revoke a privilege that the grantee has WITH GRANT OPTION Yes Optional    F041 Basic joined table Yes Mandatory    F041-01 Inner join (but not necessarily the INNER keyword) Yes Mandatory    F041-02 INNER keyword Yes Mandatory    F041-03 LEFT OUTER JOIN Yes Mandatory    F041-04 RIGHT OUTER JOIN Yes Mandatory    F041-05 Outer joins can be nested Yes Mandatory    F041-07 The inner table in a left or right outer join can also be used in an inner join Yes Mandatory    F041-08 All comparison operators are supported (rather than just =) Yes Mandatory    F051 Basic date and time Partial Mandatory No support for WITH/OUT TIMEZONE.No support for precision in TIMESTAMP.No support for TIME type.   F051-01 DATE data type (including support of DATE literal) Partial Mandatory Intervals don\u0026rsquo;t match spec syntax   F051-02 TIME data type (including support of TIME literal) with fractional seconds precision of at least 0. No Mandatory    F051-03 TIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6. Partial Mandatory No support for WITH/OUT TIMEZONE.No support for precision.Intervals don\u0026rsquo;t match spec syntax.   F051-04 Comparison predicate on DATE, TIME, and TIMESTAMP data types Partial Mandatory No support for TIME   F051-05 Explicit CAST between date-time types and character string types Partial Mandatory No support for TIME   F051-06 CURRENT_DATE Yes Mandatory    F051-07 LOCALTIME No Mandatory    F051-08 LOCALTIMESTAMP Partial Mandatory CURRENT_TIMESTAMP supported, doesn\u0026rsquo;t take precision argumentLOCALTIMESTAMP not supported    F052 Intervals and datetime arithmetic Partial Optional Interval not supported as column type, only as expression type in queries.Interval syntax differs from standard.   F054 TIMESTAMP in DATE type precedence list Yes Optional    F081 UNION and EXCEPT in views Yes Mandatory    F131 Grouped operations Yes Mandatory    F131-01 WHERE, GROUP BY, and HAVING clauses supported in queries with grouped views Yes Mandatory    F131-02 Multiple tables supported in queries with grouped views Yes Mandatory    F131-03 Set functions supported in queries with grouped views Yes Mandatory    F131-04 Subqueries with GROUP BY and HAVING clauses and grouped views Yes Mandatory    F131-05 Single row SELECT with GROUP BY and HAVING clauses and grouped views Yes Mandatory    F171 Multiple schemas per user Yes Optional    F200 TRUNCATE TABLE statement Yes Optional    F181 Multiple module support No Mandatory    F201 CAST function Yes Mandatory    F221 Explicit defaults Yes Mandatory    F261 CASE expression Yes Mandatory    F261-01 Simple CASE Yes Mandatory    F261-02 Searched CASE Yes Mandatory    F261-03 NULLIF Yes Mandatory    F261-04 COALESCE Yes Mandatory    F302 INTERSECT table operator Yes Optional    F302-01 INTERSECT DISTINCT table operator Yes Optional    F302-02 INTERSECT ALL table operator Yes Optional    F304 EXCEPT ALL table operator Yes Optional    F311 Schema definition statement Yes Mandatory    F311-01 CREATE SCHEMA Yes Mandatory    F311-02 CREATE TABLE for persistent base tables Partial Mandatory Does not create schema element creation as part of schema creation, must be done in separate statement   F311-03 CREATE VIEW Partial Mandatory Does not create schema element creation as part of schema creation, must be done in separate statement   F311-04 CREATE VIEW: WITH CHECK OPTION No Mandatory    F311-05 GRANT statement Partial Mandatory Does not create schema element creation as part of schema creation, must be done in separate statement   F312 MERGE statement Yes Optional    F313 Enhanced MERGE statement Yes Optional    F314 MERGE statement with DELETE branch Yes Optional    F321 User authorization Partial Optional Support for CURRENT_USER function, none of the rest   F381 Extended schema manipulation Partial Optional No support for scope.No support for ALTER routine.   F381-01 ALTER TABLE statement: ALTER COLUMN clause Partial Optional Syntax non-standard.No support for scope.No support for identities.No support for column generation.   F381-02 ALTER TABLE statement: ADD CONSTRAINT clause Partial Optional Same limitations as creating constraints above   F381-03 ALTER TABLE statement: DROP CONSTRAINT clause Partial Optional Same limitations as creating constraints above   F382 Alter column data type Partial Optional Syntax non-standard   F383 Set column not null clause Partial Optional Syntax non-standard   F391 Long identifiers Yes Optional    F401 Extended joined table Partial Optional NATURAL joins not supported   F401-02 FULL OUTER JOIN Yes Optional    F401-04 CROSS JOIN Yes Optional    F471 Scalar subquery values Yes Mandatory    F481 Expanded NULL predicate Yes Mandatory    F531 Temporary tabels Partial Optional GLOBAL/LOCAL scope not supported.DECLARE TEMPORARY TABLE not supported.   F555 Enhanced seconds precision Yes Optional    F763 CURRENT_SCHEMA Partial Optional CURRENT_DATABASE, which is equivalent   F812 Basic flagging No Mandatory    F841 LIKE_REGEX predicate Partial Optional use RLIKE instead   F847 Nonconstant regular expressions Yes Optional    F850 Top level in  Yes Optional    F851 in subqueries Yes Optional    F852 Top-level in views Yes Optional    F855 Nested in  Yes Optional    F856 Nested in  Yes Optional    F857 Top-level in  Yes Optional    F858 in subqueries Yes Optional    F859 Top-level in views Yes Optional    S011 Distinct data types No Mandatory    S091 Basic array support Partial Optional Syntax non-standard.No option to declare max cardinality.SIZE instead of CARDINALITY.   S091-01 Arrays of built-in data types Partial Optional Syntax non-standard   S091-03 Array expressions Partial Optional Support array element reference and cardinality (though syntax non-standard)No support for array concatenation, trimming, or max-cardinality   T021 BINARY and VARBINARY types Partial Optional BINARY only, though it acts like VARBINARY, no length parameter accepted.No support for overlay, trim, position, or LIKE.   T031 BOOLEAN data type Yes Optional    T041 Basic LOB data type support Partial Optional BINARY acts as BLOB (no size restrictions)STRING acts as CLOBNon-standard syntax   T041-01 BLOB data type Partial Optional BINARY acts as BLOB, non-standard syntax   T041-02 CLOB data type Partial Optional STRING acts as CLOB, non-standard syntax   T041-03 POSITION, LENGTH, LOWER, TRIM, UPPER, SUBSTRING for LOB data types Partial Optional No POSITIONLOWER, UPPER only applicable to STRING   T041-04 Concatenation of LOB types Yes Optional    T042 Extended LOB data type support Partial Optional Cast for BINARY and STRING supported.LIKE for STRING supported.All other advanced options not supported.   T051 Row types Partial Optional Called STRUCT rather than ROW   T071 BIGINT data type Yes Optional    T121 WITH (excluding RECURSIVE) in query expression Yes Optional    T321 Basic SQL-invoked routines No Mandatory    T321-01 User-defined functions with no overloading No Mandatory    T321-02 User-defined stored procedures with no overloading No Mandatory    T321-03 Function invocation No Mandatory    T321-04 CALL statement No Mandatory    T321-05 RETURN statement No Mandatory    T331 Basic roles Yes Optional    T351 Bracketed comments Yes Optional    T431 Extended grouping capabilities Yes Optional    T433 Multiargument GROUPING function Yes Optional    T441 ABS and MOD functions Yes Optional    T501 Enhanced EXISTS predicate Yes Optional    T581 Regular expression substring function Yes Optional    T591 UNIQUE constraints of possibly null columns Yes Optional    T611 Elementary OLAP operations Yes Optional    T612 Advanced OLAP operations Partial Optional    T613 Sampling Yes Optional    T614 NTILE function Yes Optional    T615 LEAD and LAG functions Yes Optional    T617 FIRST_VALUE and LAST_VALUE functions Yes Optional    T621 Enhanced numeric functions Yes Optional    T622 Trigonometric functions Partial Optional No sinh, cosh, tanh   T623 General logarithm functions Yes Optional    T624 Common logarithm functions Yes Optional    T631 IN predicate with one list element Yes Mandatory     Comments:                        Alan Gates Following features are supported in 3.1:          | E061-09 | Subqueries in comparison predicate |\n| E141-06 | CHECK constraints | No | Mandatory |\nPosted by vgarg at Nov 29, 2018 19:18 | | No need to declare NOT NULL with PRIMARY KEY or UNIQUE - I think this is not true. NOT NULL is not inferred on UNIQUE and needs to be explicitly declared.\nPosted by vgarg at Nov 29, 2018 19:20 | |\n| E121-02 | ORDER BY columns need not be in select list | No | Mandatory |\n Looks like this feature is partially supported. Hive allows this if there is not aggregate.\nPosted by vgarg at Nov 29, 2018 19:26 | | IIUC the requirement isn\u0026rsquo;t that you don\u0026rsquo;t need to declare not null and it is inferred, but rather that it can support unique/pk indices with nulls in them. Posted by alangates at Nov 29, 2018 20:57 | | Agreed, I missed this one. Feel free to edit it. I\u0026rsquo;ll be circling back on this and a few others shortly to fix it.\nPosted by alangates at Nov 29, 2018 20:57 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/97551656/","tags":null,"title":"Apache Hive : Supported Features:  Apache Hive 3.1"},{"categories":null,"contents":"Apache Hive : Supported Features: Apache Hive 2.1    Identifier Description Hive 2.1 Comment     E011 Numeric data types Yes    E011-01 INTEGER and SMALLINT data types (including all spellings) Yes Int instead of Integer   E011-02 REAL, DOUBLE PRECISON,and FLOAT data types Yes Double instead of Double Precision   E011-03 DECIMAL and NUMERIC data types Yes    E011-04 Arithmetic operators Yes    E011-05 Numeric comparison Yes    E011-06 Implicit casting among the numeric data types Yes    E021 Character data types Yes    E021-01 CHARACTER data type Yes Char instead of Character   E021-02 CHARACTER VARYING data type Yes Varchar instead of Character Varying   E021-03 Character literals Yes    E021-04 CHARACTER_LENGTH function Partial length UDF provided   E021-06 SUBSTRING function Yes    E021-07 Character concatenation Yes concat UDF instead of standard   E021-08 UPPER and LOWER functions Yes    E021-09 TRIM function Partial leading / trailing / both from not supported   E021-10 Implicit casting among the fixed-length and variablelength character string types Yes    E021-12 Character comparison Yes    E031 Identifiers Yes    E031-01 Delimited identifiers Partial Backtick (`) used instead of (\u0026quot;). Semicolon character (;) cannot be used in an identifier. Table and column names have additional restrictions   E031-03 Trailing underscore Yes    E051 Basic query specification Yes    E051-01 SELECT DISTINCT Yes    E051-02 GROUP BY clause Partial Empty grouping sets not supported   E051-04 GROUP BY can contain columns not in  Yes    E051-05 Select list items can be renamed Yes    E051-06 HAVING clause Yes    E051-07 Qualified * in select list Yes    E051-08 Correlation names in the FROM clause Yes    E061 Basic predicates and search conditions Yes    E061-01 Comparison predicate Yes    E061-02 BETWEEN predicate Yes    E061-03 IN predicate with list of values Yes    E061-04 LIKE predicate Yes    E061-06 NULL predicate Yes    E061-08 EXISTS predicate Yes    E061-11 Subqueries in IN predicate Yes    E061-13 Correlated subqueries Partial Only correlated subqueries that can be decorrelated with rewrite rules supported   E071 Basic query expressions Yes    E071-01 UNION DISTINCT table operator Partial Corresponding By syntax not supported   E071-02 UNION ALL table operator Partial Corresponding By syntax not supported   E071-05 Columns combined via table operators need not have exactly the same data type. Yes    E071-06 Table operators in subqueries Yes    E081 Basic Privileges Yes    E081-01 SELECT privilege Yes    E081-03 INSERT privilege at the table level Yes    E081-04 UPDATE privilege at the table level Yes    E081-08 WITH GRANT OPTION Yes    E091 Set Functions Yes    E091-01 AVG Yes    E091-02 COUNT Yes    E091-03 MAX Yes    E091-04 MIN Yes    E091-05 SUM Yes    E091-07 DISTINCT quantifier Yes    E101 Basic data manipulation Yes    E101-01 INSERT statement Yes    E101-03 Searched UPDATE statement Yes    E101-04 Searched DELETE statement Yes    E131 Null value support (nulls in lieu of values) Partial Null specification is supported   E141 Basic integrity constraints Yes    E141-03 PRIMARY KEY constraints Partial Non-validated   E141-04 Basic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action Partial Non-validated   E141-10 Names in a foreign key can be specified in any order Yes    E151 Transaction support Partial Autocommit transaction for INSERT/UPDATE/DELETE   E161 SQL comments using leading double minus Yes    F031 Basic schema manipulation Yes    F031-01 CREATE TABLE statement to create persistent base tables Yes    F031-02 CREATE VIEW statement Yes    F031-03 GRANT statement Yes    F031-04 ALTER TABLE statement: ADD COLUMN clause Yes    F031-13 DROP TABLE statement: RESTRICT clause Yes    F031-16 DROP VIEW statement: RESTRICT clause Yes    F041 Basic joined table Yes    F041-01 Inner join (but not necessarily the INNER keyword) Yes Named columns join not supported   F041-02 INNER keyword Yes    F041-03 LEFT OUTER JOIN Yes    F041-04 RIGHT OUTER JOIN Yes    F041-05 Outer joins can be nested Yes    F041-07 The inner table in a left or right outer join can also be used in an inner join Yes    F051 Basic date and time Yes    F051-01 DATE data type (including support of DATE literal) Yes    F051-03 TIMESTAMP data type (including support of TIMES- TAMP literal) with fractional seconds precision of at least 0 and 6. Yes    F051-04 Comparison predicate on DATE, TIME, and TIMES- TAMP data types Yes    F051-05 Explicit CAST between date- time types and character string types Yes    F051-06 CURRENT_DATE Yes    F052 Intervals and datetime arithmetic Yes    F081 UNION and EXCEPT in views Partial UNION only   F131 Grouped operations Yes    F131-01 WHERE, GROUP BY, and HAVING clauses supported in queries with grouped views Yes    F131-02 Multiple tables supported in queries with grouped views Yes    F131-03 Set functions supported in queries with grouped views Yes    F131-04 Subqueries with GROUP BY and HAVING clauses and grouped views Yes    F171 Multiple schemas per user Yes    F200 TRUNCATE TABLE statement Yes    F201 CAST function Yes    F261 CASE expression Yes    F261-01 Simple CASE Yes    F261-02 Searched CASE Yes    F261-04 COALESCE Yes    F311-01 CREATE SCHEMA Yes    F311-02 CREATE TABLE for persistent base tables Yes    F311-03 CREATE VIEW Yes    F311-05 GRANT statement Yes    F382 Alter column data type Yes Uses nonstandard syntax   F391 Long identifiers Yes    F401 Extended joined table Yes    F401-01 NATURAL JOIN Yes    F401-02 FULL OUTER JOIN Yes    F401-04 CROSS JOIN Yes    F403 Partitioned join tables Yes    F531 Temporary tables Yes    F555 Enhanced seconds precision Yes    F561 Full value expressions Yes    F591 Derived tables Yes    F641 Row and table constructors Yes    F651 Catalog name qualifiers Yes    F846 Octet support in regular expression operators Yes    F847 Nonconstant regular expressions Yes    F850 Top-level in  Yes    F851 in subqueries Yes    F852 Top-level in views Yes    F855 Nested in  Yes    S023 Basic structured types Yes    S091 Basic array support Yes    S091-01 Arrays of built-in data types Yes    S091-02 Arrays of distinct types Yes    S098 ARRAY_AGG Partial collect_list does the same   S201-01 Array parameters Yes    S281 Nested collection types Yes    T021 BINARY and VARBINARY data types Partial BINARY only   T031 BOOLEAN data type Yes    T051 Row types Yes    T071 BIGINT data type Yes    T121 WITH (excluding RECURSIVE) in query expression Yes    T122 WITH (excluding RECURSIVE) in subquery Yes    T172 AS subquery clause in table definition Yes    T326 Table functions Yes    T331 Basic roles Yes    T431 Extended grouping capabilities Partial Grouping sets need to be extracted manually from a bitmask   T433 Multiargument GROUPING function Yes    T441 ABS and MOD functions Partial ABS provided, MOD provided via % operator   T501 Enhanced EXISTS predicate Yes    T581 Regular expression substring function Yes    T611 Elementary OLAP operations Yes    T612 Advanced OLAP operations Partial PERCENT_RANK, CUME_DIST and ROW_NUMBER supported   T613 Sampling Yes Nonstandard syntax via TABLESAMPLE   T614 NTILE function Yes    T615 LEAD and LAG functions Yes    T616 Null treatment option for LEAD and LAG functions Yes    T617 FIRST_VALUE and LAST_VALUE functions Yes    T621 Enhanced numeric functions Yes    T631 IN predicate with one list element Yes     ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/67641451/","tags":null,"title":"Apache Hive : Supported Features: Apache Hive 2.1"},{"categories":null,"contents":"Apache Hive : Supported Features: Apache Hive 2.3    Identifier Description Hive 2.3 Comment     E011 Numeric data types Yes    E011-01 INTEGER and SMALLINT data types (including all spellings) Yes    E011-02 REAL, DOUBLE PRECISON,and FLOAT data types Yes    E011-03 DECIMAL and NUMERIC data types Yes    E011-04 Arithmetic operators Yes    E011-05 Numeric comparison Yes    E011-06 Implicit casting among the numeric data types Yes    E021 Character data types Yes    E021-01 CHARACTER data type Yes Char instead of Character   E021-02 CHARACTER VARYING data type Yes Varchar instead of Character Varying   E021-03 Character literals Yes    E021-04 CHARACTER_LENGTH function Yes    E021-05 OCTET_LENGTH function Yes    E021-06 SUBSTRING function Yes    E021-07 Character concatenation Yes    E021-08 UPPER and LOWER functions Yes    E021-09 TRIM function Partial leading / trailing / both from not supported   E021-10 Implicit casting among the fixed-length and variablelength character string types Yes    E021-12 Character comparison Yes    E031 Identifiers Yes    E031-01 Delimited identifiers Yes    E031-03 Trailing underscore Yes    E051 Basic query specification Yes    E051-01 SELECT DISTINCT Yes    E051-02 GROUP BY clause Partial Empty grouping sets not supported   E051-04 GROUP BY can contain columns not in  Yes    E051-05 Select list items can be renamed Yes    E051-06 HAVING clause Yes    E051-07 Qualified * in select list Yes    E051-08 Correlation names in the FROM clause Yes    E061 Basic predicates and search conditions Yes    E061-01 Comparison predicate Yes    E061-02 BETWEEN predicate Yes    E061-03 IN predicate with list of values Yes    E061-04 LIKE predicate Yes    E061-06 NULL predicate Yes    E061-08 EXISTS predicate Yes    E061-09 Subqueries in comparison predicate Yes    E061-11 Subqueries in IN predicate Yes    E061-13 Correlated subqueries Yes    E071 Basic query expressions Yes    E071-01 UNION DISTINCT table operator Yes    E071-02 UNION ALL table operator Yes    E071-03 EXCEPT DISTINCT table operator Yes    E071-05 Columns combined via table operators need not have exactly the same data type. Yes    E071-06 Table operators in subqueries Yes    E081 Basic Privileges Yes    E081-01 SELECT privilege Yes    E081-03 INSERT privilege at the table level Yes    E081-04 UPDATE privilege at the table level Yes    E081-08 WITH GRANT OPTION Yes    E091 Set Functions Yes    E091-01 AVG Yes    E091-02 COUNT Yes    E091-03 MAX Yes    E091-04 MIN Yes    E091-05 SUM Yes    E091-06 ALL quantifier Yes    E091-07 DISTINCT quantifier Yes    E101 Basic data manipulation Yes    E101-01 INSERT statement Yes    E101-03 Searched UPDATE statement Yes    E101-04 Searched DELETE statement Yes    E131 Null value support (nulls in lieu of values) Yes    E141 Basic integrity constraints Yes    E141-03 PRIMARY KEY constraints Partial Non-enforced   E141-04 Basic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action Partial Non-enforced   E141-08 NOT NULL inferred on PRIMARY KEY Partial Inferred on read   E141-10 Names in a foreign key can be specified in any order Yes    E151 Transaction support Partial Autocommit transaction for INSERT/UPDATE/DELETE/MERGE   E161 SQL comments using leading double minus Yes    F031 Basic schema manipulation Yes    F031-01 CREATE TABLE statement to create persistent base tables Yes    F031-02 CREATE VIEW statement Yes    F031-03 GRANT statement Yes    F031-04 ALTER TABLE statement: ADD COLUMN clause Yes    F031-13 DROP TABLE statement: RESTRICT clause Yes    F031-16 DROP VIEW statement: RESTRICT clause Yes    F041 Basic joined table Yes    F041-01 Inner join (but not necessarily the INNER keyword) Yes    F041-02 INNER keyword Yes    F041-03 LEFT OUTER JOIN Yes    F041-04 RIGHT OUTER JOIN Yes    F041-05 Outer joins can be nested Yes    F041-07 The inner table in a left or right outer join can also be used in an inner join Yes    F041-08 All comparison operators are supported (rather than just =) Yes    F051 Basic date and time Yes    F051-01 DATE data type (including support of DATE literal) Yes    F051-03 TIMESTAMP data type (including support of TIMES- TAMP literal) with fractional seconds precision of at least 0 and 6. Yes    F051-04 Comparison predicate on DATE, TIME, and TIMES- TAMP data types Yes    F051-05 Explicit CAST between date- time types and character string types Yes    F051-06 CURRENT_DATE Yes    F052 Intervals and datetime arithmetic Yes    F054 TIMESTAMP in DATE type precedence list Yes    F081 UNION and EXCEPT in views Yes    F131 Grouped operations Yes    F131-01 WHERE, GROUP BY, and HAVING clauses supported in queries with grouped views Yes    F131-02 Multiple tables supported in queries with grouped views Yes    F131-03 Set functions supported in queries with grouped views Yes    F131-04 Subqueries with GROUP BY and HAVING clauses and grouped views Yes    F171 Multiple schemas per user Yes    F200 TRUNCATE TABLE statement Yes    F201 CAST function Yes    F261 CASE expression Yes    F261-01 Simple CASE Yes    F261-02 Searched CASE Yes    F261-03 NULLIF Yes    F261-04 COALESCE Yes    F271 Compound character literals Yes    F281 LIKE enhancements Partial Escape characters not supported   F302 INTERSECT table operator Yes    F302-01 INTERSECT DISTINCT table operator Yes    F302-02 INTERSECT ALL table operator Yes    F304 EXCEPT ALL table operator Yes    F311-01 CREATE SCHEMA Partial Database is used as the equivalent   F311-02 CREATE TABLE for persistent base tables Yes    F311-03 CREATE VIEW Yes    F311-05 GRANT statement Yes    F312 MERGE statement Yes    F314 MERGE statement with DELETE branch Yes    F382 Alter column data type Yes Uses nonstandard syntax   F391 Long identifiers Yes    F401 Extended joined table Yes    F401-01 NATURAL JOIN Yes    F401-02 FULL OUTER JOIN Yes    F401-04 CROSS JOIN Yes    F403 Partitioned join tables Yes    F531 Temporary tables Yes    F555 Enhanced seconds precision Yes    F561 Full value expressions Yes    F591 Derived tables Yes    F641 Row and table constructors Yes    F651 Catalog name qualifiers Yes    F846 Octet support in regular expression operators Yes    F847 Nonconstant regular expressions Yes    F850 Top-level in  Yes    F851 in subqueries Yes    F852 Top-level in views Yes    F855 Nested in  Yes    S023 Basic structured types Yes    S091 Basic array support Yes    S091-01 Arrays of built-in data types Yes    S091-02 Arrays of distinct types Yes    S098 ARRAY_AGG Partial collect_list provides similar functionality   S201-01 Array parameters Yes    S281 Nested collection types Yes    S301 Enhanced UNNEST Partial LATERAL JOIN provides similar functionality   T021 BINARY and VARBINARY data types Partial BINARY only   T031 BOOLEAN data type Yes    T051 Row types Yes    T071 BIGINT data type Yes    T121 WITH (excluding RECURSIVE) in query expression Yes    T122 WITH (excluding RECURSIVE) in subquery Yes    T171 LIKE clause in table definition Yes    T172 AS subquery clause in table definition Yes    T281 SELECT privilege with column granularity Partial Provided by ecosystem projects like Apache Ranger and Apache Sentry   T326 Table functions Yes    T331 Basic roles Yes    T351 Bracketed comments Yes    T431 Extended grouping capabilities Partial Concatenated grouping sets unsupported   T433 Multiargument GROUPING function Yes    T441 ABS and MOD functions Yes    T501 Enhanced EXISTS predicate Yes    T551 Optional key words for default syntax Yes    T581 Regular expression substring function Yes    T611 Elementary OLAP operations Yes    T612 Advanced OLAP operations Partial PERCENT_RANK, CUME_DIST and ROW_NUMBER supported   T613 Sampling Yes Nonstandard syntax via TABLESAMPLE   T614 NTILE function Yes    T615 LEAD and LAG functions Yes    T616 Null treatment option for LEAD and LAG functions Yes    T617 FIRST_VALUE and LAST_VALUE functions Yes    T621 Enhanced numeric functions Yes    T631 IN predicate with one list element Yes     ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/73632935/","tags":null,"title":"Apache Hive : Supported Features: Apache Hive 2.3"},{"categories":null,"contents":"Apache Hive : Synchronized Metastore Cache Overview This work is to solve the consistency problem if we use HMS HA with metadata cache. Note it does not aim to address any existing consistency issues already exist in non-cached HMS. For example, it won’t fix the transaction semantic between metadata and data. If the problem exists today in non-cached HMS, it stays a problem after this work.\nThe problem we try to solve here is the cache consistency issue. We already build a HMS cache to cache the metadata. If we have multiple HMS in the cluster, the cache is not synchronized. That is, if metastore 1 changed a table/partition, metastore 2 won’t see the change immediately. There’s a background thread keep polling from the notification log and update changed entries, so the cache is eventually consistent. In this work, we want to make the cache full consistent. The idea is at read time, we will check if the cached entry is obsolete or not. However, we don’t want to penalize the read performance. We don’t want to introduce additional db call to compare the db version and cached version in order to do the check. The solution is we will use the transaction state of a query for the version check. A query will pull the transaction state of involved tables (ValidWriteIdList) from db (non-cached) anyway. So we don’t need additional db call to check the staleness of the cache.\n.\nData structure change The only data structure change is adding ValidWriteIdList into SharedCache.TableWrapper, which represents the transaction state of the cached table.\nNote there is no db table structure change, and we don’t store extra information in db. We don’t update TBLS.WRITE_ID field as we will use db as the fact of truth. We assume db always carry the latest copy and every time we fetch from db, we will tag it with the transaction state of the query.\nHow It Works Read Metastore read request will compare ValidWriteIdList parameter with the cached one. If the cached version is fresh or newer (there’s no transaction committed after the entry is cached) , Metastore will use cached copy. Otherwise, Metastore will retrieve the entry from ObjectStore.\nHere is the example for a get_table request:\n At the beginning of the query, Hive will retrieve the global transaction state and store in config (ValidTxnList.VALID_TXNS_KEY) Hive translate ValidTxnList to ValidWriteIdList of the table [12:7,8,12] (The format for writeid is [hwm:exceptions], all writeids from 1 to hwm minus exceptions, are committed. In this example, writeid 1..6,9,10,11 are committed) Hive pass the ValidWriteIdList to HMS HMS compare ValidWriteIdList [12:7,8,12] with the cached one [11:7,8] using TxnIdUtils.compare, if it is fresh or newer (Fresh or newer means no transaction committed between two states. In this example, [11:7,8] means writeid 1..6,9,10,11 are committed, the same as the requested writeid [12:7,8,12]), HMS return cached table entry If the cached ValidWriteIdList is [12:7,12], the comparison fails because writeid 8 is committed since then. HMS will fetch the table from ObjectStore HMS will eventually catch up with the newer version from notification log. HMS will serve the request from cache since then  Here is another example of get_partitions_by_expr. The API is a list query not a point lookup. There are a couple of similarities and differences to point out:\n HMS will still compare requested writeid and cached table writeid to decide if the request can serve from cache Every add/remove/alter/rename partition request will increment the table writeid. HMS will mark cached table entry invalid upon processing the first write message from notification log, and mark it valid and tag with the right writeid upon processing the commit message from notification log  Write\nEvery write request will advance the write id for the table for both DML/DDL. The writeid will be marked committed locally in HMS client. The next read request is guaranteed to read from db until the notification log catch up to the commit message of the transaction commit, since the writeid is newer than the cache (the writeid for the transaction is committed locally, but is not committed on HMS until notification log catch up).\nCache update In the previous discussion, we know if the cache is stale, HMS will serve the request from ObjectStore. We need to catch up the cache with the latest change. This can be done by the existing notification log based cache update mechanism. A thread in HMS constantly poll from notification log, update the cache with the entries from notification log. The interesting entries in notification log are table/partition writes, and corresponding commit transaction message. When processing table/partition writes, HMS will put the table/partition entry in cache. However, the entry is not immediately usable until the commit message of the corresponding writes is processed, and mark writeid of corresponding table entry committed.\nHere is a complete flow for a cache update when write happen (and illustrated in the diagram):\n The ValidWriteIdList of cached table is initially [11:7,8] HMS 1 get a alter_table request. HMS 1 puts alter_table message to notification log The transaction in HMS 1 get committed. HMS 1 puts commit message to notification log The cache update thread in HMS 2 will read the alter_table event from notification log, update the cache with the new version from notification log. However, the entry is not available for read as writeid associate with it is not updated yet A read for the entry on HMS 2 will fetch from db since the entry is not available for read The cache update thread will further read commit event from notification log, mark writeid 12 as committed, the tag of cached table entry changed to [12:7,8] The next read from HMS 2 will serve from cache  Bootstrap The use cases discussed so far are driven by a query. However, during the HMS startup, there’s a cache prewarm. HMS will fetch everything from db to cache. There is no particular query drives the process, that means we don’t have ValidWriteIdList of the query. Prewarm needs to generate ValidWriteIdList by itself. To do that, for every table, HMS will query the current global transaction state ValidTxnList (HiveTxnManager.getValidTxns), and then convert it to table specific ValidWriteIdList (HiveTxnManager.getValidWriteIds). As an optimization, we don’t have to invoke HiveTxnManager.getValidTxns per table. We can invoke it every couple of minutes. If ValidTxnList is outdated, we will get an outdated ValidWriteIdList. Next time when Hive read this entry, Metastore will fetch from the db even though it is in fact fresh. There’s no correctness issue, only impact performance in some cases. The other possibility is the entry changes after we fetches ValidWriteIdList. This is not unlikely as fetching all partitions of the table may take some time. If that happens, the cached entry is actually newer than the ValidWriteIdList. The next time Hive reads it will trigger a db read though it is not necessary. Again, there’s no correctness issue, only impact performance in some cases.\nMaintain WriteId HMS will maintain ValidWriteIdList for every cache entry when transactions are committed. The initial ValidWriteIdList is brought in by bootstrap. After that, for every commit message, HMS needs to:\n Find the table writeids associated with the txnid, this can be done by a db lookup on TXN_TO_WRITE_ID table, or by processing ALLOC_WRITE_ID_EVENT message in the notification log. I will explain later in detail Mark the writeid as committed in the ValidWriteIdList associated with the cached tables  As an optimization, we can save a db lookup in #1 by cache the writeid of modified tables of the transaction. Every modified table will generate a corresponding ALLOC_WRITE_ID_EVENT associate txnid with table writeid generated. Upon we receive commit message of the transaction, we can get the table writeids for the transaction. Thus we don’t need to do a db lookup to find the same information. However, in the initial commit message after bootstrap, we might miss some ALLOC_WRITE_ID_EVENT for the transaction. To address this issue, we will use this optimization unless we saw the open transaction event as well. Otherwise, HMS will still go to the db to fetch the writeids.\nDeal with Rename When we rename a table, writeids are renamed immediately on HMS (TxnHandler.onRename). However, cache won’t update immediately until it catches up the notification log. It is possible the cached table with the same table name is actually another table which is already dropped. To solve the issue, Hive session will fetch tableid of involved tables from db (must bypass cache) at the beginning of the transaction. It can be combined with HMS request for writeid. In every read request, HMS client need to pass tableid as well. HMS will compare the tableid with cached table. If it does not match, HMS will fetch the table from db instead.\nExternal Table Write id is not valid for external tables. And Hive won’t fetch ValidWriteIdList if the query source is external tables. Without ValidWriteIdList, HMS won’t able to check the staleness of the cache. To solve this problem, we will use the original eventually consistent model for external tables. That is, if the table is external table, Hive will pass null ValidWriteIdList to metastore API/CachedStore. Metastore cache won’t store ValidWriteIdList alongside the entry. When reading, CachedStore always retrieve the current copy. The original notification update will update the metadata of external tables, so we can eventually get updates from external changes.\nConsistency Guarantee Since the source of truth for cache is notification log, and the notification log is total ordered, the cache provides monotonic reads. The cost of that is we delay the update of cache until the notification log is caught up by the background thread. This guarantee the cache always move forward not backward. During the interim before HMS catch up the notification log, read will be served from db. The performance will suffer during this short period of time, but consider write operation is less often, the cost is minor.\nThe other benefit of this approach is it provide right semantic even if a transaction consists of more than one HMS write request. In that case, there are multiple copies of the cache entry could associate with a single writeid. For example, if there are two alter table request in the transaction, one request route to HMS 1 and the other route to HMS 2. We might not be able to tell which copy is applicable for this writeid if not carefully designed. By using notification log and only make cache available upon commit message, we can make sure we apply all request of the transaction. Once the commit message is processed, we can make sure the copy we have in cache is correct after this transaction.\nThis approach also provide read your own writes guarantee. Reads within the transaction will use a writeid list which marks the current transaction committed, that’s guaranteed to be newer than the cached entry, and thus HMS will go to db to fetch the fresh copy.\nLimitation The check based on ValidWriteIdList is limited to table/partition. We cannot use the same mechanism for other entities such as databases, functions, privileges, as Hive only generate writeid on table (partition belong to table, so we use table writeid to track partition changes). Currently we don’t cache other entities, HMW will fetch those directly from db. However, many of those entities need to be in cache. For example, get_database is invoked multiple times by every query. So we need to address this issue in the future.\nAPI changes\nWhen reading, we need to pass a writeid list so HMS can compare it with the cached version. We need to add ValidWriteIdList into metastore thrift API and RawStore interfaces for all table/partition related read calls.\nHive_metastore.thrift and HiveMetaStoreClient.java\nAdding a serialized version of ValidWriteIdList to every read HMS API.\n| hive_metastore.thriftOld API | New API | | get_table(string dbname,string tbl_name) | get_table(string dbname,string tbl_name,int tableid,string validWriteIdList) |\nActually we don’t need to add the new field into every read request because:\n Many APIs are using a request structure rather than taking individual parameters. So need to add ValidWriteIdList to the request structure instead Some APIs already take ValidWriteIdList to invalidate outdated transactional statistics. We don’t need to change the API signature, but will reuse the ValidWriteIdList to validate cached entries in CachedStore  HMS read API will remain backward compatible for external table. That is, new server can deal with old client. If the old client issue a create_table call, server side will receive the request of create_table with tableid=0 and validWriteIdList=null, and will cache or retrieve the entry regardless(with eventual consistency model). For managed table, tableid and validWriteIdList are required and HMS server will throw an exception if validWriteIdList=null.\nRawStore\nObjectStore will use the additional validWriteIdList field for all read methods to compare with cached ValidWriteIdList\n| Old API | New API | | getTable(String catName,String dbName,String tableName) | getTable(String catName,String dbName,String tableName,int tableid,String validWriteIdList) |\nHive.java The implementation details will be encapsulated in Hive.java. Which include:\n Generate new write id for every write operation involving managed tables. Since DbTxnManager cache write id for every transaction, so every query will generate at most one new write id for a single table, even if it consists of multiple Hive.java write API calls Pass the tableid and writeid to the read request of HMS client. It can be retrieved from config (for managed table, it guarantees to be there in config)  Changes in Other Components All other components invoking HMS API directly (bypass Hive.java) will be changed to invoke the newer HMS API. This includes HCatalog, Hive streaming, etc, and other projects using HMS client such as Impala.\nFor every read request involving table/partitions, HMS client (HiveMetaStoreClient) need to pass tableid and validWriteIdList string in addition to the existing arguments. tableid and validWriteIdList can be retrieved with txnMgr.getValidWriteIdsAndTblIds(). validWriteIdList can be null if it is external table, as HMS will return whatever in the cache for external table using eventual consistency. But if validWriteIdList=null for managed table, HMS will throw exception. validWriteIdList is a serialized form of ValidReaderWriteIdList. Usually ValidTxnWriteIdList can be obtained from HiveTxnManager using the following code snippet:\nHiveTxnManager txnMgr = TxnManagerFactory.getTxnManagerFactory().getTxnManager(conf); ValidTxnList txnIds = txnMgr.getValidTxns(); // get global transaction state ValidTxnWriteIdList txnWriteIdsTblIds = txnMgr.getValidWriteIdsTableIds(txnTables, txnString); // map global transaction state to table specific write id int tblId = txnWriteIdsTblIds.getTableId(fullTableName); ValidWriteIdList writeids = txnWriteIds.getTableValidWriteIdList(fullTableName); // get table specific writeid For every managed table write, advance the writeid for the table:\nAcidUtils.advanceWriteId(conf, tbl); Attachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/synchronized-metastore-cache_110692851/","tags":null,"title":"Apache Hive : Synchronized Metastore Cache"},{"categories":null,"contents":"Apache Hive : TeradataBinarySerde  Availability Overview How to export  Using TPT FastExport Using BTEQ   How to import  Using BTEQ Using TPT FastLoad   Usage  Table Creating Table Properties Teradata to Hive Type Conversion Serde Restriction    Availability Earliest version CSVSerde is available\nThe TeradataBinarySerDe is available in Hive 2.4 or greater.\nOverview Teradata can use TPT(Teradata Parallel Transporter) or BTEQ(Basic Teradata Query) to export and import data files compressed by gzip in very high speed. However such binary files are encoded in Teradata’s proprietary format and can’t be directly consumed by Hive without a customized SerDe.\nThe TeradataBinarySerde enables users to read or write Teradata binary data as Hive Tables:\n Directly consume the Teradata Binary data file which is exported by TPT/BTEQ and then registered in Hive Generate the Teradata Binary data file in Hive and directly load it via TPT/BTEQ into Teradata  How to export The TeradataBinarySerde supports data files in \u0026lsquo;Formatted\u0026rsquo; or \u0026lsquo;Formatted4\u0026rsquo; mode with the restrictions  INDICDATA format is used (please don\u0026rsquo;t use DATA) Maximum decimal digits = 38 (please don\u0026rsquo;t use 18) Date format = integer (please don\u0026rsquo;t use ANSI)  Using TPT FastExport Here is a bash script example for how to call TPT FastExport:\nTPT FastExport script example\nquery_table=foo.teradata_binary_table data_date=20180108 select_statement=\u0026quot;SELECT * FROM $query_table WHERE transaction_date BETWEEN DATE '2018-01-01' AND DATE '2018-01-08' AND is_deleted=0\u0026quot; select_statement=${select_statement//\\'/\\'\\'} # Do not put double quote here output_path=/data/foo/bar/${data_date} num_chunks=4 tbuild -C -f $td_export_template_file -v ${tpt_job_var_file} \\ -u \u0026quot;ExportSelectStmt='${select_statement}',FormatType=Formatted,DataFileCount=${num_chunks}, FileWriterDirectoryPath='${output_path},FileWriterFileName='${query_table}.${data_date}.teradata', SourceTableName='${query_table}'\u0026quot; The td_export_template_file looks like below with proper Format, MaxDecimalDigits, andDateForm in place:\ntd_export_template_file\nUSING CHARACTER SET UTF8 DEFINE JOB EXPORT_TO_BINARY_FORMAT DESCRIPTION 'Export to the INDICDATA file' ( /* https://www.info.teradata.com/HTMLPubs/DB_TTU_16_00/Load_and_Unload_Utilities/B035-2436%E2%80%90086K/2436ch03.05.3.html */ APPLY TO OPERATOR ($FILE_WRITER()[@DataFileCount] ATTR ( IndicatorMode = 'Y', OpenMode = 'Write', Format = @FormatType, DirectoryPath = @FileWriterDirectoryPath, FileName = @FileWriterFileName, IOBufferSize = 2048000 ) ) SELECT * FROM OPERATOR ($EXPORT()[1] ATTR ( SelectStmt = @ExportSelectStmt, MaxDecimalDigits = 38, DateForm = 'INTEGERDATE', /* ANSIDATE is hard to load in BTEQ */ SpoolMode = 'NoSpool', TdpId = @SourceTdpId, UserName = @SourceUserName, UserPassword = @SourceUserPassword, QueryBandSessInfo = 'Action=TPT_EXPORT;SourceTable=@SourceTableName;Format=@FormatType;' ) ); ); The login credential is supplied in tpt_job_var_file instead of via command line:\ntpt_job_var_file\n SourceUserName=\u0026lt;td_use\u0026gt; ,SourceUserPassword=\u0026lt;td_pass\u0026gt; ,SourceTdpId=\u0026lt;td_pid\u0026gt; Using BTEQ TheBTEQ script looks like below with proper INDICDATA, Format, MaxDecimalDigits, and DateForm in place and by default, recordlength=max64 (Formatted) is applied, so MAX1MB must be explicitly specified when \u0026lsquo;Formatted4\u0026rsquo; mode is required.\nBTEQ script example\nSET SESSION DATEFORM=INTEGERDATE; .SET SESSION CHARSET UTF8 .decimaldigits 38 .export indicdata recordlength=max1mb file=td_data_with_1mb_rowsize.dat select * from foo.teradata_binary_table order by test_int; .export reset How to import Using BTEQ When unicode is used, the CHAR(n) column must be specified as CHAR(n x 3) in the USING() section. For example, test_char is defined as CHAR(1) CHARACTER SET UNICODE in DDL, when loading via BTEQ, it needs to occupy up to 3 bytes, therefore it appears as CHAR(3) in the USING().\nIf n x 3 rule is not applied, BTEQ can encounter the error like \u0026ldquo;Failure 2673 The source parcel length does not match data that was defined.\u0026rdquo;\nHere is the sample BTEQ script with proper INDICDATA, Format, MaxDecimalDigits, and DateFormto load a \u0026lsquo;RECORDLENGTH=MAX1MB\u0026rsquo; data file:\nBTEQ script example\nSET SESSION DATEFORM=INTEGERDATE; .SET SESSION CHARSET UTF8 .decimaldigits 38 .IMPORT INDICDATA RECORDLENGTH=MAX1MB FILE=td_data_with_1mb_rowsize.teradata .REPEAT * USING( test_tinyint BYTEINT, test_smallint SMALLINT, test_int INTEGER, test_bigint BIGINT, test_double FLOAT, test_decimal DECIMAL(15,2), test_date DATE, test_timestamp TIMESTAMP(6), test_char CHAR(3), -- CHAR(1) will occupy 3 bytes test_varchar VARCHAR(40), test_binary VARBYTE(500) ) INSERT INTO foo.stg_teradata_binary_table ( test_tinyint, test_smallint, test_int, test_bigint, test_double, test_decimal, test_date, test_timestamp, test_char, test_varchar, test_binary ) values ( :test_tinyint, :test_smallint, :test_int, :test_bigint, :test_double, :test_decimal, :test_date, :test_timestamp, :test_char, :test_varchar, :test_binary ); .IMPORT RESET Using TPT FastLoad tbuild can load multiple gzip files in parallel, this makes TPT the best choice to bulk load big data files. Here is a bash script example for how to call TPT FastLoad.\nTPT FastLoad script example\nstaging_database=foo staging_table=stg_table_name_up_to_30_chars table_name_less_than_26chars=stg_table_name_up_to_30_c file_dir=/data/foo/bar job_id=\u0026lt;my_job_execution_id\u0026gt; tbuild -C -f $td_import_template_file -v ${tpt_job_var_file} \\ -u \u0026quot;TargetWorkingDatabase='${staging_database}',TargetTable='${staging_table}', SourceDirectoryPath='${file_dir}',SourceFileName='*.teradata.gz', FileInstances=8,LoadInstances=1, Substr26TargetTable='${table_name_less_than_26chars}', TargetQueryBandSessInfo='TptLoad=${staging_table};JobId=${job_id};'\u0026quot; The td_import_template_file looks like:\ntd_import_template_file\nUSING CHARACTER SET @Characterset DEFINE JOB LOAD_JOB DESCRIPTION 'Loading Data From File To Teradata Table' ( set LogTable=@TargetWorkingDatabase||'.'||@Substr26TargetTable||'_LT'; set ErrorTable1=@TargetWorkingDatabase||'.'||@Substr26TargetTable||'_ET'; set ErrorTable2=@TargetWorkingDatabase||'.'||@Substr26TargetTable||'_UT'; set WorkTable=@TargetWorkingDatabase||'.'||@Substr26TargetTable||'_WT'; set ErrorTable=@TargetWorkingDatabase||'.'||@Substr26TargetTable||'_ET'; set LoadPrivateLogName=@TargetTable||'_load.log' set UpdatePrivateLogName=@TargetTable||'_update.log' set StreamPrivateLogName=@TargetTable||'_stream.log' set InserterPrivateLogName=@TargetTable||'_inserter.log' set FileReaderPrivateLogName=@TargetTable||'_filereader.log' STEP PRE_PROCESSING_DROP_ERROR_TABLES ( APPLY ('release mload '||@TargetTable||';'), ('drop table '||@LogTable||';'), ('drop table '||@ErrorTable||';'), ('drop table '||@ErrorTable1||';'), ('drop table '||@ErrorTable2||';'), ('drop table '||@WorkTable||';') TO OPERATOR ($DDL); ); STEP LOADING ( APPLY $INSERT TO OPERATOR ($LOAD() [@LoadInstances]) SELECT * FROM OPERATOR ($FILE_READER() [@FileInstances]); ); ); Please set the correct values in tpt_job_var_file, such as SourceFormat, DateForm, MaxDecimalDigits. Here is an example:\ntpt_job_var_file\n Characterset='UTF8' ,DateForm='integerDate' ,MaxDecimalDigits=38 ,TargetErrorLimit=100 ,TargetErrorList=['3807','2580', '3706'] ,TargetBufferSize=1024 ,TargetDataEncryption='off' ,SourceOpenMode='Read' ,SourceFormat='Formatted' ,SourceIndicatorMode='Y' ,SourceMultipleReaders='Y' ,LoadBufferSize=1024 ,UpdateBufferSize=1024 ,LoadInstances=1 ,TargetTdpId=\u0026lt;td_pid\u0026gt; ,TargetUserName=\u0026lt;td_user\u0026gt; ,TargetUserPassword=\u0026lt;td_pass\u0026gt; Usage Table Creating Create table with specific Teradata properties\nCREATE TABLE `teradata_binary_table_1mb`( `test_tinyint` tinyint, `test_smallint` smallint, `test_int` int, `test_bigint` bigint, `test_double` double, `test_decimal` decimal(15,2), `test_date` date, `test_timestamp` timestamp, `test_char` char(1), `test_varchar` varchar(40), `test_binary` binary ) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.teradata.TeradataBinarySerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.TeradataBinaryFileInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.TeradataBinaryFileOutputFormat' TBLPROPERTIES ( 'teradata.timestamp.precision'='6', 'teradata.char.charset'='UNICODE', 'teradata.row.length'='1MB' ); Default Teradata properties\n'teradata.timestamp.precision'='6', 'teradata.char.charset'='UNICODE', 'teradata.row.length'='64KB' Table Properties    Property Name Property Value Set Default Property Value Note     teradata.row.length (64KB, 1MB) 64KB 64KB corresponds to Formatted mode1MB corresponds to Formatted4 mode   teradata.char.charset (UNICODE, LATIN) UNICODE This decides how many bytes per char for CHAR data type3 bytes per char for UNICODE2 bytes per char for LATINAll the fields with CHAR type are controlled by this property (no separate specifying supported)   teradata.timestamp.precision 0-6 6 This decides how many bytes for TIMESTAMP data type. More details is here.All the fields with TIMESTAMP are controlled by this property (no separate specifying supported)    Teradata to Hive Type Conversion    Teradata Data Type Teradata Data Type Definition Hive Type Hive Data Type Definition Note     DATE DATE DATE DATE    TIMESTAMP TIMESTAMP(X) TIMESTAMP TIMESTAMP The decoding of TIMESTAMP precision is controlled by the table property teradata.timestamp.precision   BYTEINT BYTEINT TINYINT TINYINT    SMALLINT SMALLINT SMALLINT SMALLINT    INTEGER *INTEGER INT* INT *INT   BIGINT BIGINT BIGINT BIGINT    FLOAT FLOAT DOUBLE DOUBLE    DECIMAL DECIMAL(N,M)**\u0026ndash;Default DECIMAL(5, 0) DECIMAL DECIMAL(N,M)**\u0026ndash;Default DECIMAL(10, 0)    VARCHAR VARCHAR(X) VARCHAR VARCHAR(X)    CHAR CHAR(X) CHAR CHAR(X) The decoding of each CHAR is controlled by the table property teradata.char.charset   VARBYTE VARBYTE(X) BINARY BINARY     Serde Restriction The TeradataBinarySerde has several restrictions:\n Only support simple data type listed above, other data type like INTERVAL, TIME, NUMBER, CLOB, BLOB in Teradata are not yet supported. Doesn\u0026rsquo;t support the complex data type such as ARRAY, MAP.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/teradatabinaryserde_89068127/","tags":null,"title":"Apache Hive : TeradataBinarySerde"},{"categories":null,"contents":"Apache Hive : TestingDocs Hive Testing Documents The following documents describe aspects of testing for Hive:\n Hive Developer FAQ: Testing Developer Guide: Unit Tests Unit Testing Hive SQL Running Yetus MetaStore API Tests Query File Test(qtest)  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/testingdocs_42567126/","tags":null,"title":"Apache Hive : TestingDocs"},{"categories":null,"contents":"Apache Hive : Theta Join  Preliminaries  Overview Specific Use Cases  Geo-Location Side-Table Similarity   Requirements  Goals Specific Non-Goals     Literature Review  Map-Reduce-Merge: Simplified Relational Data Processing on Large Clusters [1] Efficient Parallel Set-Similarity Joins Using MapReduce [2] Processing Theta-Joins using MapReduce [3] Efficient Multi-way Theta-Join Processing Using MapReduce [4]   Design  Map-side Reduce-side  Mapper Reducer     References  Preliminaries Overview HIVE-556 requests that Hive support non-equality joins commonly called theta joins. Theta joins cover all kinds of non-equality joins such as \u0026lt;, \u0026lt;=, \u0026gt;, \u0026gt;=, \u0026lt;\u0026gt;, LIKE, RLIKE, and Generic UDF.\nRange joins, a specific kind of theta join, is extremely useful in temporal and spatial applications. This work will focus first on range join as it’s arguably the most important theta join. After range join is implemented the remaining theta join operators can be implemented with relative ease. Supporting theta join and specifically range join is important as it will makes Hive competitive in additional use cases and removes a deficiency when compared against traditional systems.\nSpecific Use Cases Geo-Location This use case is very common today for many common users of Hive websites, mobile telephone services, online ad marketers. The user wants to join website access logs with geo-location information of ip addresses. This can be implemented as an interval join. Currently this use case is implemented by creating a custom UDF which performs the lookup. Compiling and deploying UDF’s is an error prone process and not a common task for most users of Hive.\nAs an example of this use case, we can utilize the Maxmind GeoIP by country dataset which is 79,980 records. Consider joining that dataset with a very small access log of 5,000,000 entries using a cartesian product and then filtering the result. This approach results in 399,900,000,000 tuples that need to be filtered in the mapper. Hive trunk was modified to implement map-side interval join and the previously discussed join completed in 21 seconds on a single host while the cartesian product then filter approach ran for many hours before it was killed.\nSide-Table Similarity This use case was originally proposed in HIVE-556 by it’s creator Min Zhou. The user has a side table with some entries and wants to join the side table with a main table based on similarity. The example provide was using the RLIKE operator but LIKE or a generic boolean UDF could be used as well.\nRequirements A notable non-goal of this work is n-way theta joins. The algorithm to implement n-way theta joins was inspired by the algorithm to implement 2-way theta joins. Therefore n-way theta joins can be implemented in future work.\nGoals  Map-side 2-way theta joins Reduce-side 2-way theta join  Specific Non-Goals  Map-side n-way theta joins Reduce-side n-way theta joins Additional statistic collection  Literature Review Map-Reduce-Merge: Simplified Relational Data Processing on Large Clusters [1] This work adds a Merge step to Map-Reduce which allows for easy expression of relational algebra operators. This is interesting but not immediately useful as it requires modification of the Map-Reduce framework it’s not immediately useful.\nEfficient Parallel Set-Similarity Joins Using MapReduce [2] This work studies a special type of set similarity, specifically similar strings or bit vectors. This work could be useful in implementing some operators such as LIKE. In addition this method which requires statistics to be calculated at run time results in multiple Map-Reduce jobs.\nProcessing Theta-Joins using MapReduce [3] This work proposes an algorithm 1-Bucket-Theta to perform theta joins on two relations in a single Map-Reduce job given some basic statistics, namely the cardinality of the two relations. This approach allows parallel implementation of cartesian product as well. The work also details how additional input statistics can be exploited to improve efficiency. The approach is to partition a join-matrix of the two relations.\nEfficient Multi-way Theta-Join Processing Using MapReduce [4] This work is inspired by [3] and expands the method to N-way joins. The approach used is to partition a hypercube of the relations. An approach to merge the resulting many Map-Reduce jobs into a single job is also discussed with results similar to Y-Smart [5].\nDesign Map-side A large number of theta join use cases have the nice property that only one of the relations is “large”. Therefore many theta joins can be converted to map-joins. Presently these use cases utilize a map-side cartesian product with post-join filters. As noted in the geo-location use case above some of these use cases, specifically range joins, can see several orders of magnitude speedup utilizing theta join.\nCurrently Map-side join utilizes a hashmap and a join is performed when the incoming key matches a key in the hash map. To support range join this will abstracted into a pluggable interface. The plugin can decide how two keys are joined. The equality join interface will continue to utilize a hashmap while range join can use a data structure such as an interval tree. Other such optimizations can be made. For example the not equals join condition \u0026lt;\u0026gt; can use a [view on top of a map](http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/Maps.html#filterKeys(java.util.Map, com.google.common.base.Predicate)).\nReduce-side Reduce-side joins will be implemented via 1-Bucket-Theta as described in [3]. This requires the cardinality of the two relations and therefore to perform a reduce-side theta join statistics must be turned on. Initially if the required statistics do not exist an exception will be thrown indicating the problem. After the initial implementation we can use a method to estimate the cardinality.\nAs previously mention a detailed description of 1-Bucket-Theta is located [3]. As such the discussion of the internals of the algorithm will be brief. Joining two relations S and T can be viewed as a matrix with the S, the smaller relation, on the left and T on the right.\nThe matrix is partitioned by r, the number of reducers. An example join matrix follows, with four reducers 1-4 each a separate color:\n   Row Ids T 1 T 2 T 3 T 4     S 1 1 1 2 2   S 2 1 1 2 2   S 3 3 3 4 4   S 4 3 3 4 4    In the map phase, each tuple in S is sent to all reducers which intersect the tuples’ row id. For example the S-tuple with the row id of 2, is sent to reducers 1, and 2. Similarly each tuple in T is sent to all reducers which intersect the tuples’ row id. For example, the tuple with rowid 4, is sent to reducers 2 and 4.\nIn Hive and MapReduce row ids aren’t common available. Therefore we choose a random row id between 1 and |S| (cardinality of S) for S and 1 and |T| (cardinality of T) for T. Thus a reduce-side theta join must know the estimated cardinality of each relation and statistics must be enabled. Random row id’s will result in well balanced reducer input when processing larger relations. As noted in [3], the partitioning scheme works such that if a relation is much smaller than it’s pair the smaller relation will be broadcast two all reducers. As such therefore random-skew which would occur for small relations does not impact the algorithm in practice. Additionally in Hive if a relation is small the join is converted to a map-side join and 1-Bucket-Theta is not utilized.\nMapper In the mapper the join matrix will be initialized, a random row id chosen, and then the tuple will be emitted for each reducer that intersects the row id. Hive already has a mechanism to set the hash code for a specific tuple which can be re-used here. Additionally the tuples will need to be sorted in such a way so that tuples for S arrive in the reducer first. Luckily Hive already implements this via the JoinReorder class.\nReducer The reducer is fairly simple, it buffers up the S relation and then performs the requested join on each T tuple that is received.\nReferences  Map-Reduce-Merge: Simplified Relational Data Processing on Large Clusters Efficient Parallel Set-Similarity Joins Using MapReduce Processing Theta-Joins using MapReduce Efficient Multi-way Theta-Join Processing Using MapReduce HIVE-2206  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/theta-join_33293991/","tags":null,"title":"Apache Hive : Theta Join"},{"categories":null,"contents":"Apache Hive : Top K Stats Column Level Top K Statistics  Column Level Top K Statistics  Scope Implementation Usage Example  Newly Created Tables Existing Tables   Current Status (JIRA)    This document is an addition to Statistics in Hive. It describes the support of collecting column level top K values for Hive tables (see HIVE-3421).\nScope In addition to the partition statistics, column level top K values can also be estimated for Hive tables.\nThe name and top K values of the most skewed column is stored in the partition or non-partitioned table’s skewed information, if user did not specify skew. This works for both newly created and existing tables.\nThe algorithm for computing top K is based on this paper: Efficient Computation of Frequent and Top-k Elements in Data Streams.\nImplementation Top K statistics are gathered along with partition level statistics. The interface IStatsAggregator needs to add a method aggregateStatsTopK() that reads multiple entries from the temporary storage:\n... public interface IStatsAggregator { ... /** * This method aggregates top K statistics. * * */ public List\u0026lt;String\u0026gt; aggregateStatsTopK(String keyPrefix, String statType); ... } Usage Top K statistics are disabled by default. The user can set the boolean variable hive.stats.topk.collect to be true to enable computing top K and putting top K into skewed information.\nset hive.stats.topk.collect=true; The user can also specify the number of K by setting the integer variable hive.stats.topk.num, and the minimal row percentage that a value needs to hold to be in the top K result, by setting the float variable hive.stats.topk.minpercent.\nset hive.stats.topk.num=8; set hive.stats.topk.minpercent=5.0; Another integer variable, hive.stats.topk.poolsize, specifies the number of values to be monitored while computing top K. The accuracy of top K estimate increases as this number gets larger.\nset hive.stats.topk.poolsize=200; Computing top K for a large number of partitions simultaneously can be stressful to memory. The user can specify the integer variable hive.stats.topk.maxpartnum for the maximal number of partitions to collect Top K. When this number is exceeded, top K will be disabled for all the remaining partitions.\nset hive.stats.topk.maxpartnum=10; In case of JDBC implementation of temporary stored statistics (eg. Derby or MySQL), the user should also specify the column type for top K, by setting the variable hive.stats.topk.column.type. By default, TEXT is used for MySQL, and LONG VARCHAR is used for Derby.\nset hive.stats.topk.column.type=’LONG VARCHAR’; Example The user may set top K related variables at the beginning:\nset hive.stats.topk.collect=true; set hive.stats.topk.num=4; set hive.stats.topk.minpercent=0; set hive.stats.topk.poolsize=100; Newly Created Tables Suppose a partitioned table is created without skew, and data is inserted to its partitions:\nCREATE TABLE table1 (key STRING, value STRING) PARTITIONED BY (ds STRING); INSERT OVERWRITE TABLE table1 PARTITION (ds='2012-09-07') SELECT * FROM table_src; Top K was computed for the partition while data was inserted. If the user issues the command:\nDESCRIBE FORMATTED table1 partition (ds='2012-09-07'); then among the output, the following will be displayed:\n... Skewed Columns: [value] Skewed Values: [[val_348], [val_230], [val_401], [val_70]] ... If the user issues the command:\nDESCRIBE FORMATTED table1; then among the output, there will not be skewed information, since table level top K is not available for partitioned tables.\nFor a non-partitioned table:\nCREATE TABLE table1 (key STRING, value STRING); INSERT OVERWRITE TABLE table1 SELECT * FROM table_src; If the user issues the command:\nDESCRIBE FORMATTED table1; then among the output, the following will be displayed:\n... Skewed Columns: [value] Skewed Values: [[val_348], [val_230], [val_401], [val_70]] ... When a table is created with skew:\nset hive.internal.ddl.list.bucketing.enable=true; CREATE TABLE table1 (key STRING, value STRING) PARTITIONED BY (ds STRING) SKEWED BY (key) on ('38', '49'); INSERT OVERWRITE TABLE table1 PARTITION (ds='2012-09-07') SELECT * FROM table_src; Top K will not be collected, and the user specified skewed information remains. If the user issues the command:\nDESCRIBE FORMATTED table1 partition (ds='2012-09-07'); then among the output, the following will be displayed:\n... Skewed Columns: [key] Skewed Values: [[38], [49]] ... Existing Tables Top K works the same way for ANALYZE commands as for INSERT commands.\nCurrent Status (JIRA) See HIVE-3421.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/top-k-stats_30150275/","tags":null,"title":"Apache Hive : Top K Stats"},{"categories":null,"contents":"Apache Hive : Transitivity on predicate pushdown Before Hive 0.8.0, the query\n set hive.mapred.mode=strict; create table invites (foo int, bar string) partitioned by (ds string); create table invites2 (foo int, bar string) partitioned by (ds string); select count(*) from invites join invites2 on invites.ds=invites2.ds where invites.ds='2011-01-01'; would give the error\n Error in semantic analysis: No Partition Predicate Found for Alias \u0026quot;invites2\u0026quot; Table \u0026quot;invites2\u0026quot; Here, the filter is applied to the table invites as invites.ds=\u0026lsquo;2011-01-01\u0026rsquo; but not invites2.ds=\u0026lsquo;2011-01-01\u0026rsquo;. This causes Hive to reject the query in strict mode to prevent scanning all the partitions of invites2. This can be seen by using explain plan on the query without strict mode on:\n STAGE DEPENDENCIES: Stage-1 is a root stage Stage-2 depends on stages: Stage-1 Stage-0 is a root stage STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: invites TableScan alias: invites Filter Operator predicate: expr: (ds = '2011-01-01') type: boolean Reduce Output Operator key expressions: expr: ds type: string sort order: + Map-reduce partition columns: expr: ds type: string tag: 0 value expressions: expr: ds type: string invites2 TableScan alias: invites2 Reduce Output Operator key expressions: expr: ds type: string sort order: + Map-reduce partition columns: expr: ds type: string tag: 1 Reduce Operator Tree: Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {VALUE._col2} 1 handleSkewJoin: false outputColumnNames: _col2 Select Operator Group By Operator aggregations: expr: count() bucketGroup: false mode: hash outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.SequenceFileInputFormat output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat Stage: Stage-2 Map Reduce Alias -\u0026gt; Map Operator Tree: file:/var/folders/nt/ng21tg0n1jl4547lw0k8lg6hq_nw87/T/charleschen/hive_2011-08-04_10-59-05_697_8934329734633337337/-mr-10002 Reduce Output Operator sort order: tag: -1 value expressions: expr: _col0 type: bigint Reduce Operator Tree: Group By Operator aggregations: expr: count(VALUE._col0) bucketGroup: false mode: mergepartial outputColumnNames: _col0 Select Operator expressions: expr: _col0 type: bigint outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Stage: Stage-0 Fetch Operator limit: -1 Note that there is no filter on the tablescan operation for invites2.\nIn Hive 0.8.0, support will be added for recognizing transitivity on join conditions during predicate pushdown with [HIVE-1989|https://issues.apache.org/jira/browse/HIVE-1989]. With the above example, Hive will now infer the filter invites2.ds=\u0026lsquo;2011-01-01\u0026rsquo; from the filter invites.ds=\u0026lsquo;2011-01-01\u0026rsquo; and the join condition invites.ds=invites2.ds. The explain plan now gives:\n STAGE DEPENDENCIES: Stage-1 is a root stage Stage-2 depends on stages: Stage-1 Stage-0 is a root stage STAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: invites TableScan alias: invites Filter Operator predicate: expr: (ds = '2011-01-01') type: boolean Reduce Output Operator key expressions: expr: ds type: string sort order: + Map-reduce partition columns: expr: ds type: string tag: 0 value expressions: expr: ds type: string invites2 TableScan alias: invites2 Filter Operator predicate: expr: (ds = '2011-01-01') type: boolean Reduce Output Operator key expressions: expr: ds type: string sort order: + Map-reduce partition columns: expr: ds type: string tag: 1 Reduce Operator Tree: Join Operator condition map: Inner Join 0 to 1 condition expressions: 0 {VALUE._col2} 1 handleSkewJoin: false outputColumnNames: _col2 Select Operator Group By Operator aggregations: expr: count() bucketGroup: false mode: hash outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.SequenceFileInputFormat output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat Stage: Stage-2 Map Reduce Alias -\u0026gt; Map Operator Tree: file:/var/folders/nt/ng21tg0n1jl4547lw0k8lg6hq_nw87/T/charleschen/hive_2011-08-04_10-56-09_896_8195257719501884918/-mr-10002 Reduce Output Operator sort order: tag: -1 value expressions: expr: _col0 type: bigint Reduce Operator Tree: Group By Operator aggregations: expr: count(VALUE._col0) bucketGroup: false mode: mergepartial outputColumnNames: _col0 Select Operator expressions: expr: _col0 type: bigint outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat Stage: Stage-0 Fetch Operator limit: -1 ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/transitivity-on-predicate-pushdown_27823388/","tags":null,"title":"Apache Hive : Transitivity on predicate pushdown"},{"categories":null,"contents":"Apache Hive : Tutorial Hive Tutorial   Hive Tutorial Concepts  What Is Hive What Hive Is NOT Getting Started Data Units Type System Built In Operators and Functions Language Capabilities   Usage and Examples  Concepts What Is Hive Hive is a data warehousing infrastructure based on Apache Hadoop. Hadoop provides massive scale out and fault tolerance capabilities for data storage and processing on commodity hardware.\nHive is designed to enable easy data summarization, ad-hoc querying and analysis of large volumes of data. It provides SQL which enables users to do ad-hoc querying, summarization and data analysis easily. At the same time, Hive\u0026rsquo;s SQL gives users multiple places to integrate their own functionality to do custom analysis, such as User Defined Functions (UDFs). What Hive Is NOT Hive is not designed for online transaction processing. It is best used for traditional data warehousing tasks.\nGetting Started For details on setting up Hive, HiveServer2, and Beeline, please refer to the GettingStarted guide.\nBooks about Hive lists some books that may also be helpful for getting started with Hive.\nIn the following sections we provide a tutorial on the capabilities of the system. We start by describing the concepts of data types, tables, and partitions (which are very similar to what you would find in a traditional relational DBMS) and then illustrate the capabilities of Hive with the help of some examples.\nData Units In the order of granularity - Hive data is organized into:\n Databases: Namespaces function to avoid naming conflicts for tables, views, partitions, columns, and so on. Databases can also be used to enforce security for a user or group of users. Tables: Homogeneous units of data which have the same schema. An example of a table could be page_views table, where each row could comprise of the following columns (schema):  timestamp—which is of INT type that corresponds to a UNIX timestamp of when the page was viewed. userid —which is of BIGINT type that identifies the user who viewed the page. page_url—which is of STRING type that captures the location of the page. referer_url—which is of STRING that captures the location of the page from where the user arrived at the current page. IP—which is of STRING type that captures the IP address from where the page request was made.   Partitions: Each Table can have one or more partition Keys which determines how the data is stored. Partitions—apart from being storage units—also allow the user to efficiently identify the rows that satisfy a specified criteria; for example, a date_partition of type STRING and country_partition of type STRING. Each unique value of the partition keys defines a partition of the Table. For example, all \u0026ldquo;US\u0026rdquo; data from \u0026ldquo;2009-12-23\u0026rdquo; is a partition of the page_views table. Therefore, if you run analysis on only the \u0026ldquo;US\u0026rdquo; data for 2009-12-23, you can run that query only on the relevant partition of the table, thereby speeding up the analysis significantly. Note however, that just because a partition is named 2009-12-23 does not mean that it contains all or only data from that date; partitions are named after dates for convenience; it is the user\u0026rsquo;s job to guarantee the relationship between partition name and data content! Partition columns are virtual columns, they are not part of the data itself but are derived on load. Buckets (or Clusters): Data in each partition may in turn be divided into Buckets based on the value of a hash function of some column of the Table. For example the page_views table may be bucketed by userid, which is one of the columns, other than the partitions columns, of the page_view table. These can be used to efficiently sample the data.  Note that it is not necessary for tables to be partitioned or bucketed, but these abstractions allow the system to prune large quantities of data during query processing, resulting in faster query execution.\nType System Hive supports primitive and complex data types, as described below. See Hive Data Types for additional information.\nPrimitive Types  Types are associated with the columns in the tables. The following Primitive types are supported: Integers  TINYINT—1 byte integer SMALLINT—2 byte integer INT—4 byte integer BIGINT—8 byte integer   Boolean type  BOOLEAN—TRUE/FALSE   Floating point numbers  FLOAT—single precision DOUBLE—Double precision   Fixed point numbers  DECIMAL—a fixed point value of user defined scale and precision   String types  STRING—sequence of characters in a specified character set VARCHAR—sequence of characters in a specified character set with a maximum length CHAR—sequence of characters in a specified character set with a defined length   Date and time types  TIMESTAMP — A date and time without a timezone (\u0026ldquo;LocalDateTime\u0026rdquo; semantics) TIMESTAMP WITH LOCAL TIME ZONE — A point in time measured down to nanoseconds (\u0026ldquo;Instant\u0026rdquo; semantics) DATE—a date   Binary types  BINARY—a sequence of bytes    The Types are organized in the following hierarchy (where the parent is a super type of all the children instances):\n Type   | Primitive Type |\n  | Number |\n  | DOUBLE |\n  | FLOAT |\n  | BIGINT |\n  | INT |\n  | SMALLINT |\n | TINYINT |          | STRING |\n      | BOOLEAN |\n      This type hierarchy defines how the types are implicitly converted in the query language. Implicit conversion is allowed for types from child to an ancestor. So when a query expression expects type1 and the data is of type2, type2 is implicitly converted to type1 if type1 is an ancestor of type2 in the type hierarchy. Note that the type hierarchy allows the implicit conversion of STRING to DOUBLE.\nExplicit type conversion can be done using the cast operator as shown in the #Built In Functions section below.\nComplex Types Complex Types can be built up from primitive types and other composite types using:\n Structs: the elements within the type can be accessed using the DOT (.) notation. For example, for a column c of type STRUCT {a INT; b INT}, the a field is accessed by the expression c.a Maps (key-value tuples): The elements are accessed using [\u0026lsquo;element name\u0026rsquo;] notation. For example in a map M comprising of a mapping from \u0026lsquo;group\u0026rsquo; -\u0026gt; gid the gid value can be accessed using M[\u0026lsquo;group\u0026rsquo;] Arrays (indexable lists): The elements in the array have to be in the same type. Elements can be accessed using the [n] notation where n is an index (zero-based) into the array. For example, for an array A having the elements [\u0026lsquo;a\u0026rsquo;, \u0026lsquo;b\u0026rsquo;, \u0026lsquo;c\u0026rsquo;], A[1] retruns \u0026lsquo;b\u0026rsquo;.  Using the primitive types and the constructs for creating complex types, types with arbitrary levels of nesting can be created. For example, a type User may comprise of the following fields:\n gender—which is a STRING. active—which is a BOOLEAN.  Timestamp Timestamps have been the source of much confusion, so we try to document the intended semantics of Hive.\nTimestamp (\u0026ldquo;LocalDateTime\u0026rdquo; semantics) Java\u0026rsquo;s \u0026ldquo;LocalDateTime\u0026rdquo; timestamps record a date and time as year, month, date, hour, minute, and seconds without a timezone. These timestamps always have those same values regardless of the local time zone.\nFor example, the timestamp value of \u0026ldquo;2014-12-12 12:34:56\u0026rdquo; is decomposed into year, month, day, hour, minute and seconds fields, but with no time zone information available. It does not correspond to any specific instant. It will always be the same value regardless of the local time zone. Unless your application uses UTC consistently, timestamp with local time zone is strongly preferred over timestamp for most applications. When users say an event is at 10:00, it is always in reference to a certain timezone and means a point in time, rather than 10:00 in an arbitrary time zone.\nTimestamp with local time zone (\u0026ldquo;Instant\u0026rdquo; semantics) Java\u0026rsquo;s \u0026ldquo;Instant\u0026rdquo; timestamps define a point in time that remains constant regardless of where the data is read. Thus, the timestamp will be adjusted by the local time zone to match the original point in time.\n   Type Value in America/Los_Angeles Value in America/New_York     timestamp 2014-12-12 12:34:56 2014-12-12 12:34:56   timestamp with local time zone 2014-12-12 12:34:56 2014-12-12 15:34:56    Comparisons with other tools     SQL 2003 Oracle Sybase Postgres MySQL Microsoft SQL IBM DB2 Presto Snowflake Hive \u0026gt;= 3.1 Iceberg Spark     timestamp Local Local Local Local Instant Other Local Local Local Local Local Instant   timestamp with local time zone  Instant       Instant Instant     timestamp with time zone Offset Offset Offset Instant   Offset Offset Offset  Instant    timestamp without time zone Local Local  Local   Local         Other definitions:\n Offset = Recording a point in time as well as the time zone offset in the writer\u0026rsquo;s time zone.  Built In Operators and Functions The operators and functions listed below are not necessarily up to date. (Hive Operators and UDFs has more current information.) In Beeline or the Hive CLI, use these commands to show the latest documentation:\nSHOW FUNCTIONS; DESCRIBE FUNCTION \u0026lt;function_name\u0026gt;; DESCRIBE FUNCTION EXTENDED \u0026lt;function_name\u0026gt;; Case-insensitive\nAll Hive keywords are case-insensitive, including the names of Hive operators and functions.\nBuilt In Operators  Relational Operators—The following operators compare the passed operands and generate a TRUE or FALSE value, depending on whether the comparison between the operands holds or not.     Relational Operator Operand types Description     A = B all primitive types TRUE if expression A is equivalent to expression B; otherwise FALSE   A != B all primitive types TRUE if expression A is not equivalent to expression B; otherwise FALSE   A \u0026lt; B all primitive types TRUE if expression A is less than expression B; otherwise FALSE   A \u0026lt;= B all primitive types TRUE if expression A is less than or equal to expression B; otherwise FALSE   A \u0026gt; B all primitive types TRUE if expression A is greater than expression B] otherwise FALSE   A \u0026gt;= B all primitive types TRUE if expression A is greater than or equal to expression B otherwise FALSE   A IS NULL all types TRUE if expression A evaluates to NULL otherwise FALSE   A IS NOT NULL all types FALSE if expression A evaluates to NULL otherwise TRUE   A LIKE B strings TRUE if string A matches the SQL simple regular expression B, otherwise FALSE. The comparison is done character by character. The _ character in B matches any character in A (similar to . in posix regular expressions), and the % character in B matches an arbitrary number of characters in A (similar to .* in posix regular expressions). For example, 'foobar' LIKE 'foo' evaluates to FALSE where as 'foobar' LIKE 'foo___' evaluates to TRUE and so does 'foobar' LIKE 'foo%'. To escape % use \\ (% matches one % character). If the data contains a semicolon, and you want to search for it, it needs to be escaped, columnValue LIKE 'a\\;b'   A RLIKE B strings NULL if A or B is NULL, TRUE if any (possibly empty) substring of A matches the Java regular expression B (see Java regular expressions syntax), otherwise FALSE. For example, \u0026lsquo;foobar\u0026rsquo; rlike \u0026lsquo;foo\u0026rsquo; evaluates to TRUE and so does \u0026lsquo;foobar\u0026rsquo; rlike \u0026lsquo;^f.*r$\u0026rsquo;.   A REGEXP B strings Same as RLIKE     Arithmetic Operators—The following operators support various common arithmetic operations on the operands. All of them return number types.     Arithmetic Operators Operand types Description     A + B all number types Gives the result of adding A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands, for example, since every integer is a float. Therefore, float is a containing type of integer so the + operator on a float and an int will result in a float.   A - B all number types Gives the result of subtracting B from A. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A * B all number types Gives the result of multiplying A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands. Note that if the multiplication causing overflow, you will have to cast one of the operators to a type higher in the type hierarchy.   A / B all number types Gives the result of dividing B from A. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands. If the operands are integer types, then the result is the quotient of the division.   A % B all number types Gives the reminder resulting from dividing A by B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A \u0026amp; B all number types Gives the result of bitwise AND of A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   A B all number types   A ^ B all number types Gives the result of bitwise XOR of A and B. The type of the result is the same as the common parent(in the type hierarchy) of the types of the operands.   ~A all number types Gives the result of bitwise NOT of A. The type of the result is the same as the type of A.     Logical Operators — The following operators provide support for creating logical expressions. All of them return boolean TRUE or FALSE depending upon the boolean values of the operands.     Logical Operators Operands types Description     A AND B boolean TRUE if both A and B are TRUE, otherwise FALSE   A \u0026amp;\u0026amp; B boolean Same as A AND B   A OR B boolean TRUE if either A or B or both are TRUE, otherwise FALSE   A  B   NOT A boolean TRUE if A is FALSE, otherwise FALSE   !A boolean Same as NOT A     Operators on Complex Types—The following operators provide mechanisms to access elements in Complex Types     Operator Operand types Description     A[n] A is an Array and n is an int returns the nth element in the array A. The first element has index 0, for example, if A is an array comprising of [\u0026lsquo;foo\u0026rsquo;, \u0026lsquo;bar\u0026rsquo;] then A[0] returns \u0026lsquo;foo\u0026rsquo; and A[1] returns \u0026lsquo;bar\u0026rsquo;   M[key] M is a Map\u0026lt;K, V\u0026gt; and key has type K returns the value corresponding to the key in the map for example, if M is a map comprising of {\u0026lsquo;f\u0026rsquo; -\u0026gt; \u0026lsquo;foo\u0026rsquo;, \u0026lsquo;b\u0026rsquo; -\u0026gt; \u0026lsquo;bar\u0026rsquo;, \u0026lsquo;all\u0026rsquo; -\u0026gt; \u0026lsquo;foobar\u0026rsquo;} then M[\u0026lsquo;all\u0026rsquo;] returns \u0026lsquo;foobar\u0026rsquo;   S.x S is a struct returns the x field of S, for example, for struct foobar {int foo, int bar} foobar.foo returns the integer stored in the foo field of the struct.    Built In Functions  Hive supports the following built in functions:\n(Function list in source code: FunctionRegistry.java)     Return Type Function Name (Signature) Description     BIGINT round(double a) returns the rounded BIGINT value of the double   BIGINT floor(double a) returns the maximum BIGINT value that is equal or less than the double   BIGINT ceil(double a) returns the minimum BIGINT value that is equal or greater than the double   double rand(), rand(int seed) returns a random number (that changes from row to row). Specifiying the seed will make sure the generated random number sequence is deterministic.   string concat(string A, string B,\u0026hellip;) returns the string resulting from concatenating B after A. For example, concat(\u0026lsquo;foo\u0026rsquo;, \u0026lsquo;bar\u0026rsquo;) results in \u0026lsquo;foobar\u0026rsquo;. This function accepts arbitrary number of arguments and return the concatenation of all of them.   string substr(string A, int start) returns the substring of A starting from start position till the end of string A. For example, substr(\u0026lsquo;foobar\u0026rsquo;, 4) results in \u0026lsquo;bar\u0026rsquo;   string substr(string A, int start, int length) returns the substring of A starting from start position with the given length, for example, substr(\u0026lsquo;foobar\u0026rsquo;, 4, 2) results in \u0026lsquo;ba\u0026rsquo;   string upper(string A) returns the string resulting from converting all characters of A to upper case, for example, upper(\u0026lsquo;fOoBaR\u0026rsquo;) results in \u0026lsquo;FOOBAR\u0026rsquo;   string ucase(string A) Same as upper   string lower(string A) returns the string resulting from converting all characters of B to lower case, for example, lower(\u0026lsquo;fOoBaR\u0026rsquo;) results in \u0026lsquo;foobar\u0026rsquo;   string lcase(string A) Same as lower   string trim(string A) returns the string resulting from trimming spaces from both ends of A, for example, trim(' foobar \u0026lsquo;) results in \u0026lsquo;foobar\u0026rsquo;   string ltrim(string A) returns the string resulting from trimming spaces from the beginning(left hand side) of A. For example, ltrim(\u0026rsquo; foobar \u0026lsquo;) results in \u0026lsquo;foobar '   string rtrim(string A) returns the string resulting from trimming spaces from the end(right hand side) of A. For example, rtrim(\u0026rsquo; foobar \u0026lsquo;) results in ' foobar\u0026rsquo;   string regexp_replace(string A, string B, string C) returns the string resulting from replacing all substrings in B that match the Java regular expression syntax(See Java regular expressions syntax) with C. For example, regexp_replace(\u0026lsquo;foobar\u0026rsquo;, \u0026lsquo;oo   int size(Map\u0026lt;K.V\u0026gt;) returns the number of elements in the map type   int size(Array) returns the number of elements in the array type   value of  cast( as ) converts the results of the expression expr to , for example, cast(\u0026lsquo;1\u0026rsquo; as BIGINT) will convert the string \u0026lsquo;1\u0026rsquo; to it integral representation. A null is returned if the conversion does not succeed.   string from_unixtime(int unixtime) convert the number of seconds from the UNIX epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the format of \u0026ldquo;1970-01-01 00:00:00\u0026rdquo;   string to_date(string timestamp) Return the date part of a timestamp string: to_date(\u0026ldquo;1970-01-01 00:00:00\u0026rdquo;) = \u0026ldquo;1970-01-01\u0026rdquo;   int year(string date) Return the year part of a date or a timestamp string: year(\u0026ldquo;1970-01-01 00:00:00\u0026rdquo;) = 1970, year(\u0026ldquo;1970-01-01\u0026rdquo;) = 1970   int month(string date) Return the month part of a date or a timestamp string: month(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 11, month(\u0026ldquo;1970-11-01\u0026rdquo;) = 11   int day(string date) Return the day part of a date or a timestamp string: day(\u0026ldquo;1970-11-01 00:00:00\u0026rdquo;) = 1, day(\u0026ldquo;1970-11-01\u0026rdquo;) = 1   string get_json_object(string json_string, string path) Extract json object from a json string based on json path specified, and return json string of the extracted json object. It will return null if the input json string is invalid.     The following built in aggregate functions are supported in Hive:     Return Type Aggregation Function Name (Signature) Description     BIGINT count(*), count(expr), count(DISTINCT expr[, expr_.]) count(*)—Returns the total number of retrieved rows, including rows containing NULL values; count(expr)—Returns the number of rows for which the supplied expression is non-NULL; count(DISTINCT expr[, expr])—Returns the number of rows for which the supplied expression(s) are unique and non-NULL.   DOUBLE sum(col), sum(DISTINCT col) returns the sum of the elements in the group or the sum of the distinct values of the column in the group   DOUBLE avg(col), avg(DISTINCT col) returns the average of the elements in the group or the average of the distinct values of the column in the group   DOUBLE min(col) returns the minimum value of the column in the group   DOUBLE max(col) returns the maximum value of the column in the group    Language Capabilities Hive\u0026rsquo;s SQL provides the basic SQL operations. These operations work on tables or partitions. These operations are:\n Ability to filter rows from a table using a WHERE clause. Ability to select certain columns from the table using a SELECT clause. Ability to do equi-joins between two tables. Ability to evaluate aggregations on multiple \u0026ldquo;group by\u0026rdquo; columns for the data stored in a table. Ability to store the results of a query into another table. Ability to download the contents of a table to a local (for example,, nfs) directory. Ability to store the results of a query in a hadoop dfs directory. Ability to manage tables and partitions (create, drop and alter). Ability to plug in custom scripts in the language of choice for custom map/reduce jobs.  Usage and Examples NOTE: Many of the following examples are out of date. More up to date information can be found in the LanguageManual.\nThe following examples highlight some salient features of the system. A detailed set of query test cases can be found at Hive Query Test Cases and the corresponding results can be found at Query Test Case Results.\n Creating, Showing, Altering, and Dropping Tables Loading Data Querying and Inserting Data  Creating, Showing, Altering, and Dropping Tables See Hive Data Definition Language for detailed information about creating, showing, altering, and dropping tables.\nCreating Tables An example statement that would create the page_view table mentioned above would be like:\n CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) STORED AS SEQUENCEFILE; In this example, the columns of the table are specified with the corresponding types. Comments can be attached both at the column level as well as at the table level. Additionally, the partitioned by clause defines the partitioning columns which are different from the data columns and are actually not stored with the data. When specified in this way, the data in the files is assumed to be delimited with ASCII 001(ctrl-A) as the field delimiter and newline as the row delimiter.\nThe field delimiter can be parametrized if the data is not in the above format as illustrated in the following example:\n CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' STORED AS SEQUENCEFILE; The row delimintor currently cannot be changed since it is not determined by Hive but Hadoop delimiters.\nIt is also a good idea to bucket the tables on certain columns so that efficient sampling queries can be executed against the data set. If bucketing is absent, random sampling can still be done on the table but it is not efficient as the query has to scan all the data. The following example illustrates the case of the page_view table that is bucketed on the userid column:\n CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' COLLECTION ITEMS TERMINATED BY '2' MAP KEYS TERMINATED BY '3' STORED AS SEQUENCEFILE; In the example above, the table is clustered by a hash function of userid into 32 buckets. Within each bucket the data is sorted in increasing order of viewTime. Such an organization allows the user to do efficient sampling on the clustered column—n this case userid. The sorting property allows internal operators to take advantage of the better-known data structure while evaluating queries with greater efficiency.\n CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, friends ARRAY\u0026lt;BIGINT\u0026gt;, properties MAP\u0026lt;STRING, STRING\u0026gt; ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '1' COLLECTION ITEMS TERMINATED BY '2' MAP KEYS TERMINATED BY '3' STORED AS SEQUENCEFILE; In this example, the columns that comprise of the table row are specified in a similar way as the definition of types. Comments can be attached both at the column level as well as at the table level. Additionally, the partitioned by clause defines the partitioning columns which are different from the data columns and are actually not stored with the data. The CLUSTERED BY clause specifies which column to use for bucketing as well as how many buckets to create. The delimited row format specifies how the rows are stored in the hive table. In the case of the delimited format, this specifies how the fields are terminated, how the items within collections (arrays or maps) are terminated, and how the map keys are terminated. STORED AS SEQUENCEFILE indicates that this data is stored in a binary format (using hadoop SequenceFiles) on hdfs. The values shown for the ROW FORMAT and STORED AS clauses in the above, example represent the system defaults.\nTable names and column names are case insensitive.\nBrowsing Tables and Partitions  SHOW TABLES; To list existing tables in the warehouse; there are many of these, likely more than you want to browse.\n SHOW TABLES 'page.*'; To list tables with prefix \u0026lsquo;page\u0026rsquo;. The pattern follows Java regular expression syntax (so the period is a wildcard).\n SHOW PARTITIONS page_view; To list partitions of a table. If the table is not a partitioned table then an error is thrown.\n DESCRIBE page_view; To list columns and column types of table.\n DESCRIBE EXTENDED page_view; To list columns and all other properties of table. This prints lot of information and that too not in a pretty format. Usually used for debugging.\n DESCRIBE EXTENDED page_view PARTITION (ds='2008-08-08'); To list columns and all other properties of a partition. This also prints lot of information which is usually used for debugging.\nAltering Tables To rename existing table to a new name. If a table with new name already exists then an error is returned:\n ALTER TABLE old_table_name RENAME TO new_table_name; To rename the columns of an existing table. Be sure to use the same column types, and to include an entry for each preexisting column:\n ALTER TABLE old_table_name REPLACE COLUMNS (col1 TYPE, ...); To add columns to an existing table:\n ALTER TABLE tab1 ADD COLUMNS (c1 INT COMMENT 'a new int column', c2 STRING DEFAULT 'def val'); Note that a change in the schema (such as the adding of the columns), preserves the schema for the old partitions of the table in case it is a partitioned table. All the queries that access these columns and run over the old partitions implicitly return a null value or the specified default values for these columns.\nIn the later versions, we can make the behavior of assuming certain values as opposed to throwing an error in case the column is not found in a particular partition configurable.\nDropping Tables and Partitions Dropping tables is fairly trivial. A drop on the table would implicitly drop any indexes(this is a future feature) that would have been built on the table. The associated command is:\n DROP TABLE pv_users; To dropping a partition. Alter the table to drop the partition.\n ALTER TABLE pv_users DROP PARTITION (ds='2008-08-08')  Note that any data for this table or partitions will be dropped and may not be recoverable. *  Loading Data There are multiple ways to load data into Hive tables. The user can create an external table that points to a specified location within HDFS. In this particular usage, the user can copy a file into the specified location using the HDFS put or copy commands and create a table pointing to this location with all the relevant row format information. Once this is done, the user can transform the data and insert them into any other Hive table. For example, if the file /tmp/pv_2008-06-08.txt contains comma separated page views served on 2008-06-08, and this needs to be loaded into the page_view table in the appropriate partition, the following sequence of commands can achieve this:\n CREATE EXTERNAL TABLE page_view_stg(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User', country STRING COMMENT 'country of origination') COMMENT 'This is the staging page view table' ROW FORMAT DELIMITED FIELDS TERMINATED BY '44' LINES TERMINATED BY '12' STORED AS TEXTFILE LOCATION '/user/data/staging/page_view'; hadoop dfs -put /tmp/pv_2008-06-08.txt /user/data/staging/page_view FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='US') SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'US'; * This code results in an error due to LINES TERMINATED BY limitation\nFAILED: SemanticException 6:67 LINES TERMINATED BY only supports newline \u0026lsquo;\\n\u0026rsquo; right now. Error encountered near token \u0026lsquo;\u0026lsquo;12\u0026rsquo;\u0026rsquo;\nSee\nHIVE-5999 Allow other characters for LINES TERMINATED BY Open\nHIVE-11996 Row Delimiter other than \u0026lsquo;\\n\u0026rsquo; throws error in Hive. Open\nIn the example above, nulls are inserted for the array and map types in the destination tables but potentially these can also come from the external table if the proper row formats are specified.\nThis method is useful if there is already legacy data in HDFS on which the user wants to put some metadata so that the data can be queried and manipulated using Hive.\nAdditionally, the system also supports syntax that can load the data from a file in the local files system directly into a Hive table where the input data format is the same as the table format. If /tmp/pv_2008-06-08_us.txt already contains the data for US, then we do not need any additional filtering as shown in the previous example. The load in this case can be done using the following syntax:\n LOAD DATA LOCAL INPATH /tmp/pv_2008-06-08_us.txt INTO TABLE page_view PARTITION(date='2008-06-08', country='US') The path argument can take a directory (in which case all the files in the directory are loaded), a single file name, or a wildcard (in which case all the matching files are uploaded). If the argument is a directory, it cannot contain subdirectories. Similarly, the wildcard must match file names only.\nIn the case that the input file /tmp/pv_2008-06-08_us.txt is very large, the user may decide to do a parallel load of the data (using tools that are external to Hive). Once the file is in HDFS - the following syntax can be used to load the data into a Hive table:\n LOAD DATA INPATH '/user/data/pv_2008-06-08_us.txt' INTO TABLE page_view PARTITION(date='2008-06-08', country='US') It is assumed that the array and map fields in the input.txt files are null fields for these examples.\nSee Hive Data Manipulation Language for more information about loading data into Hive tables, and see External Tables for another example of creating an external table.\nQuerying and Inserting Data  Simple Query Partition Based Query Joins Aggregations Multi Table/File Inserts Dynamic-Partition Insert Inserting into Local Files Sampling Union All Array Operations Map (Associative Arrays) Operations Custom Map/Reduce Scripts Co-Groups  The Hive query operations are documented in Select, and the insert operations are documented in Inserting data into Hive Tables from queries and Writing data into the filesystem from queries.\nSimple Query For all the active users, one can use the query of the following form:\n INSERT OVERWRITE TABLE user_active SELECT user.* FROM user WHERE user.active = 1; Note that unlike SQL, we always insert the results into a table. We will illustrate later how the user can inspect these results and even dump them to a local file. You can also run the following query in Beeline or the Hive CLI:\n SELECT user.* FROM user WHERE user.active = 1; This will be internally rewritten to some temporary file and displayed to the Hive client side.\nPartition Based Query What partitions to use in a query is determined automatically by the system on the basis of where clause conditions on partition columns. For example, in order to get all the page_views in the month of 03/2008 referred from domain xyz.com, one could write the following query:\n INSERT OVERWRITE TABLE xyz_com_page_views SELECT page_views.* FROM page_views WHERE page_views.date \u0026gt;= '2008-03-01' AND page_views.date \u0026lt;= '2008-03-31' AND page_views.referrer_url like '%xyz.com'; Note that page_views.date is used here because the table (above) was defined with PARTITIONED BY(date DATETIME, country STRING) ; if you name your partition something different, don\u0026rsquo;t expect .date to do what you think!\nJoins In order to get a demographic breakdown (by gender) of page_view of 2008-03-03 one would need to join the page_view table and the user table on the userid column. This can be accomplished with a join as shown in the following query:\n INSERT OVERWRITE TABLE pv_users SELECT pv.*, u.gender, u.age FROM user u JOIN page_view pv ON (pv.userid = u.id) WHERE pv.date = '2008-03-03'; In order to do outer joins the user can qualify the join with LEFT OUTER, RIGHT OUTER or FULL OUTER keywords in order to indicate the kind of outer join (left preserved, right preserved or both sides preserved). For example, in order to do a full outer join in the query above, the corresponding syntax would look like the following query:\n INSERT OVERWRITE TABLE pv_users SELECT pv.*, u.gender, u.age FROM user u FULL OUTER JOIN page_view pv ON (pv.userid = u.id) WHERE pv.date = '2008-03-03'; In order check the existence of a key in another table, the user can use LEFT SEMI JOIN as illustrated by the following example.\n INSERT OVERWRITE TABLE pv_users SELECT u.* FROM user u LEFT SEMI JOIN page_view pv ON (pv.userid = u.id) WHERE pv.date = '2008-03-03'; In order to join more than one tables, the user can use the following syntax:\n INSERT OVERWRITE TABLE pv_friends SELECT pv.*, u.gender, u.age, f.friends FROM page_view pv JOIN user u ON (pv.userid = u.id) JOIN friend_list f ON (u.id = f.uid) WHERE pv.date = '2008-03-03'; Aggregations In order to count the number of distinct users by gender one could write the following query:\n INSERT OVERWRITE TABLE pv_gender_sum SELECT pv_users.gender, count (DISTINCT pv_users.userid) FROM pv_users GROUP BY pv_users.gender; Multiple aggregations can be done at the same time, however, no two aggregations can have different DISTINCT columns .e.g while the following is possible\n INSERT OVERWRITE TABLE pv_gender_agg SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(*), sum(DISTINCT pv_users.userid) FROM pv_users GROUP BY pv_users.gender; however, the following query is not allowed\n INSERT OVERWRITE TABLE pv_gender_agg SELECT pv_users.gender, count(DISTINCT pv_users.userid), count(DISTINCT pv_users.ip) FROM pv_users GROUP BY pv_users.gender; Multi Table/File Inserts The output of the aggregations or simple selects can be further sent into multiple tables or even to hadoop dfs files (which can then be manipulated using hdfs utilities). For example, if along with the gender breakdown, one needed to find the breakdown of unique page views by age, one could accomplish that with the following query:\n FROM pv_users INSERT OVERWRITE TABLE pv_gender_sum SELECT pv_users.gender, count_distinct(pv_users.userid) GROUP BY pv_users.gender INSERT OVERWRITE DIRECTORY '/user/data/tmp/pv_age_sum' SELECT pv_users.age, count_distinct(pv_users.userid) GROUP BY pv_users.age; The first insert clause sends the results of the first group by to a Hive table while the second one sends the results to a hadoop dfs files.\nDynamic-Partition Insert In the previous examples, the user has to know which partition to insert into and only one partition can be inserted in one insert statement. If you want to load into multiple partitions, you have to use multi-insert statement as illustrated below.\n FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='US') SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'US' INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='CA') SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'CA' INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country='UK') SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip WHERE pvs.country = 'UK'; In order to load data into all country partitions in a particular day, you have to add an insert statement for each country in the input data. This is very inconvenient since you have to have the priori knowledge of the list of countries exist in the input data and create the partitions beforehand. If the list changed for another day, you have to modify your insert DML as well as the partition creation DDLs. It is also inefficient since each insert statement may be turned into a MapReduce Job.\nDynamic-partition insert (or multi-partition insert) is designed to solve this problem by dynamically determining which partitions should be created and populated while scanning the input table. This is a newly added feature that is only available from version 0.6.0. In the dynamic partition insert, the input column values are evaluated to determine which partition this row should be inserted into. If that partition has not been created, it will create that partition automatically. Using this feature you need only one insert statement to create and populate all necessary partitions. In addition, since there is only one insert statement, there is only one corresponding MapReduce job. This significantly improves performance and reduce the Hadoop cluster workload comparing to the multiple insert case.\nBelow is an example of loading data to all country partitions using one insert statement:\n FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt='2008-06-08', country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, pvs.country There are several syntactic differences from the multi-insert statement:\n country appears in the PARTITION specification, but with no value associated. In this case, country is a dynamic partition column. On the other hand, ds has a value associated with it, which means it is a static partition column. If a column is dynamic partition column, its value will be coming from the input column. Currently we only allow dynamic partition columns to be the last column(s) in the partition clause because the partition column order indicates its hierarchical order (meaning dt is the root partition, and country is the child partition). You cannot specify a partition clause with (dt, country=\u0026lsquo;US\u0026rsquo;) because that means you need to update all partitions with any date and its country sub-partition is \u0026lsquo;US\u0026rsquo;. An additional pvs.country column is added in the select statement. This is the corresponding input column for the dynamic partition column. Note that you do not need to add an input column for the static partition column because its value is already known in the PARTITION clause. Note that the dynamic partition values are selected by ordering, not name, and taken as the last columns from the select clause.  Semantics of the dynamic partition insert statement:\n When there are already non-empty partitions exists for the dynamic partition columns, (for example, country=\u0026lsquo;CA\u0026rsquo; exists under some ds root partition), it will be overwritten if the dynamic partition insert saw the same value (say \u0026lsquo;CA\u0026rsquo;) in the input data. This is in line with the \u0026lsquo;insert overwrite\u0026rsquo; semantics. However, if the partition value \u0026lsquo;CA\u0026rsquo; does not appear in the input data, the existing partition will not be overwritten. Since a Hive partition corresponds to a directory in HDFS, the partition value has to conform to the HDFS path format (URI in Java). Any character having a special meaning in URI (for example, \u0026lsquo;%\u0026rsquo;, \u0026lsquo;:\u0026rsquo;, \u0026lsquo;/\u0026rsquo;, \u0026lsquo;#') will be escaped with \u0026lsquo;%\u0026rsquo; followed by 2 bytes of its ASCII value. If the input column is a type different than STRING, its value will be first converted to STRING to be used to construct the HDFS path. If the input column value is NULL or empty string, the row will be put into a special partition, whose name is controlled by the hive parameter hive.exec.default.partition.name. The default value is HIVE_DEFAULT_PARTITION{}. Basically this partition will contain all \u0026ldquo;bad\u0026rdquo; rows whose value are not valid partition names. The caveat of this approach is that the bad value will be lost and is replaced by HIVE_DEFAULT_PARTITION{} if you select them Hive. JIRA HIVE-1309 is a solution to let user specify \u0026ldquo;bad file\u0026rdquo; to retain the input partition column values as well. Dynamic partition insert could potentially be a resource hog in that it could generate a large number of partitions in a short time. To get yourself buckled, we define three parameters:  hive.exec.max.dynamic.partitions.pernode (default value being 100) is the maximum dynamic partitions that can be created by each mapper or reducer. If one mapper or reducer created more than that the threshold, a fatal error will be raised from the mapper/reducer (through counter) and the whole job will be killed. hive.exec.max.dynamic.partitions (default value being 1000) is the total number of dynamic partitions could be created by one DML. If each mapper/reducer did not exceed the limit but the total number of dynamic partitions does, then an exception is raised at the end of the job before the intermediate data are moved to the final destination. hive.exec.max.created.files (default value being 100000) is the maximum total number of files created by all mappers and reducers. This is implemented by updating a Hadoop counter by each mapper/reducer whenever a new file is created. If the total number is exceeding hive.exec.max.created.files, a fatal error will be thrown and the job will be killed.   Another situation we want to protect against dynamic partition insert is that the user may accidentally specify all partitions to be dynamic partitions without specifying one static partition, while the original intention is to just overwrite the sub-partitions of one root partition. We define another parameter hive.exec.dynamic.partition.mode=strict to prevent the all-dynamic partition case. In the strict mode, you have to specify at least one static partition. The default mode is strict. In addition, we have a parameter hive.exec.dynamic.partition=true/false to control whether to allow dynamic partition at all. The default value is false prior to Hive 0.9.0 and true in Hive 0.9.0 and later. In Hive 0.6, dynamic partition insert does not work with hive.merge.mapfiles=true or hive.merge.mapredfiles=true, so it internally turns off the merge parameters. Merging files in dynamic partition inserts are supported in Hive 0.7 (see JIRA HIVE-1307 for details).  Troubleshooting and best practices:\n As stated above, there are too many dynamic partitions created by a particular mapper/reducer, a fatal error could be raised and the job will be killed. The error message looks something like:   beeline\u0026gt; set hive.exec.dynamic.partition.mode=nonstrict; beeline\u0026gt; FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt, country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country; ... 2010-05-07 11:10:19,816 Stage-1 map = 0%, reduce = 0% [Fatal Error] Operator FS_28 (id=41): fatal error. Killing the job. Ended Job = job_201005052204_28178 with errors ... The problem of this that one mapper will take a random set of rows and it is very likely that the number of distinct (dt, country) pairs will exceed the limit of hive.exec.max.dynamic.partitions.pernode. One way around it is to group the rows by the dynamic partition columns in the mapper and distribute them to the reducers where the dynamic partitions will be created. In this case the number of distinct dynamic partitions will be significantly reduced. The above example query could be rewritten to:\n beeline\u0026gt; set hive.exec.dynamic.partition.mode=nonstrict; beeline\u0026gt; FROM page_view_stg pvs INSERT OVERWRITE TABLE page_view PARTITION(dt, country) SELECT pvs.viewTime, pvs.userid, pvs.page_url, pvs.referrer_url, null, null, pvs.ip, from_unixtimestamp(pvs.viewTime, 'yyyy-MM-dd') ds, pvs.country DISTRIBUTE BY ds, country; This query will generate a MapReduce job rather than Map-only job. The SELECT-clause will be converted to a plan to the mappers and the output will be distributed to the reducers based on the value of (ds, country) pairs. The INSERT-clause will be converted to the plan in the reducer which writes to the dynamic partitions.\nAdditional documentation:\n Design Document for Dynamic Partitions  Original design doc HIVE-936   Hive DML: Dynamic Partition Inserts HCatalog Dynamic Partitioning  Usage with Pig Usage from MapReduce    Inserting into Local Files In certain situations you would want to write the output into a local file so that you could load it into an excel spreadsheet. This can be accomplished with the following command:\n INSERT OVERWRITE LOCAL DIRECTORY '/tmp/pv_gender_sum' SELECT pv_gender_sum.* FROM pv_gender_sum; Sampling The sampling clause allows the users to write queries for samples of the data instead of the whole table. Currently the sampling is done on the columns that are specified in the CLUSTERED BY clause of the CREATE TABLE statement. In the following example we choose 3rd bucket out of the 32 buckets of the pv_gender_sum table:\n INSERT OVERWRITE TABLE pv_gender_sum_sample SELECT pv_gender_sum.* FROM pv_gender_sum TABLESAMPLE(BUCKET 3 OUT OF 32); In general the TABLESAMPLE syntax looks like:\n TABLESAMPLE(BUCKET x OUT OF y) y has to be a multiple or divisor of the number of buckets in that table as specified at the table creation time. The buckets chosen are determined if bucket_number module y is equal to x. So in the above example the following tablesample clause\n TABLESAMPLE(BUCKET 3 OUT OF 16) would pick out the 3rd and 19th buckets. The buckets are numbered starting from 0.\nOn the other hand the tablesample clause\n TABLESAMPLE(BUCKET 3 OUT OF 64 ON userid) would pick out half of the 3rd bucket.\nUnion All The language also supports union all, for example, if we suppose there are two different tables that track which user has published a video and which user has published a comment, the following query joins the results of a union all with the user table to create a single annotated stream for all the video publishing and comment publishing events:\n INSERT OVERWRITE TABLE actions_users SELECT u.id, actions.date FROM ( SELECT av.uid AS uid FROM action_video av WHERE av.date = '2008-06-03' UNION ALL SELECT ac.uid AS uid FROM action_comment ac WHERE ac.date = '2008-06-03' ) actions JOIN users u ON(u.id = actions.uid); Array Operations Array columns in tables can be as follows:\nCREATE TABLE array_table (int_array_column ARRAY\u0026lt;INT\u0026gt;); Assuming that pv.friends is of the type ARRAY(i.e. it is an array of integers), the user can get a specific element in the array by its index as shown in the following command:\n SELECT pv.friends[2] FROM page_views pv; The select expression gets the third item in the pv.friends array.\nThe user can also get the length of the array using the size function as shown below:\n SELECT pv.userid, size(pv.friends) FROM page_view pv; Map (Associative Arrays) Operations Maps provide collections similar to associative arrays. Such structures can only be created programmatically currently. We will be extending this soon. For the purpose of the current example assume that pv.properties is of the type map\u0026lt;String, String\u0026gt; i.e. it is an associative array from strings to string. Accordingly, the following query:\n INSERT OVERWRITE page_views_map SELECT pv.userid, pv.properties['page type'] FROM page_views pv; can be used to select the \u0026lsquo;page_type\u0026rsquo; property from the page_views table.\nSimilar to arrays, the size function can also be used to get the number of elements in a map as shown in the following query:\n SELECT size(pv.properties) FROM page_view pv; Custom Map/Reduce Scripts Users can also plug in their own custom mappers and reducers in the data stream by using features natively supported in the Hive language. for example, in order to run a custom mapper script - map_script - and a custom reducer script - reduce_script - the user can issue the following command which uses the TRANSFORM clause to embed the mapper and the reducer scripts.\nNote that columns will be transformed to string and delimited by TAB before feeding to the user script, and the standard output of the user script will be treated as TAB-separated string columns. User scripts can output debug information to standard error which will be shown on the task detail page on hadoop.\n FROM ( FROM pv_users MAP pv_users.userid, pv_users.date USING 'map_script' AS dt, uid CLUSTER BY dt) map_output INSERT OVERWRITE TABLE pv_users_reduced REDUCE map_output.dt, map_output.uid USING 'reduce_script' AS date, count; Sample map script (weekday_mapper.py )\nimport sys import datetime for line in sys.stdin: line = line.strip() userid, unixtime = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print ','.join([userid, str(weekday)]) Of course, both MAP and REDUCE are \u0026ldquo;syntactic sugar\u0026rdquo; for the more general select transform. The inner query could also have been written as such:\n SELECT TRANSFORM(pv_users.userid, pv_users.date) USING 'map_script' AS dt, uid CLUSTER BY dt FROM pv_users; Schema-less map/reduce: If there is no \u0026ldquo;AS\u0026rdquo; clause after \u0026ldquo;USING map_script\u0026rdquo;, Hive assumes the output of the script contains 2 parts: key which is before the first tab, and value which is the rest after the first tab. Note that this is different from specifying \u0026ldquo;AS key, value\u0026rdquo; because in that case value will only contains the portion between the first tab and the second tab if there are multiple tabs.\nIn this way, we allow users to migrate old map/reduce scripts without knowing the schema of the map output. User still needs to know the reduce output schema because that has to match what is in the table that we are inserting to.\n FROM ( FROM pv_users MAP pv_users.userid, pv_users.date USING 'map_script' CLUSTER BY key) map_output INSERT OVERWRITE TABLE pv_users_reduced REDUCE map_output.dt, map_output.uid USING 'reduce_script' AS date, count; Distribute By and Sort By: Instead of specifying \u0026ldquo;cluster by\u0026rdquo;, the user can specify \u0026ldquo;distribute by\u0026rdquo; and \u0026ldquo;sort by\u0026rdquo;, so the partition columns and sort columns can be different. The usual case is that the partition columns are a prefix of sort columns, but that is not required.\n FROM ( FROM pv_users MAP pv_users.userid, pv_users.date USING 'map_script' AS c1, c2, c3 DISTRIBUTE BY c2 SORT BY c2, c1) map_output INSERT OVERWRITE TABLE pv_users_reduced REDUCE map_output.c1, map_output.c2, map_output.c3 USING 'reduce_script' AS date, count; Co-Groups Amongst the user community using map/reduce, cogroup is a fairly common operation wherein the data from multiple tables are sent to a custom reducer such that the rows are grouped by the values of certain columns on the tables. With the UNION ALL operator and the CLUSTER BY specification, this can be achieved in the Hive query language in the following way. Suppose we wanted to cogroup the rows from the actions_video and action_comments table on the uid column and send them to the \u0026lsquo;reduce_script\u0026rsquo; custom reducer, the following syntax can be used by the user:\n FROM ( FROM ( FROM action_video av SELECT av.uid AS uid, av.id AS id, av.date AS date UNION ALL FROM action_comment ac SELECT ac.uid AS uid, ac.id AS id, ac.date AS date ) union_actions SELECT union_actions.uid, union_actions.id, union_actions.date CLUSTER BY union_actions.uid) map INSERT OVERWRITE TABLE actions_reduced SELECT TRANSFORM(map.uid, map.id, map.date) USING 'reduce_script' AS (uid, id, reduced_val); ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/tutorial_27362061/","tags":null,"title":"Apache Hive : Tutorial"},{"categories":null,"contents":"Apache Hive : Type Qualifiers in Hive Intro Hive will need to support some kind of type qualifiers/parameters in its type metadata to be able to enforce type features such as decimal precision/scale or char/varchar length and collation. This involves changes to the PrimitiveTypeEntry/TypeInfo/ObjectInspectors, possibly metastore changes,\nMy impression is that the actual enforcement of the type qualifiers should be done by the ObjectInspectors/Converters/casts operations. It should be ok to do col * col when col is a decimal(2) value of 99, it would fail if you try to cast the result to decimal(2) or try to insert it to a decimal(2) column. Initial prototype work There is some initial work on this in an initial patch for HIVE-4844. There is a BaseTypeParams object to represent type parameters, with VarcharTypeParams as a varchar-specific subclass containing the string length. The PrimitiveTypeEntryTypeInfo/ObjectInspectors are augmented to contain this BaseTypeParams object if the column/expression has type parameters. There also needed to be additional PrimitiveTypeEntryTypeInfo/ObjectInspectors factory methods which take a BaseTypeParams parameter.\nSome issues/questions from this:\n  There are some cases where Hive is trying to create a PrimitiveTypeEntry or ObjectInspector based on a Java class type. Such as ObjectInspectorFactory.getReflectionObjectInspector(). In these cases, there would be no data type params available to add to the PrimitiveTypeEntry/ObjectInspector, in which case we might have to default to some kind of type attributes - max precision decimal or max length char/varchar. This happens in a few places:\n TypedSerDe (used by ThriftByteStreamTypedSerDe). Might be ok since if it\u0026rsquo;s just using Thrift types. S3LogDeserializer (in contrib). Might be ok, looks like it is only a deserializer, and for a custom S3 struct. MetadataTypedColumnsetSerDe. Might be ok, looks it might just use strings. GenericUDFUtils.ConversionHelper.ConversionHelper(), as well as GenericUDFBridge. This is used by old-style UDFs, in particular for the return type of the UDF. So in the general case it is not always possible to have type parameters for the return type of UDFs. GenericUDFs would be required if we want to be able to return a char length/decimal precision as part of the return type metadata, since they can customize the return type ObjectInspector.    If cast operators remain implemented as UDFs, then the UDF should probably be implemented as a Generic UDF so that the return type ObjectInspector can be set with the type params. In addition, the type parameters need to be somehow passed into the cast UDF before its initialize() method is called.\n  Hive code does a lot of pointer-based equality using PrimitiveTypeEntry/TypeInfo/ObjectInspector objects. So a varchar(15) object inspector is not equal to a varchar(10). This may have some advantages such as requiring conversion/length enforcement in this case, but it seems like this may not always be desirable behavior.\n  MetaStore Changes There are a few different options here:\nNo metastore changes The type qualifiers could simply be stored as part of the type string for a column. The qualifiers would be validated during when creating/altering the column, and they would need to be parsed when creating TypeInfo/ObjectInspectors. This approach has the advantage that no additional metastore changes would be needed, though it would be more difficult to query these type attributes if someone is querying the metastore directly, since parsing of the type string is required.\nAdd additional columns to COLUMNS_V2 table in metastore This approach would be similar to the attributes in the INFORMATION_SCHEMA.COLUMNS that some DBMS catalog tables have, such as those listed below:\nWe could add new columns to the COLUMNS_V2 table for any type qualifiers we are trying to support (initially looks like CHARACTER_MAXIMUM_LENGTH, NUMERIC_PRECISION, NUMERIC_SCALE). Advantages to this would be that it is easier to query these parameters than the first approach, though types with no parameters would still have these columns (set to null).\nNew table with type qualifiers in megastore Rather than having to change the COLUMNS_V2 table we could have a new table to hold the type qualifier information. This would mean no additions to the existing COLUMNS_V2 table, and non-parameterized types would have no rows in this new table. But it would mean an extra query to this new table any time we are fetching column metadata from the metastore.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/type-qualifiers-in-hive_33298524/","tags":null,"title":"Apache Hive : Type Qualifiers in Hive"},{"categories":null,"contents":"Apache Hive : Union Optimization Consider the query\nselect * from\n(subq1\nUNION ALL\nsub2) u;\nIf the parents to union were map reduce jobs, they will write the output to temporary files. The Union will then read the rows from these temporary files and write to a final directory. In effect, the results are read and written twice unnecessarily. We can avoid this by directly writing to the final directory.\nDesign The optimization will be applied if the following hold:\n The Union is followed by a select * and then a file sink. All parents of Union are file sinks.  Union may have more than 2 parents.\nLet\u0026rsquo;s say the output directory of the final file sink was dir_final. We will replace the output directories of subq1 and subq2 with dir_final/subquery_1 and dir_final/subquery_2, respectively. All other properties of the final file sink like gatherStats, etc. will also be copied. After this, we remove the union and everything below it.\nThe optimization is important for https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization , but should also be useful in other cases.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/union-optimization_29688910/","tags":null,"title":"Apache Hive : Union Optimization"},{"categories":null,"contents":"Apache Hive : Unit Testing Hive SQL  Motivations Challenges Modularisation  Encapsulation of column level logic Encapsulation of set level logic   Tools and frameworks Useful practices Relevant issues Other Hive unit testing concerns  Motivations Hive is widely applied as a solution to numerous distinct problem types in the domain of big data. Quite clearly it is often used for the ad hoc querying of large datasets. However it is also used to implement ETL type processes. Unlike ad hoc queries, the Hive SQL written for ETLs has some distinct attributes:\n It is executed repeatedly, usually on a schedule. It is frequently a large, complex body of code. It has much greater longevity, persisting in an organisation’s code base for long periods of time. It is frequently modified over time, often in an organic manner to meet changing business requirements. It is critical to the operation of the organisation in that it generates valuable data.  Code exhibiting such properties is a strong candidate for unit test coverage as it is very prone to bugs, errors, and accidental breakage, all of which can represent risk to an organisation.\nChallenges There are a number of challenges posed by both Hive and Hive SQL that can make it difficult to construct suites of unit tests for Hive based ETL systems. These can be broadly described as follows:\n Defining boundaries between components: How can and how should a problem be decomposed into smaller, testable units. The ability to do this is limited by the set of language features provided by Hive SQL. Harness provision: Providing a local execution environment that seamlessly supports Hive’s features in a local IDE setting (UDFs etc). Ideally the harness should have no environmental dependencies such as a local Hive or Hadoop installation. Developers should be able to simply check out a project and run the tests. Speed of execution: The goal is to have large numbers of isolated, small tests. Test isolation requires frequent setup and teardown and the costs incurred are multiplied the number of tests. The Hive CLI is fairly heavy process to repeatedly start and stop and so some Hive test frameworks attempt to optimise this aspect of test execution.  Modularisation By modularising processes implemented using Hive they become easier to test effectively and more resilient to change. Although Hive provides a number of vectors for modularisation it is not always clear how a large process can be decomposed. Features for encapsulation of query logic into components is separated into two perpendicular concerns: column level logic, and set level logic. Column level logic refers to the expressions applied to individual columns or groups of columns in the query, commonly described as ‘functions’. Set level logic concerns Hive SQL constructs that manipulate groupings of data such as: column projection with SELECT, GROUP BY aggregates, JOINs, ORDER BY sorting, etc. In either case we expect individual components to live in their own source file or deployable artifact and imported as needed by the composition. For Hive SQL-based components, the [SOURCE](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveInteractiveShellCommands) command provides this functionality.\nEncapsulation of column level logic In the case of column level logic Hive provides both UDFs and macros that allow the user to extract and reuse the expressions applied to columns. Once defined, UDFs and Macros can be readily isolated for testing. UDFs can be simply tested with existing Java/Python unit test tools such as JUnit whereas Macros require a Hive command line interface to execute the macro declaration and then exercise it with some sample SELECT statements.\nEncapsulation of set level logic Unlike column level logic, it is much less obvious how best to encapsulate and compose collections of set based logic. Consider the following example of a single complex query comprising joins, groupings, and column projections:\nMonolithic query\nSELECT ... FROM ( -- Query 1 SELECT ... FROM ( -- Query 2 SELECT ... FROM ( -- Query 3 SELECT ... FROM a WHERE ... -- Query 4 ) A LEFT JOIN ( -- Query 3 SELECT ... FROM b -- Query 5 ) B ON (...) -- Query 3 ) ab FULL OUTER JOIN ( -- Query 2 SELECT ... FROM c WHERE ... -- Query 6 ) C ON (...) -- Query 2 ) abc LEFT JOIN ( -- Query 1 SELECT ... FROM d WHERE ... -- Query 7 ) D ON (...) -- Query 1 GROUP BY ...; -- Query 1 This query has a very broad set of responsibilities which cannot be easily verified in isolation. On closer inspection it appears that it is in fact formed of at least 7 distinct queries. To effectively unit test this process we must encapsulate each of the subqueries into separate components so that they can be tested independently. To achieve this there are a number of approaches are open to us including:\n Sequential execution of components with intermediate tables. Views. Variable substitution of query fragments. Functional/procedural SQL approaches.  Limited testing suggests that the VIEW approach is more effective than using sequential execution of components with intermediate tables, both in terms of elegance and performance. Intermediate table solutions (including TEMPORARY tables) take longer to run, generate more I/O, and restrict query optimization opportunities. It should also be noted that views do not appear to suffer from performance issues as often prophesied; in fact the execution plans and times for views and monolithic queries were comparable. Variable substitution was also suggested as an approach for modularizing large queries, but upon inspection it was found to be unsuitable as an additional bash file is required which would make testing more complex. HPL/SQL was also considered, however it does not have the necessary pipelined function feature required for query modularization. Tools and frameworks When constructing tests it is helpful to have a framework that simplifies the declaration and execution of tests. Typically these tools allow the specification of many of the following:\n Execution environment configuration: usually hiveconf and hivevar parameters. Declaring input test data: creating or selecting files that back some source tables. Definition of the executable component of the test: normally the SQL script under test. Expectations: These can be in the form of a reference data file or alternatively fine grained assertions can be made with further queries.  The precise details are of course framework specific, but generally speaking tools manage the full lifecycle of tests by composing the artifacts provided by the developer into a sequence such as:\n Configure Hive execution environment. Setup test input data. Execute SQL script under test. Extract data written by the executed script. Make assertions on the data extracted.  At this time there are are a number of concrete approaches to choose from:\n HiveRunner: Test cases are declared using Java, Hive SQL and JUnit and can execute locally in your IDE. This library focuses on ease of use and execution speed. No local Hive/Hadoop installation required. Provides full test isolation, fine grained assertions, and seamless UDF integration (they need only be on the project classpath). The metastore is backed by an in-memory database to increase test performance. beetest: Test cases are declared using Hive SQL and \u0026lsquo;expected\u0026rsquo; data files. Test suites are executed using a script on the command line. Apparently requires HDFS to be installed in the environment in which the tests are executed. hive_test: Test cases are declared using Java, Hive SQL and JUnit and can execute locally in your IDE. HiveQLUnit: Test your Hive scripts inside your favourite IDE. Appears to use Spark to execute the tests. How to utilise the Hive project\u0026rsquo;s internal test framework.  Useful practices The following Hive specific practices can be used to make processes more amenable to unit testing and assist in the simplification of individual tests.\n Modularise large or complex queries into multiple smaller components. These are easier to comprehend, maintain, and test. Use macros or UDFs to encapsulate repeated or complex column expressions. Use Hive variables to decouple SQL scripts from specific environments. For example it might be wise to use LOCATION ${myTableLocation} in preference to LOCATION /hard/coded/path. Keep the scope of tests small. Making coarse assertions on the entire contents of a table is brittle and has a high maintenance requirement. Use the [SOURCE](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveInteractiveShellCommands) command to combine multiple smaller SQL scripts. Test macros and the integration of UDFs by creating simple test tables and applying the functions to columns in those tables. Test UDFs by invoking the lifecycle methods directly (initialize, evaluate, etc.) in a standard testing framework such as JUnit.  Relevant issues  HIVE-12703: CLI agnostic HQL import command implementation  Other Hive unit testing concerns Although not specifically related to Hive SQL, tooling exists for the testing of other aspects of the Hive ecosystem. In particular the BeeJU project provides JUnit rules to simplify the testing of integrations with the Hive Metastore and HiveServer2 services. These are useful, if for example, you are developing alternative data processing frameworks or tools that aim to leverage Hive\u0026rsquo;s metadata features.\n Comments:            Disclosure: The tools are listed according to level of experience I have with each tool, HiveRunner being the tool that I have used the most. Furthermore, I have previously contributed to the HiveRunner project. I\u0026rsquo;ve also been involved with the BeeJU project.    Posted by teabot at Nov 11, 2015 10:20 | | Where does Capybara fit into this (it at all)?\nPosted by teabot at Dec 03, 2015 11:30 |\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/unit-testing-hive-sql_61328063/","tags":null,"title":"Apache Hive : Unit Testing Hive SQL"},{"categories":null,"contents":"Apache Hive : UpdatableViews Proposal Hive will consider a view updatable if:\n The view refers to exactly one base table or updatable view in the FROM clause without a WHERE clause. Each column in the view is a column in the underlying table/updatable view with no underlying columns duplicated. Views must have the same partition columns as the underlying table/updatable view.  When inserting into a view:\n If a view does not specify all underlying columns, NULL will be inserted for each column not specified.  Given\n create table t1 (id int, key string, value string) partitioned by (ds string, hr string); create view v partitioned on (ds, hr) as select id, value, ds, hr from t1; we can insert into v where NULL will be used for key.\nNotes:\n Should we try to support views like (create view v partitioned on (ds, hr) as select id, value, ds where ds=\u0026lsquo;2011-01-01\u0026rsquo; and hr=\u0026lsquo;12\u0026rsquo;) for updating, where we infer values for ds and hr? With non-dynamic partitioning, do we require the partition be on each view in the updatable view chain? This seems burdensome if you don\u0026rsquo;t have write access to all the views? When we specify dynamic partitions for a view, do we create partitions on each view in a chain of updatable views? If we don\u0026rsquo;t, there may be strange behavior where SHOW PARTITIONS may not show anything on a view, but we can insert into such partitions of a view. If we do, drop partition on the view actually does nothing to the data.  See Hive Views for general information about views.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/updatableviews_27824044/","tags":null,"title":"Apache Hive : UpdatableViews"},{"categories":null,"contents":"Apache Hive : User and Group Filter Support with LDAP Atn Provider in HiveServer2  User and Group Filter Support with LDAP  Group Membership  hive.server2.authentication.ldap.groupDNPattern hive.server2.authentication.ldap.groupFilter hive.server2.authentication.ldap.groupMembershipKey hive.server2.authentication.ldap.groupClassKey   User Search List  hive.server2.authentication.ldap.userDNPattern hive.server2.authentication.ldap.userFilter   Custom Query String  hive.server2.authentication.ldap.customLDAPQuery Support for Groups in Custom LDAP Query   Order of Precedence    User and Group Filter Support with LDAP Starting in Hive 1.3.0, HIVE-7193 adds support in HiveServer2 for\n LDAP Group filters LDAP User filters Custom LDAP Query support.  Filters greatly enhance the functionality of the LDAP Authentication provider. They allow Hive to restrict the set of users allowed to connect to HiveServer2.\nSee Authentication/Security Configuration for general information about configuring authentication for HiveServer2. Also see Hive Configuration Properties – HiveServer2 for individual configuration parameters discussed below.\nGroup Membership This enables HiveServer2 to enforce group membership for users. The authentication request will succeed if the user belongs to one or more of the groups listed in the Hive configuration file. If the user does not belong to at least one of the groups listed, the user authentication fails.\nFour configuration parameters support group-membership based authentication:\n hive.server2.authentication.ldap.groupDNPattern hive.server2.authentication.ldap.groupFilter hive.server2.authentication.ldap.groupMembershipKey (version 2.1.0 via HIVE-13295) hive.server2.authentication.ldap.groupClassKey (version 2.1.0 via HIVE-13295)  hive.server2.authentication.ldap.groupDNPattern This value represents a pattern for “distinguishedName” (DN) for groups in the directory. This value could be a single DN if the LDAP Group entities are co-located or could be a colon separated list of all DN patterns if the groups are scattered across different trees.\nEach DN pattern can contain a “%s” in it that will be substituted with the group name (from the group filter) by the provider for group search queries.\nExample 1 (single DN):\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt; hive.server2.authentication.ldap.groupDNPattern \u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;CN=%s,OU=Groups,DC=apache,DC=org\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; This indicates that all LDAP group entries are under the directory root “OU=Groups,DC=apache,DC=org”.\nThe LDAP Authentication Provider replaces the %s with the group name in the LDAP queries to locate the group entry. For example, if a group named “group1” is being queried for, it uses \u0026ldquo;CN=group1,OU=Groups,DC=apache,DC=org\u0026rdquo;.\nExample 2 (two DNs):\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt; hive.server2.authentication.ldap.groupDNPattern \u0026lt;/name\u0026gt; \u0026lt;value\u0026gt; CN=%s,OU=Groups,DC=apache,DC=org:uid=%s,CN=Users,DC=apache,DC=org \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; The above pattern advises the LDAPAtnProvider that LDAP group entities can exist in two separate trees in the directory and can have different attributes in their DNs. (Note the colon separator.)\nhive.server2.authentication.ldap.groupFilter This value represents the group name filter that is to be enforced by the LDAPAtnProvider. All individual groups are represented using a comma separated list. The user MUST belong to one or more of these groups for the authentication request to succeed.\nExample: \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.authentication.ldap.groupFilter\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;group1,group2\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; hive.server2.authentication.ldap.groupMembershipKey This value represents the LDAP attribute on group entries in LDAP that indicates its members. (Available starting in version 2.1.0.)\nThere could be multiple entries for this attribute, one for each of its members. By default, the LDAP authentication provider assumes \u0026ldquo;member\u0026rdquo; to search for users. To alter this default, set a value/key for property for the provider to accurately search for group members.\nhive.server2.authentication.ldap.groupClassKey This value represents the LDAP objectClass each of the groups implements in LDAP. By default, the LDAP Authentication provider uses \u0026ldquo;groupOfNames\u0026rdquo; in its search for groups. (Available starting in version 2.1.0.)\nThe properties above assist in correctly finding user-group associations in LDAP.\nExample: If the LDAP Group \u0026ldquo;testGroup\u0026rdquo; has the following attributes, Hive\u0026rsquo;s LDAP Authentication provider will not be able to find group members. Setting the 2 properties will help with this.\n dn:uid=testGroup,ou=Groups,dc=domain,dc=com objectClass: group objectClass: top memberUid: uid=testUser1,ou=Users,dc=domain,dc=com memberUid: uid=testUser2,ou=Users,dc=domain,dc=com cn: HiveUserGroup  \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.authentication.ldap.groupMembershipKey\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;memberUid\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.authentication.ldap.groupClassKey\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;group\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; User Search List This enables HiveServer2 to restrict access to a specified list of users. If the user being authenticated is not part of this userlist, access will be denied.\nTwo configuration parameters support this feature:\n hive.server2.authentication.ldap.userDNPattern hive.server2.authentication.ldap.userFilter  hive.server2.authentication.ldap.userDNPattern This value represents a pattern for “distinguishedName” (DN) for users in the directory. This value could be a single DN if the LDAP User entities are co-located within a single root or could be a colon separated list of all DN patterns if the users are scattered across different trees/forests in the directory.\nEach DN pattern can contain a “%s” in it that will be substituted with the username (from the user filter) by the provider for user search queries.\nExample 1 (single DN):\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt; hive.server2.authentication.ldap.userDNPattern \u0026lt;/name\u0026gt; \u0026lt;value\u0026gt; CN=%s,CN=Users,DC=apache,DC=org \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; In the example above, all users are co-located under a single root “CN=Users,DC=apache,DC=org”. To search for user “foo”, LDAPAtnProvider attempts to find the user with DN like “CN=foo,CN=Users,DC=apache,DC=org”.\nExample 2 (two DNs):\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt; hive.server2.authentication.ldap.userDNPattern \u0026lt;/name\u0026gt; \u0026lt;value\u0026gt; CN=%s,OU=Users,DC=apache,DC=org:uid=%s,CN=UnixUsers,DC=apache,DC=org \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; The above pattern advises the LDAPAtnProvider that LDAP user entities can exist in two separate trees in the directory and can have different attributes in their DNs. (Note the colon separator.)\nhive.server2.authentication.ldap.userFilter This is a comma separated list of usernames to grant access to. The Atn provider grants access if the user being authenticated is part of this list, and denies access otherwise.\nExample:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt; hive.server2.authentication.ldap.userFilter \u0026lt;/name\u0026gt; \u0026lt;value\u0026gt; hive-admin,hive,hivetest,hive-user \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Custom Query String There are several LDAP implementations available for commercial use, with no standard set of attributes within each implementation. If either of the above filters does not meet the requirements for some unforeseen reasons, HiveServer2 can use a user-specified LDAP Query string to execute against the LDAP server. This configured query is expected to return a set of DNs that represent individual users (see below for support for groups). The returned result will then be used to adjudicate a GRANT/DENY decision to the authenticating user. To support this configuration, a new configuration property has been introduced.\nhive.server2.authentication.ldap.customLDAPQuery Example:\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.authentication.ldap.customLDAPQuery\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;![CDATA[(\u0026amp;(objectClass=person)(|(memberOf=CN=Domain Admins,CN=Users,DC=apache,DC=org)(memberOf=CN=Administrators,CN=Builtin,DC=apache,DC=org)))]]\u0026gt; \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; The above query returns all users that are members of one of the groups (Domain Admins or Administrators). This offers a lot more flexibility that allows Hive users to customize the LDAP configuration for their implementation of LDAP.\nSupport for Groups in Custom LDAP Query Version information\nAvailable starting with Hive 2.1.1 (see HIVE-14513).\nIt is not always straightforward to be able to author queries that return users. For example, to allow \u0026ldquo;all users from group1 and group2\u0026rdquo; to be authenticated, the LDAP query has to return a union of all members of group1 and group2.\nOne common configuration is that groups contain a list of users:\n \u0026quot;dn: uid=group1,ou=Groups,dc=example,dc=com\u0026quot;, \u0026quot;distinguishedName: uid=group1,ou=Groups,dc=example,dc=com\u0026quot;, \u0026quot;objectClass: top\u0026quot;, \u0026quot;objectClass: groupOfNames\u0026quot;, \u0026quot;objectClass: ExtensibleObject\u0026quot;, \u0026quot;cn: group1\u0026quot;, \u0026quot;ou: Groups\u0026quot;, \u0026quot;sn: group1\u0026quot;, \u0026quot;member: uid=user1,ou=People,dc=example,dc=com\u0026quot;, The query\n (\u0026amp;(objectClass=groupOfNames)(|(cn=group1)(cn=group2))) will return the entries\n uid=group1,ou=Groups,dc=example,dc=com uid=group2,ou=Groups,dc=example,dc=com but there is no means to form a query that would return just the values of \u0026ldquo;member\u0026rdquo; attributes. (LDAP APIs allow filtering of the attributes on the result set.)\nTo allow for such queries to return user DNs for the members of the group instead of the group DN itself, as of Hive release 2.1.1 the LDAP authentication provider will (re)use the configuration property hive.server2.authentication.ldap.groupMembershipKey. This property represents the attribute name that represents the user DN on the Group entry. In the example from above, that attribute is \u0026ldquo;member\u0026rdquo;.\nThis allows the Hive LDAP authentication provider to specify a query that returns groups and individual users as below (all users of group1 + the user user4 will be allowed to authenticate):\n\u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.server2.authentication.ldap.customLDAPQuery\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;\u0026lt;![CDATA[(|(\u0026amp;(objectClass=groupOfNames)(cn=group1))(\u0026amp;(objectClass=person)(sn=user4)))]]\u0026gt; \u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Order of Precedence The group membership parameters can be used in conjunction with the user lists to enforce a stricter access. The LDAP Atn provider adjudicates authentication decisions according to the following criteria:\n If hive-site.xml contains “hive.server2.authentication.ldap.customLDAPQuery”, only the results of this query are used to adjudicate an authentication decision. All other property values (*.groupDNPattern, *.groupFilter, *.userDNPattern, *.userFilter) are ignored entirely. If the *.groupFilter and *.userFilter parameters are both specified in the configuration file, access is granted if and only if the user being authenticated satisfies both conditions; access is denied otherwise. So the user has to be listed in the *.userFilter and the user MUST also belong to one of the groups listed in the *.groupFilter. If only one of the filters ( ( *.userDNPattern + *.userFilter ) || ( *.groupDNPattern + *.groupFilter ) ) is specified, a decision is adjudicated based on whether the user being authenticated satisfies the specified filter. If neither *.groupFilter nor *.userFilter is specified in the configuration file, the provider attempts to search for the user in the LDAP directory within the baseDN tree. Access is granted if user has been found, and denied otherwise. IMPORTANT: This implementation is a little more stringent compared to the prior implementation. In the prior implementation, if the baseDN was not provided, authentication would be granted if the provider is able to bind to LDAP with the user  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/user-and-group-filter-support-with-ldap-atn-provider-in-hiveserver2_58852417/","tags":null,"title":"Apache Hive : User and Group Filter Support with LDAP Atn Provider in HiveServer2"},{"categories":null,"contents":"Apache Hive : User FAQ Hive User FAQ  Hive User FAQ  General  I see errors like: Server access Error: Connection timed out url=http://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz How to change the warehouse.dir location for older tables? When running a JOIN query, I see out-of-memory errors. I am using MySQL as metastore and I see errors: \u0026ldquo;com.mysql.jdbc.exceptions.jdbc4.!CommunicationsException: Communications link failure\u0026rdquo; Does Hive support Unicode?   Hive SQL  Are Hive SQL identifiers (e.g. table names, column names, etc) case sensitive? What are the maximum allowed lengths for Hive SQL identifiers?   Importing Data into Hive  How do I import XML data into Hive? How do I import CSV data into Hive? How do I import JSON data into Hive? How do I import Thrift data into Hive? How do I import Avro data into Hive? How do I import delimited text data into Hive? How do I import fixed-width data into Hive? How do I import ASCII logfiles (HTTP, etc) into Hive?   Exporting Data from Hive Hive Data Model  What is the difference between a native table and an external table? What are dynamic partitions? Can a Hive table contain data in more than one format? Is it possible to set the data format on a per-partition basis?   JDBC Driver  Does Hive have a JDBC Driver?   ODBC Driver  Does Hive have an ODBC driver?      General I see errors like: Server access Error: Connection timed out url=http://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz Run the following commands:\ncd ~/.ant/cache/hadoop/core/sources\nwget \u0026lt;http://archive.apache.org/dist/hadoop/core/hadoop-0.20.1/hadoop-0.20.1.tar.gz\u0026gt;\nHow to change the warehouse.dir location for older tables? To change the base location of the Hive tables, edit the hive.metastore.warehouse.dir param. This will not affect the older tables. Metadata needs to be changed in the database (MySQL or Derby). The location of Hive tables is in table SDS and column LOCATION.\nWhen running a JOIN query, I see out-of-memory errors. This is usually caused by the order of JOIN tables. Instead of \u0026ldquo;FROM tableA a JOIN tableB b ON \u0026hellip;\u0026rdquo;, try \u0026ldquo;FROM tableB b JOIN tableA a ON \u0026hellip;\u0026rdquo;. NOTE that if you are using LEFT OUTER JOIN, you might want to change to RIGHT OUTER JOIN. This trick usually solve the problem - the rule of thumb is, always put the table with a lot of rows having the same value in the join key on the rightmost side of the JOIN.\nI am using MySQL as metastore and I see errors: \u0026ldquo;com.mysql.jdbc.exceptions.jdbc4.!CommunicationsException: Communications link failure\u0026rdquo; This is usually caused by MySQL servers closing connections after the connection is idling for some time. Run the following command on the MySQL server will solve the problem \u0026ldquo;set global wait_status=120;\u0026rdquo;\n When using MySQL as a metastore I see the error \u0026ldquo;com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Specified key was too long; max key length is 767 bytes\u0026rdquo;.\nThis is a known limitation of MySQL 5.0 and UTF8 databases. One option is to use another character set, such as \u0026lsquo;latin1\u0026rsquo;, which is known to work.  Does Hive support Unicode? You can use Unicode string on data/comments, but cannot use for database/table/column name.\nYou can use UTF-8 encoding for Hive data. However, other encodings are not supported (HIVE-7142 introduce encoding for LazySimpleSerDe, however, the implementation is not complete and not address all cases).\nHive SQL Are Hive SQL identifiers (e.g. table names, column names, etc) case sensitive? No. Hive is case insensitive.\nExecuting:\n SELECT * FROM MyTable WHERE myColumn = 3\n is strictly equivalent to\n select * from mytable where mycolumn = 3\n What are the maximum allowed lengths for Hive SQL identifiers? Importing Data into Hive How do I import XML data into Hive? How do I import CSV data into Hive? How do I import JSON data into Hive? How do I import Thrift data into Hive? How do I import Avro data into Hive? How do I import delimited text data into Hive? How do I import fixed-width data into Hive? How do I import ASCII logfiles (HTTP, etc) into Hive? Exporting Data from Hive Hive Data Model What is the difference between a native table and an external table? What are dynamic partitions? Can a Hive table contain data in more than one format? Is it possible to set the data format on a per-partition basis? JDBC Driver Does Hive have a JDBC Driver? Yes. Look out to the hive-jdbc jar. The driver is \u0026lsquo;org.apache.hadoop.hive.jdbc.HiveDriver\u0026rsquo;.\nIt supports two modes: a local mode and a remote one.\nIn the remote mode it connects to the hive server through its Thrift API. The JDBC url to use should be of the form: \u0026lsquo;jdbc:hive://hostname:port/databasename\u0026rsquo;\nIn the local mode Hive is embedded. The JDBC url to use should be \u0026lsquo;jdbc:hive://\u0026rsquo;.\nODBC Driver Does Hive have an ODBC driver? Yes. Many third-party vendors provide ODBC drivers.\nSimba provides both ODBC and JDBC drivers, and developed many of the drivers for other companies.\nhttp://www.simba.com/drivers/hive-odbc-jdbc/\nMicrosoft provides an ODBC driver for Hive in HDInsight and local clusters.\nhttps://azure.microsoft.com/en-us/documentation/articles/hdinsight-connect-excel-hive-odbc-driver/\nhttps://www.microsoft.com/en-ca/download/details.aspx?id=40886 Hortonworks provides an ODBC driver for HDP\nhttp://hortonworks.com/hdp/addons/\nCloudera provides an ODBC driver for Cloudera Enterprise\nhttp://www.cloudera.com/downloads/connectors/hive/odbc/2-5-12.html\nMapR provides ODBC drivers\nhttp://doc.mapr.com/display/MapR/Hive+ODBC+Connector\nProgress offers a DataDirect ODBC driver\nhttps://www.progress.com/odbc/apache-hadoop-hive\nAmazon provides ODBC/JDBC drivers for Amazon EMR\nhttps://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-bi-tools.html\n ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/user-faq_27362095/","tags":null,"title":"Apache Hive : User FAQ"},{"categories":null,"contents":"Apache Hive : UserGuide NOTE: This page is deprecated and merged into GettingStarted.\nUser Guide The query language specification is available at LanguageManual. Also see, GettingStarted for setup instructions.\nSupported Features Usage Examples Creating tables MovieLens User Ratings CREATE TABLE u_data ( userid INT, movieid INT, rating INT, unixtime STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' STORED AS TEXTFILE; Apache Access Log Tables add jar ../build/contrib/hive_contrib.jar; CREATE TABLE apachelog ( host STRING, identity STRING, user STRING, time STRING, request STRING, status STRING, size STRING, referer STRING, agent STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.RegexSerDe' WITH SERDEPROPERTIES ( \u0026quot;input.regex\u0026quot; = \u0026quot;([^]*) ([^]*) ([^]*) (-|\\\\[^\\\\]*\\\\]) ([^ \\\u0026quot;]*|\\\u0026quot;[^\\\u0026quot;]*\\\u0026quot;) (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\u0026quot;]*|\\\u0026quot;[^\\\u0026quot;]*\\\u0026quot;) ([^ \\\u0026quot;]*|\\\u0026quot;[^\\\u0026quot;]*\\\u0026quot;))?\u0026quot;, \u0026quot;output.format.string\u0026quot; = \u0026quot;%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s\u0026quot; ) STORED AS TEXTFILE; Control Separated Tables CREATE TABLE mylog ( name STRING, language STRING, groups ARRAY\u0026lt;STRING\u0026gt;, entities MAP\u0026lt;INT, STRING\u0026gt;) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\001' COLLECTION ITEMS TERMINATED BY '\\002' MAP KEYS TERMINATED BY '\\003' STORED AS TEXTFILE; Loading tables MovieLens User Ratings Download and extract the data:\nwget http://www.grouplens.org/system/files/ml-data.tar+0.gz tar xvzf ml-data.tar+0.gz Load it in:\nLOAD DATA LOCAL INPATH 'ml-data/u.data' OVERWRITE INTO TABLE u_data; Running queries MovieLens User Ratings SELECT COUNT(1) FROM u_data; Running custom map/reduce jobs MovieLens User Ratings Create weekday_mapper.py:\nimport sys import datetime for line in sys.stdin: line = line.strip() userid, movieid, rating, unixtime = line.split('\\t') weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print '\\t'.join([userid, movieid, rating, str(weekday)]) Use the mapper script:\nCREATE TABLE u_data_new ( userid INT, movieid INT, rating INT, weekday INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'; INSERT OVERWRITE TABLE u_data_new SELECT TRANSFORM (userid, movieid, rating, unixtime) USING 'python weekday_mapper.py' AS (userid, movieid, rating, weekday) FROM u_data; SELECT weekday, COUNT(1) FROM u_data_new GROUP BY weekday; Note: due to a bug in the parser, you must run the \u0026ldquo;INSERT OVERWRITE\u0026rdquo; query on a single line\nUsing sampling Known Issues/Bugs ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/userguide_27362066/","tags":null,"title":"Apache Hive : UserGuide"},{"categories":null,"contents":"Apache Hive : Using TiDB as the Hive Metastore database   Why use TiDB in Hive as the Metastore database? How to create a Hive cluster with TiDB  Components required Install a Hive cluster  Step 1: Deploy a TiDB cluster Step 2: Configure Hive Step 3: Initialize metadata Step 4: Launch Metastore and test     Conclusion FAQ  Why use TiDB in Hive as the Metastore database?\nTiDB is a distributed SQL database built by PingCAP and its open-source community. It is MySQL compatible and features horizontal scalability, strong consistency, and high availability. It\u0026rsquo;s a one-stop solution for both Online Transactional Processing (OLTP) and Online Analytical Processing (OLAP) workloads.\nIn scenarios with enormous amounts of data, due to TiDB\u0026rsquo;s distributed architecture, query performance is not limited to the capability of a single machine. When the data volume reaches the bottleneck, you can add nodes to improve TiDB\u0026rsquo;s storage capacity.\nBecause TiDB is compatible with the MySQL protocol, it\u0026rsquo;s easy to switch Hive\u0026rsquo;s Metastore database to TiDB. You can use TiDB as if you were using MySQL, with almost no changes:\n For the existing Hive cluster, you can use the mysqldump tool to replicate all data in MySQL to TiDB. You can use the metadata initialization tool that comes with Hive to create a new Hive cluster  How to create a Hive cluster with TiDB Creating a Hive cluster with TiDB involves the following steps:\n Meet component requirements Install a Hive cluster  Deploy a TiDB cluster Configure Hive Initialize metadata Launch Metastore and test    Components required    Component Version     Hive 3.1.2   Hadoop 2.6.0-cdh-5.16.1   TiDB 4.0   Java Development Kit (JDK) 1.8.0_221    There are no mandatory requirements for the component versions, as long as the components are compatible with each other. After you confirm that you have successfully installed Hadoop and JDK and can use them directly, you can move on to the next step.\nInstall a Hive cluster Step 1: Deploy a TiDB cluster  To set up a TiDB cluster, refer to this document. Create a Hive user in TiDB and set a password. Create a database named hive and grant privileges to the hive user.   -- Create a database for Hive Metastore. create database hive; -- Create a user and password for Hive Metastore. create user 'hive'@'%' identified by '123456'; -- Grant privileges to the user. grant all privileges on hive.* to 'hive'@'%' identified by '123456'; -- Flush privileges. flush privileges; Set the configuration item.   set global tidb_skip_isolation_level_check=1; If you don\u0026rsquo;t set the configuration item, Metastore throws the following exception when it is running: MetaException(message:The isolation level 'SERIALIZABLE' is not supported. Set tidb_skip_isolation_level_check=1 to skip this error) Step 2: Configure Hive  Download and decompress Hive. In this example, the decompression directory for Hive is ${HIVE_HOME}. To edit the hive-site.xml configuration file, run vim ${HIVE_HOME}/conf/hive-site.xml. (The configuration items only use the minimum configuration.)  \u0026lt;configuration\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionURL\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;jdbc:mysql://host:port/hive\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;TiDB address\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionUserName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;hive\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;TiDB username\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionPassword\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;123456\u0026lt;/value\u0026gt; \u0026lt;description\u0026gt;TiDB password\u0026lt;/description\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;javax.jdo.option.ConnectionDriverName\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;com.mysql.jdbc.Driver\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.uris\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;thrift://localhost:9083\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;hive.metastore.schema.verification\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;false\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/configuration\u0026gt; To edit the hive-env.sh configuration file, run vim ${HIVE_HOME}/conf/hive-env.sh.  export HADOOP_HOME=... export JAVA_HOME=... Copy mysql-connector-java-${version}.jar to the lib directory in Hive.  cp ${MYSQL_JDBC_PATH}/mysql-connector-java-${version}.jar ${HIVE_HOME}/lib Step 3: Initialize metadata You\u0026rsquo;re performing this step to create a table for Hive metadata. The SQL script is in ${HIVE_HOME}/scripts/metastore/upgrade/mysql.\nTo initialize metadata, run the following command.\n${HIVE_HOME}/bin/schematool -dbType mysql -initSchema --verbose When schemaTool completed appears in the last line, it means the metadata is successfully initialized.\nStep 4: Launch Metastore and test  Launch Metastore.  ${HIVE_HOME}/bin/hive --service metastore Start the Hive client for testing.  ${HIVE_HOME}/bin/hive Conclusion If you use MySQL as the Hive Metastore database, as data grows in Hive, MySQL might become the bottleneck for the entire system. In this case, TiDB is a good solution, because it is compatible with the MySQL protocol and has excellent horizontal scalability. Due to its distributed architecture, TiDB far outperforms MySQL on large data sets and large numbers of concurrent queries.\nThis post showed how to deploy a Hive cluster with TiDB as the Metastore database. We hope TiDB can help you horizontally scale your Hive Metastore to meet your growing business needs.\nIn addition, if you\u0026rsquo;re interested in our MySQL-to-TiDB migration story, check out this post.\nFAQ  Hive Compatibility Version？     Hive Version Status     1.x Not tested   2.0.x Tested   2.1.x Tested and verified in production   2.3.x Tested and verified in production   3.x Tested    Do the schemas in the Hive metastore database need to be changed？For Hive version 2.1.x and 2.3.x，no schema change is needed. Does the foreign key constraint for tables in Hive the metastore database affecting migrating to TiDB? For the versions tested, foreign key constraints do not impact using TiDB as Hive metastore. How to handle MetaException(message:The isolation level 'SERIALIZABLE' is not supported. Set tidb_skip_isolation_level_check=1 to skip this error)  exception? In TiDB, execute the following command set global tidb_skip_isolation_level_check=1; to skip check.  ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/using-tidb-as-the-hive-metastore-database_158872426/","tags":null,"title":"Apache Hive : Using TiDB as the Hive Metastore database"},{"categories":null,"contents":"Apache Hive : Vectorized Query Execution  Introduction Using Vectorized Query Execution  Enabling vectorized execution Supported data types and operations Seeing whether vectorization is used for a query   Limitations Version Information  Introduction Vectorized query execution is a Hive feature that greatly reduces the CPU usage for typical query operations like scans, filters, aggregates, and joins. A standard query execution system processes one row at a time. This involves long code paths and significant metadata interpretation in the inner loop of execution. Vectorized query execution streamlines operations by processing a block of 1024 rows at a time. Within the block, each column is stored as a vector (an array of a primitive data type). Simple operations like arithmetic and comparisons are done by quickly iterating through the vectors in a tight loop, with no or very few function calls or conditional branches inside the loop. These loops compile in a streamlined way that uses relatively few instructions and finishes each instruction in fewer clock cycles, on average, by effectively using the processor pipeline and cache memory. A detailed design document is attached to the vectorized query execution JIRA, at https://issues.apache.org/jira/browse/HIVE-4160.\nUsing Vectorized Query Execution Enabling vectorized execution To use vectorized query execution, you must store your data in ORC format, and set the following variable as shown in Hive SQL (see Configuring Hive):\nset hive.vectorized.execution.enabled = true;\nVectorized execution is off by default, so your queries only utilize it if this variable is turned on. To disable vectorized execution and go back to standard execution, do the following:\nset hive.vectorized.execution.enabled = false;\nAdditional configuration variables for vectorized execution are documented in Configuration Properties – Vectorization.\nSupported data types and operations The following data types are currently supported for vectorized execution:\n tinyint smallint int bigint boolean float double decimal date timestamp (see Limitations below) string  Using other data types will cause your query to execute using standard, row-at-a-time execution.\nThe following expressions can be vectorized when used on supported types:\n arithmetic: +, -, *, /, % AND, OR, NOT comparisons \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=, =, !=, BETWEEN, IN ( list-of-constants ) as filters Boolean-valued expressions (non-filters) using AND, OR, NOT, \u0026lt;, \u0026gt;, \u0026lt;=, \u0026gt;=, =, != IS [NOT] NULL all math functions (SIN, LOG, etc.) string functions SUBSTR, CONCAT, TRIM, LTRIM, RTRIM, LOWER, UPPER, LENGTH type casts Hive user-defined functions, including standard and generic UDFs date functions (YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, UNIX_TIMESTAMP) the IF conditional expression  User-defined functions are supported using a backward compatibility bridge, so although they do run vectorized, they don\u0026rsquo;t run as fast as optimized vector implementations of built-in operators and functions. Vectorized filter operations are evaluated left-to-right, so for best performance, put UDFs on the right in an ANDed list of expressions in the WHERE clause. E.g., use\ncolumn1 = 10 and myUDF(column2) = \u0026quot;x\u0026quot;\ninstead of\nmyUDF(column2) = \u0026quot;x\u0026quot; and column1 = 10\nThis will allow the optimized filter to run first, potentially eliminating many rows from consideration, before running the UDF via the bridge. The UDF will only be run for rows that pass the filter on the left hand side of the AND operation.\nUsing a built-in operator or function that is not supported for vectorization will cause your query to run in standard row-at-a-time mode. If a compile time or run time error occurs that appears related to vectorization, please file a Hive JIRA. To work around such an error, disable vectorization by setting hive.vectorized.execution.enabled to false for the specific query that is failing, to run it in standard mode.\nVectorized support continues to be added for additional functions and expressions. If you have a request for one, please comment on this page, or open a JIRA for it.\nSeeing whether vectorization is used for a query You can verify which parts of your query are being vectorized using the explain feature. For example, when Fetch is used in the plan instead of Map, it does not vectorize and the explain output will not include the \u0026ldquo;Vectorized execution: true\u0026rdquo; notation:\ncreate table vectorizedtable(state string,id int) stored as orc; insert into vectorizedtable values('haryana',1); set hive.vectorized.execution.enabled = true; explain select count(*) from vectorizedtable; The explain output contains this:\nSTAGE PLANS: Stage: Stage-1 Map Reduce Alias -\u0026gt; Map Operator Tree: alltypesorc TableScan alias: vectorizedtable Statistics: Num rows: 1 Data size: 95 Basic stats: COMPLETE Column stats: COMPLETE Select Operator Statistics: Num rows: 1 Data size: 95 Basic stats: COMPLETE Column stats: COMPLETE Group By Operator aggregations: count() mode: hash outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE Reduce Output Operator sort order: Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE value expressions: _col0 (type: bigint) Execution mode: vectorized Reduce Operator Tree: Group By Operator aggregations: count(VALUE._col0) mode: mergepartial outputColumnNames: _col0 Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE File Output Operator compressed: false Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: ListSink The notation Vectorized execution: true shows that the operator containing that notation is vectorized. Absence of this notation means the operator is not vectorized, and uses the standard row-at-a-time execution path.\nNote: In case you want to use vectorized execution for fetch then set hive.fetch.task.conversion=none\nLimitations  Timestamps only work correctly with vectorized execution if the timestamp value is between 1677-09-20 and 2262-04-11. This limitation is due to the fact that a vectorized timestamp value is stored as a long value representing nanoseconds before/after the Unix Epoch time of 1970-01-01 00:00:00 UTC. Also see HIVE-9862.  Version Information Vectorized execution is available in Hive 0.13.0 and later (HIVE-5283).\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/vectorized-query-execution_34838326/","tags":null,"title":"Apache Hive : Vectorized Query Execution"},{"categories":null,"contents":"Apache Hive : ViewDev Hive Views  Hive Views  Use Cases Scope Syntax Implementation Sketch Issues  Stored View Definition Metastore Modeling Dependency Tracking Dependency Invalidation View Modification Fast Path Execution ORDER BY and LIMIT in view definition Underlying Partition Dependencies   Metastore Upgrades  Automatic ALTER TABLE Explicit ALTER TABLE Existing Row UPDATE      Use Cases Views (http://issues.apache.org/jira/browse/HIVE-972) are a standard DBMS feature and their uses are well understood. A typical use case might be to create an interface layer with a consistent entity/attribute naming scheme on top of an existing set of inconsistently named tables, without having to cause disruption due to direct modification of the tables. More advanced use cases would involve predefined filters, joins, aggregations, etc for simplifying query construction by end users, as well as sharing common definitions within ETL pipelines.\nScope At a minimum, we want to\n add queryable view support at the SQL language level (specifics of the scoping are under discussion in the Issues section below)  updatable views will not be supported (see the Updatable Views proposal)   make sure views and their definitions show up anywhere tables can currently be enumerated/searched/described where relevant, provide additional metadata to allow views to be distinguished from tables  Beyond this, we may want to\n expose metadata about view definitions and dependencies (at table-level or column-level) in a way that makes them consumable by metadata-driven tools  Syntax  CREATE VIEW [IF NOT EXISTS] view_name [(column_name [COMMENT column_comment], ...) ] [COMMENT table_comment] AS SELECT ... DROP VIEW view_name Implementation Sketch The basics of view implementation are very easy due to the fact that Hive already supports subselects in the FROM clause.\n For CREATE VIEW v AS view-def-select, we extend SemanticAnalyzer to behave similarly to CREATE TABLE t AS select, except that we don\u0026rsquo;t actually execute the query (we stop after plan generation). It\u0026rsquo;s necessary to perform all of plan generation (even though we\u0026rsquo;re not actually going to execute the plan) since currently some validations such as type compatibility-checking are only performed during plan generation. After successful validation, the text of the view is saved in the metastore (the simplest approach snips out the text from the parser\u0026rsquo;s token stream, but this approach introduces problems described in the issues section below). For select \u0026hellip; from view-reference, we detect the view reference in SemanticAnalyzer.getMetaData, load the text of its definition from the metastore, parse it back into an AST, prepare a QBExpr to hold it, and then plug this into the referencing query\u0026rsquo;s QB, resulting in a tree equivalent to select \u0026hellip; from (view-def-select); plan generation can then be carried out on the combined tree.  Issues Some of these are related to functionality/scope; others are related to implementation approaches. Opinions are welcome on all of them.\nStored View Definition In SQL:200n, a view definition is supposed to be frozen at the time it is created, so that if the view is defined as select * from t, where t is a table with two columns a and b, then later requests to select * from the view should return just columns a and b, even if a new column c is later added to the table. This is implemented correctly by most DBMS products.\nThere are similar issues with other kinds of references in the view definition; for example, if a table or function name can be qualified, then the reference should be bound at the time the view is created.\nImplementing this typically requires expanding the view definition into an explicit form rather than storing the original view definition text directly. Doing this could require adding \u0026ldquo;unparse\u0026rdquo; support to the AST model (to be applied after object name resolution takes place), something which is not currently present (and which is also useful to have available in general).\nHowever, storing both the expanded form and the original view definition text as well can also be useful for both DESCRIBE readability as well as functionality (see later section on ALTER VIEW v RECOMPILE).\nUpdate 7-Jan-2010: Rather than adding full-blown unparse support to the AST model, I\u0026rsquo;m taking a parser-dependent shortcut. ANTLR\u0026rsquo;s TokenRewriteStream provides a way to substitute text for token subsequences from the original token stream and then regenerate a transformed version of the parsed text. So, during column resolution, we map an expression such as \u0026ldquo;t.*\u0026rdquo; to replacement text \u0026ldquo;t.c1, t.c2, t.c3\u0026rdquo;. Then once all columns have been resolved, we regenerate the view definition using these mapped replacements. Likewise, an unqualified column reference such as \u0026ldquo;c\u0026rdquo; gets replaced with the qualified reference \u0026ldquo;t.c\u0026rdquo;. The rest of the parsed text remains unchanged.\nThis approach will break if we ever need to perform more drastic (AST-based) rewrites as part of view expansion in the future.\nMetastore Modeling The metastore model will need to be augmented in order to allow view definitions to be saved. An important issue to be resolved is whether to model this via inheritance, or just shoehorn views in as a special kind of table.\nWith an inheritance model, views and base tables would share a common base class (here called ColumnSet following the convention in the Common Warehouse Metamodel for lack of a better term):\nFor a view, most of the storage descriptor (everything other than the column names and types) would be irrelevant, so this model could be further refined with such discriminations.\nView names and table names share the same namespace with respect to uniqueness (i.e. you can\u0026rsquo;t have a table and a view with the same name), so the name key uniqueness would need to be specified at the base class level.\nAlternately, if we choose to avoid inheritance, then we could just add a new viewText attribute to the existing Table class (leaving it null for base tables):\n(Storing the view definition as a table property may not work since property values are limited to VARCHAR(767), and view definitions may be much longer than that, so we\u0026rsquo;ll need to use a LOB.)\nComparison of the two approaches:\n     Inheritance Model Flat Model     JDO Support Need to investigate how well inheritance works for our purposes Nothing special   Metadata queries from existing code/tools Existing queries for tables will NOT include views in results; those that need to will have to be modified to reference base class instead Existing queries for tables WILL include views in results; those that are not supposed to will need to filter them out   Metastore upgrade on deployment Need to test carefully to make sure introducing inheritance doesn\u0026rsquo;t corrupt existing metastore instances Nothing special, just adding a new attribute    Update 30-Dec-2009: Based on a design review meeting, we\u0026rsquo;re going to go with the flat model. Prasad pointed out that in the future, for materialized views, we may need the view definition to be tracked at the partition level as well, so that when we change the view definition, we don\u0026rsquo;t have to discard existing materialized partitions if the new view result can be derived from the old one. So it may make sense to add the view definition as a new attribute of StorageDescriptor (since that is already present at both table and partition level).\nUpdate 20-Jan-2010: After further discussion with Prasad, we decided to put the view definition on the table object instead; for details, see discussion in HIVE-972. Also, per HIVE-1068, we added an attribute to store the type (view, managed table, external table) for each table descriptor.\nDependency Tracking It\u0026rsquo;s necessary to track dependencies from a view to objects it references in the metastore:\n tables: this is mandatory if we want DROP TABLE to be able to correctly CASCADE/RESTRICT to a referencing view other views: same as tables columns: this is optional (useful for lineage inspection, but not required for implementing SQL features) temporary functions: we should disallow these at view creation unless we also want a concept of temporary view (or if it\u0026rsquo;s OK for the referencing view to become invalid whenever the volatile function registry gets cleared) any other objects? (e.g. udt\u0026rsquo;s coming in as part of http://issues.apache.org/jira/browse/HIVE-779)  (Note that MySQL doesn\u0026rsquo;t actually implement CASCADE/RESTRICT: it just ignores the keyword and drops the table unconditionally, leaving the view dangling.)\nMetastore object id\u0026rsquo;s can be used for dependency modeling in order to avoid the need to update dependency records when an object is renamed. However, we\u0026rsquo;ll need to decide what kinds of objects can participate in dependencies. For example, if we restrict it to just tables and views (and assuming we don\u0026rsquo;t introduce inheritance for views), then we can use a model like the one below, in which the dependencies are tracked as (supplier,consumer) table pairs. (In this model, the TableDependency class is acting as an intersection table for implementing a many-to-many relationship between suppliers and consumers).\nHowever, if later we want to introduce persistent functions, or track column dependencies, this model will be insufficient, and we may need to introduce inheritance, with a DependencyParticipant base class from which tables, columns, functions etc all derive. (Again, need to verify that JDO inheritance will actually support what we want here.)\nUpdate 30-Dec-2009: Based on a design review meeting, we\u0026rsquo;ll start with the bare-minimum MySQL approach (with no metastore support for dependency tracking), then if time allows, add dependency analysis and storage, followed by CASCADE support. See HIVE-1073 and HIVE-1074.\nDependency Invalidation What happens when an object is modified underneath a view? For example, suppose a view references a table\u0026rsquo;s column, and then ALTER TABLE is used to drop or replace that column. Note that if the column\u0026rsquo;s datatype changes, the view definition may remain meaningful, but the view\u0026rsquo;s schema may need to be updated to match. Here are two possible options:\n Strict: prevent operations which would invalidate or change the view in any way (and optionally to provide a CASCADE flag which requests that such views be dropped automatically). This is the approach taken by SQL:200n. Lenient: allow the update to proceed (and maybe warn the user of the impact), potentially leaving the view in an invalid state. Later, when an invalid view definition is referenced, throw a validation exception for the referencing query. This is the approach taken by MySQL. In the case of datatype changes, derived column datatypes already stored in metastore for referencing views would become stale until those views were recreated.  Note that besides table modifications, other operations such as CREATE OR REPLACE VIEW have similar issues (since views can reference other views). The lenient approach provides a reasonable solution for the related issue of external tables whose schemas may be dynamic (not sure if we currently support this).\nUpdate 30-Dec-2009: Based on a design review meeting, we\u0026rsquo;ll start with the lenient approach, without any support for marking objects invalid in the metastore, then if time allows, follow up with strict support and possibly metastore support for tracking object validity. See HIVE-1077.\nView Modification In SQL:200n, there\u0026rsquo;s no standard way to update a view definition. MySQL supports both\n CREATE OR REPLACE VIEW v AS new-view-def-select ALTER VIEW v AS new-view-def-select  Note that supporting view modification requires detection of cyclic view definitions, which should be invalid. Whether this detection is carried out at the time of view modification versus reference is dependent on the strict versus lenient approaches to dependency invalidation described above.\nUpdate 30-Dec-2009: Based on a design review meeting, we\u0026rsquo;ll start with an Oracle-style ALTER VIEW v RECOMPILE, which can be used to revalidate a view definition, as well as to re-expand the original definition for clauses such as select *. Then if time allows, we\u0026rsquo;ll follow up with CREATE OR REPLACE VIEW support. (The latter is less important since we\u0026rsquo;re going with the lenient invalidation model, making DROP and re-CREATE possible without having to deal with downstream dependencies.) See HIVE-1077 and HIVE-1078.\nFast Path Execution For select * from t, hive supports fast-path execution (skipping Map/Reduce). Is it important for this to work for select * from v as well?\nUpdate 30-Dec-2009: Based on feedback in JIRA, we\u0026rsquo;ll leave this as dependent on getting the fast-path working for the underlying filters and projections.\nUpdate 6-Dec-2010: This one is addressed by Hive\u0026rsquo;s new \u0026ldquo;auto local mode\u0026rdquo; feature.\nORDER BY and LIMIT in view definition SQL:200n prohibits ORDER BY in a view definition, since a view is supposed to be a virtual (unordered) table, not a query alias. However, many DBMS\u0026rsquo;s ignore this rule; for example, MySQL allows ORDER BY, but ignores it in the case where it is superceded by an ORDER BY in the query. Should we prevent ORDER BY? This question also applies to the LIMIT clause.\nUpdate 30-Dec-2009: Based on feedback in JIRA, ORDER BY is important as forward-looking to materialized views. LIMIT may be less important, but we should probably support it too for consistency.\nUnderlying Partition Dependencies Update 30-Dec-2009: Prasad pointed out that even without supporting materialized views, it may be necessary to provide users with metadata about data dependencies between views and underlying table partitions so that users can avoid seeing inconsistent results during the window when not all partitions have been refreshed with the latest data. One option is to attempt to derive this information automatically (using an overconservative guess in cases where the dependency analysis can\u0026rsquo;t be made smart enough); another is to allow view creators to declare the dependency rules in some fashion as part of the view definition. Based on a design review meeting, we will probably go with the automatic analysis approach once dependency tracking is implemented. The analysis will be performed on-demand, perhaps as part of describing the view or submitting a query job against it. Until this becomes available, users may be able to do their own analysis either via empirical lineage tools or via view-\u0026gt;table dependency tracking metadata once it is implemented. See HIVE-1079.\nUpdate 1-Feb-2011: For the latest on this, see PartitionedViews.\nMetastore Upgrades Since we are adding new columns to the TBLS table in the metastore schema, existing metastore deployments will need to be upgraded. There are two ways this can happen.\nAutomatic ALTER TABLE If the following property is set in the Hive configuration file, JDO will notice the difference between the persistent schema and the model and ALTER the tables automatically:\n \u0026lt;property\u0026gt; \u0026lt;name\u0026gt;datanucleus.autoCreateSchema\u0026lt;/name\u0026gt; \u0026lt;value\u0026gt;true\u0026lt;/value\u0026gt; \u0026lt;/property\u0026gt; Explicit ALTER TABLE However, if the datanucleus.autoCreateSchema property is set to false, then the ALTER statements must be executed explicitly. (An administrator may have set this property for safety in production configurations.)\nIn this case, execute a script such as the following against the metastore database:\n ALTER TABLE TBLS ADD COLUMN VIEW_ORIGINAL_TEXT MEDIUMTEXT; ALTER TABLE TBLS ADD COLUMN VIEW_EXPANDED_TEXT MEDIUMTEXT; ALTER TABLE TBLS ADD COLUMN TBL_TYPE VARCHAR(128); The syntax here is for MySQL, so you may need to adjust it (particularly for CLOB datatype).\nNote that it should be safe to execute this script and continue operations BEFORE upgrading Hive; the old Hive version will simply ignore/nullify the columns it doesn\u0026rsquo;t recognize.\nExisting Row UPDATE After the tables are altered, the new columns will contain NULL values for existing rows describing previously created tables. This is correct for VIEW_ORIGINAL_TEXT and VIEW_EXPANDED_TEXT (since views did not previously exist), but is incorrect for the TBL_TYPE column introduced by HIVE-1068. The new Hive code is capable of handling this (automatically filling in the correct value for the new field when a descriptor is retrieved), but it does not \u0026ldquo;fix\u0026rdquo; the stored rows. This could be an issue if in the future other tools are used to retrieve information directly from the metastore database rather than accessing the metastore API.\nThe script below can be used to fix existing rows after the tables have been altered. It should be run AFTER all Hive instances directly accessing the metastore database have been upgraded (otherwise new null values could slip in and remain forever). For safety, it is view-aware just in case a CREATE VIEW statement has already been executed, meaning it can be rerun any time after the upgrade.\n UPDATE TBLS SET TBL_TYPE='MANAGED_TABLE' WHERE VIEW_ORIGINAL_TEXT IS NULL AND NOT EXISTS( SELECT * FROM TABLE_PARAMS WHERE TABLE_PARAMS.TBL_ID=TBLS.TBL_ID AND PARAM_KEY='EXTERNAL' AND PARAM_VALUE='TRUE' ); UPDATE TBLS SET TBL_TYPE='EXTERNAL_TABLE' WHERE EXISTS( SELECT * FROM TABLE_PARAMS WHERE TABLE_PARAMS.TBL_ID=TBLS.TBL_ID AND PARAM_KEY='EXTERNAL' AND PARAM_VALUE='TRUE' ); For MySQL, note that the \u0026ldquo;safe updates\u0026rdquo; feature will need to be disabled since these are full-table updates.\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/viewdev_27362067/","tags":null,"title":"Apache Hive : ViewDev"},{"categories":null,"contents":"Apache Hive : WebHCat This is the manual for WebHCat, previously known as Templeton. WebHCat is the REST API for HCatalog, a table and storage management layer for Hadoop.  Using WebHCat Installation Configuration Reference  See the HCatalog Manual for general HCatalog documentation.\nNavigation Links Next: Using WebHCat\nGeneral: HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat_33299069/","tags":null,"title":"Apache Hive : WebHCat"},{"categories":null,"contents":"Apache Hive : WebHCat Configure WebHCat Configuration  WebHCat Configuration  Configuration Files Configuration Variables  Default Values      Configuration Files The configuration for WebHCat (Templeton) merges the normal Hadoop configuration with the WebHCat-specific variables. Because WebHCat is designed to connect services that are not normally connected, the configuration is more complex than might be desirable.\nThe WebHCat-specific configuration is split into two layers:\n webhcat-default.xml – All the configuration variables that WebHCat needs. This file sets the defaults that ship with WebHCat and should only be changed by WebHCat developers. Do not copy this file or change it to maintain local installation settings. Because webhcat-default.xml is present in the WebHCat war file, editing a local copy of it will not change the configuration. webhcat-site.xml – The (possibly empty) configuration file in which the system administrator can set variables for their Hadoop cluster. Create this file and maintain entries in it for configuration variables that require you to override default values based on your local installation.  Note\nThe WebHCat server will require restart after any change to the configuration.\nThe configuration files are loaded in this order with later files overriding earlier ones:\n To find the configuration files, WebHCat first attempts to load a file from the CLASSPATH and then looks in the directory specified in the TEMPLETON_HOME environment variable.  Configuration files may access the special environment variable env for all environment variables. For example, the Pig executable could be specified using:\n${env.PIG_HOME}/bin/pig Configuration variables that use a filesystem path try to have reasonable defaults. However, it\u0026rsquo;s always safe to specify the full and complete path if there is any uncertainty.\nLog File Location\nThe webhcat-log4j.properties file sets the location of the log files created by WebHCat and some other properties of the logging system.\nConfiguration Variables    Name Description     templeton.port The HTTP port for the main server.   templeton.hadoop.config.dir The path to the Hadoop configuration.   Obsolete: templeton.jar The path to the WebHCat jar file. (Not used in recent releases, so removed in Hive 0.14.0.)   templeton.libjars Jars to add to the classpath.   templeton.override.jars Jars to add to the HADOOP_CLASSPATH for all Map Reduce jobs. These jars must exist on HDFS.   templeton.override.enabled Enable the override path in templeton.override.jars.   templeton.streaming.jar The HDFS path to the Hadoop streaming jar file.   templeton.hadoop The path to the Hadoop executable.   templeton.pig.archive The path to the Pig archive.   templeton.pig.path The path to the Pig executable.   templeton.hcat The path to the HCatalog executable.   templeton.hive.archive The path to the Hive archive.   templeton.hive.path The path to the Hive executable.   templeton.hive.properties Properties to set when running Hive (during job submission). This is expected to be a comma-separated prop=value list. If some value is itself a comma-separated list, the escape character is \u0026lsquo;' (from Hive 0.13.1 onward).To use it in a cluster with Kerberos security enabled, set hive.metastore.sasl.enabled=false and add hive.metastore.execute.setugi=true. Using localhost in metastore URI does not work with Kerberos security.   templeton.exec.encoding The encoding of the stdout and stderr data.   templeton.exec.timeout How long in milliseconds a program is allowed to run on the WebHCat box.   templeton.exec.max-procs The maximum number of processes allowed to run at once.   templeton.exec.max-output-bytes The maximum number of bytes from stdout or stderr stored in ram.   templeton.controller.mr.child.opts Java options to be passed to WebHCat controller map task.   templeton.exec.envs The environment variables passed through to exec.   templeton.zookeeper.hosts ZooKeeper servers, as comma-separated host:port pairs.   templeton.zookeeper.session-timeout ZooKeeper session timeout in milliseconds.   templeton.callback.retry.interval How long to wait between callback retry attempts in milliseconds.   templeton.callback.retry.attempts How many times to retry the callback.   templeton.storage.class The class to use as storage.   templeton.storage.root The path to the directory to use for storage.   templeton.hdfs.cleanup.interval The maximum delay between a thread\u0026rsquo;s cleanup checks.   templeton.hdfs.cleanup.maxage The maximum age of a WebHCat job.   templeton.zookeeper.cleanup.interval The maximum delay between a thread\u0026rsquo;s cleanup checks.   templeton.zookeeper.cleanup.maxage The maximum age of a WebHCat job.   templeton.kerberos.secret The secret used to sign the HTTP cookie value. The default value is a random value. Unless multiple WebHCat instances need to share the secret the random value is adequate.   templeton.kerberos.principal The Kerberos principal to used by the server. As stated by the Kerberos SPNEGO specification, it should be USER/${HOSTNAME}@{REALM}. It does not have a default value.   templeton.kerberos.keytab The keytab file containing the credentials for the Kerberos principal.   templeton.hadoop.queue.name MapReduce queue name where WebHCat map-only jobs will be submitted to. Can be used to avoid a deadlock where all map slots in the cluster are taken over by Templeton launcher tasks.Versions: Hive 0.12.0 and later.   templeton.mapper.memory.mb WebHCat controller job\u0026rsquo;s Launch mapper\u0026rsquo;s memory limit in megabytes. When submitting a controller job, WebHCat will overwrite mapreduce.map.memory.mb with this value. If empty, WebHCat will not set mapreduce.map.memory.mb when submitting the controller job, therefore the configuration in mapred-site.xml will be used.Versions: Hive 0.14.0 and later.   templeton.frame.options.filter Adds web server protection from clickjacking using X-Frame-Options header. The possible values are DENY, SAMEORIGIN, ALLOW-FROM .Versions: Hive 3.0.0 and later.    Default Values Some of the default values for WebHCat configuration variables depend on the release number. For the default values in the Hive release you are using, see the webhcat-default.xml file. It can be found in the SVN repository at:\n http://svn.apache.org/repos/asf/hive/branches/branch-\u0026lt;release_number\u0026gt;/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml  where \u0026lt;release_number\u0026gt; is 0.11, 0.12, and so on. Prior to Hive 0.11, WebHCat was in the Apache incubator.\nFor example:\n Hive 0.12.0: http://svn.apache.org/repos/asf/hive/branches/branch-0.12/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml Hive 0.13.0: http://svn.apache.org/repos/asf/hive/branches/branch-0.13/hcatalog/webhcat/svr/src/main/config/webhcat-default.xml  Default values prior to Hive 0.11 are listed in the HCatalog 0.5.0 documentation:\n HCatalog 0.5.0: WebHCat Configuration Variables   Navigation Links Previous: Installation\nNext: Reference\nHive configuration: Configuring Hive, Hive Configuration Properties, Thrift Server Setup\nGeneral: WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-configure_34015738/","tags":null,"title":"Apache Hive : WebHCat Configure"},{"categories":null,"contents":"Apache Hive : WebHCat InstallWebHCat WebHCat Installation  WebHCat Installation  WebHCat Installed with Hive WebHCat Installation Procedure Server Commands Requirements Hadoop Distributed Cache Permissions Secure Cluster Proxy User Support    WebHCat Installed with Hive Version\nWebHCat and HCatalog are installed with Hive, starting with Hive release 0.11.0.\nIf you install Hive from the binary tarball, the WebHCat server command webhcat_server.sh is in the hcatalog/sbin directory.\nHive installation is documented here.\nWebHCat Installation Procedure Note: WebHCat was originally called Templeton. For backward compatibility the name still appears in URLs, log file names, variable names, etc.\n Ensure that the required related installations are in place, and place required files into the Hadoop distributed cache. Download and unpack the HCatalog distribution. Set the TEMPLETON_HOME environment variable to the base of the HCatalog REST server installation. This will usually be same as HCATALOG_HOME. This is used to find the WebHCat (Templeton) configuration. Set JAVA_HOME, HADOOP_PREFIX, and HIVE_HOME environment variables. Review the configuration and update or create webhcat-site.xml as required. Ensure that site-specific component installation locations are accurate, especially the Hadoop configuration path. Configuration variables that use a filesystem path try to have reasonable defaults, but it\u0026rsquo;s always safe to specify a full and complete path. Verify that HCatalog is installed and that the hcat executable is in the PATH. Build HCatalog using the command ant jar from the top level HCatalog directory. Start the REST server with the command \u0026ldquo;hcatalog/sbin/webhcat_server.sh start\u0026rdquo; for Hive 0.11.0 releases and later, or \u0026ldquo;sbin/webhcat_server.sh start\u0026rdquo; for installations prior to HCatalog merging with Hive. Check that your local install works. Assuming that the server is running on port 50111, the following command would give output similar to that shown.  % curl -i http://localhost:50111/templeton/v1/status HTTP/1.1 200 OK Content-Type: application/json Transfer-Encoding: chunked Server: Jetty(7.6.0.v20120127) {\u0026quot;status\u0026quot;:\u0026quot;ok\u0026quot;,\u0026quot;version\u0026quot;:\u0026quot;v1\u0026quot;} % Server Commands  Start the server: sbin/webhcat_server.sh start (HCatalog 0.5.0 and earlier – prior to Hive release 0.11.0)  hcatalog/sbin/webhcat_server.sh start (Hive release 0.11.0 and later)   Stop the server: sbin/webhcat_server.sh stop (HCatalog 0.5.0 and earlier – prior to Hive release 0.11.0)  hcatalog/sbin/webhcat_server.sh stop (Hive release 0.11.0 and later)   End-to-end build, run, test: ant e2e  Requirements  Ant, version 1.8 or higher Hadoop, version 1.0.3 or higher ZooKeeper is required if you are using the ZooKeeper storage class. (Be sure to review and update the ZooKeeper-related WebHCat configuration.) HCatalog, version 0.5.0 or higher. The hcat executable must be both in the PATH and properly configured in the WebHCat configuration. Permissions must be given to the user running the server. (See below.) If running a secure cluster, Kerberos keys and principals must be created. (See below.) Hadoop Distributed Cache. To use Hive, Pig, or Hadoop Streaming resources, see instructions below for placing the required files in the Hadoop Distributed Cache.  Hadoop Distributed Cache The server requires some files be accessible on the Hadoop distributed cache. For example, to avoid the installation of Pig and Hive everywhere on the cluster, the server gathers a version of Pig or Hive from the Hadoop distributed cache whenever those resources are invoked. After placing the following components into HDFS please update the site configuration as required for each.\n Hive: Download the Hive tar.gz file and place it in HDFS. For example, for Hive version 0.11.0:  hadoop fs -put /tmp/hive-0.11.0.tar.gz /apps/templeton/hive-0.11.0.tar.gz  Pig: Download the Pig tar.gz file and place it into HDFS. For example, for Pig version 0.11.1:  hadoop fs -put /tmp/pig-0.11.1.tar.gz /apps/templeton/pig-0.11.1.tar.gz  Hadoop Streaming: Place hadoop-streaming-*.jar into HDFS. Use the following command:  hadoop fs -put \u0026lt;hadoop streaming jar\u0026gt; \\ \u0026lt;templeton.streaming.jar\u0026gt;/hadoop-streaming-*.jar where \u0026lt;templeton.streaming.jar\u0026gt; is a property value defined in webhcat-default.xml which can be overridden in the webhcat-site.xml file, and  is the Hadoop streaming jar in your Hadoop version:\n+ `hadoop-1.*/contrib/streaming/hadoop-streaming-*.jar` in the Hadoop 1.x tar + `hadoop-2.*/share/hadoop/tools/lib/hadoop-streaming-*.jar` in the Hadoop 2.x tar For example, ``` hadoop fs -put hadoop-2.1.0/share/hadoop/tools/lib/hadoop-streaming-2.1.0.jar \\ /apps/templeton/hadoop-streaming.jar ```   Override Jars: Place override jars required (if any) into HDFS. Note: Hadoop versions prior to 1.0.3 required a patch (HADOOP-7987) to properly run WebHCat. This patch is distributed with WebHCat (located at templeton/src/hadoop_temp_fix/ugi.jar) and should be placed into HDFS, as reflected in the current default configuration.  hadoop fs -put ugi.jar /apps/templeton/ugi.jar The location of these files in the cache, and the location of the installations inside the archives, can be specified using the following WebHCat configuration variables. (See the Configuration documentation for more information on changing WebHCat configuration parameters.) Some default values vary depending on release number; defaults shown below are for the version of WebHCat that is included in Hive release 0.11.0. Defaults for the previous release are shown in the HCatalog 0.5.0 documentation.\n   Name Default (Hive 0.11.0) Description     templeton.pig.archive hdfs:///apps/templeton/pig-0.11.1.tar.gz The path to the Pig archive.   templeton.pig.path pig-0.11.1.tar.gz/pig-0.11.1/bin/pig The path to the Pig executable.   templeton.hive.archive hdfs:///apps/templeton/hive-0.11.0.tar.gz The path to the Hive archive.   templeton.hive.path hive-0.11.0.tar.gz/hive-0.11.0/bin/hive The path to the Hive executable.   templeton.streaming.jar hdfs:///apps/templeton/hadoop-streaming.jar The path to the Hadoop streaming jar file.   templeton.override.jars hdfs:///apps/templeton/ugi.jar Jars to add to the HADOOP_CLASSPATH for all Map Reduce jobs. These jars must exist on HDFS. This is not needed for Hadoop versions 1.0.1 and newer.    Permissions Permission must be given for the user running the WebHCat executable to run jobs for other users. That is, the WebHCat server will impersonate users on the Hadoop cluster.\nCreate (or assign) a Unix user who will run the WebHCat server. Call this USER. See the Secure Cluster section below for choosing a user on a Kerberos cluster.\nModify the Hadoop core-site.xml file and set these properties:\n   Variable Value     hadoop.proxyuser.USER.groups A comma-separated list of the Unix groups whose users will be impersonated.   hadoop.proxyuser.USER.hosts A comma-separated list of the hosts that will run the HCatalog and JobTracker servers.    Secure Cluster To run WebHCat on a secure cluster follow the Permissions instructions above but create a Kerberos principal for the WebHCat server with the name USER/host@realm.\nAlso, set the WebHCat configuration variables templeton.kerberos.principal and templeton.kerberos.keytab.\nProxy User Support Proxy User Support in WebHCat allows the caller of WebHCat to instruct WebHCat to run commands on the Hadoop cluster as a particular user.\nThe canonical example is Joe using Hue to submit a MapReduce job through WebHCat. For the following description, assume Joe has the Unix name \u0026lsquo;joe\u0026rsquo;, Hue is \u0026lsquo;hue\u0026rsquo; and WebHCat is \u0026lsquo;hcat\u0026rsquo;. If Hue specifies \u0026lsquo;doAs=joe\u0026rsquo; when calling WebHCat, WebHCat submits the MR job as \u0026lsquo;joe\u0026rsquo; so that the Hadoop cluster can perform securitiy checks with respect to \u0026lsquo;joe\u0026rsquo;. If the doAs value is not specified, the MR job will be submitted as user \u0026lsquo;hue\u0026rsquo;.\nTo set up Proxy User Support, make the following edits in configuration files.\nIn hive-site.xml, set:\n   Variable Value     hive.security.metastore.authorization.manager org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider   hive.security.metastore.authenticator.manager org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator   hive.metastore.pre.event.listeners org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener   hive.metastore.execute.setugi true    In webhcat-site.xml, set:\n   Variable Value     webhcat.proxyuser.hue.groups A comma-separated list of the Unix groups whose users may be impersonated by \u0026lsquo;hue\u0026rsquo;.   webhcat.proxyuser.hue.hosts A comma-separated list of the hosts which are allowed to submit requests by \u0026lsquo;hue\u0026rsquo;. In the canonical example, this would be the servers running Hue.    In core-site.xml, make sure the following are also set:\n   Variable Value     hadoop.proxyuser.hcat.group A comma-separated list of the Unix groups whose users may be impersonated by \u0026lsquo;hcat\u0026rsquo;.   hadoop.proxyuser.hcat.hosts A comma-separated list of the hosts which are allowed to submit requests by \u0026lsquo;hcat\u0026rsquo;.    Navigation Links Previous: Using WebHCat\nNext: Configuration\nHive installation: Installing Hive\nHCatalog installation: Installation from Tarball\nGeneral: WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-installwebhcat_34015585/","tags":null,"title":"Apache Hive : WebHCat InstallWebHCat"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Reference: WebHCat Resources This overview page lists all of the WebHCat resources. (DDL resources are listed here and on another overview page. For information about HCatalog DDL commands, see HCatalog DDL. For information about Hive DDL commands, see Hive Data Definition Language.)\n    Category Resource (Type) Description     General :version (GET) Return a list of supported response types.     status (GET) Return the WebHCat server status.     version (GET) Return a list of supported versions and the current version.    version/hive (GET) Return the Hive version being run. (Added in Hive 0.13.0.)    version/hadoop (GET) Return the Hadoop version being run. (Added in Hive 0.13.0.)   DDL ddl (POST) Perform an HCatalog DDL command.     ddl/database (GET) List HCatalog databases.     ddl/database/:db (GET) Describe an HCatalog database.     ddl/database/:db (PUT) Create an HCatalog database.     ddl/database/:db (DELETE) Delete (drop) an HCatalog database.     ddl/database/:db/table (GET) List the tables in an HCatalog database.     ddl/database/:db/table/:table (GET) Describe an HCatalog table.     ddl/database/:db/table/:table (PUT) Create a new HCatalog table.     ddl/database/:db/table/:table (POST) Rename an HCatalog table.     ddl/database/:db/table/:table (DELETE) Delete (drop) an HCatalog table.     ddl/database/:db/table/:existingtable/like/:newtable (PUT) Create a new HCatalog table like an existing one.     ddl/database/:db/table/:table/partition (GET) List all partitions in an HCatalog table.     ddl/database/:db/table/:table/partition/:partition (GET) Describe a single partition in an HCatalog table.     ddl/database/:db/table/:table/partition/:partition (PUT) Create a partition in an HCatalog table.     ddl/database/:db/table/:table/partition/:partition (DELETE) Delete (drop) a partition in an HCatalog table.     ddl/database/:db/table/:table/column (GET) List the columns in an HCatalog table.     ddl/database/:db/table/:table/column/:column (GET) Describe a single column in an HCatalog table.     ddl/database/:db/table/:table/column/:column (PUT) Create a column in an HCatalog table.     ddl/database/:db/table/:table/property (GET) List table properties.     ddl/database/:db/table/:table/property/:property (GET) Return the value of a single table property.     ddl/database/:db/table/:table/property/:property (PUT) Set a table property.   MapReduce mapreduce/streaming (POST) Create and queue Hadoop streaming MapReduce jobs.     mapreduce/jar (POST) Create and queue standard Hadoop MapReduce jobs.   Pig pig (POST) Create and queue Pig jobs.   Hive hive (POST) Run Hive queries and commands.   Queue(deprecated in Hive 0.12,removed in Hive 0.14) queue (GET) Return a list of all job IDs. (Removed in Hive 0.14.0.)     queue/:jobid (GET) Return the status of a job given its ID. (Removed in Hive 0.14.0.)     queue/:jobid (DELETE) Kill a job given its ID. (Removed in Hive 0.14.0.)   Jobs(Hive 0.12 and later) jobs (GET) Return a list of all job IDs.     jobs/:jobid (GET) Return the status of a job given its ID.     jobs/:jobid (DELETE) Kill a job given its ID.    Navigation Links Previous: Configuration\nNext: GET :version\nOverview of DDL resources: WebHCat Reference: DDL\nHCatalog DDL commands: HCatalog DDL\nHive DDL commands: Hive Data Definition Language\nGeneral: WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference_34015762/","tags":null,"title":"Apache Hive : WebHCat Reference"},{"categories":null,"contents":"Apache Hive : WebHCat Reference AllDDL WebHCat Reference: DDL Resources This is an overview page for the WebHCat DDL resources. The full list of WebHCat resources is on this overview page.\n For information about HCatalog DDL commands, see HCatalog DDL. For information about Hive DDL commands, see Hive Data Definition Language.     Object Resource (Type) Description     DDL Command ddl (POST) Perform an HCatalog DDL command.   Database ddl/database (GET) List HCatalog databases.     ddl/database/:db (GET) Describe an HCatalog database.     ddl/database/:db (PUT) Create an HCatalog database.     ddl/database/:db (DELETE) Delete (drop) an HCatalog database.   Table ddl/database/:db/table (GET) List the tables in an HCatalog database.     ddl/database/:db/table/:table (GET) Describe an HCatalog table.     ddl/database/:db/table/:table (PUT) Create a new HCatalog table.     ddl/database/:db/table/:table (POST) Rename an HCatalog table.     ddl/database/:db/table/:table (DELETE) Delete (drop) an HCatalog table.     ddl/database/:db/table/:existingtable/like/:newtable (PUT) Create a new HCatalog table like an existing one.   Partition ddl/database/:db/table/:table/partition (GET) List all partitions in an HCatalog table.     ddl/database/:db/table/:table/partition/:partition (GET) Describe a single partition in an HCatalog table.     ddl/database/:db/table/:table/partition/:partition (PUT) Create a partition in an HCatalog table.     ddl/database/:db/table/:table/partition/:partition (DELETE) Delete (drop) a partition in an HCatalog table.   Column ddl/database/:db/table/:table/column (GET) List the columns in an HCatalog table.     ddl/database/:db/table/:table/column/:column (GET) Describe a single column in an HCatalog table.     ddl/database/:db/table/:table/column/:column (PUT) Create a column in an HCatalog table.   Property ddl/database/:db/table/:table/property (GET) List table properties.     ddl/database/:db/table/:table/property/:property (GET) Return the value of a single table property.     ddl/database/:db/table/:table/property/:property (PUT) Set a table property.    Navigation Links Previous: GET version Next: POST ddl\nHCatalog DDL commands: HCatalog DDL Hive DDL commands: Hive Data Definition Language\nGeneral: WebHCat Reference – WebHCat (Templeton) Manual – HCatalog Manual – Hive Home\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-allddl_34016001/","tags":null,"title":"Apache Hive : WebHCat Reference AllDDL"},{"categories":null,"contents":"Apache Hive : WebHCat Reference DDL DDL Command — POST ddl  DDL Command — POST ddl  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description Performs an HCatalog DDL command. The command is executed immediately upon request. Responses are limited to 1 MB. For requests which may return longer results consider using the Hive resource as an alternative.\nURL http://www.myserver.com/templeton/ddl\nParameters    Name Description Required? Default     exec The HCatalog ddl string to execute Required None   group The user group to use when creating a table Optional None   permissions The permissions string to use when creating a table. The format is \u0026ldquo;rwxrw-r-x\u0026rdquo;. Optional None    The standard parameters are also supported.\nResults    Name Description     stdout A string containing the result HCatalog sent to standard out (possibly empty).   stderr A string containing the result HCatalog sent to standard error (possibly empty).   exitcode The exitcode HCatalog returned.    Example Curl Command % curl -s -d 'exec=show tables;' \\ 'http://localhost:50111/templeton/v1/ddl?user.name=ekoifman' Version information\nPrior to Hive 0.13.0, user.name was specified in POST requests as a form parameter: curl -d user.name=*\u0026lt;user\u0026gt;*.\nIn Hive 0.13.0 onward, user.name should be specified in the query string (as shown above): 'http://.../templeton/v1/ddl?user.name=*\u0026lt;name\u0026gt;*'. Specifying user.name as a form parameter is deprecated.\nJSON Output { \u0026quot;stdout\u0026quot;: \u0026quot;important_table my_other_table my_table my_table_2 pokes \u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated... Hive history file=/tmp/ctdean/hive_job_log_ctdean_201111111258_2117356679.txt OK Time taken: 1.202 seconds \u0026quot;, \u0026quot;exitcode\u0026quot;: 0 } JSON Output (error) { \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated... Hive history file=/tmp/ctdean/hive_job_log_ctdean_201204051246_689834593.txt FAILED: Parse Error: line 1:5 Failed to recognize predicate 'tab'... \u0026quot;, \u0026quot;exitcode\u0026quot;: 11 } Navigation Links Previous: GET version/hadoop\nNext: GET ddl/database\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-ddl_34015990/","tags":null,"title":"Apache Hive : WebHCat Reference DDL"},{"categories":null,"contents":"Apache Hive : WebHCat Reference DeleteDB Delete Database — DELETE ddl/database/:db  Delete Database — DELETE ddl/database/:db  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description Delete a database.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db\nParameters    Name Description Required? Default     :db The database name Required None   ifExists Hive returns an error if the database specified does not exist, unless ifExists is set to true. Optional false   option Parameter set to either \u0026ldquo;restrict\u0026rdquo; or \u0026ldquo;cascade\u0026rdquo;. Restrict will remove the schema if all the tables are empty. Cascade removes everything including data and definitions. Optional None   group The user group to use Optional None   permissions The permissions string to use. The format is \u0026ldquo;rwxrw-r-x\u0026rdquo;. Optional None    The standard parameters are also supported.\nResults    Name Description     database The database name    Example Curl Command % curl -s -X DELETE \u0026quot;http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=ctdean\u0026quot; JSON Output { \u0026quot;database\u0026quot;:\u0026quot;newdb\u0026quot; } JSON Output (error) { \u0026quot;errorDetail\u0026quot;: \u0026quot; NoSuchObjectException(message:There is no database named my_db) at org.apache.hadoop.hive.metastor... \u0026quot;, \u0026quot;error\u0026quot;: \u0026quot;There is no database named newdb\u0026quot;, \u0026quot;errorCode\u0026quot;: 404, \u0026quot;database\u0026quot;: \u0026quot;newdb\u0026quot; } Navigation Links Previous: PUT ddl/database/:db Next: GET ddl/database/:db/table\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-deletedb_34016281/","tags":null,"title":"Apache Hive : WebHCat Reference DeleteDB"},{"categories":null,"contents":"Apache Hive : WebHCat Reference DeleteJob Delete Job — DELETE queue/:jobid  Delete Job — DELETE queue/:jobid  Description URL Parameters Results Example  Curl Command JSON Output      Description Kill a job given its job ID. Substitute \u0026ldquo;:jobid\u0026rdquo; with the job ID received when the job was created.\nVersion: Deprecated in 0.12.0\nDELETE queue/:jobid is deprecated starting in Hive release 0.12.0. Users are encouraged to use DELETE jobs/:jobid instead. (See HIVE-4443.)\nDELETE queue/:jobid is equivalent to DELETE jobs/:jobid – check [DELETE jobs/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-deletejobid_34835045/) for documentation.\nVersion: Obsolete in 0.14.0\nDELETE queue/:jobid will be removed in Hive release 0.14.0. (See HIVE-6432.)\nUse [DELETE jobs/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-deletejobid_34835045/) instead.\nURL http://www.myserver.com/templeton/v1/queue/:jobid\nParameters    Name Description Required? Default     :jobid The job ID to delete. This is the ID received when the job was created. Required None    The standard parameters are also supported.\nResults    Name Description     status A JSON object containing the job status information. See the Hadoop documentation (Class JobStatus) for more information.   profile A JSON object containing the job profile information. WebHCat passes along the information in the JobProfile object, which is subject to change from one Hadoop version to another. See the Hadoop documentation (API docs) for org.apache.hadoop.mapred.JobProfile for more information.   id The job ID.   parentId The parent job ID.   percentComplete The job completion percentage, for example \u0026ldquo;75% complete\u0026rdquo;.   exitValue The job\u0026rsquo;s exit value.   user User name of the job creator.   callback The callback URL, if any.   completed A string representing completed status, for example \u0026ldquo;done\u0026rdquo;.    Example Curl Command % curl -s -X DELETE 'http://localhost:50111/templeton/v1/queue/job_201111111311_0009?user.name=ctdean' JSON Output { \u0026quot;status\u0026quot;: { \u0026quot;startTime\u0026quot;: 1321047216471, \u0026quot;username\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201111111311\u0026quot;, \u0026quot;id\u0026quot;: 9 }, \u0026quot;jobACLs\u0026quot;: { }, \u0026quot;schedulingInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;failureInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201111111311_0009\u0026quot;, \u0026quot;jobPriority\u0026quot;: \u0026quot;NORMAL\u0026quot;, \u0026quot;runState\u0026quot;: 1, \u0026quot;jobComplete\u0026quot;: false }, \u0026quot;profile\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://localhost:50030/jobdetails.jsp?jobid=job_201111111311_0009\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201111111311\u0026quot;, \u0026quot;id\u0026quot;: 9 }, \u0026quot;queueName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;jobFile\u0026quot;: \u0026quot;hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201111111311_0009/job.xml\u0026quot;, \u0026quot;jobName\u0026quot;: \u0026quot;streamjob3322518350676530377.jar\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201111111311_0009\u0026quot; } \u0026quot;id\u0026quot;: \u0026quot;job_201111111311_0009\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;job_201111111311_0008\u0026quot;, \u0026quot;percentComplete\u0026quot;: \u0026quot;10% complete\u0026quot;, \u0026quot;exitValue\u0026quot;: 0, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;callback\u0026quot;: null, \u0026quot;completed\u0026quot;: \u0026quot;false\u0026quot; } Note\nThe job is not immediately deleted, therefore the information returned may not reflect deletion, as in our example. Use GET queue/:jobid to monitor the job and confirm that it is eventually deleted.\nNavigation Links Previous: GET queue/:jobid\nNext: GET jobs\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaced in Hive 0.12.0 by: DELETE jobs/:jobid\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-deletejob_34017204/","tags":null,"title":"Apache Hive : WebHCat Reference DeleteJob"},{"categories":null,"contents":"Apache Hive : WebHCat Reference DeleteJobID Delete Job — DELETE jobs/:jobid  Delete Job — DELETE jobs/:jobid  Description URL Parameters Results Example  Curl Command JSON Output      Description Kill a job given its job ID. Substitute \u0026ldquo;:jobid\u0026rdquo; with the job ID received when the job was created.\nVersion: Hive 0.12.0 and later\nDELETE jobs/:jobid is introduced in Hive release 0.12.0. It is equivalent to [DELETE queue/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-deletejob_34017204/) in prior releases.\nDELETE queue/:jobid is now deprecated (HIVE-4443) and will be removed in Hive 0.14.0 (HIVE-6432).\nURL http://www.myserver.com/templeton/v1/jobs/:jobid\nParameters    Name Description Required? Default     :jobid The job ID to delete. This is the ID received when the job was created. Required None    The standard parameters are also supported.\nResults    Name Description     status A JSON object containing the job status information. See the Hadoop documentation (Class JobStatus) for more information.   profile A JSON object containing the job profile information. WebHCat passes along the information in the JobProfile object, which is subject to change from one Hadoop version to another. See the Hadoop documentation (API docs) for org.apache.hadoop.mapred.JobProfile for more information.   id The job ID.   parentId The parent job ID.   percentComplete The job completion percentage, for example \u0026ldquo;75% complete\u0026rdquo;.   exitValue The job\u0026rsquo;s exit value.   user User name of the job creator.   callback The callback URL, if any.   completed A string representing completed status, for example \u0026ldquo;done\u0026rdquo;.    Example Curl Command % curl -s -X DELETE 'http://localhost:50111/templeton/v1/jobs/job_201111111311_0009?user.name=ctdean' JSON Output { \u0026quot;status\u0026quot;: { \u0026quot;startTime\u0026quot;: 1321047216471, \u0026quot;username\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201111111311\u0026quot;, \u0026quot;id\u0026quot;: 9 }, \u0026quot;jobACLs\u0026quot;: { }, \u0026quot;schedulingInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;failureInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201111111311_0009\u0026quot;, \u0026quot;jobPriority\u0026quot;: \u0026quot;NORMAL\u0026quot;, \u0026quot;runState\u0026quot;: 1, \u0026quot;jobComplete\u0026quot;: false }, \u0026quot;profile\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://localhost:50030/jobdetails.jsp?jobid=job_201111111311_0009\u0026quot;, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201111111311\u0026quot;, \u0026quot;id\u0026quot;: 9 }, \u0026quot;queueName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;jobFile\u0026quot;: \u0026quot;hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201111111311_0009/job.xml\u0026quot;, \u0026quot;jobName\u0026quot;: \u0026quot;streamjob3322518350676530377.jar\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201111111311_0009\u0026quot; } \u0026quot;id\u0026quot;: \u0026quot;job_201111111311_0009\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;job_201111111311_0008\u0026quot;, \u0026quot;percentComplete\u0026quot;: \u0026quot;10% complete\u0026quot;, \u0026quot;exitValue\u0026quot;: 0, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;callback\u0026quot;: null, \u0026quot;completed\u0026quot;: \u0026quot;false\u0026quot; } Note\nThe job is not immediately deleted, therefore the information returned may not reflect deletion, as in our example. Use GET jobs/:jobid to monitor the job and confirm that it is eventually deleted.\nNavigation Links Previous: GET jobs/:jobid\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaces deprecated resource: DELETE queue/:jobid\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-deletejobid_34835045/","tags":null,"title":"Apache Hive : WebHCat Reference DeleteJobID"},{"categories":null,"contents":"Apache Hive : WebHCat Reference DeletePartition Delete Partition — DELETE ddl/database/:db/table/:table/partition/:partition  Delete Partition — DELETE ddl/database/:db/table/:table/partition/:partition  Description URL Parameters Results Example  Curl Command JSON Output      Description Delete (drop) a partition in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/partition/:partition\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :partition The partition name, col_name=\u0026lsquo;value\u0026rsquo; list. Be careful to properly encode the quote for http, for example, country=%27algeria%27. Required None   ifExists Hive returns an error if the partition specified does not exist, unless ifExists is set to true. Optional false   group The user group to use Optional None   permissions The permissions string to use. The format is \u0026ldquo;rwxrw-r-x\u0026rdquo;. Optional None    The standard parameters are also supported.\nResults    Name Description     partition The partition name   table The table name   database The database name    Example Curl Command % curl -s -X DELETE \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/partition/country=%27algeria%27?user.name=ctdean' JSON Output { \u0026quot;partition\u0026quot;: \u0026quot;country='algeria'\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } Navigation Links Previous: PUT ddl/database/:db/table/:table/partition/:partition Next: GET ddl/database/:db/table/:table/column\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-deletepartition_34016611/","tags":null,"title":"Apache Hive : WebHCat Reference DeletePartition"},{"categories":null,"contents":"Apache Hive : WebHCat Reference DeleteTable Delete Table — DELETE ddl/database/:db/table/:table  Delete Table — DELETE ddl/database/:db/table/:table  Description URL Parameters Results Example  Curl Command JSON Output      Description Delete (drop) an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   ifExists Hive 0.70 and later returns an error if the table specified does not exist, unless ifExists is set to true. Optional false   group The user group to use Optional None   permissions The permissions string to use. The format is \u0026ldquo;rwxrw-r-x\u0026rdquo;. Optional None    The standard parameters are also supported.\nResults    Name Description     table The table name   database The database name    Example Curl Command % curl -s -X DELETE 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean' JSON Output { \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } Navigation Links Previous: POST ddl/database/:db/table/:table Next: PUT ddl/database/:db/table/:existingtable/like/:newtable\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-deletetable_34016561/","tags":null,"title":"Apache Hive : WebHCat Reference DeleteTable"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetColumn Describe Column — GET ddl/database/:db/table/:table/column/:column  Describe Column — GET ddl/database/:db/table/:table/column/:column  Description URL Parameters Results Example  Curl Command JSON Output      Description Describe a single column in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/column/:column\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :column The column name Required None    The standard parameters are also supported.\nResults    Name Description     database The database name   table The table name   column A JSON object containing the column name, type, and comment (if any)    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/column/price?user.name=ctdean' JSON Output { \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;column\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;price\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The unit price\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot; } } Navigation Links Previous: GET ddl/database/:db/table/:table/column Next: PUT ddl/database/:db/table/:table/column/:column\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getcolumn_34016979/","tags":null,"title":"Apache Hive : WebHCat Reference GetColumn"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetColumns List Columns — GET ddl/database/:db/table/:table/column  List Columns — GET ddl/database/:db/table/:table/column  Description URL Parameters Results Example  Curl Command JSON Output      Description List the columns in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/column\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None    The standard parameters are also supported.\nResults    Name Description     columns A list of column names and types   database The database name   table The table name    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table/column?user.name=ctdean' JSON Output { \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bigint\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The user name\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;my_p\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;my_q\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;my_table\u0026quot; } Navigation Links Previous: DELETE ddl/database/:db/table/:table/partition/:partition Next: GET ddl/database/:db/table/:table/column/:column\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getcolumns_34016970/","tags":null,"title":"Apache Hive : WebHCat Reference GetColumns"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetDB Describe Database — GET ddl/database/:db  Describe Database — GET ddl/database/:db  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description Describe a database. (Note: This resource has a \u0026ldquo;format=extended\u0026rdquo; parameter however the output structure does not change if it is used.)\nURL http://www.myserver.com/templeton/v1/ddl/database/:db\nParameters    Name Description Required? Default     :db The database name Required None    The standard parameters are also supported.\nResults    Name Description     location The database location   params The database parameters   comment The database comment   database The database name    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=ctdean' JSON Output { \u0026quot;location\u0026quot;:\u0026quot;hdfs://localhost:9000/warehouse/newdb.db\u0026quot;, \u0026quot;params\u0026quot;:\u0026quot;{a=b}\u0026quot;, \u0026quot;comment\u0026quot;:\u0026quot;Hello there\u0026quot;, \u0026quot;database\u0026quot;:\u0026quot;newdb\u0026quot; } JSON Output (error) { \u0026quot;error\u0026quot;: \u0026quot;No such database: newdb\u0026quot;, \u0026quot;errorCode\u0026quot;: 404 } Navigation Links Previous: GET ddl/database Next: PUT ddl/database/:db\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getdb_34016250/","tags":null,"title":"Apache Hive : WebHCat Reference GetDB"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetDBs List Databases — GET ddl/database  List Databases — GET ddl/database  Description URL Parameters Results Example  Curl Command JSON Output      Description List the databases in HCatalog.\nURL http://www.myserver.com/templeton/v1/ddl/database\nParameters    Name Description Required? Default     like List only databases whose names match the specified pattern. Optional \u0026ldquo;*\u0026rdquo; (List all)    The standard parameters are also supported.\nResults    Name Description     databases A list of database names.    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database?user.name=ctdean\u0026amp;like=n*' JSON Output { \u0026quot;databases\u0026quot;: [ \u0026quot;newdb\u0026quot;, \u0026quot;newdb2\u0026quot; ] } Navigation Links Previous: POST ddl\nNext: GET ddl/database/:db\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getdbs_34016238/","tags":null,"title":"Apache Hive : WebHCat Reference GetDBs"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetPartition Describe Partition — GET ddl/database/:db/table/:table/partition/:partition  Describe Partition — GET ddl/database/:db/table/:table/partition/:partition  Description URL Parameters Results Example  Curl Command JSON Output      Description Describe a single partition in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/partition/:partition\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :partition The partition name, col_name=\u0026lsquo;value\u0026rsquo; list. Be careful to properly encode the quote for http, for example, country=%27algeria%27. Required None    The standard parameters are also supported.\nResults    Name Description     database The database name   table The table name   partition The partition name   partitioned True if the table is partitioned   location Location of table   outputFormat Output format   columns List of column names, types, and comments   owner The owner\u0026rsquo;s user name   partitionColumns List of the partition columns   inputFormat Input format    Example Curl Command % curl -s \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/mytest/partition/country=%27US%27?user.name=ctdean' JSON Output { \u0026quot;partitioned\u0026quot;: true, \u0026quot;location\u0026quot;: \u0026quot;hdfs://ip-10-77-6-151.ec2.internal:8020/apps/hive/warehouse/mytest/loc1\u0026quot;, \u0026quot;outputFormat\u0026quot;: \u0026quot;org.apache.hadoop.hive.ql.io.RCFileOutputFormat\u0026quot;, \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;i\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;int\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;j\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bigint\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;ip\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;IP Address of the User\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;owner\u0026quot;: \u0026quot;rachel\u0026quot;, \u0026quot;partitionColumns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;country\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;inputFormat\u0026quot;: \u0026quot;org.apache.hadoop.hive.ql.io.RCFileInputFormat\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;mytest\u0026quot;, \u0026quot;partition\u0026quot;: \u0026quot;country='US'\u0026quot; } Navigation Links Previous: GET ddl/database/:db/table/:table/partition Next: PUT ddl/database/:db/table/:table/partition/:partition\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getpartition_34016592/","tags":null,"title":"Apache Hive : WebHCat Reference GetPartition"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetPartitions List Partitions — GET ddl/database/:db/table/:table/partition  List Partitions — GET ddl/database/:db/table/:table/partition  Description URL Parameters Results Example  Curl Command JSON Output      Description List all the partitions in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/partition\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None    The standard parameters are also supported.\nResults    Name Description     partitions A list of partition values and of partition names   database The database name   table The table name    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table/partition?user.name=ctdean' JSON Output { \u0026quot;partitions\u0026quot;: [ { \u0026quot;values\u0026quot;: [ { \u0026quot;columnName\u0026quot;: \u0026quot;dt\u0026quot;, \u0026quot;columnValue\u0026quot;: \u0026quot;20120101\u0026quot; }, { \u0026quot;columnName\u0026quot;: \u0026quot;country\u0026quot;, \u0026quot;columnValue\u0026quot;: \u0026quot;US\u0026quot; } ], \u0026quot;name\u0026quot;: \u0026quot;dt='20120101',country='US'\u0026quot; } ], \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;my_table\u0026quot; } Navigation Links Previous: PUT ddl/database/:db/table/:existingtable/like/:newtable Next: GET ddl/database/:db/table/:table/partition/:partition\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getpartitions_34016583/","tags":null,"title":"Apache Hive : WebHCat Reference GetPartitions"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetProperties List Properties — GET ddl/database/:db/table/:table/property  List Properties — GET ddl/database/:db/table/:table/property  Description URL Parameters Results Example  Curl Command JSON Output      Description List all the properties of an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/property\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None    The standard parameters are also supported.\nResults    Name Description     properties A list of the table\u0026rsquo;s properties in name: value pairs   database The database name   table The table name    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property?user.name=ctdean' JSON Output { \u0026quot;properties\u0026quot;: { \u0026quot;fruit\u0026quot;: \u0026quot;apple\u0026quot;, \u0026quot;last_modified_by\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;hcat.osd\u0026quot;: \u0026quot;org.apache.hcatalog.rcfile.RCFileOutputDriver\u0026quot;, \u0026quot;color\u0026quot;: \u0026quot;blue\u0026quot;, \u0026quot;last_modified_time\u0026quot;: \u0026quot;1331620706\u0026quot;, \u0026quot;hcat.isd\u0026quot;: \u0026quot;org.apache.hcatalog.rcfile.RCFileInputDriver\u0026quot;, \u0026quot;transient_lastDdlTime\u0026quot;: \u0026quot;1331620706\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;Best table made today\u0026quot;, \u0026quot;country\u0026quot;: \u0026quot;Albania\u0026quot; }, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } Navigation Links Previous: PUT ddl/database/:db/table/:table/column/:column Next: GET ddl/database/:db/table/:table/property/:property\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getproperties_34016995/","tags":null,"title":"Apache Hive : WebHCat Reference GetProperties"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetProperty Property Value — GET ddl/database/:db/table/:table/property/:property  Property Value — GET ddl/database/:db/table/:table/property/:property  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description Return the value of a single table property.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/property/:property\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :property The property name Required None    The standard parameters are also supported.\nResults    Name Description     property The requested property\u0026rsquo;s name: value pair   database The database name   table The table name    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property/fruit?user.name=ctdean' JSON Output { \u0026quot;property\u0026quot;: { \u0026quot;fruit\u0026quot;: \u0026quot;apple\u0026quot; }, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } JSON Output (error) { \u0026quot;error\u0026quot;: \u0026quot;Table test_table does not exist\u0026quot;, \u0026quot;errorCode\u0026quot;: 404, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot; } Navigation Links Previous: GET ddl/database/:db/table/:table/property Next: PUT ddl/database/:db/table/:table/property/:property\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-getproperty_34017004/","tags":null,"title":"Apache Hive : WebHCat Reference GetProperty"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetTable Describe Table — GET ddl/database/:db/table/:table  Describe Table — GET ddl/database/:db/table/:table  Description URL Parameters Results Example  Curl Command (simple) JSON Output (simple) Curl Command (extended) JSON Output (extended) JSON Output (error)      Description Describe an HCatalog table. Normally returns a simple list of columns (using \u0026ldquo;desc table\u0026rdquo;), but the extended format will show more information (using \u0026ldquo;show table extended like\u0026rdquo;).\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table\nhttp://www.myserver.com/templeton/v1/ddl/database/:db/table/:table?format=extended\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   format Set \u0026ldquo;format=extended\u0026rdquo; to see additional information (using \u0026ldquo;show table extended like\u0026rdquo;) Optional Not extended    The standard parameters are also supported.\nResults    Name Description     columns A list of column names and types   database The database name   table The table name   partitioned (extended only) True if the table is partitioned   location (extended only) Location of table   outputFormat (extended only) Output format   owner (extended only) The owner\u0026rsquo;s user name   partitionColumns (extended only) List of the partition columns   inputFormat (extended only) Input format    Example Curl Command (simple) % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table?user.name=ctdean' JSON Output (simple) { \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bigint\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;user\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The user name\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;my_p\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;my_q\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;my_table\u0026quot; } Curl Command (extended) % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean\u0026amp;format=extended' JSON Output (extended) { \u0026quot;partitioned\u0026quot;: true, \u0026quot;location\u0026quot;: \u0026quot;hdfs://ip-10-77-6-151.ec2.internal:8020/apps/hive/warehouse/test_table\u0026quot;, \u0026quot;outputFormat\u0026quot;: \u0026quot;org.apache.hadoop.hive.ql.io.RCFileOutputFormat\u0026quot;, \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bigint\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;price\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The unit price\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot; } ], \u0026quot;owner\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;partitionColumns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;country\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;inputFormat\u0026quot;: \u0026quot;org.apache.hadoop.hive.ql.io.RCFileInputFormat\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot; } JSON Output (error) { \u0026quot;error\u0026quot;: \u0026quot;Table xtest_table does not exist\u0026quot;, \u0026quot;errorCode\u0026quot;: 404, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;xtest_table\u0026quot; } Navigation Links Previous: GET ddl/database/:db/table Next: PUT ddl/database/:db/table/:table\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-gettable_34016519/","tags":null,"title":"Apache Hive : WebHCat Reference GetTable"},{"categories":null,"contents":"Apache Hive : WebHCat Reference GetTables List Tables — GET ddl/database/:db/table  List Tables — GET ddl/database/:db/table  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description List the tables in an HCatalog database.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table\nParameters    Name Description Required? Default     :db The database name Required None   like List only tables whose names match the specified pattern Optional \u0026ldquo;*\u0026rdquo; (List all tables)    The standard parameters are also supported.\nResults    Name Description     tables A list of table names   database The database name    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table?user.name=ctdean\u0026amp;like=m*' JSON Output { \u0026quot;tables\u0026quot;: [ \u0026quot;my_table\u0026quot;, \u0026quot;my_table_2\u0026quot;, \u0026quot;my_table_3\u0026quot; ], \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } JSON Output (error) { \u0026quot;errorDetail\u0026quot;: \u0026quot; org.apache.hadoop.hive.ql.metadata.HiveException: ERROR: The database defaultsd does not exist. at org.apache.hadoop.hive.ql.exec.DDLTask.switchDatabase(DDLTask.java:3122) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:224) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:134) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:57) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1332) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1123) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:931) at org.apache.hcatalog.cli.HCatDriver.run(HCatDriver.java:42) at org.apache.hcatalog.cli.HCatCli.processCmd(HCatCli.java:247) at org.apache.hcatalog.cli.HCatCli.processLine(HCatCli.java:203) at org.apache.hcatalog.cli.HCatCli.main(HCatCli.java:162) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156) \u0026quot;, \u0026quot;error\u0026quot;: \u0026quot;FAILED: Error in metadata: ERROR: The database defaultsd does not exist.\u0026quot;, \u0026quot;errorCode\u0026quot;: 500, \u0026quot;database\u0026quot;: \u0026quot;defaultsd\u0026quot; } Navigation Links Previous: DELETE ddl/database/:db Next: GET ddl/database/:db/table/:table\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-gettables_34016290/","tags":null,"title":"Apache Hive : WebHCat Reference GetTables"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Hive Hive Job — POST hive  Hive Job — POST hive  Description URL Parameters Results Example  Curl Command JSON Output Example Results      Description Runs a Hive query or set of commands.\nVersion: Hive 0.13.0 and later\nAs of Hive 0.13.0, GET version/hive displays the Hive version used for the query or commands.\nURL http://www.myserver.com/templeton/v1/hive\nParameters    Name Description Required? Default     execute String containing an entire, short Hive program to run. One of either \u0026ldquo;execute\u0026rdquo; or \u0026ldquo;file\u0026rdquo; is required. None   file HDFS file name of a Hive program to run. One of either \u0026ldquo;execute\u0026rdquo; or \u0026ldquo;file\u0026rdquo; is required. None   define Set a Hive configuration variable using the syntax define=NAME=VALUE. See a note CURL and \u0026ldquo;=\u0026rdquo;. Optional None   arg Set a program argument. This parameter was introduced in Hive 0.12.0. (See HIVE-4444.) Optional in Hive 0.12.0+ None   files Comma-separated files to be copied to the map reduce cluster. This parameter was introduced in Hive 0.12.0. (See HIVE-4444.) Optional in Hive 0.12.0+ None   statusdir A directory where WebHCat will write the status of the Hive job. If provided, it is the caller\u0026rsquo;s responsibility to remove this directory when done. Optional None   enablelog If statusdir is set and enablelog is \u0026ldquo;true\u0026rdquo;, collect Hadoop job configuration and logs into a directory named $statusdir/logs after the job finishes. Both completed and failed attempts are logged. The layout of subdirectories in $statusdir/logs is: logs/$job_id (directory for $job_id) logs/$job_id/job.xml.html logs/$job_id/$attempt_id (directory for $attempt_id) logs/$job_id/$attempt_id/stderr logs/$job_id/$attempt_id/stdout logs/$job_id/$attempt_id/syslog This parameter was introduced in Hive 0.12.0. (See HIVE-4531.) Optional in Hive 0.12.0+ None   callback Define a URL to be called upon job completion. You may embed a specific job ID into this URL using $jobId. This tag will be replaced in the callback URL with this job\u0026rsquo;s job ID. Optional None    The standard parameters are also supported.\nResults    Name Description     id A string containing the job ID similar to \u0026ldquo;job_201110132141_0001\u0026rdquo;.   info A JSON object containing the information returned when the job was queued. See the Hadoop documentation (Class TaskController) for more information.    Example Curl Command % curl -s -d execute=\u0026quot;select+*+from+pokes;\u0026quot; \\ -d statusdir=\u0026quot;pokes.output\u0026quot; \\ 'http://localhost:50111/templeton/v1/hive?user.name=ekoifman' Version information\nPrior to Hive 0.13.0, user.name was specified in POST requests as a form parameter: curl -d user.name=*\u0026lt;user\u0026gt;*.\nIn Hive 0.13.0 onward, user.name should be specified in the query string (as shown above): 'http://.../templeton/v1/hive?user.name=*\u0026lt;name\u0026gt;*'. Specifying user.name as a form parameter is deprecated.\nJSON Output { \u0026quot;id\u0026quot;: \u0026quot;job_201111111311_0005\u0026quot;, \u0026quot;info\u0026quot;: { \u0026quot;stdout\u0026quot;: \u0026quot;templeton-job-id:job_201111111311_0005 \u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;exitcode\u0026quot;: 0 } } Example Results % hadoop fs -ls pokes.output Found 2 items -rw-r--r-- 1 ctdean supergroup 610 2011-11-11 13:22 /user/ctdean/pokes.output/stderr -rw-r--r-- 1 ctdean supergroup 15 2011-11-11 13:22 /user/ctdean/pokes.output/stdout % hadoop fs -cat pokes.output/stdout 1 a 2 bb 3 ccc Navigation Links Previous: POST pig\nNext: GET queue\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-hive_34017180/","tags":null,"title":"Apache Hive : WebHCat Reference Hive"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Job Job Information — GET jobs/:jobid  Job Information — GET jobs/:jobid  Description URL Parameters Results Example  Curl Command JSON Output      Description Check the status of a job and get related job information given its job ID. Substitute \u0026ldquo;:jobid\u0026rdquo; with the job ID received when the job was created.\nVersion: Hive 0.12.0 and later\nGET jobs/:jobid is introduced in Hive release 0.12.0. It is equivalent to [GET queue/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-jobinfo_34017194/) in prior releases.\nGET queue/:jobid is now deprecated (HIVE-4443) and will be removed in Hive 0.14.0 (HIVE-6432).\nURL http://www.myserver.com/templeton/v1/jobs/:jobid\nParameters    Name Description Required? Default     :jobid The job ID to check. This is the ID received when the job was created. Required None    The standard parameters are also supported.\nResults    Name Description     status A JSON object containing the job status information. See the Hadoop documentation (Class JobStatus) for more information.   profile A JSON object containing the job profile information. WebHCat passes along the information in the JobProfile object, which is subject to change from one Hadoop version to another. See the Hadoop documentation (API docs) for org.apache.hadoop.mapred.JobProfile for more information.   id The job ID.   parentId The parent job ID.   percentComplete The job completion percentage, for example \u0026ldquo;75% complete\u0026rdquo;.   exitValue The job\u0026rsquo;s exit value.   user User name of the job creator.   callback The callback URL, if any.   completed A string representing completed status of the process launched by the Launcher task. For example when a MapReduce job is submitted via WebHCat, the Launcher invokes a \u0026ldquo;hadoop jar\u0026rdquo; command and then when that process exits the completed string is set to \u0026ldquo;done\u0026rdquo;. Note that this is not the same as the job status (see status).   userargs A JSON object repesenting the argument names and values for the job submission request.    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/jobs/job_201112212038_0004?user.name=ctdean' JSON Output { \u0026quot;status\u0026quot;: { \u0026quot;startTime\u0026quot;: 1324529476131, \u0026quot;username\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201112212038\u0026quot;, \u0026quot;id\u0026quot;: 4 }, \u0026quot;jobACLs\u0026quot;: { }, \u0026quot;schedulingInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;failureInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot;, \u0026quot;jobPriority\u0026quot;: \u0026quot;NORMAL\u0026quot;, \u0026quot;runState\u0026quot;: 2, \u0026quot;jobComplete\u0026quot;: true }, \u0026quot;profile\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201112212038\u0026quot;, \u0026quot;id\u0026quot;: 4 }, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;queueName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;jobFile\u0026quot;: \u0026quot;hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml\u0026quot;, \u0026quot;jobName\u0026quot;: \u0026quot;PigLatin:DefaultJobName\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot; }, \u0026quot;id\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;job_201112212038_0003\u0026quot;, \u0026quot;percentComplete\u0026quot;: \u0026quot;100% complete\u0026quot;, \u0026quot;exitValue\u0026quot;: 0, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;callback\u0026quot;: null, \u0026quot;completed\u0026quot;: \u0026quot;done\u0026quot;, \u0026quot;userargs\u0026quot; =\u0026gt; { \u0026quot;callback\u0026quot; =\u0026gt; null, \u0026quot;define\u0026quot; =\u0026gt; [], \u0026quot;execute\u0026quot; =\u0026gt; \u0026quot;select a,rand(b) from mynums\u0026quot;, \u0026quot;file\u0026quot; =\u0026gt; null, \u0026quot;statusdir\u0026quot; =\u0026gt; null, \u0026quot;user.name\u0026quot; =\u0026gt; \u0026quot;hadoopqa\u0026quot;, }, } Navigation Links Previous: GET jobs\nNext: DELETE jobs/:jobid\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaces deprecated resource: GET queue/:jobid\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-job_34835065/","tags":null,"title":"Apache Hive : WebHCat Reference Job"},{"categories":null,"contents":"Apache Hive : WebHCat Reference JobIDs List JobIDs — GET queue  List JobIDs — GET queue  Description URL Parameters Results Example  Curl Command JSON Output      Description Return a list of all job IDs.\nVersion: Deprecated in 0.12.0\nGET queue is deprecated starting in Hive release 0.12.0. (See HIVE-4443.) Users are encouraged to use [GET jobs](https://hive.apache.org/docs/latest/webhcat-reference-jobs_34835057/) instead.\nVersion: Obsolete in 0.14.0\nGET queue will be removed in Hive release 0.14.0. (See HIVE-6432.)\nUse [GET jobs](https://hive.apache.org/docs/latest/webhcat-reference-jobs_34835057/) instead.\nURL http://www.myserver.com/templeton/v1/queue\nParameters    Name Description Required? Default     showall If showall is set to \u0026ldquo;true\u0026rdquo;, then the request will return all jobs the user has permission to view, not only the jobs belonging to the user. This parameter is not available in releases prior to Hive 0.12.0. (See HIVE-4442.) Optional in Hive 0.12.0+ false    The standard parameters are also accepted.\nResults    Name Description     ids A list of all job IDs either belonging to the user, or (Hive 0.12.0 and later) job IDs the user has permission to view if showall is true.    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/queue?user.name=ctdean' JSON Output { \u0026quot;job_201111111311_0008\u0026quot;, \u0026quot;job_201111111311_0012\u0026quot; } Navigation Links Previous: POST hive\nNext: GET queue/:jobid\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaced in Hive 0.12.0 by: GET jobs\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-jobids_34017187/","tags":null,"title":"Apache Hive : WebHCat Reference JobIDs"},{"categories":null,"contents":"Apache Hive : WebHCat Reference JobInfo Job Information — GET queue/:jobid  Job Information — GET queue/:jobid  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (Hive 0.12.0 and later)      Description Check the status of a job and get related job information given its job ID. Substitute \u0026ldquo;:jobid\u0026rdquo; with the job ID received when the job was created.\nVersion: Deprecated in 0.12.0\nGET queue/:jobid is deprecated starting in Hive release 0.12.0. Users are encouraged to use GET jobs/:jobid instead. (See HIVE-4443.)\nGET queue/:jobid is equivalent to GET jobs/:jobid – check [GET jobs/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-job_34835065/) for documentation.\nVersion: Obsolete in 0.14.0\nGET queue/:jobid will be removed in Hive release 0.14.0. (See HIVE-6432.)\nUse [GET jobs/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-job_34835065/) instead.\nURL http://www.myserver.com/templeton/v1/queue/:jobid\nParameters    Name Description Required? Default     :jobid The job ID to check. This is the ID received when the job was created. Required None    The standard parameters are also supported.\nResults    Name Description     status A JSON object containing the job status information. See the Hadoop documentation (Class JobStatus) for more information.   profile A JSON object containing the job profile information. WebHCat passes along the information in the JobProfile object, which is subject to change from one Hadoop version to another. See the Hadoop documentation (API docs) for org.apache.hadoop.mapred.JobProfile for more information.   id The job ID.   parentId The parent job ID.   percentComplete The job completion percentage, for example \u0026ldquo;75% complete\u0026rdquo;.   exitValue The job\u0026rsquo;s exit value.   user User name of the job creator.   callback The callback URL, if any.   completed A string representing completed status, for example \u0026ldquo;done\u0026rdquo;.    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/queue/job_201112212038_0004?user.name=ctdean' JSON Output { \u0026quot;status\u0026quot;: { \u0026quot;startTime\u0026quot;: 1324529476131, \u0026quot;username\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201112212038\u0026quot;, \u0026quot;id\u0026quot;: 4 }, \u0026quot;jobACLs\u0026quot;: { }, \u0026quot;schedulingInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;failureInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot;, \u0026quot;jobPriority\u0026quot;: \u0026quot;NORMAL\u0026quot;, \u0026quot;runState\u0026quot;: 2, \u0026quot;jobComplete\u0026quot;: true }, \u0026quot;profile\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201112212038\u0026quot;, \u0026quot;id\u0026quot;: 4 }, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;queueName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;jobFile\u0026quot;: \u0026quot;hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml\u0026quot;, \u0026quot;jobName\u0026quot;: \u0026quot;PigLatin:DefaultJobName\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot; }, \u0026quot;id\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;job_201112212038_0003\u0026quot;, \u0026quot;percentComplete\u0026quot;: \u0026quot;100% complete\u0026quot;, \u0026quot;exitValue\u0026quot;: 0, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;callback\u0026quot;: null, \u0026quot;completed\u0026quot;: \u0026quot;done\u0026quot; } JSON Output (Hive 0.12.0 and later) Version: Hive 0.12.0 and later\nStarting in Hive release 0.12.0, GET queue/:jobid returns user arguments as well as status information (HIVE-5031).\n{ \u0026quot;status\u0026quot;: { \u0026quot;startTime\u0026quot;: 1324529476131, \u0026quot;username\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201112212038\u0026quot;, \u0026quot;id\u0026quot;: 4 }, \u0026quot;jobACLs\u0026quot;: { }, \u0026quot;schedulingInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;failureInfo\u0026quot;: \u0026quot;NA\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot;, \u0026quot;jobPriority\u0026quot;: \u0026quot;NORMAL\u0026quot;, \u0026quot;runState\u0026quot;: 2, \u0026quot;jobComplete\u0026quot;: true }, \u0026quot;profile\u0026quot;: { \u0026quot;url\u0026quot;: \u0026quot;http://localhost:50030/jobdetails.jsp?jobid=job_201112212038_0004\u0026quot;, \u0026quot;jobID\u0026quot;: { \u0026quot;jtIdentifier\u0026quot;: \u0026quot;201112212038\u0026quot;, \u0026quot;id\u0026quot;: 4 }, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;queueName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;jobFile\u0026quot;: \u0026quot;hdfs://localhost:9000/tmp/hadoop-ctdean/mapred/staging/ctdean/.staging/job_201112212038_0004/job.xml\u0026quot;, \u0026quot;jobName\u0026quot;: \u0026quot;PigLatin:DefaultJobName\u0026quot;, \u0026quot;jobId\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot; }, \u0026quot;id\u0026quot;: \u0026quot;job_201112212038_0004\u0026quot;, \u0026quot;parentId\u0026quot;: \u0026quot;job_201112212038_0003\u0026quot;, \u0026quot;percentComplete\u0026quot;: \u0026quot;100% complete\u0026quot;, \u0026quot;exitValue\u0026quot;: 0, \u0026quot;user\u0026quot;: \u0026quot;ctdean\u0026quot;, \u0026quot;callback\u0026quot;: null, \u0026quot;completed\u0026quot;: \u0026quot;done\u0026quot;, \u0026quot;userargs\u0026quot; =\u0026gt; { \u0026quot;callback\u0026quot; =\u0026gt; null, \u0026quot;define\u0026quot; =\u0026gt; [], \u0026quot;enablelog\u0026quot; =\u0026gt; \u0026quot;false\u0026quot;, \u0026quot;execute\u0026quot; =\u0026gt; \u0026quot;select a,rand(b) from mynums\u0026quot;, \u0026quot;file\u0026quot; =\u0026gt; null, \u0026quot;files\u0026quot; =\u0026gt; [], \u0026quot;statusdir\u0026quot; =\u0026gt; null, \u0026quot;user.name\u0026quot; =\u0026gt; \u0026quot;hadoopqa\u0026quot;, }, } Navigation Links Previous: GET queue\nNext: DELETE queue/:jobid\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaced in Hive 0.12.0 by: GET jobs/:jobid\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-jobinfo_34017194/","tags":null,"title":"Apache Hive : WebHCat Reference JobInfo"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Jobs List JobIDs — GET jobs  List JobIDs — GET jobs  Description URL Parameters Results Examples  Curl Command JSON Output Curl Command (showall) JSON Output (showall) Curl Command (fields) JSON Output (fields)      Description Return a list of all job IDs.\nVersion: Hive 0.12.0 and later\nGET jobs is introduced in Hive release 0.12.0. It is equivalent to [GET queue](https://hive.apache.org/docs/latest/webhcat-reference-jobids_34017187/) in prior releases.\nGET queue is now deprecated (HIVE-4443) and will be removed in Hive 0.14.0 (HIVE-6432).\nURL http://www.myserver.com/templeton/v1/jobs\nParameters    Name Description Required? Default     fields If fields set to \u0026ldquo;\u0026rdquo;, the request will return full details of the job. If fields is missing, will only return the job ID. Currently the value can only be \u0026ldquo;\u0026rdquo;, other values are not allowed and will throw exception. Optional None   showall If showall is set to \u0026ldquo;true\u0026rdquo;, the request will return all jobs the user has permission to view, not only the jobs belonging to the user. Optional false   jobid If jobid is present, only the records whose job ID is lexicographically greater than jobid are returned. For example, if jobid = \u0026ldquo;job_201312091733_0001\u0026rdquo;, the jobs whose job ID is greater than \u0026ldquo;job_201312091733_0001\u0026rdquo; are returned. The number of records returned depends on the value of numrecords.This parameter is not available in releases prior to Hive 0.13.0. (See HIVE-5519.) Optional in Hive 0.13.0+ None   numrecords If the jobid and numrecords parameters are present, the top numrecords records appearing after jobid will be returned after sorting the job ID list lexicographically. If the jobid parameter is missing and numrecords is present, the top numrecords will be returned after lexicographically sorting the job ID list. If the jobid parameter is present and numrecords is missing, all the records whose job ID is greater than jobid are returned.This parameter is not available in releases prior to Hive 0.13.0. (See HIVE-5519.) Optional in Hive 0.13.0+ All    The standard parameters are also accepted.\nResults Returns an array of jobs either belonging to the user, or which the user has permission to view (showall=\u0026ldquo;true\u0026rdquo;), based on the filter conditions specified by the user.\nEvery element inside the array includes:\n   Name Description     id Job ID.   detail Job details if showall is set to \u0026ldquo;true\u0026rdquo;; otherwise \u0026ldquo;null\u0026rdquo;. For more information about what details it contains, check [GET jobs/:jobid](https://hive.apache.org/docs/latest/webhcat-reference-job_34835065/).    Examples Curl Command % curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=daijy' JSON Output [ {\u0026quot;id\u0026quot;:\u0026quot;job_201304291205_0015\u0026quot;,\u0026quot;detail\u0026quot;:null} ] Curl Command (showall) % curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=daijy\u0026amp;showall=true' JSON Output (showall) [ {\u0026quot;id\u0026quot;:\u0026quot;job_201304291205_0014\u0026quot;,\u0026quot;detail\u0026quot;:null}, {\u0026quot;id\u0026quot;:\u0026quot;job_201111111311_0015\u0026quot;,\u0026quot;detail\u0026quot;:null}, ] Curl Command (fields) % curl -s 'http://localhost:50111/templeton/v1/jobs?user.name=daijy\u0026amp;fields=*' JSON Output (fields) Hive 0.12.0 bug\nIn release 0.12.0 the first line of JSON output for the fields parameter gives the parent jobid instead of the actual jobid (HIVE-5510). The example below shows the correct jobid, as displayed in release 0.13.0 and later.\n[{\u0026quot;id\u0026quot;:\u0026quot;job_201304291205_0016\u0026quot;, \u0026quot;detail\u0026quot;:{ \u0026quot;status\u0026quot;:{ \u0026quot;jobACLs\u0026quot;:{ \u0026quot;MODIFY_JOB\u0026quot;:{\u0026quot;allAllowed\u0026quot;:false,\u0026quot;aclstring\u0026quot;:\u0026quot; \u0026quot;}, \u0026quot;VIEW_JOB\u0026quot;:{\u0026quot;allAllowed\u0026quot;:false,\u0026quot;aclstring\u0026quot;:\u0026quot; \u0026quot;}}, \u0026quot;runState\u0026quot;:2, \u0026quot;startTime\u0026quot;:1367264912274, \u0026quot;schedulingInfo\u0026quot;:\u0026quot;NA\u0026quot;, \u0026quot;failureInfo\u0026quot;:\u0026quot;NA\u0026quot;, \u0026quot;jobPriority\u0026quot;:\u0026quot;NORMAL\u0026quot;, \u0026quot;username\u0026quot;:\u0026quot;daijy\u0026quot;, \u0026quot;jobID\u0026quot;:{\u0026quot;id\u0026quot;:16,\u0026quot;jtIdentifier\u0026quot;:\u0026quot;201304291205\u0026quot;}, \u0026quot;jobId\u0026quot;:\u0026quot;job_201304291205_0016\u0026quot;, \u0026quot;jobComplete\u0026quot;:true}, \u0026quot;profile\u0026quot;:{ \u0026quot;user\u0026quot;:\u0026quot;daijy\u0026quot;, \u0026quot;jobFile\u0026quot;:\u0026quot;hdfs://localhost:8020/Users/daijy/hadoop-1.0.3/tmp/mapred/staging/ daijy/.staging/job_201304291205_0016/job.xml\u0026quot;, \u0026quot;url\u0026quot;:\u0026quot;http://localhost:50030/jobdetails.jsp?jobid=job_201304291205_0016\u0026quot;, \u0026quot;queueName\u0026quot;:\u0026quot;default\u0026quot;, \u0026quot;jobName\u0026quot;:\u0026quot;word count\u0026quot;, \u0026quot;jobID\u0026quot;:{\u0026quot;id\u0026quot;:16,\u0026quot;jtIdentifier\u0026quot;:\u0026quot;201304291205\u0026quot;}, \u0026quot;jobId\u0026quot;:\u0026quot;job_201304291205_0016\u0026quot;}, \u0026quot;id\u0026quot;:\u0026quot;job_201304291205_0016\u0026quot;, \u0026quot;parentId\u0026quot;:\u0026quot;job_201304291205_0015\u0026quot;, \u0026quot;percentComplete\u0026quot;:\u0026quot;map 100% reduce 100%\u0026quot;, \u0026quot;exitValue\u0026quot;:0, \u0026quot;user\u0026quot;:\u0026quot;daijy\u0026quot;, \u0026quot;callback\u0026quot;:\u0026quot;http://daijymacpro.local:57815/templeton/$jobId\u0026quot;, \u0026quot;completed\u0026quot;: \u0026quot;done\u0026quot;, \u0026quot;userargs\u0026quot; =\u0026gt; { \u0026quot;callback\u0026quot; =\u0026gt; null, \u0026quot;define\u0026quot; =\u0026gt; [], \u0026quot;enablelog\u0026quot; =\u0026gt; \u0026quot;false\u0026quot;, \u0026quot;execute\u0026quot; =\u0026gt; \u0026quot;select a,rand(b) from mynums\u0026quot;, \u0026quot;file\u0026quot; =\u0026gt; null, \u0026quot;files\u0026quot; =\u0026gt; [], \u0026quot;statusdir\u0026quot; =\u0026gt; null, \u0026quot;user.name\u0026quot; =\u0026gt; \u0026quot;hadoopqa\u0026quot;, }, }]    Navigation Links Previous: DELETE queue/:jobid\nNext: GET jobs/:jobid\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaces deprecated resource: GET queue\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-jobs_34835057/","tags":null,"title":"Apache Hive : WebHCat Reference Jobs"},{"categories":null,"contents":"Apache Hive : WebHCat Reference MapReduceJar MapReduce Job — POST mapreduce/jar  MapReduce Job — POST mapreduce/jar  Description URL Parameters Results Example  Code and Data Setup Curl Command JSON Output      Description Creates and queues a standard Hadoop MapReduce job.\nVersion: Hive 0.13.0 and later\nAs of Hive 0.13.0, GET version/hadoop displays the Hadoop version used for the MapReduce job.\nURL http://www.myserver.com/templeton/v1/mapreduce/jar\nParameters    Name Description Required? Default     jar Name of the jar file for Map Reduce to use. Required None   class Name of the class for Map Reduce to use. Required None   libjars Comma separated jar files to include in the classpath. Optional None   files Comma separated files to be copied to the map reduce cluster. Optional None   arg Set a program argument. Optional None   define Set a Hadoop configuration variable using the syntax define=NAME=VALUE Optional None   statusdir A directory where WebHCat will write the status of the Map Reduce job. If provided, it is the caller\u0026rsquo;s responsibility to remove this directory when done. Optional None   enablelog If statusdir is set and enablelog is \u0026ldquo;true\u0026rdquo;, collect Hadoop job configuration and logs into a directory named $statusdir/logs after the job finishes. Both completed and failed attempts are logged. The layout of subdirectories in $statusdir/logs is: logs/$job_id (directory for $job_id) logs/$job_id/job.xml.html logs/$job_id/$attempt_id (directory for $attempt_id) logs/$job_id/$attempt_id/stderr logs/$job_id/$attempt_id/stdout logs/$job_id/$attempt_id/syslog This parameter was introduced in Hive 0.12.0. (See HIVE-4531.) Optional in Hive 0.12.0+ None   callback Define a URL to be called upon job completion. You may embed a specific job ID into this URL using $jobId. This tag will be replaced in the callback URL with this job\u0026rsquo;s job ID. Optional None   usehcatalog Specify that the submitted job uses HCatalog and therefore needs to access the metastore, which requires additional steps for WebHCat to perform in a secure cluster. (See HIVE-5133.) This parameter will be introduced in Hive 0.13.0. Also, if webhcat-site.xml defines the parameters templeton.hive.archive, templeton.hive.home and templeton.hcat.home then WebHCat will ship the Hive tar to the target node where the job runs. (See HIVE-5547.) This means that Hive doesn\u0026rsquo;t need to be installed on every node in the Hadoop cluster. This is independent of security, but improves manageability. The webhcat-site.xml parameters are documented in webhcat-default.xml. Optional in Hive 0.13.0+ false    The standard parameters are also supported.\nResults    Name Description     id A string containing the job ID similar to \u0026ldquo;job_201110132141_0001\u0026rdquo;.   info A JSON object containing the information returned when the job was queued. See the Hadoop documentation (Class TaskController) for more information.    Example Code and Data Setup % hadoop fs -put wordcount.jar . % hadoop fs -put transform.jar . % hadoop fs -ls . Found 2 items -rw-r--r-- 1 ctdean supergroup 23 2011-11-11 13:29 /user/ctdean/wordcount.jar -rw-r--r-- 1 ctdean supergroup 28 2011-11-11 13:29 /user/ctdean/transform.jar Curl Command % curl -s -d jar=wordcount.jar \\ -d class=org.myorg.WordCount \\ -d libjars=transform.jar \\ -d arg=wordcount/input \\ -d arg=wordcount/output \\ 'http://localhost:50111/templeton/v1/mapreduce/jar?user.name=ekoifman' Version information\nPrior to Hive 0.13.0, user.name was specified in POST requests as a form parameter: curl -d user.name=*\u0026lt;user\u0026gt;*.\nIn Hive 0.13.0 onward, user.name should be specified in the query string (as shown above): 'http://.../templeton/v1/mapreduce/jar?user.name=*\u0026lt;name\u0026gt;*'. Specifying user.name as a form parameter is deprecated.\nJSON Output { \u0026quot;id\u0026quot;: \u0026quot;job_201111121211_0001\u0026quot;, \u0026quot;info\u0026quot;: { \u0026quot;stdout\u0026quot;: \u0026quot;templeton-job-id:job_201111121211_0001 \u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;exitcode\u0026quot;: 0 } } Navigation Links Previous: POST mapreduce/streaming\nNext: POST pig\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-mapreducejar_34017030/","tags":null,"title":"Apache Hive : WebHCat Reference MapReduceJar"},{"categories":null,"contents":"Apache Hive : WebHCat Reference MapReduceStream MapReduce Streaming Job — POST mapreduce/streaming  MapReduce Streaming Job — POST mapreduce/streaming  Description URL Parameters Results Example  Code and Data Setup Curl Command JSON Output Example Results      Description Create and queue a Hadoop streaming MapReduce job.\nVersion: Hive 0.13.0 and later\nAs of Hive 0.13.0, GET version/hadoop displays the Hadoop version used for the MapReduce job.\nURL http://www.myserver.com/templeton/v1/mapreduce/streaming\nParameters    Name Description Required? Default     input Location of the input data in Hadoop. Required None   output Location in which to store the output data. If not specified, WebHCat will store the output in a location that can be discovered using the queue resource. Optional See description   mapper Location of the mapper program in Hadoop. Required None   reducer Location of the reducer program in Hadoop. Required None   file Add an HDFS file to the distributed cache. Optional None   define Set a Hadoop configuration variable using the syntax define=NAME=VALUE Optional None   cmdenv Set an environment variable using the syntax cmdenv=NAME=VALUE Optional None   arg Set a program argument. Optional None   statusdir A directory where WebHCat will write the status of the Map Reduce job. If provided, it is the caller\u0026rsquo;s responsibility to remove this directory when done. Optional None   enablelog If statusdir is set and enablelog is \u0026ldquo;true\u0026rdquo;, collect Hadoop job configuration and logs into a directory named $statusdir/logs after the job finishes. Both completed and failed attempts are logged. The layout of subdirectories in $statusdir/logs is: logs/$job_id (directory for $job_id) logs/$job_id/job.xml.html logs/$job_id/$attempt_id (directory for $attempt_id) logs/$job_id/$attempt_id/stderr logs/$job_id/$attempt_id/stdout logs/$job_id/$attempt_id/syslog This parameter was introduced in Hive 0.12.0. (See HIVE-4531.) Optional in Hive 0.12.0+ None   callback Define a URL to be called upon job completion. You may embed a specific job ID into this URL using $jobId. This tag will be replaced in the callback URL with this job\u0026rsquo;s job ID. Optional None    The standard parameters are also supported.\nResults    Name Description     id A string containing the job ID similar to \u0026ldquo;job_201110132141_0001\u0026rdquo;.   info A JSON object containing the information returned when the job was queued. See the Hadoop documentation (Class TaskController) for more information.    Example Code and Data Setup % cat mydata/file01 mydata/file02 Hello World Bye World Hello Hadoop Goodbye Hadoop % hadoop fs -put mydata/ . % hadoop fs -ls mydata Found 2 items -rw-r--r-- 1 ctdean supergroup 23 2011-11-11 13:29 /user/ctdean/mydata/file01 -rw-r--r-- 1 ctdean supergroup 28 2011-11-11 13:29 /user/ctdean/mydata/file02 Curl Command % curl -s -d input=mydata \\ -d output=mycounts \\ -d mapper=/bin/cat \\ -d reducer=\u0026quot;/usr/bin/wc -w\u0026quot; \\ 'http://localhost:50111/templeton/v1/mapreduce/streaming?user.name=ekoifman' Version information\nPrior to Hive 0.13.0, user.name was specified in POST requests as a form parameter: curl -d user.name=*\u0026lt;user\u0026gt;*.\nIn Hive 0.13.0 onward, user.name should be specified in the query string (as shown above): 'http://.../templeton/v1/mapreduce/streaming?user.name=*\u0026lt;name\u0026gt;*'. Specifying user.name as a form parameter is deprecated.\nJSON Output { \u0026quot;id\u0026quot;: \u0026quot;job_201111111311_0008\u0026quot;, \u0026quot;info\u0026quot;: { \u0026quot;stdout\u0026quot;: \u0026quot;packageJobJar: [] [/Users/ctdean/var/hadoop/hadoop-0.20.205.0/share/hadoop/contrib/streaming/hadoop-streaming-0.20.205.0.jar... templeton-job-id:job_201111111311_0008 \u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;11/11/11 13:26:43 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments 11/11/11 13:26:43 INFO mapred.FileInputFormat: Total input paths to process : 2 \u0026quot;, \u0026quot;exitcode\u0026quot;: 0 } } Example Results % hadoop fs -ls mycounts Found 3 items -rw-r--r-- 1 ctdean supergroup 0 2011-11-11 13:27 /user/ctdean/mycounts/_SUCCESS drwxr-xr-x - ctdean supergroup 0 2011-11-11 13:26 /user/ctdean/mycounts/_logs -rw-r--r-- 1 ctdean supergroup 10 2011-11-11 13:27 /user/ctdean/mycounts/part-00000 % hadoop fs -cat mycounts/part-00000 8 Navigation Links Previous: PUT ddl/database/:db/table/:table/property/:property\nNext: POST mapreduce/jar\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-mapreducestream_34017023/","tags":null,"title":"Apache Hive : WebHCat Reference MapReduceStream"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Pig Pig Job — POST pig  Pig Job — POST pig  Description URL Parameters Results Example  Code and Data Setup Curl Command JSON Output      Description Create and queue a Pig job.\nURL http://www.myserver.com/templeton/v1/pig\nParameters    Name Description Required? Default     execute String containing an entire, short Pig program to run. One of either \u0026ldquo;execute\u0026rdquo; or \u0026ldquo;file\u0026rdquo; is required. None   file HDFS file name of a Pig program to run. One of either \u0026ldquo;execute\u0026rdquo; or \u0026ldquo;file\u0026rdquo; is required. None   arg Set a program argument. If -useHCatalog is included, then usehcatalog is interpreted as \u0026ldquo;true\u0026rdquo; (Hive 0.13.0 and later). Optional None   files Comma separated files to be copied to the map reduce cluster. Optional None   statusdir A directory where WebHCat will write the status of the Pig job. If provided, it is the caller\u0026rsquo;s responsibility to remove this directory when done. Optional None   enablelog If statusdir is set and enablelog is \u0026ldquo;true\u0026rdquo;, collect Hadoop job configuration and logs into a directory named $statusdir/logs after the job finishes. Both completed and failed attempts are logged. The layout of subdirectories in $statusdir/logs is: logs/$job_id (directory for $job_id) logs/$job_id/job.xml.html logs/$job_id/$attempt_id (directory for $attempt_id) logs/$job_id/$attempt_id/stderr logs/$job_id/$attempt_id/stdout logs/$job_id/$attempt_id/syslog This parameter was introduced in Hive 0.12.0. (See HIVE-4531.) Optional in Hive 0.12.0+ None   callback Define a URL to be called upon job completion. You may embed a specific job ID into this URL using $jobId. This tag will be replaced in the callback URL with this job\u0026rsquo;s job ID. Optional None   usehcatalog Specify that the submitted job uses HCatalog and therefore needs to access the metastore, which requires additional steps for WebHCat to perform in a secure cluster. (See HIVE-5133.) This parameter will be introduced in Hive 0.13.0. It can also be set to \u0026ldquo;true\u0026rdquo; by including -useHCatalog in the arg parameter. Also, if webhcat-site.xml defines the parameters templeton.hive.archive, templeton.hive.home and templeton.hcat.home then WebHCat will ship the Hive tar to the target node where the job runs. (See HIVE-5547.) This means that Hive doesn\u0026rsquo;t need to be installed on every node in the Hadoop cluster. It does not ensure that Pig is installed on the target node in the cluster. This is independent of security, but improves manageability. The webhcat-site.xml parameters are documented in webhcat-default.xml. Optional in Hive 0.13.0+ false    The standard parameters are also supported.\nResults    Name Description     id A string containing the job ID similar to \u0026ldquo;job_201110132141_0001\u0026rdquo;.   info A JSON object containing the information returned when the job was queued. See the Hadoop documentation (Class TaskController) for more information.    Example Code and Data Setup % cat id.pig A = load 'passwd' using PigStorage(':'); B = foreach A generate $0 as id; dump B; % cat fake-passwd ctdean:Chris Dean:secret pauls:Paul Stolorz:good carmas:Carlos Armas:evil dra:Deirdre McClure:marvelous % hadoop fs -put id.pig . % hadoop fs -put fake-passwd passwd Curl Command % curl -s -d file=id.pig \\ -d arg=-v \\ 'http://localhost:50111/templeton/v1/pig?user.name=ekoifman' Version information\nPrior to Hive 0.13.0, user.name was specified in POST requests as a form parameter: curl -d user.name=*\u0026lt;user\u0026gt;*.\nIn Hive 0.13.0 onward, user.name should be specified in the query string (as shown above): 'http://.../templeton/v1/pig?user.name=*\u0026lt;name\u0026gt;*'. Specifying user.name as a form parameter is deprecated.\nJSON Output { \u0026quot;id\u0026quot;: \u0026quot;job_201111101627_0018\u0026quot;, \u0026quot;info\u0026quot;: { \u0026quot;stdout\u0026quot;: \u0026quot;templeton-job-id:job_201111101627_0018 \u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;exitcode\u0026quot;: 0 } } Navigation Links Previous: POST mapreduce/jar\nNext: POST hive\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-pig_34017169/","tags":null,"title":"Apache Hive : WebHCat Reference Pig"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PostTable Rename Table — POST ddl/database/:db/table/:table  Rename Table — POST ddl/database/:db/table/:table  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description Rename an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table\nParameters    Name Description Required? Default     :db The database name Required None   :table The existing (old) table name Required None   rename The new table name Required None   group The user group to use Optional None   permissions The permissions string to use. The format is \u0026ldquo;rwxrw-r-x\u0026rdquo;. Optional None    The standard parameters are also supported.\nResults    Name Description     table The new table name   database The database name    Example Curl Command % curl -s -d rename=test_table_2 \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ekoifman' Version information\nPrior to Hive 0.13.0, user.name was specified in POST requests as a form parameter: curl -d user.name=*\u0026lt;user\u0026gt;*.\nIn Hive 0.13.0 onward, user.name should be specified in the query string (as shown above): 'http://.../templeton/v1/ddl/...?user.name=*\u0026lt;name\u0026gt;*'. Specifying user.name as a form parameter is deprecated.\nJSON Output { \u0026quot;table\u0026quot;: \u0026quot;test_table_2\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } JSON Output (error) { \u0026quot;error\u0026quot;: \u0026quot;Table test_table does not exist\u0026quot;, \u0026quot;errorCode\u0026quot;: 404, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table_2\u0026quot; } Navigation Links Previous: PUT ddl/database/:db/table/:table\nNext: DELETE ddl/database/:db/table/:table\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-posttable_34016548/","tags":null,"title":"Apache Hive : WebHCat Reference PostTable"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PutColumn Create Column — PUT ddl/database/:db/table/:table/column/:column  Create Column — PUT ddl/database/:db/table/:table/column/:column  Description URL Parameters Results Example  Curl Command JSON Output      Description Create a column in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/column/:column\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :column The column name Required None   group The user group to use Optional None   permissions The permissions string to use Optional None   type The type of column to add, like \u0026ldquo;string\u0026rdquo; or \u0026ldquo;int\u0026rdquo; Required None   comment The column comment, like a description Optional None    The standard parameters are also supported.\nResults    Name Description     column The column name   table The table name   database The database name    Example Curl Command % curl -s -X PUT -HContent-type:application/json \\ -d '{\u0026quot;type\u0026quot;: \u0026quot;string\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The brand name\u0026quot;}' \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/column/brand?user.name=ctdean' JSON Output { \u0026quot;column\u0026quot;: \u0026quot;brand\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } } Navigation Links Previous: GET ddl/database/:db/table/:table/column/:column Next: GET ddl/database/:db/table/:table/property\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-putcolumn_34016987/","tags":null,"title":"Apache Hive : WebHCat Reference PutColumn"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PutDB Create Database — PUT ddl/database/:db  Create Database — PUT ddl/database/:db  Description URL Parameters Results Example  Curl Command JSON Output      Description Create a database.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db\nParameters    Name Description Required? Default     :db The database name Required None   group The user group to use Optional None   permissions The permissions string to use Optional None   location The database location Optional None   comment A comment for the database, like a description Optional None   properties The database properties Optional None    The standard parameters are also supported.\nResults    Name Description     database The database name    Example Curl Command % curl -s -X PUT -HContent-type:application/json \\ -d '{ \u0026quot;comment\u0026quot;:\u0026quot;Hello there\u0026quot;, \u0026quot;location\u0026quot;:\u0026quot;hdfs://localhost:9000/user/hive/my_warehouse\u0026quot;, \u0026quot;properties\u0026quot;:{\u0026quot;a\u0026quot;:\u0026quot;b\u0026quot;}}' \\ 'http://localhost:50111/templeton/v1/ddl/database/newdb?user.name=rachel' JSON Output { \u0026quot;database\u0026quot;:\u0026quot;newdb\u0026quot; } Navigation Links Previous: GET ddl/database/:db Next: DELETE ddl/database/:db\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-putdb_34016273/","tags":null,"title":"Apache Hive : WebHCat Reference PutDB"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PutPartition Create Partition — PUT ddl/database/:db/table/:table/partition/:partition  Create Partition — PUT ddl/database/:db/table/:table/partition/:partition  Description URL Parameters Results Example  Curl Command JSON Output      Description Create a partition in an HCatalog table.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/partition/:partition\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :partition The partition name, col_name=\u0026lsquo;value\u0026rsquo; list. Be careful to properly encode the quote for http, for example, country=%27algeria%27. Required None   group The user group to use Optional None   permissions The permissions string to use Optional None   location The location for partition creation Required None   ifNotExists If true, return an error if the partition already exists. Optional False    The standard parameters are also supported.\nResults    Name Description     partition The partition name   table The table name   database The database name    Example Curl Command % curl -s -X PUT -HContent-type:application/json -d '{\u0026quot;location\u0026quot;: \u0026quot;loc_a\u0026quot;}' \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/partition/country=%27algeria%27?user.name=ctdean' JSON Output { \u0026quot;partition\u0026quot;: \u0026quot;country='algeria'\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } Navigation Links Previous: GET ddl/database/:db/table/:table/partition/:partition Next: DELETE ddl/database/:db/table/:table/partition/:partition\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-putpartition_34016600/","tags":null,"title":"Apache Hive : WebHCat Reference PutPartition"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PutProperty Set Property — PUT ddl/database/:db/table/:table/property/:property  Set Property — PUT ddl/database/:db/table/:table/property/:property  Description URL Parameters Results Example  Curl Command JSON Output      Description Add a single property on an HCatalog table. This will also reset an existing property.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table/property/:property\nParameters    Name Description Required? Default     :db The database name Required None   :table The table name Required None   :property The property name Required None   group The user group to use Optional None   permissions The permissions string to use Optional None   value The property value Required None    The standard parameters are also supported.\nResults    Name Description     database The database name   table The table name   property The property name    Example Curl Command % curl -s -X PUT -HContent-type:application/json -d '{ \u0026quot;value\u0026quot;: \u0026quot;apples\u0026quot; }' \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/property/fruit?user.name=ctdean' JSON Output { \u0026quot;property\u0026quot;: \u0026quot;fruit\u0026quot;, \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } Navigation Links Previous: GET ddl/database/:db/table/:table/property/:property Next: POST mapreduce/streaming\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-putproperty_34017012/","tags":null,"title":"Apache Hive : WebHCat Reference PutProperty"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PutTable Create Table — PUT ddl/database/:db/table/:table  Create Table — PUT ddl/database/:db/table/:table  Description URL Parameters Results Example  Curl Command Curl Command (using clusteredBy) JSON Output JSON Output (error)      Description Create a new HCatalog table. For more information, please refer to the Hive documentation for CREATE TABLE.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:table\nParameters    Name Description Required? Default     :db The database name. Required None   :table The new table name. Required None   group The user group to use when creating a table. Optional None   permissions The permissions string to use when creating a table. Optional None   external Allows you to specify a location so that Hive does not use the default location for this table. Optional false   ifNotExists If true, you will not receive an error if the table already exists. Optional false   comment Comment for the table. Optional None   columns A list of column descriptions, including name, type, and an optional comment. Optional None   partitionedBy A list of column descriptions used to partition the table. Like the columns parameter this is a list of name, type, and comment fields. Optional None   clusteredBy An object describing how to cluster the table including the parameters columnNames, sortedBy, and numberOfBuckets. The sortedBy parameter includes the parameters columnName and order (ASC for ascending or DESC for descending). For further information please refer to the examples below or to the Hive documentation. Optional None   format Storage format description including parameters for rowFormat, storedAs, and storedBy. For further information please refer to the examples below or to the Hive documentation. Optional None   location The HDFS path. Optional None   tableProperties A list of table property names and values (key/value pairs). Optional None    The standard parameters are also supported.\nResults    Name Description     table The new table name.   database The database name.    Example Curl Command % curl -s -X PUT -HContent-type:application/json -d '{ \u0026quot;comment\u0026quot;: \u0026quot;Best table made today\u0026quot;, \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bigint\u0026quot; }, { \u0026quot;name\u0026quot;: \u0026quot;price\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The unit price\u0026quot; } ], \u0026quot;partitionedBy\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;country\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;format\u0026quot;: { \u0026quot;storedAs\u0026quot;: \u0026quot;rcfile\u0026quot; } }' \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table?user.name=ctdean' Curl Command (using clusteredBy) % curl -s -X PUT -HContent-type:application/json -d '{ \u0026quot;comment\u0026quot;: \u0026quot;Best table made today\u0026quot;, \u0026quot;columns\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;id\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;bigint\u0026quot;}, { \u0026quot;name\u0026quot;: \u0026quot;price\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;float\u0026quot;, \u0026quot;comment\u0026quot;: \u0026quot;The unit price\u0026quot; } ], \u0026quot;partitionedBy\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;country\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;string\u0026quot; } ], \u0026quot;clusteredBy\u0026quot;: { \u0026quot;columnNames\u0026quot;: [\u0026quot;id\u0026quot;], \u0026quot;sortedBy\u0026quot;: [ { \u0026quot;columnName\u0026quot;: \u0026quot;id\u0026quot;, \u0026quot;order\u0026quot;: \u0026quot;ASC\u0026quot; } ], \u0026quot;numberOfBuckets\u0026quot;: 10 }, \u0026quot;format\u0026quot;: { \u0026quot;storedAs\u0026quot;: \u0026quot;rcfile\u0026quot;, \u0026quot;rowFormat\u0026quot;: { \u0026quot;fieldsTerminatedBy\u0026quot;: \u0026quot;\\u0001\u0026quot;, \u0026quot;serde\u0026quot;: { \u0026quot;name\u0026quot;: \u0026quot;org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe\u0026quot;, \u0026quot;properties\u0026quot;: { \u0026quot;key\u0026quot;: \u0026quot;value\u0026quot; } } } } } ' \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table_c?user.name=ctdean' JSON Output { \u0026quot;table\u0026quot;: \u0026quot;test_table\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } JSON Output (error) { \u0026quot;statement\u0026quot;: \u0026quot;use default; create table test_table_c(id bigint, price float comment ...\u0026quot;, \u0026quot;error\u0026quot;: \u0026quot;unable to create table: test_table_c\u0026quot;, \u0026quot;exec\u0026quot;: { \u0026quot;stdout\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;stderr\u0026quot;: \u0026quot;WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated... Hive history file=/tmp/ctdean/hive_job_log_ctdean_201204051335_2016086186.txt SLF4J: Class path contains multiple SLF4J bindings. SLF4J: Found binding in ... SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation. OK Time taken: 0.448 seconds FAILED: Error in semantic analysis: Operation not supported. HCatalog doesn't allow Clustered By in create table. \u0026quot;, \u0026quot;exitcode\u0026quot;: 10 } } Navigation Links Previous: GET ddl/database/:db/table/:table Next: POST ddl/database/:db/table/:table\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-puttable_34016540/","tags":null,"title":"Apache Hive : WebHCat Reference PutTable"},{"categories":null,"contents":"Apache Hive : WebHCat Reference PutTableLike Create Table Like — PUT ddl/database/:db/table/:existingtable/like/:newtable  Create Table Like — PUT ddl/database/:db/table/:existingtable/like/:newtable  Description URL Parameters Results Example  Curl Command JSON Output      Description Create a new HCatalog table like an existing one.\nURL http://www.myserver.com/templeton/v1/ddl/database/:db/table/:existingtable/like/:newtable\nParameters    Name Description Required? Default     :db The database name Required None   :existingtable The existing table name Required None   :newtable The new table name Required None   group The user group to use when creating a table Optional None   permissions The permissions string to use when creating a table Optional None   external Allows you to specify a location so that Hive does not use the default location for this table. Optional false   ifNotExists If true, you will not receive an error if the table already exists. Optional false   location The HDFS path Optional None    The standard parameters are also supported.\nResults    Name Description     table The new table name   database The database name    Example Curl Command % curl -s -X PUT -HContent-type:application/json -d {} \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table/like/test_table_2?user.name=ctdean' JSON Output { \u0026quot;table\u0026quot;: \u0026quot;test_table_2\u0026quot;, \u0026quot;database\u0026quot;: \u0026quot;default\u0026quot; } Navigation Links Previous: DELETE ddl/database/:db/table/:table Next: GET ddl/database/:db/table/:table/partition\nGeneral: DDL Resources – WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-puttablelike_34016572/","tags":null,"title":"Apache Hive : WebHCat Reference PutTableLike"},{"categories":null,"contents":"Apache Hive : WebHCat Reference ResponseTypes Response Types — GET :version  Response Types — GET :version  Description URL Parameters Results Example  Curl Command JSON Output JSON Output (error)      Description Returns a list of the response types supported by WebHCat (Templeton).\nURL http://www.myserver.com/templeton/:version\nParameters    Name Description Required? Default     :version The WebHCat version number. (Currently this must be \u0026ldquo;v1\u0026rdquo;.) Required None    The standard parameters are also supported.\nResults    Name Description     responseTypes A list of all supported response types    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1' JSON Output { \u0026quot;responseTypes\u0026quot;: [ \u0026quot;application/json\u0026quot; ] } JSON Output (error) { \u0026quot;error\u0026quot;: \u0026quot;null for uri: http://localhost:50111/templeton/v2\u0026quot; } Navigation Links Previous: Reference: WebHCat Resources\nNext: GET status\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-responsetypes_34015937/","tags":null,"title":"Apache Hive : WebHCat Reference ResponseTypes"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Status Current Status — GET status  Current Status — GET status  Description URL Parameters Results Example  Curl Command JSON Output      Description Returns the current status of the WebHCat (Templeton) server. Useful for heartbeat monitoring.\nURL http://www.myserver.com/templeton/v1/status\nParameters Only the standard parameters are accepted.\nResults    Name Description     status \u0026ldquo;ok\u0026rdquo; if the WebHCat server was contacted.   version String containing the version number similar to \u0026ldquo;v1\u0026rdquo;.    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/status' JSON Output { \u0026quot;status\u0026quot;: \u0026quot;ok\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot; } Navigation Links Previous: Response Types (GET :version)Next: GET version\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-status_34015941/","tags":null,"title":"Apache Hive : WebHCat Reference Status"},{"categories":null,"contents":"Apache Hive : WebHCat Reference Version List Versions — GET version  List Versions — GET version  Description URL Parameters Results Example  Curl Command JSON Output      Description Returns a list of supported versions and the current version.\nURL http://www.myserver.com/templeton/v1/version\nParameters Only the standard parameters are accepted.\nResults    Name Description     supportedVersions A list of all supported versions.   version The current version.    Example Curl Command % curl -s 'http://localhost:50111/templeton/v1/version' JSON Output { \u0026quot;supportedVersions\u0026quot;: [ \u0026quot;v1\u0026quot; ], \u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot; } Navigation Links Previous: GET status\nNext: GET version/hive\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-version_34015986/","tags":null,"title":"Apache Hive : WebHCat Reference Version"},{"categories":null,"contents":"Apache Hive : WebHCat Reference VersionHadoop Hadoop Version — GET version/hadoop  Hadoop Version — GET version/hadoop  Description URL Parameters Results Example  Curl Command JSON Output      Description Return the version of Hadoop being run when WebHCat creates a MapReduce job (POST mapreduce/jar or mapreduce/streaming).\nVersion: Hive 0.13.0 and later\nGET version/hadoop is introduced in Hive release 0.13.0 (HIVE-6226).\nURL http://www.myserver.com/templeton/v1/version/hadoop\nParameters Only the standard parameters are accepted.\nResults Returns the Hadoop version.\nExample Curl Command % curl -s 'http://localhost:50111/templeton/v1/version/hadoop?user.name=ekoifman' JSON Output [ {\u0026quot;module\u0026quot;:\u0026quot;hadoop\u0026quot;,\u0026quot;version\u0026quot;:\u0026quot;2.4.1-SNAPSHOT} ]    Navigation Links Previous: GET version/hive\nNext: POST ddl\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaces deprecated resource: GET queue\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-versionhadoop_44303410/","tags":null,"title":"Apache Hive : WebHCat Reference VersionHadoop"},{"categories":null,"contents":"Apache Hive : WebHCat Reference VersionHive Hive Version — GET version/hive  Hive Version — GET version/hive  Description URL Parameters Results Example  Curl Command JSON Output      Description Return the version of Hive being run when WebHCat issues Hive queries or commands (POST hive).\nVersion: Hive 0.13.0 and later\nGET version/hive is introduced in Hive release 0.13.0 (HIVE-6226).\nURL http://www.myserver.com/templeton/v1/version/hive\nParameters Only the standard parameters are accepted.\nResults Returns the Hive version.\nExample Curl Command % curl -s 'http://localhost:50111/templeton/v1/version/hive?user.name=ekoifman' JSON Output [ {\u0026quot;module\u0026quot;:\u0026quot;hive\u0026quot;,\u0026quot;version\u0026quot;:\u0026quot;0.14.0-SNAPSHOT\u0026quot;} ]    Navigation Links Previous: GET version\nNext: GET version/hadoop\nGeneral: WebHCat Reference – WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nReplaces deprecated resource: GET queue\n","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-reference-versionhive_44303406/","tags":null,"title":"Apache Hive : WebHCat Reference VersionHive"},{"categories":null,"contents":"Apache Hive : WebHCat UsingWebHCat Using the HCatalog REST API (WebHCat)  Using the HCatalog REST API (WebHCat)  Introduction to WebHCat URL Format Security  Standard Parameters  Specifying user.name   Security Error Response   WebHDFS and Code Push Error Codes and Responses Log Files Project Name    Version information\nThe HCatalog project graduated from the Apache incubator and merged with the Hive project on March 26, 2013.\nHive version 0.11.0 is the first release that includes HCatalog and its REST API, WebHCat.\nIntroduction to WebHCat This document describes the HCatalog REST API, WebHCat, which was previously called Templeton.\nAs shown in the figure below, developers make HTTP requests to access Hadoop MapReduce (or YARN), Pig, Hive, and HCatalog DDL from within applications. Data and code used by this API are maintained in HDFS. HCatalog DDL commands are executed directly when requested. MapReduce, Pig, and Hive jobs are placed in queue by WebHCat (Templeton) servers and can be monitored for progress or stopped as required. Developers specify a location in HDFS into which Pig, Hive, and MapReduce results should be placed.\nWebHCat or Templeton?\nFor backward compatibility, the original name Templeton is still used for WebHCat in some contexts. See #Project Name below.\nURL Format HCatalog\u0026rsquo;s REST resources are accessed using the following URL format:\nhttp://yourserver/templeton/v1/resource\nwhere \u0026ldquo;yourserver\u0026rdquo; is replaced with your server name, and \u0026ldquo;resource\u0026rdquo; is replaced with the HCatalog resource name.\nFor example, to check if the server is running you could access the following URL:\nht``tp://www.myserver.com/templeton/v1/status\nSee Reference: WebHCat Resources for information about the individual REST resources.\nSecurity The current version supports two types of security:\n Default security (without additional authentication) Authentication via Kerberos  Standard Parameters Every REST resource can accept the following parameters to aid in authentication:\n user.name: The user name as a string. Only valid when using default security. SPNEGO credentials: When running with Kerberos authentication.  Specifying user.name The user.name parameter is part of POST parameters for POST calls, and part of the URL for other calls.\nFor example, to specify user.name in a GET :table command:\n% curl -s 'http://localhost:50111/templeton/v1/ddl/database/default/table/my_table?user.name=ctdean' And to specify user.name in a POST :table command:\n% curl -s -d user.name=ctdean \\ -d rename=test_table_2 \\ 'http://localhost:50111/templeton/v1/ddl/database/default/table/test_table' Security Error Response If the user.name parameter is not supplied when required, the following error will be returned:\n{ \u0026quot;error\u0026quot;: \u0026quot;No user found. Missing user.name parameter.\u0026quot; } WebHDFS and Code Push Data and code that are used by HCatalog\u0026rsquo;s REST resources must first be placed in Hadoop. When placing files into HDFS is required you can use whatever method is most convienient for you. We suggest WebHDFS since it provides a REST interface for moving files into and out of HDFS.\nError Codes and Responses The server returns the following HTTP status codes.\n 200 OK: Success! 400 Bad Request: The request was invalid. 401 Unauthorized: Credentials were missing or incorrect. 404 Not Found: The URI requested is invalid or the resource requested does not exist. 500 Internal Server Error: We received an unexpected result. 503 Busy, please retry: The server is busy.  Other data returned directly by the server is returned in JSON format. JSON responses are limited to 1MB in size. Responses over this limit must be stored into HDFS using provided options instead of being directly returned. If an HCatalog DDL command might return results greater than 1MB, it\u0026rsquo;s suggested that a corresponding Hive request be executed instead.\nLog Files The server creates three log files when in operation:\n templeton.log is the log4j log. This the main log the application writes to. templeton-console.log is what Java writes to stdout when the server is started. It is a small amount of data, similar to \u0026ldquo;hcat.out\u0026rdquo;. tempelton-console-error.log is what Java writes to stderr, similar to \u0026ldquo;hcat.err\u0026rdquo;.  In the tempelton-log4j.properties file you can set the location of these logs using the variable templeton.log.dir. This log4j.properties file is set in the server startup script.\nHive log files are described in the Hive Logging section of Getting Started.\nProject Name The original work to add REST APIs to HCatalog was called Templeton. For backward compatibility the name still appears in URLs, log file names, etc. The Templeton name is taken from a character in the award-winning children\u0026rsquo;s novel Charlotte\u0026rsquo;s Web, by E. B. White. The novel\u0026rsquo;s protagonist is a pig named Wilbur. Templeton is a rat who helps Wilbur by running errands and making deliveries as requested by Charlotte while spinning her web.\n Navigation Links Next: WebHCat Installation\nGeneral: WebHCat Manual – HCatalog Manual – Hive Wiki Home – Hive Project Site\nAttachments: ","date":"12","image":null,"permalink":"https://hive.apache.org/docs/latest/webhcat-usingwebhcat_34015492/","tags":null,"title":"Apache Hive : WebHCat UsingWebHCat"},{"categories":null,"contents":"Introduction  Run Apache Hive inside docker container in pseudo-distributed mode, inorder to provide the following Quick-start/Debugging/Prepare a test env for Hive\nQuickstart  STEP 1: Pull the image  Pull the image from DockerHub: https://hub.docker.com/r/apache/hive/tags. Here are the latest images:  4.0.0 3.1.3    docker pull apache/hive:4.0.0  \nSTEP 2: Export the Hive version export HIVE_VERSION=4.0.0  \nSTEP 3: Launch the HiveServer2 with an embedded Metastore. This is lightweight and for a quick setup, it uses Derby as metastore db.\ndocker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hive4 apache/hive:${HIVE_VERSION}  \nSTEP 4: Connect to beeline docker exec -it hiveserver2 beeline -u \u0026#39;jdbc:hive2://hiveserver2:10000/\u0026#39;  \nNote: Launch Standalone Metastore To use standalone Metastore with Derby, docker run -d -p 9083:9083 --env SERVICE_NAME=metastore --name metastore-standalone apache/hive:${HIVE_VERSION}  \nDetailed Setup  - Build image Apache Hive relies on Hadoop, Tez and some others to facilitate reading, writing, and managing large datasets. The /packaging/src/docker/build.sh provides ways to build the image against specified version of the dependent, as well as build from source.\n- Build from source mvn clean package -pl packaging -DskipTests -Pdocker  \n- Build with specified version There are some arguments to specify the component version:\n-hadoop \u0026lt;hadoop version\u0026gt; -tez \u0026lt;tez version\u0026gt; -hive \u0026lt;hive version\u0026gt; If the version is not provided, it will read the version from current pom.xml: project.version, hadoop.version and tez.version for Hive, Hadoop and Tez respectively. For example, the following command uses Hive 4.0.0, Hadoop hadoop.version and Tez tez.version to build the image,\n./build.sh -hive 4.0.0 If the command does not specify the Hive version, it will use the local apache-hive-${project.version}-bin.tar.gz(will trigger a build if it doesn\u0026rsquo;t exist), together with Hadoop 3.3.6 and Tez 0.10.3 to build the image,\n./build.sh -hadoop 3.3.6 -tez 0.10.3 After building successfully, we can get a Docker image named apache/hive by default, the image is tagged by the provided Hive version.\nRun services  Before going further, we should define the environment variable HIVE_VERSION first. For example, if -hive 4.0.0 is specified to build the image,\nexport HIVE_VERSION=4.0.0 or assuming that you\u0026rsquo;re relying on current project.version from pom.xml,\nexport HIVE_VERSION=$(mvn -f pom.xml -q help:evaluate -Dexpression=project.version -DforceStdout)  \n- Metastore For a quick start, launch the Metastore with Derby,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --name metastore-standalone apache/hive:${HIVE_VERSION} Everything would be lost when the service is down. In order to save the Hive table\u0026rsquo;s schema and data, start the container with an external Postgres and Volume to keep them,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \\  --env SERVICE_OPTS=\u0026#34;-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore_db -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=password\u0026#34; \\  --mount source=warehouse,target=/opt/hive/data/warehouse \\  --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \\  --name metastore-standalone apache/hive:${HIVE_VERSION} If you want to use your own hdfs-site.xml or yarn-site.xml for the service, you can provide the environment variable HIVE_CUSTOM_CONF_DIR for the command. For instance, put the custom configuration file under the directory /opt/hive/conf, then,\ndocker run -d -p 9083:9083 --env SERVICE_NAME=metastore --env DB_DRIVER=postgres \\  -v /opt/hive/conf:/hive_custom_conf --env HIVE_CUSTOM_CONF_DIR=/hive_custom_conf \\  --mount type=bind,source=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar,target=/opt/hive/lib/postgres.jar \\  --name metastore apache/hive:${HIVE_VERSION} For Hive releases before 4.0, if you want to upgrade the existing external Metastore schema to the target version, then add --env SCHEMA_COMMAND=upgradeSchema to the command. To skip schematool initialisation or upgrade for metastore use --env IS_RESUME=\u0026quot;true\u0026quot;, for verbose logging set --env VERBOSE=\u0026quot;true\u0026quot;.\n \n- HiveServer2 Launch the HiveServer2 with an embedded Metastore,\ndocker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 --name hiveserver2-standalone apache/hive:${HIVE_VERSION} or specify a remote Metastore if it\u0026rsquo;s available,\ndocker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 \\  --env SERVICE_OPTS=\u0026#34;-Dhive.metastore.uris=thrift://metastore:9083\u0026#34; \\  --env IS_RESUME=\u0026#34;true\u0026#34; \\  --name hiveserver2-standalone apache/hive:${HIVE_VERSION} To save the data between container restarts, you can start the HiveServer2 with a Volume,\ndocker run -d -p 10000:10000 -p 10002:10002 --env SERVICE_NAME=hiveserver2 \\  --env SERVICE_OPTS=\u0026#34;-Dhive.metastore.uris=thrift://metastore:9083\u0026#34; \\  --mount source=warehouse,target=/opt/hive/data/warehouse \\  --env IS_RESUME=\u0026#34;true\u0026#34; \\  --name hiveserver2 apache/hive:${HIVE_VERSION}  \n- HiveServer2, Metastore To get a quick overview of both HiveServer2 and Metastore, there is a docker-compose.yml placed under packaging/src/docker for this purpose, specify the POSTGRES_LOCAL_PATH first:\nexport POSTGRES_LOCAL_PATH=your_local_path_to_postgres_driver Example:\nmvn dependency:copy -Dartifact=\u0026#34;org.postgresql:postgresql:42.5.1\u0026#34; \u0026amp;\u0026amp; \\ export POSTGRES_LOCAL_PATH=`mvn help:evaluate -Dexpression=settings.localRepository -q -DforceStdout`/org/postgresql/postgresql/42.5.1/postgresql-42.5.1.jar If you don\u0026rsquo;t install maven or have problem in resolving the postgres driver, you can always download this jar yourself, change the POSTGRES_LOCAL_PATH to the path of the downloaded jar. Then,\ndocker compose up -d HiveServer2, Metastore and Postgres services will be started as a consequence. Volumes are used to persist data generated by Hive inside Postgres and HiveServer2 containers:\n hive_db  The volume persists the metadata of Hive tables inside Postgres container.   warehouse  The volume stores tables' files inside HiveServer2 container.    To stop/remove them all,\ndocker compose down Usage   HiveServer2 web  Accessed on browser at http://localhost:10002/   Beeline: docker exec -it hiveserver2 beeline -u \u0026#39;jdbc:hive2://hiveserver2:10000/\u0026#39; # If beeline is installed on host machine, HiveServer2 can be simply reached via: beeline -u \u0026#39;jdbc:hive2://localhost:10000/\u0026#39;  Run some queries show tables; create table hive_example(a string, b int) partitioned by(c int); alter table hive_example add partition(c=1); insert into hive_example partition(c=1) values(\u0026#39;a\u0026#39;, 1), (\u0026#39;a\u0026#39;, 2),(\u0026#39;b\u0026#39;,3); select count(distinct a) from hive_example; select sum(b) from hive_example;   sys Schema and information_schema Schema Hive Schema Tool is located in the Docker Image at /opt/hive/bin/schematool.\nBy default, system schemas such as information_schema for HiveServer2 are not created. To create system schemas for a HiveServer2 instance, users need to configure the Hive Metastore Server used by HiveServer2 to use a database other than the embedded Derby. The following text discusses how to configure HiveServer2 when the Hive Metastore Server is in different locations.\nHiveServer2 with embedded Hive Metastore Server Assuming Maven and Docker CE are installed, a possible use case is as follows. Create a compose.yaml file in the current directory,\nservices: some-postgres: image: postgres:17.2-bookworm environment: POSTGRES_PASSWORD: \u0026#34;example\u0026#34; hiveserver2-standalone: image: apache/hive:4.0.1 depends_on: - some-postgres environment: SERVICE_NAME: hiveserver2 DB_DRIVER: postgres SERVICE_OPTS: \u0026gt;--Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://some-postgres:5432/postgres -Djavax.jdo.option.ConnectionUserName=postgres -Djavax.jdo.option.ConnectionPassword=example volumes: - ~/.m2/repository/org/postgresql/postgresql/42.7.5/postgresql-42.7.5.jar:/opt/hive/lib/postgres.jar Then execute the shell command as follows to initialize the system schemas in HiveServer2.\nmvn dependency:get -Dartifact=org.postgresql:postgresql:42.7.5 docker compose up -d docker compose exec hiveserver2-standalone /bin/bash /opt/hive/bin/schematool -initSchema -dbType hive -metaDbType postgres -url jdbc:hive2://localhost:10000/default exit HiveServer2 using a remote Hive Metastore Server Assuming Maven and Docker CE are installed, a possible use case is as follows. Create a compose.yaml file in the current directory,\nservices: some-postgres: image: postgres:17.2-bookworm environment: POSTGRES_PASSWORD: \u0026#34;example\u0026#34; metastore-standalone: image: apache/hive:4.0.1 depends_on: - some-postgres environment: SERVICE_NAME: metastore DB_DRIVER: postgres SERVICE_OPTS: \u0026gt;--Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://some-postgres:5432/postgres -Djavax.jdo.option.ConnectionUserName=postgres -Djavax.jdo.option.ConnectionPassword=example volumes: - ~/.m2/repository/org/postgresql/postgresql/42.7.5/postgresql-42.7.5.jar:/opt/hive/lib/postgres.jar hiveserver2-standalone: image: apache/hive:4.0.1 depends_on: - metastore-standalone environment: SERVICE_NAME: hiveserver2 IS_RESUME: true SERVICE_OPTS: \u0026gt;--Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://some-postgres:5432/postgres -Djavax.jdo.option.ConnectionUserName=postgres -Djavax.jdo.option.ConnectionPassword=example -Dhive.metastore.uris=thrift://metastore-standalone:9083 volumes: - ~/.m2/repository/org/postgresql/postgresql/42.7.5/postgresql-42.7.5.jar:/opt/hive/lib/postgres.jar Then execute the shell command as follows to initialize the system schemas in HiveServer2.\nmvn dependency:get -Dartifact=org.postgresql:postgresql:42.7.5 docker compose up -d docker compose exec hiveserver2-standalone /bin/bash /opt/hive/bin/schematool -initSchema -dbType hive -metaDbType postgres -url jdbc:hive2://localhost:10000/default exit ","date":"12","image":null,"permalink":"https://hive.apache.org/development/quickstart/","tags":null,"title":"QuickStarted"},{"categories":null,"contents":"The Apache Hive ™ data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage. A command line tool and JDBC driver are provided to connect users to Hive.\nGetting Started With Apache Hive Software   Check out the Getting Started Guide. Learn more About Hive\u0026rsquo;s Functionality. Read the Getting Started Guide to learn how to install Hive The User and Hive SQL documentation shows how to program Hive  Quick start with Docker  Checkout the quickstart with Docker here: DOCKER_QUICKSTART\nGetting Involved With The Apache Hive Community  Apache Hive is an open source project run by volunteers at the Apache Software Foundation. Previously it was a subproject of Apache® Hadoop®, but has now graduated to become a top-level project of its own. We encourage you to learn about the project and contribute your expertise.\n Give us feedback or submit bug reports: What can we do better? Join the mailing list and meet our community Read through our Contributor\u0026rsquo;s Guides about where to find the source or submit patches Become a Hive Fan on Facebook Follow @ApacheHive on Twitter  ","date":"10","image":null,"permalink":"https://hive.apache.org/development/gettingstarted/","tags":null,"title":"Getting Started"},{"categories":null,"contents":"People  Apache Hive is a community developed project. The list below is a partial list of contributors to the project, for a complete list you would have to look at all contributors to our issue tracker, mailing list and version control.\nHive PMC     Apache username Name Organization Role     aihuaxu Aihua Xu Cloudera    anishek Anishek Agarwal     athusoo Ashish Thusoo Qubole    ayushsaxena Ayush Saxena Cloudera    brock Brock Noland StreamSets    ctang Chaoyu Tang Cloudera    cws Carl Steinbach LinkedIn    denys Denys Kuzmenko Cloudera    daijy Daniel Dai Hortonworks    ecapriolo Edward Capriolo     gopalv Gopal Vijayaraghavan Hortonworks    gunther Gunther Hagleitner Hortonworks    hashutosh Ashutosh Chauhan Hortonworks    heyongqiang Yongqiang He Dropbox    jcamacho Jesus Camacho Rodriguez Hortonworks    jdere Jason Dere Hortonworks    jpullokk Laljo John Pullokkaran Hortonworks    jssarma Joydeep Sensarma Qubole    jxiang Jimmy Xiang Cloudera    kevinwilfong Kevin Wilfong Facebook    leftyl Lefty Leverenz IBM    namit Namit Jain Nutanix    navis Navis Ryu NexR    ngangam Naveen Gangam Cloudera VP   nzhang Ning Zhang Facebook    omalley Owen O\u0026rsquo;Malley LinkedIn    prasadm Prasad Mujumdar Cloudera    prasanth_j Prasanth Jayachandran Hortonworks    pvary Peter Vary Cloudera    pxiong Pengcheng Xiong Hortonworks    rhbutani Harish Butani Hortonworks    rmurthy Raghotham Murthy Facebook    sershe Sergey Shelukhin Hortonworks    spena Sergio Peña Cloudera    sunchao Chao Sun Cloudera    szehon Szehon Ho Cloudera    thejas Thejas Nair Hortonworks    vgumashta Vaibhav Gumashta Hortonworks    vikram Vikram Dixit Hortonworks    xuefu Xuefu Zhang Alibaba Inc    ychena Yongzhi Chen Cloudera    zabetak Stamatis Zampetakis Cloudera      Hive Committers     Apache username name organization     amareshwari Amareshwari Sriramadasu InMobi   apivovarov Alexander Pivovarov Foster City Hadoop Lab LLC   asherman Andrew Sherman Cloudera   ayushsaxena Ayush Saxena Cloudera   bharos92 Bharath Krishna Cloudera   chengxiang Chengxiang Li Intel   chinnaraol Chinna Rao Lalam Intel   cdrome Chris Drome Oath   difin Dmitriy Fingerman Cloudera   djaiswal Deepak Jaiswal Hortonworks   dmtolpeko Dmitry Tolpeko EPAM   dongc Dong Chen Intel   ehans Eric Hanson Microsoft   gangtimliu Gang Tim Liu Facebook   harisankar Hari Sankar Sivarama Subramaniyan Hortonworks   janaki Janaki Lahorani Cloudera   jitendra Jitendra Pandey Hortonworks   kgyrtkirk Zoltan Haindrich Hortonworks   kuczoram Marta Kuczora Cloudera   larsfrancke Lars Francke Freelancer   mithun Mithun Radhakrishnan Oath   mmccline Matt McCline Hortonworks   mohits Mohit Sabharwal Cloudera   okumin Shohei Okumiya Treasure Data   rbalamohan Rajesh Balamohan Hortonworks   remusr Remus Rusanu Hortonworks   sankarh Sankar Hariappan Hortonworks   sbadhya Sourabh Badhya Cloudera   sdong Siying Dong Facebook   simhadrig Simhadri Govindappa    sseth Siddharth Seth Hortonworks   szita Adam Szita Cloudera   tchoi Teddy Choi Hortonworks   vgarg Vineet Garg Hortonworks   weiz Wei Zheng Hortonworks   xuf Ferdinand Xu Intel   yhuai Yin Huai Databricks    PMC members are also Hive committers.\nCatalog Committers     Apache username name organization     toffer Francis Christopher Liu Yahoo!   avandana Vandana Ayyalasomayajula Yahoo!   travis Travis Crawford Twitter   mithun Mithun Radhakrishnan Oath     Contributors  A list of Hive contributors and their contributions is available from Jira.\nEmeritus Hive PMC Members   Alan Gates Dhruba Borthakur Prasad Chakka Johan Oskarsson Zheng Shao John Sichi Sushanth Sowmyan Paul Yang  ","date":"14","image":null,"permalink":"https://hive.apache.org/community/people/","tags":null,"title":"People"},{"categories":null,"contents":"Issue Tracking  Hive tracks both bugs and enhancement requests using Apache JIRA. We welcome input, however, before filing a request, please make sure you do the following:\n Search the JIRA database. Check the user mailing list, both by searching the archives and by asking questions.  ","date":"14","image":null,"permalink":"https://hive.apache.org/community/issuetracking/","tags":null,"title":"Issue Tracking"},{"categories":null,"contents":"Mailing Lists  We welcome you to join our mailing lists and let us know about your thoughts or ideas about Hive.\nUser Mailing List  The user list is for general discussion or questions on using Hive. Hive developers monitor this list and provide assistance when needed.\n Subscribe: user-subscribe@hive.apache.org Post: user@hive.apache.org Unsubscribe: user-unsubscribe@hive.apache.org Archives: Apache  Developer Mailing List  The developer list is for Hive developers to discuss ongoing work, make decisions, and vote on technical issues.\n Subscribe: dev-subscribe@hive.apache.org Post: dev@hive.apache.org Unsubscribe: dev-unsubscribe@hive.apache.org Archives: Apache  Issues Mailing List  The issues list receives all notifications from the JIRA issue tracker.\n Subscribe: issues-subscribe@hive.apache.org Post: issues@hive.apache.org Unsubscribe: issues-unsubscribe@hive.apache.org Archives: Apache  Commits Mailing List  The commits list receives notifications with diffs when changes are committed to the Hive source tree.\n Subscribe: commits-subscribe@hive.apache.org Unsubscribe: commits-unsubscribe@hive.apache.org Archives: Apache  Security Mailing List  The security mailing list is a private list for discussion of potential security vulnerabilities issues. Please post potential security vulnerabilities to this list so that they may be investigated and fixed before the vulnerabilities is published.\nNote: This mailing list is NOT for end-user questions and discussion on security. Please use the user mailing list for such issues.\nThe Hive security mailing list is : security@hive.apache.org.\nIn order to post to the list, it is NOT necessary to first subscribe to it.\n","date":"14","image":null,"permalink":"https://hive.apache.org/community/mailinglists/","tags":null,"title":"Mailing Lists"},{"categories":null,"contents":"Version Control  The Hive source code resides in Apache\u0026rsquo;s Hive GitHub\n Anonymous clone via http - https://github.com/apache/hive.git Authenticated clone via ssh - git@github.com:apache/hive.git Instructions: Apache committer git instructions  ","date":"14","image":null,"permalink":"https://hive.apache.org/development/versioncontrol/","tags":null,"title":"Version Control"},{"categories":null,"contents":"Privacy Policy  Information about your use of this website is collected using server access logs and a tracking cookie. The collected information consists of the following:\n The IP address from which you access the website; The type of browser and operating system you use to access our site; The date and time you access our site; The pages you visit; and The addresses of pages from where you followed a link to our site.  Part of this information is gathered using a tracking cookie set by the Google Analyticsservice and handled by Google as described in their privacy policy. See your browser documentation for instructions on how to disable the cookie if you prefer not to share this data with Google.\nWe use the gathered information to help us make our site more useful to visitors and to better understand how and when our site is used. We do not track or collect personally identifiable information or associate gathered data with any personally identifying information from other sources.\nBy using this website, you consent to the collection of this data in the manner and for the purpose described above.\n","date":"13","image":null,"permalink":"https://hive.apache.org/general/privacypolicy/","tags":null,"title":"Privacy Policy"},{"categories":null,"contents":"Downloads  All recent supported releases may be downloaded from Apache mirrors: Download a release now! Old releases can be found in the archives.\nNews    08 October 2024: EOL for release 3.x line   The Apache Hive Community has voted to declare the 3.x release line as End of Life (EOL). This means no further updates or releases will be made for this series. We urge all Hive 3.x users to upgrade to the latest versions promptly to benefit from new features and ongoing support.\n  2 October 2024: release 4.0.1 available  As from Hive 4.x, we encourage all users to move the workload to Tez to benefit from performance gain and support. This release works with Hadoop 3.3.6, Tez 0.10.4. You can look at the complete JIRA change log for this release.    20 May 2024: EOL for release 2.x line   The Apache Hive Community has voted to declare the 2.x release line as End of Life (EOL). This means no further updates or releases will be made for this series. We urge all Hive 2.x users to upgrade to the latest versions promptly to benefit from new features and ongoing support.\n  9 May 2024: release 2.3.10 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    11 April 2024: EOL for release 1.x line   The Apache Hive Community has voted to declare the 1.x release line as End of Life (EOL). This means no further updates or releases will be made for this series. We urge all Hive 1.x users to upgrade to the latest versions promptly to benefit from new features and ongoing support.\n  29 March 2024: release 4.0.0 available  This release works with Hadoop 3.3.6, Tez 0.10.3 You can look at the complete JIRA change log for this release.    14 August 2023: release 4.0.0-beta-1 available  This release works with Hadoop 3.3.1 You can look at the complete JIRA change log for this release.    16 November 2022: release 4.0.0-alpha-2 available  This release works with Hadoop 3.3.1 You can look at the complete JIRA change log for this release.    08 April 2022: release 3.1.3 available  This release works with Hadoop 3.x.y You can look at the complete JIRA change log for this release.    30 March 2022: release 4.0.0-alpha-1 available  This release works with Hadoop 3.x.y You can look at the complete JIRA change log for this release.    9 June 2021: release 2.3.9 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    17 January 2021: release 2.3.8 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    18 April 2020: release 2.3.7 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    26 August 2019: release 3.1.2 available  This release works with Hadoop 3.x.y. You can look at the complete JIRA change log for this release.    23 August 2019: release 2.3.6 available  This release works with Hadoop 2.x.y. You can look at the complete JIRA change log for this release.    14 May 2019: release 2.3.5 available  This release works with Hadoop 2.x.y. You can look at the complete JIRA change log for this release.    7 November 2018: release 2.3.4 available  This release works with Hadoop 2.x.y. You can look at the complete JIRA change log for this release.    1 November 2018: release 3.1.1 available  This release works with Hadoop 3.x.y. You can look at the complete JIRA change log for this release.    30 July 2018: release 3.1.0 available  This release works with Hadoop 3.x.y. You can look at the complete JIRA change log for this release.    21 May 2018 : release 3.0.0 available  This release works with Hadoop 3.x.y. The on-disk layout of Acid tables has changed with this release. Any Acid table partition that had Update/Delete/Merge statement executed since the last Major compaction must execute Major compaction before upgrading to 3.0. No more Update/Delete/Merge may be executed against these tables since the start of Major compaction. Not following this may lead to data corruption. Tables/partitions that only contain results of Insert statements are fully compatible and don\u0026rsquo;t need to be compacted. You can look at the complete JIRA change log for this release.    3 April 2018 : release 2.3.3 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    18 November 2017 : release 2.3.2 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    24 October 2017 : release 2.3.1 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    25 July 2017 : release 2.2.0 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    17 July 2017 : release 2.3.0 available  This release works with Hadoop 2.x.y You can look at the complete JIRA change log for this release.    07 April 2017 : release 1.2.2 available  This release works with Hadoop 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    8 December 2016 : release 2.1.1 available  This release works with Hadoop 2.x.y. Hive 1.x line will continue to be maintained with Hadoop 1.x.y support. You can look at the complete JIRA change log for this release.    20 June 2016 : release 2.1.0 available  This release works with Hadoop 2.x.y. Hive 1.x line will continue to be maintained with Hadoop 1.x.y support. You can look at the complete JIRA change log for this release.    25 May 2016 : release 2.0.1 available  This release works with Hadoop 2.x.y. Hive 1.x line will continue to be maintained with Hadoop 1.x.y support. You can look at the complete JIRA change log for this release.    15 February 2016 : release 2.0.0 available  This release works with Hadoop 2.x.y. Hive 1.x line will continue to be maintained with Hadoop 1.x.y support. You can look at the complete JIRA change log for this release.    28 Jan 2016 : hive-parent-auth-hook made available  This is a hook usable with hive to fix an authorization issue. Users of Hive 1.0.x,1.1.x and 1.2.x are encouraged to use this hook. More details can be found in the README inside the tar.gz file.    27 June 2015 : release 1.2.1 available  This release works with Hadoop 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    21 May 2015 : release 1.0.1, 1.1.1, and ldap-fix are available  These two releases works with Hadoop 1.x.y, 2.x.y. They are based on Hive 1.0.0 and 1.1.0 respectively, plus a fix for a LDAP vulnerability issue. Hive users for these two versions are encouraged to upgrade. Users of previous versions can download and use the ldap-fix. More details can be found in the README attached to the tar.gz file. You can look at the complete JIRA change log for release 1.0.1 and release 1.1.1    18 May 2015 : release 1.2.0 available  This release works with Hadoop 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    8 March 2015: release 1.1.0 available  This release works with Hadoop 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    4 February 2015: release 1.0.0 available  This release works with Hadoop 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    12 November, 2014: release 0.14.0 available  This release works with Hadoop 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    6 June, 2014: release 0.13.1 available  This release works with Hadoop 0.20.x, 0.23.x.y, 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    21 April, 2014: release 0.13.0 available  This release works with Hadoop 0.20.x, 0.23.x.y, 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    15 October, 2013: release 0.12.0 available  This release works with Hadoop 0.20.x, 0.23.x.y, 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    15 May, 2013: release 0.11.0 available  This release works with Hadoop 0.20.x, 0.23.x.y, 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    March, 2013: HCatalog merges into Hive  Old HCatalog releases may still be downloaded.    11 January, 2013: release 0.10.0 available  This release works with Hadoop 0.20.x, 0.23.x.y, 1.x.y, 2.x.y You can look at the complete JIRA change log for this release.    ","date":"13","image":null,"permalink":"https://hive.apache.org/general/downloads/","tags":null,"title":"Downloads"},{"categories":null,"contents":"Recent versions:  javadoc and sources jars for use in an IDE are also available via Nexus  Hive 4.0.1 Javadocs Hive 4.0.0 Javadocs Hive 4.0.0-beta-1 Javadocs Hive 4.0.0-alpha-2 Javadocs Hive 3.1.3 Javadocs Hive 4.0.0-alpha-1 Javadocs Hive 3.1.2 Javadocs Hive 3.0.0 Javadocs Hive 2.3.9 Javadocs Hive 2.2.0 Javadocs Hive 2.1.1 Javadocs Hive 1.2.2 Javadocs  ","date":"29","image":null,"permalink":"https://hive.apache.org/docs/javadocs/","tags":null,"title":"Javadocs"},{"categories":null,"contents":"Capture Lineage Information In Hive Hooks Background In Hive, lineage information is captured in the form of LineageInfo object. This object is created in the SemanticAnalyzer and is passed to the HookContext object. Users can use the following existing Hooks or implement their own custom hooks to capture this information and utilize it.\nExisting Hooks  org.apache.hadoop.hive.ql.hooks.PostExecutePrinter org.apache.hadoop.hive.ql.hooks.LineageLogger org.apache.atlas.hive.hook.HiveHook  To facilitate the capture of lineage information in a custom hook or in a use case where the existing hooks are not set in hive.exec.post.hooks, a new configuration hive.lineage.hook.info.enabled was introduced in HIVE-24051. This configuration is set to false by default.\nTo provide filtering capability on query type in the lineage information, a new configuration hive.lineage.hook.info.query.type was introduced in HIVE-28409, with default value as \u0026ldquo;ALL\u0026rdquo;. Users can tune the configuration accordingly to capture lineage information only for the required query types. In HIVE-28409, the previously introduced configuration hive.lineage.hook.info.enabled was marked as deprecated.\nNOTE: HIVE-28409, will be available in Hive-4.1.0 release.\nUsage example:\nhive.lineage.hook.info.query.type=ALL -- will capture lineage info for all the queries. hive.lineage.hook.info.query.type=CREATE_VIEW,CREATE_TABLE_AS_SELECT -- will capture lineage info related to these 2 particulare query types only. hive.lineage.hook.info.query.type=NONE -- will not capture lineage info for any query. Previously, to capture lineage information, users has 2 ways:\n Set any of the above mentioned existing hooks in hive.exec.post.hooks configuration. Set hive.lineage.hook.info.enabled as true in cluster and restart HiveServer2 service. (Valid since Hive-4.0.0 release).  NOTE: Just by enabling hive.lineage.hook.info.enabled, lineage information for \u0026ldquo;Create View\u0026rdquo; query type won\u0026rsquo;t be captured, user has to set the existing hooks in hive.exec.post.hooks along with their custom hook class name.\nChanges done in HIVE-28768 The hardcoded values of the existing hooks that capture lineage information in SemanticAnalyzer and Optimizer code has been removed and to determine, whether lineage information should be captured or not, the value of hive.lineage.hook.info.query.type configuration is checked. The default value of hive.lineage.hook.info.query.type has been set to \u0026ldquo;NONE\u0026rdquo;.\nImplications of HIVE-28768 on users  Users migrating directly from Hive-3.x to HIVE-4.1.0 will observe breaking changes in the way lineage information is captured. Setting hive.exec.post.hooks to any of the existing hooks will not capture lineage information anymore. Users will have to make use of hive.lineage.hook.info.query.type configuration to capture lineage information. Users migrating from Hive-4.0.x to Hive-4.1.0 who don\u0026rsquo;t have hive.lineage.hook.info.enabled set to true, will also observe breaking changes in the way lineage information is captured.   NOTE: Recommended way to capture lineage information is though hive.lineage.hook.info.query.type configuration as hive.lineage.hook.info.enabled is marked as deprecated and is subjected to be removed in future release\n ","date":"01","image":null,"permalink":"https://hive.apache.org/docs/latest/capture-lineage-info/","tags":null,"title":""}]